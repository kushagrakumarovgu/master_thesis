review,reasoning,sentiment_score,politeness_score
"This paper presents a new contribution for a largely understudied problem of label shift (also called target shift), a situation occurring when the class proportions vary between the training and test sets. The proposed contribution builds upon a recent work on the subject by Lipton et al., 2018 and addresses several of its weaknesses. The paper also gives several improved generalisation bounds w.r.t. that of Lipton et al. that are further used as guidelines to tune the regularisation parameter based on the size of source and target samples. Finally, the empirical results show that the proposed algorithm outperforms that of Lipton et al. especially in cases where the shift in proportions becomes quite important. 

*Pros: 
   - A work in an area with very view contributions and a certain lack of theoretical results
    -Theoretical results that are actually used in the algorithmic implementation and that allow to define the regularisation parameter based on the size of the available samples
    -Improved empirical results


*Cons: 
    -An incomplete state-of-the-art section that does not cite several important contributions on the subject;
    -Lack of baselines due to the incomplete state-of-the-art section;
    -Lack of clear comparison with Lipton et al. both in terms of the proposed method and the obtained theoretical guarantees.  


*Detailed comments:
This paper is rather interesting and well-written.

I have several major concerns regarding this paper. They can be summarised as follows:

    There is an important part of literature review on target shift that is missing in this paper. Even though, the paper mentioned the work of Chang, 2005 and Zhang, 2013, it completely ignores several other highly relevant methods such as [1,2]. These works also propose algorithms that allow to estimate class proportions that vary between training and test data. This estimation can then be used for cost-sensitive learning to correct the target shift. The paper should mention this work and add the corresponding methods to the baselines for comparison. 

    Several statements that justify the contribution of this paper are unsupported. For instance, the paper states that the estimator obtained with the inverse of the confusion matrix can be arbitrary bad when the sample size and/or the singular values are small. However, this exact dependence can be found in Lemma 1 for the proposed contribution also! This is repeated in the beginning of Section 2.2 to justify the regularised version of the estimator but once again no evidence was provided to support the claim. The obtained bound for the regularised algorithm also has these two terms and thus it is not clear why the regularised algorithm is supposed to work better. 

    The paper may want to clearly state the differences between the proposed algorithm and that of Lipton et al. and also between the obtained error bounds. The paper states that it achieves a k*log(k) improvement over Lipton et al. bounds but as fair as I can see this improvement is achieved only when h_0 is an ideal estimator. Furthermore, Lipton et al.’s bounds are linear in k while the proposed bounds replace this term with log(k/delta) so that when \delta is small, ie the bound holds with high probability, the bound becomes much worse. I would suggest to add a brief discussion on the relationship between the two to better highlight the original contribution of the paper. 

    The proofs are quite badly written with many lacking results used to move from one inequality to another. For instance, Lemma 2 is proved using the theorem 1.4[Matrix Bernstein] and dilation technique from Tropp but it is not clear which results the authors are using in particular; Theorem 1.4 is related to the largest eigenvalue of the sum of matrices while the authors obtain an inequality for the norm of the sum without any further comment on how this transition was made. Also, I do not see why delta is smaller than 1/2 in Lemma 2. 


*Minor comments:

   - p.1: expected have -> expected to have
   - p.4: we are instead only gave access -> given access to 
   - I do not understand Figure 1. Should it be n_q*n_p on the y axis ?
   - The inequality for n_q next to Figure 1 is derived from the bound (6). Why it is independent of k?
   - Why the authors choose to the black box predictor h0 to be a two-layer fully connected neural? Is there any particular reason to use this classification model?

[1] Class Proportion Estimation with Application to Multiclass Anomaly Rejection, AISTATS14
[2] Mixture Proportion Estimation via Kernel Embeddings of Distributions, ICML16","The review acknowledges the paper's contribution to an understudied problem and its merits (improved bounds, empirical results). However, it raises significant concerns about missing citations, unsupported claims, and unclear comparisons and proofs. The tone, while direct, maintains a professional and constructive approach, suggesting areas for improvement rather than outright rejection.",20.0,60.0
"The paper considers deep nets with piecewise linear activation functions, which are known to give rise to piecewise linear input-output mappings, and proposes loss functions which discourage datapoints in the input space from lying near the boundary between linear regions. These loss functions are well-motivated theoretically, and have the intended effect of increasing the distance to the nearest boundary and reducing the number of distinct linear regions. 

My only concern is that while their method appears to effectively increase the l_1 and l_2 margin (as they have defined it), the utility of doing so is not clearly demonstrated. If improving the quality or validity of local linearization for explaining predictions is one of the main motivations for this work, showing that the proposed method does so would strengthen the overall message. However, I do feel that “establishing robust derivatives over larger regions” is an important problem in its own right. 

With the exception of some minor typos, the exposition is clear and the theoretical claims all appear correct. The authors may have missed some relevant recent work [1], but their contributions are complementary. It is not immediately clear that the parallel computation of gradients proposed in section 4.1 is any faster than standard backpropagation, as this has to be carried out separately for each linear region. A basic complexity analysis or running time comparison would help clarify this. I think I am missing the point of the gradient visualizations in figure 4, panels b-e and g-j. 


[1] Elsayed, Gamaleldin F., et al. ""Large Margin Deep Networks for Classification."" arXiv preprint arXiv:1803.05598 (To appear in NIPS 2018).","The reviewer provides constructive criticism and acknowledges the merits of the paper (""well-motivated theoretically"", ""contributions are complementary"", ""exposition is clear""). While they raise a valid concern about the utility of the proposed method, they also acknowledge its potential value (""an important problem in its own right""). The suggestions for improvement are presented politely and objectively. Overall, the tone is balanced and encouraging.",50.0,80.0
"In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training.  Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. 

a) In the paper, the authors mentioned that ""This simple metric is non-parametric and we found that the results are not sensitive to the selection of k"". Can authors provide more details, e.g., empirical results, about it? What is its rationale?

b) In the paper, ""We find that these blind-spots are prevalent and can be easily found without resorting to complex
generative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model."" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? 

c) The linear transformation x^\prime = \alpha x + \beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\ell_infty$ attack satisfies  max_{\alpha, \beta} f(\alpha x + \beta) subject to \| \alpha x + \beta \|\leq \epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. 

d) ""Because we scale the image by a factor of \alpha, we also set a stricter criterion of success, ..., perturbation must be less
than \alpha \epsilon to be counted as a successful attack."" I did not get the point. Even if you have a scaling factor in x^\prime = \alpha x + \beta, the universal perturbation rule should still be | x - x^\prime  |_\infty \leq \epsilon. The metric the authors used would result in a higher attack success rate, right? 
","The reviewer acknowledges the interesting ideas and empirical results presented in the paper, indicating a somewhat positive sentiment. However, they also raise several concerns and ask for clarifications, suggesting that the paper is not without flaws. The tone is professional and the reviewer provides constructive criticism, making it clear they are engaging with the work in good faith. Therefore, the sentiment is slightly positive, and the politeness is highly positive.",60.0,80.0
"The paper considers a number of streaming learning settings with various forms of dataset shift/drift of interest for continual learning research, and proposes a novel regularization-based objective enabled by a replay memory managed using the well known reservoir sampling algorithm.

Pros:
The new objective is not too surprising, but figuring out how to effectively implement this objective in a streaming setting is the strong point of this paper. 

Task labels are not used, yet performance seems superior to competing methods, many of which use task labels.

Results are good on popular benchmarks, I find the baselines convincing in the supervised case.

Cons:
Despite somewhat frequent usage, I would like to respectfully point out that Permuted MNIST experiments are not very indicative for a majority of desiderata of interest in continual learning, and i.m.h.o. should be used only as a prototyping tool. To pick one issue, such results can be misleading since the benchmark allows for “trivial” solutions which effectively freeze the upper part of the network and only change first (few) layer(s) which “undo” the permutation. This is an artificial type of dataset shift, and is not realistic for the type of continual learning issues which appear even in single task deep reinforcement learning, where policies or value functions represented by the model need to change substantially across learning.

I was pleased to see the RL experiments, which I find more convincing because dataset drifts/shifts are more interesting. Also, such applications of continual learning solutions are attempting to solve a ‘real problem’, or at least something which researchers in that field struggle with. That said, I do have a few suggestions. At first glance, it’s not clear whether anything is learned in the last 3 versions of Catcher, also what the y axis actually means. What is good performance for each game is very specific to your actual settings so I have no reference to compare the scores with. The sequence of games is progressively harder, so it makes sense that scores are lower, but it’s not clear whether your approach impedes learning of new tasks, i.e. what is the price to pay for not forgetting?

This is particularly important for the points you’re trying to make because a large number of competing approaches either saturate the available capacity and memory with the first few tasks, or they faithfully model the recent ones. Any improvement there is worth a lot of attention, given proper comparisons. Even if this approach does not strike the ‘optimal’ balance, it is still worth knowing how much training would be required to reach full single-task performance on each game variant, and what kind of forgetting that induces. 
","The reviewer provides both positive and negative points about the paper. They praise the novelty and performance of the proposed method but raise concerns about the Permuted MNIST experiments and suggest improvements for the RL experiments. The language used is polite, with phrases like ""I would like to respectfully point out"" and ""I was pleased to see."" Overall, the review leans towards the positive side, acknowledging the strengths while suggesting constructive improvements.",60.0,80.0
"This paper puts forward a new global+local memory pointer network to tackle task-oriented dialogue problem.

The idea of introducing global memory is novel and experimental results show its effectiveness to encode external knowledge in most cases.

Here're some comments:
1. In global memory pointer, the users employ non-normalized probability (non-softmax). What is the difference in performance if one uses softmax?

2. In (11), there's no linear weights. Will higher weights in global/local help?

3. As pointed out in ablation study, it's weird that in task5 global memory pointer does not help.

4. The main competitor of this algorithm is mem2seq. While mem2seq includes DSTC2 and In-car Assistant, and especially in-car assistant provides the first example dialogue, why does the paper not include expeirments on these two datasets?","The reviewer finds the core idea of the paper novel and acknowledges the effectiveness of the proposed approach. They also provide constructive criticism and ask relevant questions to improve the paper. This suggests a positive but also analytical stance. The language is formal, objective, and respectful, without any harsh or condescending remarks.",60.0,80.0
"This paper reinvestigate several recent works on network pruning and find that the common belief about the necessity to train a large network before pruning may not hold. The authors find that training the pruned model from scratch can achieve similar, if not better, performance given enough time of training. Based on these observations, the author conclude that training a larger model followed by pruning is not necessary for obtaining an efficient model with similar performance. In other words, the pruned architecture is more important than the weights inherited from the large model. It reminds researchers to perform stronger baselines before showing complex pruning methods. 

The paper is well organized and written. It re-evaluate the recent progresses made on this topic. Instead of comparing approaches by simply using the numbers from previous paper, the authors perform extensive experiments to verify whether training the pruned network from scratch would work. The results are very interesting, it suggests the researchers to tune the baseline “hardly” and stick to simple approach. However, here are some places that I have concerns with:

1. The two “common beliefs” actually state one thing, that is the weights of a pre-trained larger model can potentially help optimization for a smaller model. 

2. I don’t quite agree with that “training” is the first step of a pruning pipeline as illustrated in Figure 1.  Actually the motivation or the common assumption for pruning is that there are already existing trained models (training is already finished) with good performance. If a trained model does not even exist, then one can certainly train various thin/smaller model from scratch as before, this is still a trial and error process. 

3. “The value of pruning”. The goal of pruning is to explore a “thin” or “shallower” version of it with similar accuracy while avoiding the exhaustive architecture search with heavy training processes. Thus the first value of pruning is to explore efficient architecture while avoiding heavy training. Therefore, it should be fast and efficient, ideally with no retraining or little fine-tuning. When the pruning method is too complex to implement or requires much more time than training from scratch, it could be an overkill and adds little value, especially when the performance is not better enough. Therefore, it is more informative if the authors would report the time/complexities for pruning/fine-tuning .

4. The second value of pruning lies at understand the redundancy of the model and providing insights for more efficient architecture designs. 

5. Comparing to random initialization, pruning simply provide an initialization point inherited from the larger network. The essential question the author asked is whether a subset of pre-trained weights can outperform random initialization. This seems to be a common belief in transfer learning, knowledge distillation and the studies on initialization. The authors conclude that the accuracy of an architecture is determined by the architecture itself, but not the initialization. If this is true, training from scratch should have similar (but not better) result as fine-tuning a pruned model.  As the inherited weights can also be viewed as a “random” initialization. Both methods should reach equivalent good solution if they are trained with enough number of epochs. Can this be verified with experiments?

6. The experiments might not be enough to reject the common belief. The experiments only spoke that the pruned architectures can still be easily trained and encounter no difficulties during the optimization. One conjecture is that the pruned models in the previous work still have enough capacity for keeping good accuracy. What if the models are significantly pruned (say more than 70% of channels got pruned), is training from scratch still working well? It would add much value if the author can identify when training from scratch fails to match the performance obtained by pruning and fine-tuning.

7. In Section 4.1, “scratch-trained models achieve at least the same level of accuracy as fine-tuned models”. First, the ResNet-34-pruned A/B for this comparison does not have significant FLOPs reduction (10% and 24% FLOPs reduction). Fine-tuning still has advantage as it only takes ¼ of training time compare to scratch-E. Second, it is interesting that fine-tuning has generally smaller variance than stratch-E (except VGG-19). Would this imply that fine-tuning a pruned model produce more stable result? It would be more complete if there is variance analysis for the imagenet result. 

8. What is the training/fine-tuning hyperparameters used in section 4.1?  Note that in the experiment of Li et al, 2017, scratch-E takes 164 epochs to train from scratch, while fine-tuning takes only 40 epochs. Like suggested above, if we fine-tune it with more epochs, would it achieve equivalent performance? Also, what is the hyperparameter used in scratch-E? Note that the original paper use batch size 128. If the authors adopts a smaller batch-size for scratch-E, then it has in more iterations and could certainly result in better performance according to recent belief that small batch-size generates better.

9. The conclusion of section 5 is not quite clear or novel. Using uniform pruning ratio for pruning is expected to perform worse than automatic pruning methods as it does not consider the importance difference of each layer and. This comes back to my point 3 & 4 about the value of pruning, that is the value of pruning lies at the analysis of the redundancy of the network. There are a number of works worked on analyzing the importance of different layers of filters. So I think the “hypothesis” of “the value of automatic pruning methods actually lies in the resulting architecture rather than the inherited weight” is kind of straightforward. Also, why not use FLOPs as x-axis in Figure 3?


Minor: It might be more accurate to use “L1-norm based Filter Pruning (Li et al., 2017)” as literally “channels” usually refers to feature maps, which are by-products of the model but not the model itself.

I  will revise my score if authors can address above concerns.


--------- review after rebuttal----------
#1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. 

#5 “fine-tuning with enough epochs”. 
I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that “training from scratch is better when the number of epochs is large enough”. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates “fine-tuning is faster to converge”.  However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of “scratch B”, as ResNet-56 B just reduce 27% FLOPs. 

The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training.

Questions:
- Are both methods using the same learning rate schedule between epoch 160 and epoch 320?
- The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5,  but it turns out to be 92.68(±0.19) in reply#8.
- It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison.


#6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. 

The authors find that “when the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning”.  This could be due to following reasons: 
      1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning. 
      2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? 

Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios.

-------- update ----------------

The authors addressed most of my concerns. Some questions are still remaining in my comment “Review after rebuttal”,  specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version.

However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7.
","The review is quite balanced. It acknowledges the paper's strengths, such as its organization, writing, and interesting results that challenge common beliefs. The reviewer finds the topic and findings valuable. However, they also raise several valid concerns and suggest areas for improvement. They question some of the authors' assumptions and interpretations, point out potential flaws in the experimental methodology, and call for further investigation and clarification on certain aspects. The reviewer's tone is critical but professional and aims to improve the paper's quality. They even offer to revise their score based on the authors' responses to their concerns, indicating a willingness for constructive dialogue. Overall, the review leans towards the positive side due to the paper's valuable contributions, but the numerous concerns and suggestions for improvement keep it from being overwhelmingly positive.",60.0,70.0
"This paper proposes a method to synthesize speech from text input, with the style of an input voice provided with the text. Thus, we provide content - text - and style - voice. It leverages recent - phenomenal - progress in TTS with Deep Neural Networks as seen from exemplar works such as Tacotron (and derivatives), DeepVoice, which use seq2seq RNNs and Wavenet families of models. The work is extremely relevant in that audio data is hard to generate (expensive) and content-style modeling could be useful in a number of practical areas in synthetic voice generation. It is also quite applicable in the related problem of voice conversion. The work also uses some quite complex - (and very interesting!) - proposals to abstract style, and paste with content using generative modeling. I am VERY excited by this effort in that it puts together a number of sophisticated pieces together, in what I think is a very sensible way to implement a solution to this very difficult problem. However, I would like clarifications and explanations, especially in regards to the architecture.  

Description of problem: The paper proposes a fairly elaborate setup to inject voice style (speech) into text. At train time it takes in text samples $x_{txt}$, paired voice samples (utterances that have $x_{txt}$ as content) $s+$ and unpaired voice samples $s-$, and produces two voice samples $x+$ (for paired  <txt, utterance>) and $x-$ (for unpaired txt/utterance). The idea is that at test time, we pass in a text sample $x_{txt}$ and an UNPAIRED voice sample $x_{aud}$ and the setup produces voice in the style of $x_{aud}$ but whose content is $x_{txt}$, in other words it generates synthetic speech saying $x_{txt}$. The paper goes on to show performance metrics based on an autoencoder loss, WER and t-SNE embeddings for various attributes. 

Context:  The setup seems to be built upon the earlier work by Taigman et al (2016) which has the extremely interesting conception of using a {\it ternary} discriminator loss to carry out domain adaptation between images. This previous work was prior to the seminal CycleGAN work for image translation, which many speech works have since used. Interestingly, the Taigman work also hints at a 'common' latent representation a la UNIT using coupled VAE-GANs with cycle consistency (also extremely pertinent), but done differently. In addition to the GAN framework by Taigman et al, since this work is built upon Tacotron and the GST (Global Style Tokens) work that followed it, the generative setup is a sophisticated recurrent attention based seq2seq model.

Formulation:
A conditional formulation is used wherein the content c (encoding generated by text) is passed along with other inputs in the generator and discriminator. The formulation in Taigman assumes that there is an invariant representation in both (image) domains with shared features. To this, style embeddings (audio) gets added on and then gets passed into the generator to generate the speech. Both c and s seem to be encoder outputs in the formulation. The loss components of what they call ‘adversarial’, ‘collaborative’ and ‘style’ losses. 

Adversarial losses
The ternary loss for D consists of 

Discriminator output from ‘paired’ style embedding (i.e. text matching the content of paired audio sample)
Discriminator output from ‘unpaired’ style embedding (i.e text paired with random sample of some style)
Discriminator output from target ground truth style. The paper uses x_+, so I would think that it uses the paired sample (i.e. from the source) style.

Generator loss (also analogous to Taigman et al) consists of generations from paired and unpaired audio, possibly a loose analogue to source and target domains, although in this case we can’t as such think of ‘+’ as the source domain, since the input is text. 

Collaborative losses 
This has two components, one for style (Gatys et al 2016) and a reconstruction component. The reconstruction component again has two terms, one to reconstruct the paired audio output ‘x+=x_audio+’ - so that the input content is reproduced -  and the other to encourage reconstruction of the latent code. 

Datasets and Results:
They use two datasets: one, an internal ‘EMT-4’ dataset with 20k+ English speakers, and the other, the VCTK corpus. Comparisons are made with a few good baselines in Tacotron2, GST and DeepVoice2. 

One comparison technique to test disentanglement ability is to compare autoencoder reconstructions with the idea that a setup that has learnt to disentangle would produce higher reconstruction error because it has learnt to separate style and content. 

t-SNE embeddings are presented to show visualizations of various emotion styles (neutral, angry, sad and happy), and separation of male and female voices. A WER metric is also presented so that generations are passed into a classifier (an ASR system trained on Wavenet). All the metrics above seem to compare excellently (better than?) with the others. 

Questions and clarifications:

(Minor) There’s a typo in page 2, line 2. x_{aud}^+ should be x_{aud}^-.

Clarification on formulation: Making the analogy (is that even the right way of looking at this?) that the ‘source’ domain is ‘+’, and the target domain is ‘-’, in equation (5), the last term of the ternary discriminator has the source domain (x_{aud}^+) in it, while the Taigman et al paper uses the target term. Does this matter? I would think ‘no’, because we have a large number of terms here and each individual term in and of itself might not be relevant, nor is the current work a direct translation of the Taigman et al work. Nevertheless, I would like clarification, if possible, on the discrepancy and why we use the ‘+’ samples. 

Clarification on reconstruction loss: I think the way it is presented, equation (8) is misleading. Apparently, we are sampling from the latent space of style and content embeddings for paired data. The notation seems to be quite consistent with that of the VAE, where we have a reconstruction and a recognition model, and in effect the equation (8) is sampling from the latent space in a stochastic way. However, as far as I can see, the latent space here produces deterministic embeddings, in that c = f(x_{txt}) and s = g(x_{aud}^+), with the distribution itself being a delta function. Also, the notation q used in this equation most definitely indicates a variational distribution, which I would think is misleading (unless I have misinterpreted what the style tokens mean). At any rate, it would help to show how the style token is computed and why it is not deterministic. 

Clarification on latent reconstruction loss: In equation (9), how is the latent representation ‘l’ computed? While I can intuitively see that the latent space ‘l’ (or z, in more common notation) would be the ‘same’ between real audio samples and the ‘+’, ‘-’ fake samples, it seems to me that they would be related to s (as the paper says, ‘C’ and ‘Enc_s’ share all conv layers) and the text. But what, in physical terms is it producing? Is it like the shared latent space in the UNIT work, or the invariant representation in Taigman? This could be made clearer with an block diagram for the architecture. 

(Major) Clarification on network architecture
The work references Tacotron’s GST work (Wang et al 2018) and the related Skerry-Ryan work as the stem architecture with separate networks for style embeddings and for content (text). While the architecture itself might be available in the stem work by Wang et al, I think we need some diagrams for the current work as well for a high level picture. Although it is mentioned in words in section 3.3, I do not get a clear idea of what the encoder/decoder architectures look like. I was also surprised in not seeing attention plots which are ubiquitous in this kind of work. Furthermore, in the notes to the ‘inference’ network ‘C’ it is stated that C and Enc_s share all conv layers. Again, a diagram might be helpful - this also applies for the discriminator. 

Clarification on stability/mode collapse: Could the authors clarify how easily this setup trained in this adversarial setup? 

Note on latent representation: To put the above points in perspective, a small note on what this architecture does in regards to the meaning of the latent codes would be useful. The Taigman et al 2016 paper talks about the f-constancy condition (and 'invariance'). Likewise, in the UNIT paper by Ming-Yu Liu - which is basically a set of coupled VAEs + cycle consistency losses, there is the notion of a shared latent space. A little discussion on these aspects would make the paper much more insightful to the domain adaptation practitioner.

Reference: This reference - Adversarial feature matching for text generation - (https://arxiv.org/abs/1706.03850) contains a reconstruction stream (as perhaps many other papers) and might be useful for instruction. 

Other relevant works in speech and voice conversion: This work comes to mind, using the StarGAN setup, also containing a survey of relevant approach in voice conversion. Although the current work is for TTS, I think it would be useful to include speech papers carrying out domain adaptation for other tasks.

StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks. 
https://arxiv.org/abs/1806.02169 

I would rate this paper as being acceptable if the authors clarify my concerns, and in particular, about the architecture. It is also hard to hard to assess reproducibility in a complex architecture such as this. ","The reviewer expresses excitement and finds the work very interesting and relevant. They praise the sophisticated approach and sensible implementation. However, they also raise several questions and require clarifications, particularly regarding the architecture and its relation to previous work. The tone is constructive and inquisitive, not dismissive. Therefore, the sentiment is positive, but not overly enthusiastic due to the need for clarifications. The language is polite and academic, with no signs of rudeness.",75.0,90.0
"The authors propose in this paper a series of results on the approximation capabilities of neural networks based on ReLU using quantized weights. Results include upper bounds on the depth and on the number of weights needed to reach a certain approximation level given the number of distinct weights usable. The paper is clear and as far as I know the results are both new and significant. My only negative remark is about the appendix that could be clearer. In particular, I think that figure 2 obscures the proof of Proposition 1 rather than the contrary. I think it might be much clearer to give an explicit neural network approximation of x^2 for say r=2, for instance.","The reviewer explicitly states the paper is ""clear"" and the results ""new and significant."" This indicates a positive sentiment. The suggestion regarding the appendix is constructive and aims to improve the paper. The language used is formal and polite throughout the review.",75.0,75.0
"The English, grammar and writing style is very good, as are the citations.
The technical quality appears to me to be very good (I am not an expert in Poincare spaces).
The authors demonstrate a good knowledge of the mathematical theory with the constructions made in Section 6.
The experimental write-up has been abbreviated.  The lexical entailment results Tables 6 and 7 are just sitting there without discussion, as far as I can see, as are the qualitative results Tables 4 and 5.   The entailment results are quite complex and really need supporting interpretation.  For instance, for Hyperlex, WN-Poincare is 0.512, above yours.
For your entailment score you say ""For simplicity, we propose dropping the dependence in μ"".  This needs more justification and discussion as it is counter-intuitive for those not expert in  Poincare spaces.
Section 6.2 presents the entailment score.  Note Nickel etal. give us a nice single formula.  You however, provide 4 paragraphs of construction from which an astute reader would then have to work on to extract your actual method.  I would prefer to see a summary algorithm given somewhere.  Perhaps you need another appendix.
RADAGRAD is discussed in Section 5, but I'd have preferred to see it discussed again in Section 8 and discussed to highlight what was indded done and the differences.  It certainly makes the paper non-reproducible.
A significant part of the theory in earlier sections is about the 50x2D method, but in experiments this doesn't seem to work as well.  Can you justify this some other how:  its much faster, its more interpretable?  Otherwise, I'm left thinking, why not delete this stuff?
The paper justifies its method with a substantial and winning comparison against vanilla GloVe.  That by itself is a substantial contribution.
But now, one is then hit with a raft of questions.  Embedding methods are popping up like daisies all over the fields of academia.  Indeed, word similarity and lexical entailment tasks themselves are proliferating too.  To me, its really unclear what one needs to achieve in the empirical section of a paper.  To make it worse, some folks use 500D, some 100D, some 50D, so results aren't always comparible.  Demonstrating one's work is state-of-the-art against all comers is a massive implementation effort.  I notice some papers now just compare against one other (e.g., Klami etal. ECML-PKDD, 2018).

My overall feeling is that this paper tries to compress too much into a small space (8 pages).
I think it really needs to be longer to present what is shown.   Moreover, I would want to see the inclusion of the work on 50x2D justified. So my criticisms are about the way the paper is written, not about the quality of the work.  
Moroever, though, one needs to consider comparisons against models other than GloVe.

Addendum:  You know, what I really love about ICLR is the effort authors make to refresh their paper and respond to reviewers.  You guys did a great job.  Really impressed.  50x2D now clarified and some of the hasty/unexplained bits fixed.","The reviewer provides a generally positive review, acknowledging the technical quality of the work and its contribution compared to GloVe. However, they express concerns about the paper's length, clarity, and lack of comparison with other embedding methods. The reviewer finds the writing quality good and their criticisms are constructive, suggesting improvements rather than outright rejection. The addendum further highlights the positive shift in the reviewer's opinion after the authors addressed their concerns, indicating a collaborative and appreciative tone.",60.0,80.0
"# 1. Summary
This paper presents a model for future video prediction, which integrates 3D convolutions into RNNs. The internal operations of the RNN are modified by adding historical records controlled via a gate-controlled self-attention module. The authors show that the model is effective also for other tasks such as early activity recognition.

Strengths:
* Nice extensive experimentation on video prediction and early activity recognition tasks and comparison with recent papers
* Each choice in the model definition are motivated, although some clarity is still missing (see below)

Weaknesses:
* Novelty: the proposed model is a small extension of a previous work (Wang et al., 2017) 


# 2. Clarity and Motivation
In general, the paper is clear and general motivation makes sense, however some points need to be improved with further discussion and motivation:

A) Page 2 “Unlike the conventional memory transition function, it learns the size of temporal interactions. For longer sequences, this allows attending to distant states containing salient information”: This is not obvious. Can the authors add more details and motivate these two sentences? How is long-term relations are learned given Eq. 1? 
B) Page 5 “These two terms are respectively designed for short-term and long-term video modeling”: How do you make sure that Recall(.) does not focus on the short-term modeling instead? Not clear why this should model long-term relations.
C) Page 5 and Eq 1: motivation why layer norm is required when defining C_t^k is not clear
D) What if the Recall is instead modeled as attention? The idea is to consider only C_{1:t-1}^k (not consider R_t) and have an attentional model that learn what to recall based only on C. Also, why does Recall need to depend on R_t?
E) Page 5 “to minimize the l1 + l2 loss over every pixel in the frame”: this sentence is not clear. How does it relate to Eq. 2?


# 3. Novelty
Novelty is the major concern of this paper. Although the introduced new concepts and ideas are interesting, the work seems to be an extension of ST-LSTM and PredRNN where Eq 1 is slightly modified by introducing Recall. 
In addition the existing relation between the proposed model and ST-LSTM is not clearly state. Page 2, first paragraph: here the authors should state that model is and extension of ST-LSTM and highlight what are the difference and advantage of the new model.


# 4. Significance of the work
This paper deals with an interesting and challenging topic (video prediction) as well as it shows some results on the early activity recognition task. These are definitively nice problem which are far to be solved. From the application perspective this work is significant, however from the methodological perspective it lacks a bit of significance because of the novelty issues highlighted above.


# 5. Experimentation
The experiments are robust with nice comparisons with recent methods and ablation study motivating the different components of the model (Table 1 and 2). Some suggested improvements:

A) Page 7 “Seq 1 and Seq 2 are completely irrelevant, and ahead of them, another sub-sequence called prior context is given as the input, which is exactly the same as Seq 2”: The COPY task is a bit unclear and need to be better explained. Why are Seq. 1 and 2 irrelevant? I would suggest to rephrase this part.
B) Sec. 4.2, “Dataset and setup”: which architecture has been used here?
C) Sec. 4.3, “Hyper-parameters and Baselines“: the something-something dataset is more realising that the other two “toy” dataset. Why did the authors choose to train a 2 layers 3D-CNN encoders, instead of using existing pretrained 3D CNNs? I would suspect that the results can improve quite a bit.


# 6. Others
* The term “self-supervised auxiliary learning” is introduced in the abstract, but at this point it’s meaning is not clear. I’d suggest to either remove it or explain its meaning.
* Figure 1(a): inconsistent notation with 2b. Also add citation (Wang et al., 2017) since it ie the same model of that paper

-------
# Post-discussion
I increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors.
","The reviewer acknowledges the limitations in novelty but recognizes the value of the experimental results and the incremental improvements made. The tone is generally constructive, suggesting specific areas for improvement rather than outright criticism. While there are concerns about novelty, the reviewer's willingness to increase their rating after discussion indicates a generally positive outlook on the paper's contributions.",50.0,70.0
"The paper aims to come up with a criterion for evaluating the quality of samples produced by a Generative Adversarial Network. The main goal is that the criterion should not reward trivial sample generation algorithms such as the one which generates samples uniformly at random from the samples in the training set. I personally feel that if sample generation is the only goal, then this trivial algorithm is perfectly fine because, statistically, the empirical distribution is in many, though not all, ways, a good estimator of the underlying true probability measure (this is the idea that is used in the statistical technique of Bootstrap for example). However the underlying goal in unsupervised learning problems where GANs are used is hardly sample generation. The GANs also output a whole function in the form of a generative network which converts random samples into samples from the underlying generating distribution. This generative network is arguably more important and more useful than just the samples that it generates. An evaluation scheme for GANs should focus on the generative network directly rather than on a set of its generating samples. 

Even if one were to regard the premise of the paper as valuable, the paper still does a poor job meeting its objective. A measure D_CNN is proposed as a benchmark. It must be remarked that D_CNN is not even properly defined (for example, there is a function \Delta in its definition but it is never explained what this function is). D_CNN is a variant of the existing notion of Neural Network Divergences. Only a numerical study (with no theory) is done to illustrate the utility of D_CNN for evaluating samples generated by GANs. The entire paper is very anecdotal with very little rigorous theory. ","The review starts with a negative sentiment, disagreeing with the fundamental premise of the paper. The reviewer finds the paper's objective potentially misguided. Furthermore, the reviewer criticizes the paper for poor execution, lack of clarity, and lack of theoretical rigor. The language used, while critical, maintains a professional and academic tone. There are no personal attacks or disrespectful language.",-60.0,60.0
"The paper is nice thread, easy to follow.

The paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. 

Overall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution.

I have following questions to the authors:
1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100
2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great.
3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen?
4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training?
5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)?
6. In section 5.2, page 8 , can authors provide their intuition behind the results: ""We found that the improvement on VAT is not drastic – our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%"" - why did fast-SWA not improve much?","The reviewer uses phrases like ""nice thread"", ""very well described"", ""easy to follow"", ""really helpful"", ""valid contribution"", and ""useful insight"", which all point towards a positive sentiment. The reviewer also provides constructive questions, indicating a willingness to further engage with the work. There is no use of harsh language, and the questions are framed politely and professionally. Therefore, the sentiment is positive, and the language is polite.",80.0,90.0
"The paper presents a methodology for improved program synthesis by generating datasets for program induction and synthetic tasks from uniform distributions. This method is evaluate on two problem settings. 

The methodology is presented in section 3. Even though the outline does not seem to be complicated, the presentation in section 3 leaves me puzzled. The the second paragraph two sets of silent variables are introduced X_1,...,X_n and Z_1,...,Z_m but never used again the rest of the paper. In the third and forth paragraph details about the Karel domain are presented without the Karel domain having been introduced. It seems you are using rejection sampling to sample from a uniform distribution. Why can you not sample from a uniform distribution directly? What do you mean with the notation X(s)? What are you proving in Appendix? Would maybe be clearer if you presented it as a theorem/lemma.

The remaining part of the paper evaluates this methodology on two specific problem settings, the Karel domain and Calculator domain. The generalization performance is increased when trained on datasets generated by the method presented in the paper. However, I cannot find and strong arguments in the paper why this property should generalize to other problem settings. To me the analysis and experimental results seems to be tailored to the two problems settings used in the paper.

==== After revision ====

The authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.","The review starts with a somewhat neutral tone, pointing out the paper's focus and evaluation. However, it quickly expresses significant confusion and questions about the methodology section, indicating a negative sentiment. While the reviewer acknowledges improvements after revision, their confidence remains low, suggesting lingering concerns. The language used, while direct and critical, maintains a professional and respectful tone, avoiding personal attacks or overly harsh language.",-20.0,60.0
"1) Summary
This paper presents a graph neural network based architecture that is trained to locate and model the interactions of agents in an environment directly from pixels. They propose an architecture that is a composition of recurrent neural networks where each models a single object independently and communicate with other for the overall environment modeling. The model is trained with a variational recurrent neural network objective that allows for stochasticity in the predictions while at the same time allows to model the current and future steps simultaneously. In experiments, they show the advantage of using the proposed model for tasks of tracking as well as forecasting of agents locations.



2) Pros:
+ Novel recurrent neural network architecture to model structured dynamics of agents in an environment.
+ Outperforms baseline methods.
+ New dataset for partially observable prediction research.

3) Cons:

Forecasting task:
- The authors argue that a discretization needs to be performed because of the many possible futures given the past, and also provide an error measure based on likelihood. However, if trajectories are actually generated from these distributions, I suspect the many possible futures generated will be very shaky. Can the authors provide trajectories sampled from this? If sampling trajectories does not make sense somehow, can the authors comment on how we can sample multiple trajectories?

Lack of baselines:
- The authors mention social LSTM and social GAN in the related work, however, no comparison is provided. From a quick glance, the authors of these papers work on trajectories. However, the “social” principle in those papers is general since it’s done from the computed feature vector. Could it have not been used on top of one of the baselines? If not, could the authors provide a reason why this is not the case?


Additional comments:
As the authors mention, it would be nice to extend this paper to an unsupervised or semi-supervised task. Here are a couple of papers that may interest you:
https://arxiv.org/abs/1804.04412
https://arxiv.org/abs/1705.02193
https://arxiv.org/abs/1806.07823

4) Conclusion
Overall, the paper is well written, easy to understand, and seems to be simple enough to quickly reproduce. Additionally, the proposed dataset may be of use for the community. If the authors are able to successfully address the issues mentioned, I am willing to improve my score.","The reviewer provides both positive feedback (novel architecture, good performance, new dataset) and constructive criticism (lack of trajectory sampling, missing baselines). They also offer helpful suggestions for future work. While they have concerns, they are open to increasing their score based on the authors' response, indicating a positive attitude overall.",60.0,80.0
"The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines.
Overall, it is a high-quality paper. 
There are only some minor comments for improving the paper:
ν	Page 6, there is a typo. “for node v by employing …”  should be “for node u”
ν	Page 6, “Both GAT and GaAN has”   should be  “Both GAT and GaAN have”
ν	In section 5.1, it will be great if authors can explain more what are the “association events” and “communication events” with more details in these two evaluation datasets.
","The review starts with very positive statements about the paper's quality, writing, and contribution. The reviewer finds the approach appropriate, the discussion clear, and the results promising. While the reviewer lists minor suggestions for improvement, these are presented constructively and aim to further enhance the paper. The language used throughout the review is professional and courteous.",85.0,90.0
"Paper summary: 

This paper presents a deep-learning based method for super-resolving low-resolution labels into high-resolution labels given the joint distribution between those low- and high- resolution labels. This is useful for many semantic segmentation tasks where high-resolution ground truth data is hard and expensive to collect. Its main contribution is a novel loss function that allows to minimize the distance between the distribution determined by a set of model outputs and the corresponding distribution given by low-resolution label over the same set of outputs. The paper also thoroughly evaluates the proposed method for two main tasks, the first being a land cover mapping task and the second being a medical imaging problem.

For the land cover application, adding low-resolution data to high-resolution data worsens the results when evaluating on the geographic area from which the high-resolution data was taken. However, when testing the model on new geographic areas and only adding the low-resolution data from this new area in training makes significant improvements.

Generally the paper is very well written, well structured, all explanations are clear, examples and figures are presented when needed and convey helpful information for the reader. The overall idea is fun, original, useful (especially in remote sensing) and is presented in a a convincing way. All major claims are supported by experimental evaluation. There are nevertheless a few concerns:

Major Concerns:

On a conceptual level, the main concern is that the paper assumes we are given a joint distribution of low and high resolution labels, “where we are given the joint distribution P(Y,Z)”, which seems the main limitation of this method. In fact, to correctly estimat this joint distribution either requires additional knowledge about low-resolution data such as the example presented on the NCLD data : “For instance, the “Developed, Medium Intensity” class [...] of the coarse classes”, or it requires actual high-resolution labelled data to correctly estimate this joint distribution. I think the paper would greatly benefit from including a section that discusses the impact of this limitation.

Another point is footnote 3 on page 5. This argument is valid but it would be more convincing to give a thorough explanation on why the choice of the presented loss function is better compared to the KL divergence based loss function or at least some evidence that the two perform similarly when evaluating the method.

Minor Concerns: 

-	“such as CRFs or iterative evaluation” I would include a citation on this type of work.
-	Format of some references in the text need to be corrected, e.g. “into different land cover classes Demir et al. (2018); Kuo et al. (2018); Davydow et al. (2018); Tian et al. (2018).” 

","The reviewer explicitly states that the paper is ""very well written"", ""well structured"", ""clear"", ""original"", ""useful"", and ""convincing"". They find the idea ""fun"" and the presentation compelling. While they raise valid concerns, they are framed as constructive feedback for improvement rather than harsh criticism. The reviewer acknowledges the paper's strengths before pointing out areas for enhancement, indicating a positive overall sentiment. Therefore, the sentiment leans significantly towards the positive side. The language used is respectful, professional, and constructive throughout, indicating a high level of politeness.",75.0,90.0
"This paper introduces a new framework to interactively interact document retriever and reader for open-domain question answering. While retriever-reader framework was often used for open-domain QA, this bi-directional interaction between the retriever and the reader is novel and effective because
1) If the retriever fails to retrieve the right document at the first step, the reader can give a signal to the retriever so that the retriever can recover its mistake at the next step
2) The idea of `reader state` from the reader to the retriever is new
3) The retriever use question-independent representation of paragraphs, which does not require different representation depending on the question and makes the framework easily scalable.

Strengths
1) The idea of multi-step & bi-directional interaction between the retriever and the reader is novel enough (as mentioned above). The paper contains enough literature studies on existing retriever-reader framework in open-domain setting, and clearly demonstrates how their framework is different from them.
2) The authors run the experiments on 4 different dataset, which supports the argument about the framework’s effectiveness.

Weakness
1) The authors seem to highlight multi-step `reasoning`, while it is not `reasoning` in my opinion. Multi-step reasoning refers to the task which you need evidence from different documents, and/or you need to find first evident to find the second evidence from a different document. I don’t think the dataset here are not multi-step reasoning dataset, and the authors seem not to claim it either. Therefore, I recommend using another term (maybe `multi-step interaction`?) instead of `multi-step reasoning`.
2) While the idea of multi-step interaction and how it benefits the overall performance is interesting, the analysis is not enough. Figure 3 in the paper does not have enough description — for example, I got the left example means step 2 recovers the mistake from step 1, but what does the right example mean?

Questions on result comparison
1) On TriviaQA (both open and full), the authors mentioned the result is on hidden test set — did you submit it to the leaderboard? I don’t see the same numbers on the TriviaQA leaderboard. Also, the authors claim they are SOTA on TriviaQA, but there are higher numbers on the leaderboard (which are submitted prior to the ICLR deadline).
2) There are other published papers with higher result on Quasar-T, SearchQA and TriviaQA (such as https://aclanthology.info/papers/P18-1161/p18-1161 and https://arxiv.org/abs/1805.08092) which the authors did not compare with.
3) In Section 4.2, is there a reason for the specific comparison to AQA (5th line), though AQA is not SOTA on SearchQA? I don’t think it means latent space is better than natural language space. They are totally different model and the only intersection is they contains interaction between two submodules.
4) In Section 5, the authors mentioned their framework outperforms previous SOTA by 15% margin on TriviaQA, but what is that? I don’t see 15% margin in Table 2.

Marginal comments:
1) If I understood correctly, `TriviaQA-open` and `TriviaQA-full` in the paper are officially called `TriviaQA-full` and `open-domain TriviaQA`. How about changing the term for readers to better understand the task? Also, in Section 4, the authors said TriviaQA-open is larger than web/wiki setting, but to my knowledge, this setting is part of the wiki setting.
2) It would be great if the authors make the capitalization consistent. e.g. EM, Quasar-T, BiDAF. Also, the authors can use EM instead of `exact match` after they mentioned EM refers to exact match in Section 4.2.

Overall comment
The idea in the paper is interesting, and their model and experiments are concrete. My only worries is that the terms in the paper are confusing and performance comparison are weak. I would like to update the score when the authors update the paper.


Update 11/27/2018
Thanks for the authors for updating the paper. The updated paper have more clear comparisons with other models, with more & stronger experiments with the additional dataset. Also, the model is claimed to perform multi-step interaction rather than multi-step reasoning, which clearly resolves my initial concern. The analysis, especially ablations in varying number of iterations, was helpful to understand how their framework benefits. I believe these make the paper stronger along with its initial novelty in the framework. In this regard, I vote for acceptance.","The reviewer finds the paper's core idea interesting and novel, praising its contribution to the retriever-reader framework. The experiments are well-executed, and the use of multiple datasets strengthens the paper's claims. However, the reviewer initially expresses concerns about the use of the term ""reasoning"" and points out weaknesses in the analysis and result comparison. The reviewer finds some of the terminology confusing and believes the performance comparison to other models could be more robust.  After the authors' update, the reviewer's concerns were addressed, leading to a positive final sentiment and a vote for acceptance.",75.0,75.0
"The authors design a program synthesizer that tries to satisfy per-instance specific syntactic and functional constraints,
based on sampling trajectories from an RL agent that at each time-step expands a partial-program.

The agent is trained with policy gradients with a reward shaped as the ratio of input/output examples that the synthesized program satisfies.

With the 'out-of-box' evaluation, the authors show that their agent can explore more efficiently the harder problems than their non-learning alternatives even from scratch.
(My intuition is that the agent learns to generate the most promising programs)
It would be good to have a Monte Carlo Tree Search baseline on the'out-of-box' evaluation, to detect exploration exploitation trade-offs.

The authors show with the 'meta-solver' approach that the agent can generalize to and also speed up unseen (albeit easy-ish in the authors words) instances.

Clarity: Paper is clear and nicely written.

Significance: Imagine a single program synthesizer that could generate C++/Java/Python/DSLs  programs and learn from all its successes and failures! This is a step towards that.

Pros:
+ Generating spec-following programs for different grammars.
+ partial tree expansion takes care of syntactic constraints.
Neutral
· The grammar and specification diversity may be too low to feel impressive.
· It would have been nicer by computing likelihood for unseen instances with unique and known solutions (that is, without finetuning).
Cons:
- No Tree Search baseline.
- No results on programs with control flow/internal state.","The reviewer provides constructive criticism, acknowledges the significance of the work, and lists both positive and negative aspects. They find the paper well-written and the research a step in the right direction. While they suggest improvements and have some reservations, their overall tone is encouraging and positive.",65.0,80.0
"In many machine learning applications, sorting is an important step such as ranking. However, the sorting operator is not differentiable with respect to its inputs. The main idea of the paper is to introduce a continuous relaxation of the sorting operator in order to construct an end-to-end gradient-based optimization. This relaxation is introduced as \hat{P}_{sort(s)} (see Equation 4). The paper also introduces a stochastic extension of its method 
using Placket-Luce distributions and Monte Carlo. Finally, the introduced deterministic and stochastic methods are evaluated experimentally in 3 different applications: 1. sorting handwritten numbers, 2. Quantile regression, and 3. End-to-end differentiable k-Nearest Neighbors.

The introduction of the differentiable approximation of the sorting operator is interesting and seems novel. However, the paper is not well-written and it is hard to follow the paper especially form Section 4 and on. It is not clear how the theoretical results in Section 3 and 4 are used for the experiments in Section 6. For instance:
** In page 4, what is ""s"" in the machine learning application?
** In page 4, in Equation 6, what are theta, s, L and f exactly in our machine learning applications?

Remark: 
** The phrase ""Sorting Networks"" in the title of the paper is confusing. This term typically refers to a network of comparators applied to a set of N wires (See e.g. [1])
** Page 2 -- Section 2 PRELIMINARIES -- It seems that sort(s) must be [1,4,2,3].

[1] Ajtai M, Komlós J, Szemerédi E. An 0 (n log n) sorting network. InProceedings of the fifteenth annual ACM symposium on Theory of computing 1983 Dec 1 (pp. 1-9). ACM
","The reviewer acknowledges the novelty of the paper's main idea, describing it as ""interesting"" and ""seem[ing] novel."" This suggests a somewhat positive sentiment. However, the reviewer also expresses concerns about the clarity and organization of the paper, particularly from Section 4 onwards. The numerous pointed questions and the use of the term ""confusing"" indicate a negative sentiment related to the paper's presentation. Overall, the sentiment appears to be mixed but leans slightly positive due to the acknowledgment of the core idea's value. 

The language used is direct and critical, but it maintains a professional and respectful tone. The reviewer points out flaws and provides constructive criticism without resorting to personal attacks or disrespectful language. The use of phrases like ""it is not clear"" and ""it seems"" suggests an attempt to be objective and avoid harsh accusations.",20.0,60.0
"The paper proposes a method for neural network training under a hard energy constraint (i.e. the method guarantees the energy consumption to be upper bounded). Based on a systolic array hardware architecture the authors model the energy consumption of transferring the weights and activations into different levels of memory (DRAM, Cache, register file) during inference. The energy consumption is therefore determined by the number of nonzero elements in the weight and activation tensors. To minimize the network loss under an energy constraint, the authors develop a training framework including a novel greedy algorithm to compute the projection of the weight tensors to the energy constraint.

Pros:

The proposed method allows to accurately impose an energy constraint (in terms of the proposed model), in contrast to previous methods, and also yields a higher accuracy than these on some data sets. The proposed solution seems sound (although I did not check the proofs in detail, and I am not very familiar with hardware energy consumption subtleties).

Questions:

The experiments in Sec. 6.2 suggest that the activation mask is mainly beneficial when the data is highly structured. How are the benefits (in terms of weight and activation sparsity) composed in the experiments on Imagenet? How does the weight sparsity of the the proposed method compare to the related methods in these experiments? Is weight sparsity in these cases a good proxy for energy consumption?

How does the activation sparsity (decay) parameter (\delta) q affect the accuracy-energy consumption tradeoff for the two data sets?

The authors show that the weight projection problem can be solved efficiently. How does the guarantee translate into wall-clock time?

Filter pruning methods [1,2] reduce both the size of the weight and activation tensors, while not requiring to solve a complicated projection problem or introducing activation masks. It would be good to compare to these methods, or at least comment on the gains to be expected under the proposed energy consumption model.

Knowledge distillation has previously been observed to be quite helpful when constraining neural network weights to be quantized and/or sparse, see [3,4,5]. It might be worth mentioning this.

Minor comments:
- Sec. 3.4. 1st paragraph: subscript -> superscript
- Sec. 6.2 first paragraph: pattens -> patterns, aliened -> aligned

[1] He, Y., Zhang, X., & Sun, J. (2017). Channel pruning for accelerating very deep neural networks. ICCV 2017.
[2] Li, H., Kadav, A., Durdanovic, I., Samet, H., & Graf, H. P. Pruning filters for efficient convnets. ICLR 2017.
[3] Mishra, A., & Marr, D. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. ICLR 2018.
[4] Tschannen, M., Khanna, A., & Anandkumar, A. StrassenNets: Deep learning with a multiplication budget. ICML 2018.
[5] Zhuang, B., Shen, C., Tan, M., Liu, L., & Reid, I. Towards effective low-bitwidth convolutional neural networks. CVPR 2018.","The reviewer acknowledges the strengths of the paper, such as the accurate imposition of energy constraints and improved accuracy compared to previous methods. They also find the solution sound. However, they raise several pertinent questions that suggest areas for improvement and further investigation. While the reviewer provides constructive criticism and suggestions, the overall tone suggests a positive outlook on the paper's potential. The language used is polite and professional throughout.",60.0,80.0
"** Summary **
The authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.

** Quality & Clarity **
The paper is well written and features an extensive set of experiments.

** Originality **
I am not aware of similar work and believe the idea is novel.

** Significance **
Several recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.","The review is positive about the paper. It highlights the novelty of the idea, the quality of the writing, and the potential significance of the work. The reviewer finds the paper well-written, the idea novel, and believes the work can be quite helpful. All these are positive signals. There is no negative wording used in the review.",85.0,100.0
"This is a paper that communicates a large scale experiment on human object/semantic representations and a model of such representations.   The experiment could have been more carefully controlled (and described in the paper) and the modeling work is inconclusive.

Quality, 
The experiment design is conventional, based on rating pair-wise similarity among triplets. Compared to earlier experiments, this data has more objects and more triplets.  Additional control experiments on smaller subsets have been carried out to further address hypotheses.  The description of the experiment could have been more careful: What are the precise instructions, how are the object/images presented (it is well known that relative positions, asymmetry, etc can play an important role), are there any temporal/learning effects (how clear is the task to the workers?).
The modeling work is basic and contains a number of steps that have unknown influence on the final outcome. For example model dimension: Is you claim that ""D=49"" is a law of human nature?  Model predictive performance seems excellent, that is interesting! But we do not know how robust this is to the many heuristics

Clarity, 
The presentation of the inference process is clear. Not so clear what the uncertainties are

Originality 
Limited. Mainly related to scale. But the data quality is unclear. The modeling approach involves a number of untested heuristics (non-negative, exponentiation etc). 

Significance 
Mostly related to the data.  I did not understand if it is planned to release the data.

Pros and cons 

+Large scale experiment
+simple model, seem to have good accuracy

-experiment needs more careful description
-too many heuristics in model and inference, unclear how general the conclusions are

Other comments:
References have many issues

The authors have done a good job in the revision and have clarified points that were unclear in the first version. 
I have remaining reservations on significance, but move rating up a notch to reflect the extensive improvements and  the authors' confirmation that they will release the data.


","The reviewer acknowledges the scale of the experiment and the good performance of the simple model as positive aspects. However, they express several concerns about the experimental design, lack of clarity in methodology, and the presence of untested heuristics in the modeling. The reviewer's statement about moving the rating up due to the authors' commitment to releasing the data and addressing some clarity issues suggests a shift towards a more positive sentiment. Overall, the tone is critical but professional and ultimately leans towards encouraging further development rather than outright rejection.",20.0,60.0
"The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.

- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?
- (minor) I do not think that the size of the search space a very meaningful metric

Pros:
- Good exposition
- Interesting and fairly elegant idea
- Good experimental results

Cons
- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers
- No source code available

Some typos:

- Fo example, when proxy strategy -> Fo*r* example
- normal training in following ways. -> in *the* following ways
- we can then derive optimized compact architecture.","The review is overall positive. The reviewer finds the idea interesting and elegant and praises the exposition and results. While some shortcomings are mentioned (limited settings and lack of source code), these are framed as relatively minor and common in the field. The tone is constructive and professional throughout.",75.0,80.0
"The paper presents a novel end-to-end mechanistic generative model of electron flow in a particular type of chemical reaction (“Linear Electron Flow” reactions) . Interestingly, modeling the flow of electrons aids in the prediction of the final product of a chemical reaction over and above problems which attack this “product prediction problem” directly. The method is also shown to generalize well to held-out reactions (e.g. from a chemistry textbook).

General Impressions

+ For me the biggest selling point is that it improves performance in predicting the ultimate reaction outcome. It should do because it provides strictly more supervision, but it’s great that it actually does. 
+ Because it models the reaction mechanism the model is interpretable, and it’s possible to enforce constraints, e.g. that dynamics are physically possible.
+ Generalises outside of the dataset to textbook problems :-)
+ Well-founded modeling choices and neural network architectures.
- Only applies to a very particular type of reaction (heterolytic LEF). 
- Requires supervision on the level of electron paths. This seems to inhibit applying the model to more datasets or extending it to other types of reactions.
- Furthermore the supervision extraction does not seem take advantage of symmetries noted in the section(s) about difficulty evaluating inference. 
- It would be nice to margin out the electron flow model and just maximize the marginal likelihood for the product prediction problem.

Novelty
I’m not an expert on the literature of applying machine learning to the problems of reaction {product, mechanism} prediction but the paper appears to conduct a thorough review of the relevant methods and occupy new territory in terms of the modeling strategy while improving over SOTA performance.

Clarity
The writing/exposition is in general extremely clear. Nicely done. There are some suggestions/questions which I think if addressed would improve clarity.

Ways to improve the paper
1. Better motivate the use of machine learning on this problem. What are the limitations of the arrow-pushing models? 

2. Explain more about the Linear Electron Flow reactions, especially:
- Why does the work only consider “heterolytic” LEF reactions, what other types of LEF reactions are omitted?
- Is the main blocker to extending the model on the modeling front or the difficulties of extracting ground-truth targets? It appears to be the latter but this could be made more clear. Also that seems to be a pretty severe limitation to making the algorithm more general. Could you comment on this?

Questions
1. Is splitting up the electron movement model into bond “removal” and “addition” steps just a matter of parameterization or is that physically how the movements work? 

2. It appears that Jin et al reports Top 6/8/10 whereas this work reports Top 1/3/5 accuracy on the USPTO dataset. It would be nice if there was overlap :-). Do your Top 6/8/10 results with the WLDN model agree with the Jin et al paper?


Nits
Section 2.3, first paragraph “...(LEF) topology is by far the most important”: Could you briefly say why? It’s already noted that they’re the most common in the database. Why?

Section 3.ElectionMovement, first paragraph. “Observer that since LEF reactions are a single path of electrons…”. Actually, it’s not super clear what this means from the brief description of LEF. Can you explain these reactions in slightly more detail?

Section 3.ElectionMovement, second paragraph. “Differently, the above distribution can be split…”. Awkward phrasing. How about “In contrast, the above distribution can be split…”. 

Section 3.Training, last sentence “...minibatches of size one reaction”. Slightly awkward phrasing. Maybe “...minibatches consisting of a single reaction”?

Section 5.2, second sentence. “However, underestimates the model’s actual predictive accuracy…”. It looks like a word accidentally got deleted here or something.

Section 5.2, paragraph 4. “To evaluate if our model predicts the same major project”... Did you mean “the same major product”?

","The review starts with positive remarks, highlighting the strengths of the paper such as improved performance, interpretability, and generalization ability. While it lists some weaknesses, these are presented as areas for improvement rather than fundamental flaws. The reviewer acknowledges the novelty of the work and praises the clarity of the writing. The tone throughout is constructive and suggestive, aiming to help the authors improve their work. The use of 'please' in the questions further indicates politeness.",60.0,80.0
"This paper presents a methodology to infer shape programs that can describe 3D objects. The key intuition of the shape programs is to integrate bottom-up low-level feature recognition with symbolic high-level program structure, which allows the shape programs to capture both high-level structure and the low-level geometry of the shapes. The paper proposes a domain-specific language for 3D shapes that consists of “For” loops for capturing high-level regularity, and associates objects with both their geometric and semantic attributes. It then proposes an end-to-end differentiable architecture to learn such 3D programs from shapes using an interesting self-supervised mechanism. The neural program generator proposes a program in the DSL that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution. The technique is evaluated on both synthetic and ShapeNet tasks, and leads to significant improvements compared to Tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions. In addition, the technique is also paired with MarrNet to allow for a better 3D reconstruction from 2D images.

Overall, this paper presents an elegant idea to describe 3D shapes as a DSL program that captures both geometric and spatial abstractions, and at the same time captures regularities using loops. CSGNet [Sharma et al. 2018] also uses programs to describe 2D and 3D shapes, but the DSL used here is richer as it captures more high-level regularities using loops and also semantic relationships such as top, support etc. The idea of training a neural program executor and using it for self-supervised training is quite elegant. I also liked the idea of guided adaption to make the program generator generalize beyond the synthetic template programs. Finally, the results show impressive improvements and generalization capability of the model.

Can the authors comment on some notion of completeness of the proposed DSL? In other words, is this the only set of operators, shapes, and semantics needed to represent all of ShapeNet objects? Also, it might be interesting to comment more on how this particular DSL was derived. Some of the semantics operator such as “Support”, “Locker”, etc. look overly specific to chair and tables. Is there a way to possibly learn such abstractions automatically?

What is the total search space of programs in this DSL? How would a naive random search perform in this synthesis task?

I also particularly liked the decomposition of programs into draw and compound statements, and the corresponding program generator decomposition into 2 steps BlockLSTM and StepLSTM. At inference time, does the model use some form of beam search to sample block programs or are the results corresponding to top-1 prediction?

Would it be possible to compare the results to the technique presented in CSGNet [Sharma et al. 2018]?  There are some key differences in terms of using lower-level DSL primitives and using REINFORCE for training the program generator, but it would be good to measure how well having higher-level primitives improve the results.

I presume the neural program executor module was trained using a manually-written shape program interpreter. How difficult is it to write such an interpreter? Also, how easy/difficult is to extend the DSL with new semantics operator and then write the corresponding interpreter extension?

Minor typos:
page 3: consists a variable → consists of a variable
page 5: We executes → We execute
page 6: synthetica dataset → synthetic dataset
","The review is overwhelmingly positive. The reviewer uses phrases like ""elegant idea,"" ""quite elegant,"" ""impressive improvements,"" and ""I also liked."" They clearly state the paper's strengths and express interest in its components. While the reviewer poses several questions, these are presented constructively to further the work and do not detract from the positive sentiment. The language throughout is polite and professional, typical of academic discourse.",85.0,95.0
"This paper describes a method for improving the (sequence-length) scalability of the Transformer architecture, with applications to modeling long-range interactions in musical sequences. The proposed improvement is applied to both global and local relative attention formulations of self-attention, and consists of a clever re-use (and re-shaping) of intermediate calculations. The result shaves a factor of L (sequence length) from the (relative) memory consumption, facilitating efficient training of long sequences. The method is evaluated on MIDI(-like) data of Bach chorales and piano performances, and compares favorably to prior work in terms of perplexity and a human listener evaluation.

The results in this paper seem promising, though difficult to interpret.  The quantitative evaluation consists of perplexity
scores (Tables 2 and 3), and the qualitative listening study is analyzed by pairwise comparisons between methods. While the proposed method achieves the highest win-rate in the listening study, other results in the study (LSTM vs Transformer) run contrary to the ranking given by the perplexity scores in Table 3. This immediately raises the question of how perceptually relevant the (small) differences in perplexity might be, which in turn clouds the overall interpretation of the results. Of course, perplexity is not the whole story here: the focus of the paper seems to be on efficiency, not necessarily accuracy, but one might expect improved efficiency to afford higher model capacity and improve on accuracy.


The core contributions of this work are described in sections 3.4 and 3.5, and while I get the general flavor of the idea, I find the exposition here both terse and difficult to follow. Figures 1 and 2 should illustrate the core concept, but they lack axis labels (and generally sufficient detail to decode properly), and seem to use the opposite color schemes from each-other to convey the same ideas.  Concrete image maps using real data (internal feature activations) may have been easier to read here, along with an equation that describes how the array indices map after skewing.

The description in 3.4 of the improved memory enhancement is also somewhat difficult to follow.  The claim is a reduction from O(DL^2) to O(DL), but table 1 lists this as O(DL^2) to O(DL + L^2).  In general, I would expect L to dominate D, which still leaves the memory usage in quadratic space, so it's not clear how or why this constitutes an improvement. The improvement due to moving from global to local attention is clear, but this does not appear to be a contribution of this work.","The reviewer acknowledges the paper's promising results but expresses concerns about the interpretation of those results and the clarity of the paper's exposition. The reviewer finds the results difficult to interpret due to discrepancies between perplexity scores and listening study outcomes. The reviewer also criticizes the clarity of the paper, particularly in sections 3.4 and 3.5, citing terse language, confusing figures, and unclear explanations. The tone is critical but professional and aims to improve the paper's clarity and impact. Therefore, the sentiment is slightly positive, leaning towards neutral, and the politeness is neutral.",20.0,0.0
"After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. 

#####
The paper proposes a variational framework for learning a Model of both the environment and the actor's policy in Reinforcement Learning. Specifically, the model is a deterministic RNN which at every step takes as input also a new stochastic latent variable z_t. Compared to more standard approaches, the prior over z_t is not standard normal but depends on the previously hidden state. The inference model combines information from the forward generative hidden state and a backward RNN that looks only at future observations. Finally, an auxiliary loss is added to the model that tries to predict the future states of the backward RNN using the latent variable z_t.  The idea of the paper is quite well presented and concise. 

The paper tests the proposed framework on several RL benchmarks. Using it for imitation learning outperforms two baseline models: behaviour cloning and behaviour cloning trained with an auxiliary loss of predicting the next observation. Although the results are good, it would have been much better if there was also a comparison against a Generative model (identical to the one proposed) without the auxiliary loss added? The authors claim that the results of the experiment suggest that the auxiliary loss is indeed helping, where I find the evidence unconvincing given that there is no comparison against this obvious baseline. Extra comparison against the method from [1] or GAIL would make the results even stronger, but it is understandable that one can not compare against everything, hence I do not see this as a major issue. 
The authors also compare on long-horizon video prediction. Although their method outperforms the method proposed in Ha & Schmidhuber, this by no means suggests that the method is really that superior. I would argue that in terms of future video prediction that [3] provides significantly better results than the World Models, nevertheless, at least one more baseline would have supported the authors claims much better. 
On the Model-Based planning, the authors outperform SeCTAR model on the BabyAI tasks and the Wheeled locomotion. This result is indeed interesting and shows that the method is viable for planning. However, given that similar result has been shown in [1] regarding the planning framework it is unclear how novel the result is. 

In conclusion, the paper presents a generative model for training a model-based approach with an auxiliary loss. The results look promising, however, stronger baselines and better ablation of how do different components actually contribute would make the paper significantly stronger than it is at the moment. Below are a few further comments on some specific parts of the paper. 

A few comments regarding relevant literature: 

Both in the introduction and during the main text the authors have not cited [1] which I think is a very closely related method. In this work similarly, a generative model of future segments is learned using a variational framework. In addition, the MPC procedure that the authors present in this paper is not novel, but has already been proposed and tried in [1] - optimizing over the latent variables rather than the actions directly, and there have been named Latent Action Priors. 

The data gathering process is also not a new idea and using the error in a dynamics model for exploration is a well-known method, usually referred to as curiosity, for instance see [2] and some of the cited papers as Pathak et. al., Stadie et. al. - these all should be at least cited in section 3.2.2 as well not only in the background section regarding different topics. 


On the auxiliary loss:

The authors claim that they train the auxiliary loss using Variational Inference, yet they drop the KL term, which is ""kinda"" an important feature of VI. Auxiliary losses are well understood that often help in RL, hence there is no need to over-conceptualize the idea of adding the extra term log p(b|z) as a VI and then doing something else. It would be much more clear and concise just to introduce it as an extra term and motivate it without referring to the VI framework, which the authors do not use for it (they still use it for the main generative model). The only way that this would have been acceptable if the experiment section contained experiments with the full VI objective as equation (6) suggest and without the sharing of the variational priors and posteriors and compared them against what they have done in the current version of the manuscript. 


A minor mistake seems to be that equation (5) and (7) have double counted log p(z_t|h_t-1) since they are written as an explicit term as well as they appear in the KL(q(z_t|..)|p(z_t|h_t-1)). 



[1] Prediction and Control with Temporal Segment Models [Nikhil Mishra, Pieter Abbeel, Igor Mordatch, 2017]

[2] Large-Scale Study of Curiosity-Driven Learning [Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A. Efros, 2018]

[3] Action-Conditional Video Prediction using Deep Networks in Atari Games [Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh, 2015]
","The review starts with a positive note, acknowledging the authors' efforts in addressing the reviewer's initial concerns. This suggests a positive sentiment. However, the reviewer also points out limitations in the work and suggests areas for improvement, indicating that the sentiment is not overly positive. The language used is formal, professional, and respectful, without resorting to harsh or critical language.",60.0,80.0
"Pros:
The paper formulates the driving policy problem as a model-based RL problem. Most related work on driving policy has been traditional robotics planning methods such as RRT or model-free RL such as policy gradient methods.

The policy is learned through unrolling a learned model of the environment dynamics over multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory.

The cost combine the objective the policy seeks to optimize (proximity to other cars) and an uncertainty cost representing the divergence from the states it is trained on.

Cons:

The model based RL formulation is pretty standard except that the paper has a additional model uncertainty cost.

Realistically, the output of driving policy should be planning decision, i.e. the waypoints instead of steering angles and acceleration / deceleration commands. There does not seem to be a need to solve the control problem using learning since PID and iLQR has solved the control problem very well. 

The paper did not seem to reach a conclusion on why stochastic forward model does not yield a clear improvement over the deterministic model. This may be due to the limitation of the dataset or the prediction horizon which seems to be 2 second. 

The dataset is only 45 minutes which captured by a camera looking down a small section of the road. So the policies learned might only do lane following and occasionally doing collision avoidance. I would encourage the authors to look into more diverse dataset. See the paper DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents, CVPR 2017.

Overall, the paper makes an interesting contribution: formulate the driving policy problem as a model-based RL problem. The techniques used are pretty standard. There are some insights in the experimental section. However, due to the limitation of the dataset, it is not clear how much the results can generalize to complex settings such as nudging around other cars, cutting in, pedestrian crossing, etc.

Response to rebuttal:
It is good to know that the authors have a new modified VAE posterior distribution for the stochastic model which can achieve significant gain over the deterministic model. Is this empirical and specific to this dataset? Without knowing the details, it is not clear how general this new stochastic model is.

I agree that it is worthwhile to test the model using the 45 minute dataset. However, I still believe the dataset is very limiting and it is not clear how much the experimental results can apply to other large realistic datasets.

My rating stays the same.

","The review acknowledges the interesting contribution of the paper but highlights significant limitations, particularly regarding the dataset and the lack of clear improvement with the stochastic model. While the reviewer acknowledges some insights and the use of standard techniques, the overall tone suggests that the paper might need further development and stronger results. The reviewer maintains their rating even after the authors' rebuttal, indicating unresolved concerns. Therefore, the sentiment is leaning towards the negative side due to the limitations and the persistent concerns. The language used is polite, providing constructive criticism and suggestions for improvement without resorting to harsh or disrespectful language.",-10.0,60.0
"The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. 

* pros: 
- the paper is well written. 
- the idea is simple but BRILLIANT. 
- the used techniques are good (especially to learn word clusters). 
- the experimental results  (speed up softmax at test time) are impressive. 

* cons: 
- the model is not end-to-end because word clusters are not continuous. But it not an important factor. 
- it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time.
- it would be better if the authors show some clusters for both input examples and corresponding word clusters.


","The reviewer explicitly states positive sentiment with ""the idea is simple but BRILLIANT"" and ""the experimental results (speed up softmax at test time) are impressive."" They also start with positive framing, calling the idea ""interesting."" While they list some cons, they are generally framed neutrally or even positively (e.g., ""it's not an important factor""). The language is polite and professional throughout, using constructive criticism and avoiding harsh language.",75.0,80.0
"In the submitted manuscript, the authors introduce a novel deep learning architecture to solve the  problem of supervised learning with sparse and irregularly sampled multivariate time series, with a specific interest in EHRs. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network, and it is tested on two classification/regression tasks.

The manuscript is interesting and well written: the problem is properly located into context with extensive bibliography, the method is sufficiently detailed and the experimental comparative section is rich and supportive of the authors’ claim. However, there are a couple of issues that need to be discussed: 

	▪	the reported performances represent only a limited improvement over the comparing baselines, indicating that the proposed model is promising but it is still immature
	▪	the model is sharing many characteristics with (referenced) published methods, which the proposed algorithm is a smart combination of - thus, overall, the novelty of the introduced method is somewhat limited.


######### 

After considering the proposed improvements, I decided to raise my mark to 6. Thanks for the good job done!
","The review starts with positive statements, highlighting the manuscript's interest, clarity, and experimental support. However, it also points out limitations, mainly the limited performance improvement and the lack of significant novelty. The concluding sentence, despite mentioning ""good job,"" suggests a moderate initial rating (now raised to 6) implying the positives outweigh the negatives but not by a large margin.",50.0,70.0
"[Summary]:
This paper tackles the problem of automatic robot design. The most popular approach to doing this has been evolutionary methods which work by evolving morphology of agents in a feed-forward manner using a propagation and mutation rules. This is a non-differentiable process and relies on maintaining a large pool of candidates out of which best ones are chosen with the highest fitness. In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable. This paper uses graph network to train each morphology using RL. Thereby, allowing the controller to share parameters and reuse information across generations. This expedites the score function evaluation improving the time complexity of the evolutionary process.

[Strengths]:
This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms. Paper is quite easy to follow.

[Weaknesses and Clarifications]:
=> Robot design area has been explored extensively in classical work of Sims (1994) etc. using ES. Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.
=> Environment: The experimental section of the paper can be further improved. The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?
=> Baselines: The comparison provided in the paper is weak. At first, it compares to random graph search and ES. But there are better baselines possible. One such example would be to have a network for each body part and share parameters across each body part. This network takes some identifying information (ID, shape etc.) about body part as input. As more body parts are added, more such network modules can be added. How would the given graph network compare to this? This baseline can be thought of a shared parameter graph with no message passing.
=> The results shown in Figure-4 (Section-4.2) seems unclear to me. As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process. However, the original performance of the hand-engineered design is surprisingly bad (see first data point in any plot in Figure-4). Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?

[Recommendation]:
I request the authors to address the comments raised above. Overall, this is a reasonable paper but experimental section needs much more attention.",,,
"[REVISION]
The work is thorough and some of my minor concerns have been addressed, so I am increasing my score to 6. I cannot go beyond because of the incremental nature of the work, and the very limited applicability of the used continual learning setup from this paper.

[OLD REVIEW]
The paper proposes a novel, regularization based, approach to the sequential learning problem using a fixed size model. The main idea is to add extra terms to the loss encouraging representation sparsity and combating catastrophic forgetting. The approach fairs well compared to other regularization based approaches on MNIST and CIFAR-100 sequential learning variants.

Pros:
Thorough experiments, competitive baselines and informative ablation study.
Good performance on par or superior to baselines.
Clear paper, well written.

Cons:
The approach, while competitive in performance, does not seem to fix any significant issues with baseline methods. For example, task boundaries are still used, which limits applicability; in many scenarios which do have a continual learning problem there are no clear task boundaries, such as data distribution drift in both supervised and reinforcement learning.
Since models used in the work are very different from SOTA models on those particular tasks, it is hard to determine from the paper how the proposed method influences these models. In particular, it is not clear whether these changes to the loss would still allow top performance on regular classification tasks, e.g. CIFAR-10 or MNIST even without sequential learning, or in multitask learning settings. 

Summary:
Although the work is substantial and experiments are thorough, I have reservations about extrapolating from the results to settings which do have a continual learning problem. Although I am convinced results are slightly superior to baselines, and I appreciate the lengthy amount of work which went into proving that, the paper does not go sufficiently beyond previous work.
","The reviewer acknowledges the thoroughness of the work and the improvement from a previous version. However, they maintain reservations about the limited applicability of the approach, leading to a score of 6 (which we can assume is out of 10, thus slightly above neutral). The language used is professional and polite throughout, without any personal attacks or dismissive language.",10.0,80.0
"This work demonstrates that a particle dynamics model can be learned to approximate the interaction of various objects. The resulting differentiable simulator has a strong inductive bias, which makes it possible to efficiently solve complex manipulation tasks over deformable objects.

# Quality

This work is an impressive proof-of-concept of the capabilities of differentiable programming for learning complex (physical) processes, such as particle dynamics. In my opinion, the resulting particle interaction network would deserve publication for itself. However, this work goes already one step further and demonstrates that the resulting differentiable simulator can be used for the manipulation of deformable objects.

The method is evaluated on a well-rounded set of experiments which demonstrates its potential. More real-world experiments would be welcome to leave any doubt.

EDIT: However, the current manuscript lacks a proper comparison with (cited) previous work, such as 1806.08047. 

# Clarity

The paper is well written, although I do feel it was difficult to remain within the 8-page limit given the breadth of the work.

# Originality

As far as I know, this work is (very) original. (That being said, I am not too familiar with the related work.)

EDIT: This work is actually quite similar to 1806.08047. A proper discussion of the differences should be included. 

# Significance

This work will certainly be of interest for several research communities, including deep learning, physics, control and robotics.
","The review starts with very positive statements, highlighting the impressiveness and originality of the work. While it mentions the need for more real-world experiments, this is not framed as a major drawback. However, the edits pointing out the lack of comparison with a specific previous work bring the overall sentiment down a bit. The language used is entirely professional and polite.",60.0,100.0
"Paper Summary -
The authors trained RNNs to recognize formal languages defined by random regular expressions, then measured the accuracy of decoders that predict states of the minimal deterministic finite automata (MDFA) from the RNN hidden states. They then perform a greedy search over partitions of the set of MDFA states to find the groups of states which, when merged into a single decoder target, maximize prediction accuracy. For both the MDFA and the merged classes prediction problems, linear decoders perform as well as non-linear decoders.
Clarity - The paper is very clear, both in its prose and maths.
Originality - I don't know of any prior work that approaches the relationship between RNNs and automata in quite this way.
Quality/Significance - I have one major concern about the interpretation of the experiments in this paper.

The paper seems to express the following logic:
1 - linear (and non-linear) decoders aren't so good at predicting MDFA states from RNN hidden states
2 - if we make an ""abstract"" finite automata (FA) by merging states of the MDFA to optimize decoder performance, the linear (and non-linear) decoders are much better at predicting this new, smaller FA's states.
3 - thus, trained RNNs implement something like an abstract FA to recognize formal languages.

However, a more appropriate interpretation of these experiments seems to be:
1 - (same)
2 - if we find the output classes the decoder is most often confused between, then merge them into one class, the decoder's performance increases -- trivially. in other words, you just removed the hardest parts of the classification problem, so performance increased. note: performance also increases because there are fewer classes in the merged-state FA prediction problem (e.g., chance accuracy is higher).
3 - thus, from these experiments it's hard to say much about the relationship between trained RNNs and finite automata.

I see that the ""accuracy"" measurement for the merged-state FA prediction problem, \rho, is somewhat more complicated than I would have expected; e.g., it takes into account \delta and f(h_t) as well as f(h_{t+1}). Ultimately, this formulation still asks whether any state in the merged state-set that contains f(h) transitions under the MDFA to the any state in the merged state-set that contains f(h_{t+1}). As a result, as far as I can tell the basic logic of the interpretation I laid out still applies.

Perhaps I've missed something -- I'll look forward to the author response which may alleviate my concern.

Pros - very clearly written, understanding trained RNNs is an important topic
Cons - the basic logic of the conclusion may be flawed (will await author response)

Minor -
The regular expression in Figure 6 (Top) is for phone numbers instead of emails.
""Average linear decoding accuracy as a function of M in the MDFA"" -- I don't think ""M"" was ever defined. From contexts it looks like it's the number of nodes in the MDFA.
""Average ratio of coarseness"" -- It would be nice to be explicit about what the ""ratio of coarseness"" is. I'm guessing it's (number of nodes in MDFA)/(number of nodes in abstracted DFA).
What are the integers and percentages inside the circles in Figure 6?
Figures 4 and 5 are difficult to interpret because the same (or at least very similar) colors are used multiple times.
I don't see ""a"" (as in a_t in the equations on page 3) defined anywhere. I think it's meant to indicate a symbol in the alphabet \Sigma. Maybe I missed it.","The reviewer expresses a major concern about the interpretation of the experiments, suggesting an alternative explanation that undermines the paper's conclusion. While they acknowledge the clarity and originality, their doubt about the core argument significantly impacts the overall sentiment. The tone remains professional and polite throughout, focusing on the scientific content rather than personal criticism.",-20.0,80.0
"This paper aims at matching people's voices to the images of their faces. It describes a method to train shared embeddings of voices and face images. The speech and image features go through separate neural networks until a shared embedding layer. Then a classification network is built on top of the embeddings from both networks.  The classification network predicts various combinations of covariates of faces and voices: gender, nationality, and identity.  The input to the classification network is then used as a shared representation for performing retrieval and matching tasks.

Compared with similar work from Nagrani et al (2018) who generate paired inputs of voices and faces and train a network to classify if the pair is matched or not, the proposed method doesn't require paired inputs.  It does, however, require inputs that are labeled with the same covariates across modalities.  My feeling is that paired positive examples are easier to obtain (e.g., from unlabeled video) than inputs labeled with these covariates, although paired negative examples require labeling and so may be as difficult to obtain.

Several different evaluations are performed, comparing networks that were trained to predict all subsets of identity, gender, and nationality.  These include identifying a matching face in a set of faces (1,2 or N faces) for a given voice, or vice versa. Results show that the network that predicts identity+gender tends to work best under a variety of careful examinations of various stratifications of the data.  These stratifications also show that while gender is useful overall, it is not when the gender of imposters is the same as that of the target individual.  The results also show that even when evaluating the voices and faces not shown in the training data, the model can achieve 83.2% AUC on unseen/unheard individuals, which outperforms the state-of-the-art method from Nagrani et al (2018).

An interesting avenue of future work would be using the prediction of these covariates to initialize a network and then refine it using some sort of ranking loss like the triplet loss, contrastive loss, etc.


Writing:
* Overall, ciations are all given in textual form Nagrani et al (2018) (in latex this is \citet{} or \cite{}), when many times parenthetical citations (Nagrani et al, 2018) (in latex this is \citep{}) would be more appropriate.
* The image of the voice waveform in Figures 1 and 2 should be replaced by log Mel-spectrograms in order to illustrate the network's input.
* ""state or art"" instead of ""state-of-the-art"" on page 3. 
* In subsection 2.4: ""mGiven"" is written instead of ""Given"". 
* On Page 6 Section 3.1 ""1:2 matching"" paragraph. ""Nagrani et al."" is written twice. * * Page 6 mentions that there is a row labelled ""SVHF-Net"" in table 2, but there is no such row is this table. 
* Page 7 line 1, “G,N” should be ""G, N"".
","The review is positive overall. The reviewer acknowledges the novelty of the approach compared to prior work and highlights the strong performance on unseen data. While the reviewer provides constructive criticism and points out areas for improvement, it is done in a neutral and professional manner. The suggestions focus on clarifying the presentation and enhancing the technical aspects of the work.",65.0,75.0
"This paper describes two distinct contributions: a new compound criterion for comparing a temporal clustering to a ground truth clustering and a new bayesian temporal clustering method. Globally the paper is clear and well illustrated. 
1) About the new criterion:
*pros: *
 a) as clearly pointed out by the authors, using standard non temporal clustering comparison metrics for temporal clustering evaluation is in a way ""broken by design"" as standard metrics disregard the very specificity of the problem. Thus the introduction of metrics that take explicitly into account time is extremely important.
 b) the proposed criterion combines two parts that are very important: finding the length of the stable intervals (i.e. intervals whose instants are all classified into a single cluster) and finding the sequence of labels. 
*cons:*
 a) while the criterion seems new it is also related to criteria used in the segmentation literature (see among many other https://doi.org/10.1080/01621459.2012.737745) and it would have been a good idea to discuss the relation between temporal clustering and segmentation, even briefly.
b) the reliance on a tradeoff parameter in the final criterion is a major problem: how shall one chose the parameter (more on this below)? The paper does not explore the effect of modifying the parameter.
c) in the experimental section, TSS is mostly compared to NMI and to optimal matching (called Munkres here). Even considering the full list of criteria in the appendix, the normalized rand index (NRI) seems to be missing. This is a major oversight as the NRI is very adapted to comparing clusterings with different number of clusters, contrarily to NMI. In addition, the authors claim that optimal matching is completely opaque and difficult to analyse, while on the contrary it gives a proper way of comparing clusters from different clusterings, enabling fine grain analysis. 

2) about the new model
*pros*: 
 a) as far as I know, this is indeed a new model
 b) the way the model is structured emphasizes segmentation rather than temporal dependency: the so called procedure is arbitrary and no dependency is assumed from one segment to another. In descriptive analysis this is highly desirable (as opposed to say HMM which focuses on temporal dependencies). 
*cons*
a) the way the length of the segments in the sequence are generated (with sorting) this a bit convolved. Why not generating directly those lengths? What is the distribution of those lengths under the sampling model? Is this adapted? 
b) I find the experimental evaluation acceptable but a bit poor. In particular, nothing is said on how a practitioner would tune the parameters. I can accept that the model will be rather insensitive to hyper-parameters alpha and beta, but I've serious doubt about the number of clusters, especially as the evaluation is done here in the best possible setting. In addition, the other beta parameter (of TSS) is not studied. 

Minor point:
- do not use beta for two different things (the balance in TSS and the prior parameter in the model)","The review is quite balanced, presenting both positive and negative points for each contribution. While it acknowledges the significance and clarity of the work, it also raises valid concerns and suggests areas for improvement. The language is formal, academic, and direct, but not disrespectful. The reviewer maintains a professional and constructive tone throughout.",0.0,50.0
"General comment
==============
The authors used policy gradient optimization for generating RNA sequences that fold into a target secondary structure, reporting clear accuracy and runtime improvements over the previous state-of-the-art. The authors used BOHR for optimizing hyper-parameters and present a new dataset for evaluating RNA design methods. The paper is well motivated and mostly clearly written. However, the methodological contributions are limited and I have some important concerns about their evaluation. Overall, I feel it’s a good paper for an ICLR workshop or biological journal if the authors address the outstanding comments.

Major comments
=============
1. The methodological contributions are limited. The authors used existing approaches (policy gradient optimization and BOHR for hyperparameter optimization) but do not report new methods, e.g. for sequence modeling. Performing hyper-parameter optimization is in my eyes not novel, but common practice in the field. It would me more informative if the authors compared reinforcement learning to other approaches for (conditional) sequence generations, e.g. RNNs, autoregressive models, VAEs, or GANs, which have been previously reported for biological sequence generation (e.g. http://arxiv.org/abs/1804.01694).

2. Did the authors split all three datasets (Eterna, Rfam-Taneda, Rfam-learn-test) into train, eval, and test set, trained their method on the training set, optimized hyper-parameters on the eval set, and measured generalization and runtime on the test set? This is not described clearly enough in section 5. I suggest to summarize the number of sequences for each dataset and split in a table.

3. Did the authors also optimize the most important hyperparameters of RL-LS and other methods? Otherwise it is unclear if the performance gain is due to hyperparameter optimization or the method itself.

4. The time measurement (x-axis figure 3) is unclear. Is it the time that methods were given to solve a particular target structure and does figure 3 show the average number of solved structures in the test for a the time shown on the x-axis? 

5. Were all methods compared on the same hardware (section 5; 20 cores; Broadwell E5-2630v4 2.2 GHz CPUs) and can they be parallelized over multiple CPU or GPU cores? This is essential for a fair runtime comparison.

6. The term ‘run’ (“unreliable outcomes in single runs”, section 4) is unclear. Is it a single sample from the model (one rollout), a particular hyperparameter configuration, or training the model once for a single target structure? This must be clarified for understanding the evaluation.

7. How does the accuracy and runtime or LEARNA scale depending on the sequence (structure) length?

8. How sensitive is the model performance depending on the context size k for representing the current state? Did the authors try to encode the entire target structure with, e.g. recurrent models, instead of using a window centered on the current position?

9. The authors should more clearly describe the local optimization step (section 3.1; reward). Were all nucleotides that differ mutated independently, or enumerated exhaustively? The latter would have a high runtime of O(3^d), where d is the number of nucleotides that differ. When do the authors start with the local optimization? 

Minor comments
=============
10. The authors should replace ‘450x’ faster in the abstract by ‘clearly’ faster since the evaluation does not show that LEARNA is 450x faster than all other methods.

11. Does “At its most basic form” (introduction) mean that alternative RNA nucleotides exist? If so, this should be cited.

12. The authors should more clearly motive in the introduction why they created a new dataset.

13. The authors should mention in section 2.1 that the dot-bracket notation is not the only notation for representing RNA structures (https://www.tbi.univie.ac.at/RNA/ViennaRNA/doc/html/rna_structure_notations.html).

14. The authors should define the hamming distance (section 2.1). Do other distance metrics exist?

15. For the Traveling Salesman Problem (section 2.2) should the reward be the *negative* tour length?

16. The authors should more clearly describe the embedding layer (section 4). Are nucleotides one-hot encoded or represented as integers (0, 1  for ‘(‘ and ‘.’)?","The reviewer acknowledges the paper's strengths, such as its clear improvements in accuracy and runtime, well-motivated research question, and clear writing. However, they also express reservations about the limited methodological novelty and raise several important questions about the evaluation. The overall tone suggests that the reviewer leans towards acceptance but requires the authors to address the raised concerns. Therefore, the sentiment is moderately positive, and the language used is polite and constructive.",60.0,70.0
"** review score incremented following discussion below **

Strengths:

Well written and clear paper
Intuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for 

Weaknesses:

Cost matrices choices feel a bit arbitrary in experiments
CIFAR experiments still use very small norm-balls

The submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary).

While the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. 

The experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the “hardness” of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST?

Minor comments:


P2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict.
P3: typo in “optimzation” 
P5: trade off -> trade-off 
P8: the font used in Figure 2 is small and hard to read when printed.
","The review is overall positive. The reviewer finds the paper well-written and the intuition strong. They highlight the strength of the work building upon seminal research. While they have questions and seek clarification on certain aspects like cost matrix choices and epsilon parameter justification, their tone is constructive and aimed at improving the paper. The presence of minor comments further suggests a positive disposition towards the paper.",65.0,90.0
"This paper presents an algorithm to find adversarial attacks to binary neural networks.  Binary neural networks uses sign functions as nonlinearities, making the network essentially discrete.  Previous attempts at finding adversarial attacks for binary neural networks either rely on relaxation which cannot find very good adversarial examples, or calling a mixed integer linear programming (MILP) solver which doesn’t scale.  This paper proposes to decompose the problem and iteratively find desired representations layer by layer from the top to the input.  This so called Integer Propagation (IProp) algorithm is more efficient than solving the full MILP as it solves much smaller MILP problems, one for each layer, thus each step can be solved relatively quickly.  The authors then proposed a few more improvements to the IProp algorithm, including ways to do local adjustments to the solutions, and warming starting from an existing solution.  Experiments on binary neural nets trained for MNIST and Fashion MNIST show the superiority of the proposed method over MILP and relaxation based algorithms.

Overall I found the paper to be very clear and the proposed method is sound.  I think combining ideas from discrete / combinatorial optimization with deep learning is an important research direction and can shed light on training and verifying models with discrete components, like the hard nonlinearities in the binary neural nets studied in this paper.

In terms of the particular proposed approach, it is hard for me to imagine the blind IProp that does not take the input into account until the last layer is ever going to work.  The small step size modifications make a lot more sense.  Regarding the selection of the set S, in the paper the authors simply sampled elements to be in S uniformly, but it seems possible to make use of the information from the forward pass, and choose the hidden units that are the closed to reaching the desired activations.  Would that be any better?

A few minor comments:
- when reporting warm start results, it would be good to also show the performance of the FGSM solution used for warm starting, in addition to the other two results shown in Figure 6 to have a more complete comparison
- the hidden units h_{l,j} were formulated to be in {0, 1} in equation (7), but everywhere else in the paper they are assumed to be in {-1, +1}, which is not consistent and slightly confusing.

Overall I think this is a solid paper and support accepting it for publication.","The reviewer explicitly states ""Overall I think this is a solid paper and support accepting it for publication."" which clearly indicates a positive sentiment. They also provide constructive criticism and suggestions for improvement, which further supports a positive but not overly enthusiastic sentiment. The language used is polite and professional throughout, with constructive criticism offered respectfully.",75.0,90.0
"Overall, the paper is well-written and of high quality, therefore I recommend acceptance. 

Pros:
+ The work gives an accessible but still rigorous introduction to the literature on VIs which I find highly valuable, as it creates a bridge between the classical mathematical programming literature and applications in AI. 

+ The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4)

Cons:
- I'm a bit skeptical about the experiments on GANs. They indicate that for the specific choice of architectures and hyper-parameters ""ExtraAdam"" works better, but the chosen architectures are not state-of-the art. What would convince me if the algorithm can be used to improve a current best inception score of 8.2 reached with SNGANs. Also with WGAN-GP, scores of ~7.8 are reported which are much higher than the 6.4 reported in the paper. But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a ""plug-and-play"" fashion.

- Proposition 2 is a bit misleading. While for \eta \in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \eta > 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view. Right now, only the advantages of extrapolation method and disadvantages of implicit method are mentioned which I find unfair for the implicit method.

- The theory is presented for variational inequalities with monotone operators. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?
","The review starts with a clear positive statement recommending acceptance and praising the work's quality and accessibility. While it lists some cons, these are presented constructively as suggestions for improvement rather than harsh criticisms. The reviewer acknowledges the strengths of the paper and focuses on making it even better. The language is professional and polite throughout, without resorting to negativity.",75.0,80.0
"The paper proposes ""style transfer"" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.

This is an interesting application, and somewhat different from ""style transfer"" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.

The architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.

Other than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.
","The review starts with positive remarks, highlighting the interesting application and compelling results. However, it expresses uncertainty and lack of familiarity with the technical details, suggesting the methodological contributions are unclear. The reviewer's polite but critical tone contributes to a moderately positive sentiment.",50.0,70.0
"Summary:
The authors propose an extension of dual learning (DL). In DL, one leverages the duality of a dataset, by predicting both forward and backward, e.g. English to German, and German back to English. It’s been shown that training models using this duality is beneficial. This paper extends DL by introducing multiple models for the forward and backward, and using their output to regularise the training of the two main agents.

The authors show that this setup improves on the SotA, at only a training computation expense (inference/test time remains the same).

Review:
The paper shows extensive experimentation and improves the previous result in all cases. The proposed method is a straightforward extension and can be readily implemented and used.

I have difficulty understanding equation 8 and the paragraph below. It seems like the authors use an equal weighting for the additional agents, however they mention using Monte Carlo to “tackle the intractability resulting form the summation over the exponentially large space y”. According to the paper the size of y is the dataset, is it exponentially large? Do the authors describe stochastic gradient descent? Also what do the authors mean by offline sampling? Do they compute the targets for f_0 and g_0 beforehand using f_1…n and g_1…n?

The results mention computational cost a few times, I was wondering if the authors could comment on the increase in computational cost? e.g. how long does “pre-training” take versus training the dual? Can the training of the pre-trained agents be parallelised? Would it be possible to use dropout to more computationally efficient obtain the result of an ensemble?

In general I think the authors did an excellent job validating their method on various different datasets. I also think the above confusions can be cleared up with some editing. However the general contribution of the paper is not enough, the increase in performance is minimal and the increased computational cost/complexity substantial. I do think this is a promising direction and encourage the authors to explore further directions of multi-agent dual learning.

Textual Notes:
- Pg2, middle of paragraph 1: “which are pre-trained with parameters fixed along the whole process”. This is unclear, do you mean trained before optimising f_0 and g_0 and subsequently held constant?
- Pg2, middle last paragraph: “typical way of training ML models”. While the cross entropy loss is a popular loss, it is not “typical”.
- Pg 3, equation 4, what does “briefly” mean above the equal sign?
- Perhaps a title referring to ensemble dual learning would be more appropriate, given the possible confusion with multi agent reinforcement learning. 


################
Revision:

I would like to thank the authors for the extensive revision, additional explanations/experiments, and pointing out extensive relevant literature on BLUE scores. The revision and comments are much appreciated. I have increased my score from 4 to 6.","The reviewer acknowledges the paper's strengths, such as extensive experimentation and improvement over previous work. However, they also express concerns about clarity, computational cost, and the significance of the performance increase. While encouraging further exploration, the reviewer ultimately finds the contribution not substantial enough due to the mentioned drawbacks. The tone is critical but professional and provides constructive feedback. Therefore, the sentiment leans slightly towards the negative side, and the politeness remains neutral.",-20.0,0.0
"
SUMMARY:
This work is about learning state-transition models in complex domains represented as sets of objects, their properties, ``""deictic"" reference functions between sets of objects, and possible actions (or action templates). A parametric model for the actions is assumed, and these parameters act on a neural net that learns the transition model (probabilistic rule) from the current state to the next one.  It is basically this nonlinear transition model implemented by a network which makes this work different from previous models described in the literature. The relational transition model proposed is sparse, based on the assumption that actions have only ``local effects on related objects. The prediction model itself is basically a Gaussian distribution whose mean and variances are represented by neural nets. For jointly learning multiple rules, a clustering strategy is presented which assigns experience samples to transition rules. The method is applied to simulated data in the context of predicting pushing stacks of blocks on a cluttered table top.

EVALUATION: 
The type of problems addressed in this paper is challenging and highly relevant for solving problems in the ``real'' world. Although the method proposed is in some sense a direct generalization of the work in [Pasula et al.], it still contains many novel and interesting aspects.Any single part of the model (like the use of Gaussians parametrized by functions implemented via neural nets) is somehow ``standard in deep latent variable models, but in complex real-world rule-learning problems the whole system presented  defines  certainly a big improvement over the state-of-the-art, which in my opinion has the potential to indeed advance this field of research.     
 ","The review is positive about the work, highlighting its relevance, novelty, and potential impact. The reviewer uses phrases like ""highly relevant,"" ""novel and interesting aspects,"" ""big improvement over the state-of-the-art,"" and ""potential to indeed advance this field of research,"" all indicating a positive sentiment. The language is formal and respectful throughout.",85.0,90.0
"This article addresses the problem of domain adaptation of semantic segmentation methods from autonomous vehicle simulators to the real world. The key contribution of this paper is the use of privileged information for performing the adaptation. The method is of those called unsupervised domain adaptation as no labels from the target domain are used for the adaptation. The method is based on a GAN with: a) A generator that transforms the simulation images to real appearance; b) A discriminator that distinguish between real and fake images;  c) a privileged network that learns to perform depth estimation; and d) the task networks that learns to perform semantic segmentation. Privileged information is very few exploited in simulations and I consider it an important way of further exploit these simulators.

The article is clear, short, well written and very easy to understand. The method is effective as it is able to perform domain adaptation and improve over the compared methods. There is also an ablation study to evaluate the contribution of each module. This ablation study shows that the privileged information used helps to better perform the adaptation. The state of the art is comprehensive and the formulation seams correct.  The datasets used for the experiments (Synthia, Cityscapes and Vistas) is very adequate as they are the standard ones.

Some minor concerns:
 - The use of 360x640 as resolution
 - The use of FCN8 instead of something based on Resnet or densenet

I would like some more details on what is happening with Vistas dataset. SPIGAN-no-PI underperforms the source model. By looking at Figure 4 we can observe that the transformation of the images is not working properly as many artifacts appear. In SPIGAN those artifacts does not appear and then the adaptation works better. Could it be a problem in the training?


","The reviewer explicitly states positive aspects like ""The article is clear, short, well written and very easy to understand"" and ""The method is effective..."". They also validate the methodology and results. While they ask for clarification and point out areas of improvement, these are framed constructively and not as harsh criticisms. Thus, the sentiment is positive, and the language is polite.",75.0,75.0
"In the paper, the authors try to analyze the convergence of stochastic gradient descent based method with stagewise learning rate and average solution in practice. The paper is very easy to follow, and the experimental results are clear. The following are my concerns:

1. In function (3), for any x in R^d, if \hat x  = prox_\gamma f (x), then f(\hat x ) <= f(x). This inequality looks not correct to me. If x = argmin_x f(x), the above inequality is obviously wrong.  It looks like that function (3) is a very important basis for the whole paper.
 
2. By using the weakly convex assumption and solving f_s, the authors transform a nonconvex nonsmooth problem to a convex problem. However, the paper didn't mention how to select \gamma in the algorithm. This parameter is nontrivial, if you set a small value, the problem is not convex and the analysis does not hold. In the experiment, the authors tune \gamma from 1 to 2000, which means that u < 1 or u < 1/2000.  Given neural network is a u-weakly convex problem or u-smooth problem, the theory does not match the experiment. 

3. The authors propose a universal stagewise optimization framework and mention that the stagewise ADAGRAD obtains faster convergence than other analysis. My question is that, if it is a generic framework, how about the convergence rate for other methods? is there also acceleration for SGD or momentum SGD? 
","The review starts with a positive sentiment, highlighting the paper's clarity and clear results. However, it then raises serious concerns about a fundamental inequality that seems incorrect, impacting the paper's basis. The reviewer also questions the practical application of the theoretical framework and the lack of clarity on convergence rates for other methods. The tone is polite and professional, directly addressing the authors and their work without resorting to harsh language.",20.0,60.0
"edit:  the authors nicely revised the submission, I think it is a very good paper. I increased my rating.

-----

This paper presents a method that learns to reproduce 'block towers' from a given image. A perception model, a physics engine model, and a rendering engine are first trained together on pairs of images.
The perception model predicts a representation of the scene decomposed into objects;  the physics engine predicts the object representation of a scene from an initial object representation; the rendering engine predicts an image given an object representation.

Each training pair of images is made of the first image of a sequence when introducing an object into a scene, and of the last image of the sequence, after simulating the object's motion with a physics engine. The 3 parts of the pipeline (perception, physics, rendering) are trained together on this data.

To validate the learned pipeline, it is used to recreate scenes from reference images, by trying to introduce objects in an empty scene until the given scene can be reproduced. It outperforms a related pipeline that lacks a scene representation based on objects.

This is a very interesting paper, with new ideas:
- The object-based scene representation makes a lot of sense, compared to the abstract representation used in recent work. 
- The training procedure, based on observing the result of an action, is interesting as the examples are easy to collect (except for the fact that the ground truth segmentation of the images is used as input, see below).

However, there are several things that are swept 'under the carpet' in my opinion, and this should be fixed if the paper is accepted.

* the input images are given in the form of a set of images, one image corresponding to the object segmentation. This is mentioned only once (briefly) in the middle of the paragraph for Section 2.1, while this should be mentioned in the introduction, as this makes the perception part easier. There is actually a comment in the discussion section and the authors promised to clarify this aspect, which should indeed be more detailed. For example, do the segments correspond to the full objects, or only the visible parts?

* The training procedure is explained only in Section 4.1. Before reaching this part, the method remained very mysterious to me. The text in Section 4.1 should be moved much earlier in the paper, probably between current sections 2.3 and 2.4, and briefly explained in the introduction as well.
This training procedure is in fact fully supervised - which is fine with me: Supervision makes learning 'safer'. What is nice here is that the training examples can be collected easily - even if the system was not running in a simulation.

* if I understand correctly the planning procedure, it proceeds as follows:
- sampling 'actions' that introduce 1 object at a time (?)
- for each sampled action, predicting the scene representation after the action is performed, by simulating it with the learned pipeline, 
- keeping the action that generates a scene representation close to the scene representation computed for the goal image of the scene.
- performing the selected action in a simulator, and iterate until the number of performed actions is the same as the number of objects (which is assumed to be known).

-> how do you compare the scene representation of the goal image and the predicted one before the scene is complete? Don't you need some robust distance instead of the MSE?
-> are the actions really sampled randomly?  How many actions do you need to sample for the examples given in the paper?

I also have one question about the rendering engine:  Why using the weighted average of the object images? Why not using the intensity of the object with the smallest predicted depth?  It should generate sharper images. Does using the weighted average make the convergence easier?
","The review starts with positive remarks, highlighting interesting aspects and novelty. However, it then lists several concerns and questions, indicating that while the reviewer finds merit in the work, it needs significant clarifications and improvements. The 'edit' at the beginning suggests a positive shift after revision, further supporting a generally positive but not overwhelmingly enthusiastic sentiment. The language is constructive and professional throughout.",60.0,80.0
"Pros:
1. This work presents a novel construction of the popularly-used attention modules. It points out the problems lied in existing design that attention vectors are only computed based on parametric functions, instead of considering the interactions among each attention step and output variables. To achieve that, the authors re-write the joint distribution as a product of tractable terms at each timestamp and fully exploit the dependencies among attention and output variables across the sequence. The motivation is clear, and the proposed strategy is original and to the point. This makes the work relative solid and interesting for a publication. Furthermore, the authors propose 3 different formulation for prior attention, making the work even stronger.
2. The technical content looks good, with each formula written clearly and with sufficient deductive steps. Figure 1 provides clear illustration on the comparison with traditional attentions and shows the advantage of the proposed model.
3. Extensive experiments are conducted including 5 machine translation tasks as well as another morphological inflection task. These results make the statement more convincing. The authors also conducted further experiments to analyze the effectiveness, including attention entropy evaluation.

Cons:
1. The rich information contained in the paper is not very well-organized. It takes some time to digest, due to some unclear or missing statements. Specifically, the computation for prior attention should be ordered in a subsection with a section name. The 3 different formulations should be first summarized and started with the same core formula as (4). In this way, it will become more clear of where does eq(6) come from or used for. Currently, this part is confusing.
2. Many substitutions of variables take place without detailed explanation, e.g., y_{<t} with s_t, a with x_{a} in (11) etc. Could you explain before making these substitutions?
3. As mentioned, the PAM actually computes hard attentions. It should be better to make the statement more clear by explicitly explaining eq(11) on how it assembles hard attention computation.

QA:
1. In the equation above (3) that computes prior(a_t), can you explain how P(a_{t-1}|y_{<t}) approximates P(a_{<t}|y_{<t})? What's the assumption?
2. How is eq(5) computed using first order Taylor expansion? How to make Postr inside the probability? And where does x_a' come from?
3. Transferring from P(y) on top of page 3 to eq(11), how do you substitute y_{<t}, a_t with s_t, x_j? Is there a typo for x_j?
4. Can you explain how is the baseline Prior-Joint constructed? Specifically, how to compute prior using soft attention without postr?","The review is positive overall. The reviewer highlights the novelty and originality of the work, praising its clarity, technical soundness, and extensive experiments. While the reviewer points out areas for improvement in organization and clarity, these are presented constructively with specific examples and suggestions for improvement. The language used is formal, objective, and respectful throughout.",75.0,90.0
"The main contributions of this work are essentially on the theoretical aspects. It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent. The authors need to describe in detail the algorithmic novelty of their work.

The definition of “recovering true factor exactly” need to be given. The proposed algorithm involves several tuning parameters, when alternating between two updating rules, an IHT-based update for coefficients and a gradient descent-based update for the dictionary. Therefore, an appropriate choice of their values need to be given.

In the algorithm, the authors need to define the HT function in (3) and (4).

In the experiments, the authors compare the proposed method to only the one proposed by Arora et al. 2015. We think that this is not enough, and more extensive experimental results would provide a better paper. 

There are some typos that can be easily found, such as “of the out algorithm”.","The review starts by stating that the main contributions are theoretical, hinting at a lack of practical significance. It then criticizes the originality of the algorithm and points out several areas needing clarification and improvement. The reviewer suggests the algorithm is not novel, lacks clarity in definitions and parameter choices, and needs more comprehensive experimental comparisons. While the language is critical, it avoids personal attacks and maintains a professional tone, focusing on concrete suggestions for improvement. Therefore, the sentiment is rather negative, but the politeness is relatively neutral.",-50.0,20.0
"==========================
I have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation. 
==========================
Contributions:

The main contribution of this paper is the proposed RelGAN. First, instead of using a standard LSTM as generator, the authors propose using a relational memory based generator. Second, instead of using a single CNN as discriminator, the authors use multiple embedded representations. Third, Gumbel-softmax relaxation is also used for training GANs on discrete textual data. The authors also claim the proposed model has the ability to control the trade-off between sample quality and diversity via a single adjustable parameter. 

Detailed comments:

(1) Novelty: This paper is not a breakthrough paper, mainly following previous work and propose new designs to improve the performance. However, it still contains some novelty inside, for example, the model choice of the generator and discriminator. I think the observation that the temperature control used in the Gumbel-softmax can reflect the trade-off between quality and diversity is interesting. 

However, I feel the claim in the last sentence of the abstract and introduction is a little bit strong. Though this paper seems to be the first to really use Gumbel-softmax for text generation, similar techniques like using annealed softmax to approximate argmax has already been used in previous work (Zhang et al., 2017). Since this is similar to Gumbel-softmax, I think this may need additional one or two sentences to clarify this for more careful discussion.  

Further, I would also recommend the authors discuss the following paper [a] to make this work more comprehensive as to the discussion of related work. [a] also uses annealed softmax approximation, and also divide the GAN approaches as RL-based and RL-free, similar in spirit as the discuss in this paper. 

[a] Adversarial Text Generation via Feature-Mover's Distance, NIPS 2018.

(2) Presentation: This paper is carefully written and easy to follow. I enjoyed reading the paper. 

(3) Evaluation: Experiments are generally well-executed, with ablation study also provided. However, human evaluation is lacked, which I think is essential for this line of work. I have a few questions listed below. 

Questions:

(1) In section 2.4, it mentions that the generator needs pre-training. So, my question is: does the discriminator also need pre-training? If so, how the discriminator is pre-trained?

(2) In Table 1 & 2 & 3, how does your model compare with MaskGAN? If this can be provided, it would be better. 

(3) Instead of using NLL_{gen}, a natural question is: what are the self-BLEU score results since it was used in previous work?

(4) The \beta_max value used in the synthetic and real datasets is quite different. For example, \beta_max = 1 or 2 in synthetic data, while \beta_max = 100 or 1000 is used in real data. What is the observation here? Can the authors provide some insights into this?

(5) I feel Figure 3 is interesting. As the authors noted, NLL_gen measures diversity, NLL_oracle measures quality. Looking at Figure 3, does this mean GAN model produces higher quality samples than MLE pretrained models, while GAN models also produces less diverse samples than MLE models? This is due to NLL_gen increases after pretraining, while NLL_oracle further decreases after pretraining. However, this conclusion also seems strange. Can the authors provide some discussion on this? 

(6) Can human evaluation be performed since automatic metrics are not reliable enough?
","The reviewer explicitly recommends acceptance and expresses general satisfaction with the authors' responses. While they note the work is not ""breakthrough"", they find novelty and value in the specific model designs and observations. The reviewer's detailed questions and suggestions for improvement are constructive rather than harshly critical. ",75.0,90.0
"Pros:
- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. 
- The empirical and theoretical analyses are clear, seem thorough, and make sense.
- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).
Cons:
- The premises of the analyses are not very convincing, limiting the significance of the paper.
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. 
- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper.
- Some parts of the paper feel long-winded and aimless.

[Quality]
See above pros and cons.
A few less important disagreement I have with the paper:
- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.

[Clarity]
In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.
Section 2 background takes too much space.
Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.
Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.
A few editorial issues:
- On page 4 footnote 2, as far as I know the paper did not define BPD.
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.

[Originality]
I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:
    Vít Škvára et al. Are generative deep models for novelty detection truly better? 
    ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.
A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).

[Significance] 
The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.
However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.
","The reviewer highlights both positive and negative aspects of the paper. While they find the findings interesting and the analysis clear, they express significant concerns about the paper's premises, limited generalizability due to relying on a single dataset pair, and the theoretical analysis based on approximations. The reviewer also points out redundancies and a lack of clear direction in some sections. The language used is critical but professional and not disrespectful. Therefore, the sentiment leans towards the negative side due to the significant concerns raised, despite acknowledging the interesting findings. The politeness remains neutral as the critique is presented professionally.",-25.0,0.0
"Summary: the paper introduces a new way of fine-tuning neural networks. Instead of re-training the whole model or fine-tuning the last few layers, the authors propose to fine-tune a small set of model patches that affect the network at different layers. The results show that this way of fine-tuning is superior to above mentioned typical ways either in accuracy or in the number of tuned parameters in three different settings: transfer learning, multi-task learning and domain adaptation.

Quality: the introduced way of fine-tuning is interesting alternative to the typical last layer re-training. I like that the authors present an intuition behind their approach and justify it by an illustrative example. The experiments are fair, assuming the authors explain the choice of hyper-parameters during the revision.

Clarity: in general the paper is well-written. The discussion of multi-task and domain adaptation parts can be improved though.

Originality: the contributions are novel to my best knowledge.

Significance: high, I believe the paper may facilitate a further developments in the area.

I ask the authors to address the following during the rebuttal stage:
* explain the choice of the hyper-parameters of RMSProp (paragraph under Table 1).
* fix Figure 3, it's impossible to read in the paper-printed version
* explain how the average number of parameters per model in computed in Tables 4 and 5. E.g. 700K params/model in the first column of Table 4 is misleading - I suppose the shared parameters are not taken into account. The same holds for 0 in the second column, etc.
* add a proper discussion for domain adaptation part. The simple ""The results are shown in Table 5"" is not enough. 
* consider leaving the discussion of cost-efficient model cascades out. The presented details are too condensed and do not add value to the paper.
* explain how different resolutions are managed by the same model in the domain adaptation experiments.","The reviewer explicitly states positive aspects like ""interesting alternative"", ""fair experiments"", ""the contributions are novel"", and ""high significance"". They also mention that the paper might facilitate further developments, which is a strong positive signal. While the reviewer lists several points for improvement, these are constructive and specific, suggesting a desire to see the paper published in improved form. Overall, the tone is professional and polite.",75.0,80.0
"This paper proposed a new network structure to learn GAN with incomplete data, and it is a nice extension of AmbientGAN. Two theorems are provided for better understanding the potential effect of the missing values. Improved results compared with state-of-the-art methods on MNIST, CIFAR-10 and CelebA are presented. Overall, the paper is well organized, and the experiment results are sufficient to demonstrate the advantages of the proposed method. I particular like figure5 where AmbientGAN failed in this case.

 Several suggestions about improving the paper. I notice the images used in the experiments are small size. It would be interesting to test the performance on a larger image. Another direction would be testing the robustness of the model, for example, what will happen if the observation is also noisy? Some discussion about the potential extensions will also be helpful. For example, can the proposed network be used to solve the compressive sensing problem with a real value mask instead of binary valued. 

I did not dive into the detail of the prove of theorems. And it seems valid by reading through each step.  Although these two theorems are not directly related to the properties of the proposed network structure. But it does provide some nice intuition.
","The review starts with positive statements, highlighting the merits of the paper such as being 'a nice extension', 'well-organized', and having 'sufficient' results. The reviewer also explicitly mentions liking a specific figure. This suggests a positive sentiment. However, the review also provides constructive suggestions for improvement, indicating that it's not overwhelmingly positive. 

The language used is polite and constructive. The reviewer phrases suggestions as possibilities for improvement rather than criticisms, using phrases like 'it would be interesting' and 'another direction would be'. The tone remains respectful throughout.",60.0,80.0
"Review:

This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning.
	
Quality: 

	The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods

Clarity: 

	The paper is well written in general with a few typos, e.g., 

	""The weights of the Bi-LSTM θ, is learned during the search process. The weights θ determines""

Originality: 

	The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks.

	Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening.
	
Significance:

	Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results.

	It also seems that the authors have not repeated the experiments several times since there are no error bars in the results.
	This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels.

	Besides this, the authors may want to cite this paper

	Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).	

	which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time).

Pros:

	- Well written paper.
		
	- Simply idea.

	- Extensive experiments.

Cons:
	
	- The proposed  approach is a combination of well known methods.

	- The significance of the results is in question since the authors do not include error bars in the experiments.","The reviewer acknowledges the paper's quality, clarity, and extensive experiments as positive aspects. However, they also point out a lack of originality and question the significance of the results due to missing error bars and comparisons. The language used is professional and not explicitly negative, but the concerns raised suggest a slightly negative sentiment overall.",-10.0,60.0
"

UPDATE: I've increased my rating based on the authors' thorough responses and the updates they've made to the paper. However, I still have a concern over the static nature of the experimental environments.

=====================

This paper proposes the use of iterative, linguistic corrections to guide (ie, condition and adjust) an RL policy. A major challenge in learning language-guided policies is grounding the language in environment states and agent actions. The authors tackle this challenge with a meta-learning approach.

The approach is fairly complex, blending imitation and supervised learning. It operates on a training set from a distribution of virtual pick-move-place tasks. The policy to be learned operates on this set and collects data, via something close to DAgger, for later supervised learning on the task distribution. The supervised-learning data comprises trajectories augmented with linguistic subgoal annotations, which are referred to as policy ""corrections."" By ingesting its past trajectories and the correction information, the policy is meant to learn to solve the task and to ground the corrections at the same time, end-to-end. Correction annotations are derived from an expert policy.

The idea of guiding a policy through natural language and the requisite grounding of language in environment states and policy actions have been investigated previously: for example, by supervised pretraining on a language corpus, as in the cited work of Andreas et al. (2018). The alternative meta-learning approach proposed here is both well-motivated and original.

Generally, I found the paper clear and easy to read. The authors explain convincingly the utility of guiding policies through language, especially with respect to the standard mechanisms of reward functions (sparse, engineered) and demonstrations (expertise required). The paper is also persuasive on the utility of iterative, interactive correction versus a fully-specified language instruction given a priori. The meta-learning algorithm and training/test setup are both explained well, despite their complexity. On the other hand, most architectural details necessary to reproduce the work are missing, at least from the main text. This includes various tensor dimensions, the structure of the network for perceiving the state, etc.

I like the proposed experimental setting. It enables meta-learning on sequential decision making problems in a partially observable environment, which seems useful to the research community at large. Ultimately, however, this paper's significance is not evident to me, mainly because the proposed method lacks thorough experimental validation. No standard baselines are evaluated on the task (with or without meta-learning), nor is a detailed analysis of the learned policies undertaken. The ablation study is useful, and a good start, but insufficient in my opinion. Unfortunately, the results are merely suggestive rather than convincing.

Some things I'd like to see in an expanded results section before recommending this paper include:
- Comparison to an RL baseline that attempts to learn the full task, without meta-training or language corrections.
- Comparison to a baseline that learns from intermediate rewards. Instead of annotating data with corrections, you could provide +/- scalar rewards throughout each trajectory based on progress towards the goal (since you know the optimal policy). How effective might this be compared to using the corrections?
- Comparison to a baseline that does some kind of pretraining on the language corrections, as in Andreas et al. (2018).
- Quantification of how much meta-training data is required. What is the sample complexity like with/without language corrections?

I also have concerns about the need for near-optimal agents on each task -- this seems very expensive and inefficient. The expert policy is obtained via RL on each individual task using ""ground truth"" rewards. It is not specified what these rewards are, nor is it stated how near to optimal the resulting policy is nor how this nearness affects the overall meta-learning process.

Its unclear to me how the ""full information"" baseline processes and conditions on the full set of subgoals/corrections. Are they read as a single concatenated string converted to one vector by the bi-LSTM?

There also might be an issue with the experimental setup, unless I've misunderstood it. The authors state that ""the agent only needs 2 corrections where the first correction is the location of the goal object and the second is the location of the goal square."" But if the specific rooms, indicated by colors, do not change location from task to task (and they appear not to from all the figures), then the agent can learn the room locations during meta-training and these two ""corrections"" tell it everything it needs to know to solve the task.

Pros:
- Appealing, well-motivated idea for training policies via language.
- Clear, pleasant writing and good communication of a complicated algorithm.
- Good experimental setup that should be useful in other research (except for possible issue with static room locations).

Cons:
- The need for a near-optimal policy for each task. 
- Overall complexity of the training process.
- The so-called corrections are actually linguistic statements of subgoals computed from the optimal policy. There is much talk in the introduction of interactive policy correction by humans, which is an important goal and interesting problem, but the present paper does not actually investigate human interaction. This comes as a letdown after the loftiness of the introduction.
- Various details needed for reproduction are lacking. Maybe they're in the supplementary material; if so, please state that in the main text.
- Major lack of comparisons to alternative approaches.","The reviewer finds the paper's core idea ""appealing and well-motivated"" and praises the writing clarity. They also acknowledge the originality of the meta-learning approach. However, they express significant reservations about the experimental validation, lack of standard baselines, and potential flaws in the setup.  The reviewer's updated rating suggests a more positive stance after the authors' revisions, but concerns remain. The language is generally constructive and polite, suggesting areas for improvement rather than outright criticism.",40.0,70.0
"The paper considers a multiclass classification problem in which labels are grouped in a given number M of subsets c_j, which contain all individual labels as singletons. Training takes place through an active learning setting in which all training examples x_i are initially provided without their ground truth labels y_i. The learner issues queries of the form (x_i,c_j) where c_j is one of the given subsets of labels. The annotator only replies yes/no according to whether the true label y_i of x_i belongs to c_j or not. Hence, for each training example the learner maintains a ""version space"" containing all labels that are consistent with the answers received so far for that example. The active learning process consists of the following steps: (1) use the current learning model to score queries (x_i,c_j); (2) query the best (x_i,c_j); (3) update the model.
In their experiments, the authors use a mini-batched version, where queries are issued and re-ranked several times before updating the model. Assuming the learner generates predictive models which map examples to probability distributions over the class labels, several uncertainty measures can be used to score queries: expected info gain, expected remaining classes, expected decrease in remaining classes. Experiments are run using the Res-18 neural network architecture over CIFAR10, CIFAR100, and Tiny ImageNet, with training sets of 50k, 50k, and 100k examples. The subsets c_j are computed using the Wordnet hierarchy on the label names resulting in 27, 261, and 304 subsets for the three datasets. The experiments show the advantage of performing adaptive queries as opposed to several baselines: random example selection with binary search over labels, active learning over the examples with binary search over the labels, and others. 

This paper develops a natural learning strategy combining two known approaches: active learning and learning with partial labels. The main idea is to exploit adaptation in both choosing examples and queries. The experimental approach is sound and the results are informative. In general, a good experimental paper with a somewhat incremental conceptual contribution.

In (2) there is t+1 on the left-hand side and t on the right-hand side, as if it were an update. Is it a typo?

In 3.1, how is the standard multiclass classifier making use of the partially labeled examples during training?

How are the number of questions required to exactly label all training examples computed? Why does this number vary across the different methods?

What specific partial feedback strategies are used by AQ for labeling examples?

EDC seems to consistently outperform ERC for small annotation budgets. Any intuition why this happens?","The review finds the paper to be technically sound with a solid experimental approach, leading to informative results. However, it also notes that the conceptual contribution is somewhat incremental. The reviewer asks specific questions to improve clarity and understanding, indicating a constructive intent. Overall, the tone is professional and polite.",50.0,70.0
"This paper provides several theoretical and practical insights on the impact of data distribution to adversarial robustness of trained networks. The paper reads well and provides analysis on two datasets MNIST and CIFAR10. I particularly like the result demonstrating that a lossless transformation on the data distribution could significantly impact the robustness of an adversarial trained models. The idea of using smoothness and saturation to bridge the gap between the MNIST and CIFAR10 datasets was also very interesting. One thing that is not clear from the paper is how one could use the findings from this paper and put it into practice. In other words, it would help if the authors could provide some insights on how to improve a model robustness w.r.t the changes in the data distribution. The authors did an attempt toward this in section 5, but that seems to only cover three factors that do not cause the difference in robustness.","The review starts with positive statements, praising the paper's insights, readability, analysis, and interesting ideas. The reviewer explicitly expresses liking specific results. While the reviewer asks for clarification and suggests improvements, they do so constructively and respectfully. The language remains polite throughout, using phrases like ""it would help if..."" and ""The authors did an attempt..."" which are suggestive, not demanding.",75.0,80.0
"
- Summary
This paper proposes a multi-objective evolutionary algorithm for the neural architecture search. Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search. The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods. In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.

- Pros
  - The proposed method does not require to be initialized with well-performing architectures.
  - This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.

- Cons
  - Judging from Table 1, the proposed method does not seem to provide a large contribution. For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.
  - It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.
  - In the case of the search space II, how many GPU days does the proposed method require? 
  - About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.
","The review is rather critical of the paper's contribution and finds the results not overly impressive. While it does highlight some positives, like the lack of need for pre-initialization and the reasonable approach to network size control, the cons section is longer and points out significant shortcomings. The reviewer also asks for clarifications, indicating they found parts of the paper unclear. The language, however, remains polite and professional throughout, focusing on the content and not resorting to personal remarks.",-20.0,80.0
"In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.

The paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  

","The review starts with positive statements - ""interesting and well-written."" However, it expresses uncertainty about the novelty and suggests comparisons with other techniques. Overall, the tone is constructive and leans slightly towards the positive side due to the initial praise.",20.0,80.0
"This paper investigates adversarial examples for audio data. The standard defense techniques proposed for images are studied in the context of audio. It is shown that these techniques are somewhat robust to adversarial attacks, but fail against adaptive attacks. A method exploiting the temporal dependencies of the data is then presented and shown to be robust to adversarial examples and to adaptive attacks.

The paper addresses an important issue, and the two main findings of the paper, the transformation methods used in computer vision are not useful against audio adversarial example and using temporal dependencies improves the defense capability are significant. The proposed TD method is novel.

The first part of the paper is easy to read (Section 1-3), but Section 4 is hard to follow, for the following reasons:
* Section 4.1 presents the metrics used in the evaluation, which is nice. But in the following subsections, other metrics are used: effectiveness ratio, detection rate and relative perturbation. They should be clearly defined in 4.1, and the authors should discuss why they used these metrics.
* Section 4.2 should be reorganized as it is hard to follow: there are three types of attack, so one subsection per attack should make the section clearer.
* It's not always clear what each attack is doing and why it is used. I suggest the authors to have a separate subsection with the description of each attack and the motivation of why it is used.

Because of the above, it's hard to clearly assess the performance of each method for each attack, it would be better to have a Table that summarizes the results for the transformation methods. Also, I don't understand the statement in 4.2: ""We report that the autoencoder works fine for transforming benign instances (57.6% WER in Common Voice compared to 27.5%)"": given that it's not an attack, the PER should be the same with and without transform, as we don't want the transform to affect non-adversarial examples ? Please clarify that.
The experiments on the proposed TD method are clear enough to show the viability of the approach.

Overall, the findings of this paper are significant and it is good step towards audio adversarial examples defense. But the experimental part is hard to follow and does not bring a clear picture. I am still willing to accept the paper if the authors improve and clarify Section 4.

Revision after rebuttal:
The new version is definitely clearer and easier to read, hence I support the paper for acceptance and change my rating to 7. 
There are still minor improvements that can be done in Section 4 to improve the overall clarity:
* About the metrics, the ""Average attack success rate"" and the ""Target command recognition rate"" should be clearly defined, probably under the description of the attack methods.
* The Adaptive attack approach could be introduced unter ""Attack methods"" in 4.1.
* Table 4 is not easy to read, the authors should improve it.
* The first paragraph in Section 4 (""The presentation flows ..."") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of Section 4 or to Section 5.","The reviewer acknowledges the significance of the paper's findings and the novelty of the proposed method. They use phrases like ""important issue"", ""significant findings"", ""novel method"", and ""good step towards..."" indicating a positive sentiment. However, they also point out a lack of clarity in Section 4, making it difficult to assess the work fully. This suggests a somewhat reserved positive sentiment rather than an overtly enthusiastic one. The language remains polite throughout, offering constructive criticism and suggestions for improvement.",60.0,80.0
"This is an exciting paper with a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied. The authors demonstrate the reliability of their method on a large dataset of acoustic instruments and report human evaluation metrics. I expect their proposed method of preprocessing audio to become standard practice.

Why didn't you train a WaveNet on the high-resolution instantaneous frequency representations? In addition to conditioning on the notes, this seems like it would be the right fair comparison. 

I'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?

Small comments:

- Figure 1 & 2: label the x-axis as time. Makes it a lot easier to understand.

- I appreciate the plethora of metrics. The inception score you propose is interesting. Very cool that number of statistically-different bins tracks human eval!

- sentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar. 

- Figure 5 is low-res. Please fix. All the other figures are beautiful - nice work!","The review starts with very positive sentiment, using phrases like ""exciting paper"", ""simple idea"", ""demonstrate the reliability"", and ""expect their proposed method to become standard practice."" This suggests a score close to 100 for sentiment. However, the reviewer also raises critical questions and requests clarifications, indicating that the paper is not perfect. Therefore, a slightly lower score like 85 is more appropriate. 

The language used is polite and professional throughout. The reviewer phrases their suggestions constructively, using ""Why didn't you..."" and ""If you can..."" instead of harsh imperatives. The reviewer also acknowledges the authors' efforts with phrases like ""nice work!"" and ""I appreciate..."". This justifies a politeness score close to 100.",85.0,90.0
"This paper tries to analyze the impact of the staleness on machine learning models in different settings, including model complexity, optimization methods or the number of workers. In this work, they study the convergence behaviors of a wide array of ML models and algorithms under delayed updates, and propose a new convergence analysis of asynchronous SGD method for non-convex optimization.

The following are my concerns:
1. ""For CNNs and DNNs, the staleness slows down deeper models much more than shallower counterparts."" I think it is straightforward. I want to see the theoretical analysis of the relation between model complexity and staleness.  
2. ""Different algorithms respond to staleness very differently"".  This finding is quite interesting. Is there any theoretical analysis of this phenomenon?  
3. The ""gradient coherence""  in the paper is not new. I am certain that ""gradient coherence"" is very similar to the ""sufficient direction"" in [1]. 
4. What is the architecture of the network? in the paper, each worker p can communicate with other workers p'. Does it mean that it is a grid network? or it is just a start network. 
5. in the top of page 3, why the average delay under the model is 1/2s +1, isn't it (s-1)/2? 
6.  on page 5, ""This is perhaps not surprising, given the fact that deeper models pose more optimization challenges even under the sequential settings."" why it is obvious opposite to your experimental results in figure 1(a)? Could you explain why shallower CNN requires more iterations to get the same accuracy? it is a little counter-intuitive.
7. I don't understand what does ""note that s = 0 execution treats each worker’s update as separate updates instead of one large batch in other synchronous systems"" mean in the footnote of page 5.


Above all, this paper empirically analyzes the effect of the staleness on the model and optimization methods. It would be better if there is some theoretical analysis to support these findings.

[1] Training Neural Networks Using Features Replay  https://arxiv.org/pdf/1807.04511.pdf


===after rebuttal===
All my concerns are addressed. I will upgrade the score.
","The reviewer acknowledges the paper's empirical analysis of staleness effects but expresses a desire for more theoretical grounding. The reviewer raises several specific questions and concerns, indicating a critical but engaged stance. The reviewer's willingness to upgrade the score after rebuttal suggests an overall positive disposition towards the paper's potential. The language used is direct and professional, focusing on technical aspects and suggestions for improvement.",50.0,70.0
"This paper proposes a List Conditional Variational Autoencoder approach for the slate recommendation problem. Particularly, it learns the joint document distribution on the slate conditioned on user responses, and directly generates full slates. The experiments show that the proposed method surpasses greedy ranking approaches.

Pros:
+ nice motivation, and the connection with industrial recommendation systems where candidate nomination and ranker is being used is engaging
+ it provides a conditional generative modeling framework for slate recommendation
+ the simulation experiments very clearly show that the expected number of clicks as obtained by the proposed List-CVAE is much higher compared to the chosen baselines. A similar story is shown for the YOOCHOOSE challenge dataset.

Cons: 
- Do the experiments explicitly compare with the nomination & ranking industry standard?
- Comparison with other slate recommendation approaches besides the greedy baselines?
- Comparison with non-slate recommendation models of Figure 1?

Overall, this is a very nicely written paper, and the experiments both in the simulated and real dataset shows the promise of the proposed approach.","The review starts with a neutral summary of the paper's contributions. It then lists both positive (""Pros"") and negative (""Cons"") aspects, which is generally positive as it indicates constructive criticism. The final statement is positive, highlighting the paper's quality and promising results. Overall, the tone is balanced and professional, suggesting minor revisions are needed rather than a complete rejection.",60.0,80.0
"This paper proposed a new training algorithm, G-SGD, by exploring the positively scale-invariant space for relu neural networks. The basic idea is to identify the basis paths in the path graph, and convert the weight update in SGD to the weight rescaling in G-SGD. My major concerns are as follows:

1. Empirical significance of G-SGD: While the idea of exploring the structures of relu neural networks for training based on group theory on graphs is interesting, I do not see significant improvement over SGD. The accuracy differences in Table 1 are marginal, training/testing behaviors in Fig. 3 are very similar, and more importantly there is no evidence to support the claims ""with little extra cost"" in the abstract/Sec. 4.3 in terms of computation. Therefore, I do not see the truly contribution of the proposed method.

PS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve >96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score.

2. In Alg. 3 I do not quite understand how to apply step 3 to step 4. The connection needs more explanation.","The reviewer acknowledges the interesting idea but expresses major concerns about the significance and clarity of the work. They find the improvements marginal and lack evidence for some claims. The language, while direct, maintains a professional and constructive tone, suggesting areas for improvement and further explanation. The updated review shows the reviewer is willing to be convinced and acknowledges the authors' efforts to address the initial concerns, even though some doubts remain.",10.0,60.0
"The paper presents a new convolution-like operation for parameterized manifolds, and demonstrates its effectiveness on learning problems involving spherical signals. The basic idea is to define the MeshConvolution as a linear combination (with learnable coefficients) of differential operators (identity, gradient, and Laplacian). These operators can be efficiently approximated using the 1-hop neighbourhood of a vertex in the mesh.

In general I think this is a strong paper, because it presents a simple and intuitive idea, and shows that it works well on a range of different problems. The paper is well written and mostly easy to follow. The appendix contains a wealth of detail on network architectures and training procedures.

What is not clear to me is how exactly the differential operators are computed, and how the MeshConvolution layer is implemented. The authors write that ""differential operators can be efficiently computed using Finite Element basis, or derived by Discrete Exterior Calculus"", but no references or further detail is provided. The explanation of the derivative computation is:
""The first derivative can be obtained by first computing the per-face gradients, and then using area-weighted average to obtain per-vertex gradients. The dot product between the per-vertex gradient value and the corresponding x and y vector fields are then computed to acquire grad_x F and grad_y F.""
What are per-face gradients and how are they computed? Is the signal sampled on vertices or on faces? What area is used for weighting? What is the exact formula? What vector fields are you referring to? (I presume these are the coordinate vector fields). In eq. 5, what are F_i and F_j? What is the intuition behind the cotangent formula (eq. 5), and where can I read more? etc.

Please provide a lot more detail here, delegating parts to an appendix if necessary. Providing code would be very helpful as well.

A second (minor) concern I have is to do with the coordinate-dependence of the method. Because the MeshConvolution is defined in terms of (lat / lon) coordinates in a non-invariant manner, and the sphere does not admit a global chart, the method will have a singularity at the poles. This is confirmed by the fact that in the MNIST experiment, digits are rotated to the equator ""to prevent coordinate singularity at the poles"". I think that for many applications, this is not a serious problem, but it would still be nice to be transparent and mention this as a limitation of the method when comparing to related work.

In ""Steerable CNNs"", Cohen & Welling also used a linear combination of basis kernels, so this could be mentioned in the related work under ""Reparameterized Convolutional Kernel"".

To get a feel for the differential operators, it may be helpful to show the impulse response (at different positions on the sphere if it matters).

In experiment 4.1 as well as in the introduction, it is claimed that invariant/equivariant models cannot distinguish rotated versions of the same input, such as a 6 and a 9. Although indeed an invariant model cannot, equivariant layers do preserve the ability to discriminate transformed versions of the same input, by e.g. representing a 9 as an upside-down 6. So by replacing the final invariant pooling layer and instead using a fully connected one, it should be possible to deal with this issue in such a network. This should be mentioned in the text, and could be evaluated experimentally.

In my review I have listed several areas for improvement, but as mentioned, overall I think this is a solid paper.","The reviewer explicitly states ""In general I think this is a strong paper"" and praises the clarity, simplicity, and effectiveness of the presented method. This points to a positive sentiment. However, the reviewer also lists several concerns and suggestions for improvement, indicating that the paper is not perfect. Therefore, the sentiment is scored as moderately positive. The language used is constructive, professional, and respectful throughout the review, indicating a polite tone.",60.0,80.0
"This paper introduces a verifier that obtains improvement on both the precision of the incomplete verifiers and the scalability of the complete verifiers. The proposed approaches combines over-parameterization, mixed integer linear programming, and linear programming relaxation. 

This paper is well written and well organized. I like the simple example exposed in section 2, which is a friendly start. However, I begun to lose track after that. As far as I can understand, the next section listed several techniques to be deployed. But I failed to see enough justification or reasoning why these techniques are important. My background is more theoretical, but I'm looking for theorems here, considering the complicatedness of neural network. All I am looking for is probably some high level explanation.

Empirically, the proposed approach is more robust while time consuming that the AI2 algorithm. However, the contribution and the importance of this paper still seems incremental to me.  I probably have grumbled too much about the lack of reasonings. As this paper is purely empirical, which is totally fine and could be valuable and influential as well.  In that case, I found the current experiment unsatisfying and would love to see more extensive experimental results. 
","The reviewer starts with a positive note, praising the paper's clarity and organization. However, they express concern about the lack of theoretical justification for the chosen techniques, desiring more in-depth explanations. While acknowledging the value of empirical work, they find the experiments somewhat lacking and call for more extensive results. Overall, the sentiment leans towards the neutral/slightly positive side due to the initial praise and acknowledgment of potential impact, but the numerous concerns hold it back from being overly positive.",40.0,70.0
"Summary:
This paper presents a Two-Timescale Network (TTN) that enables linear methods to be used to learn values. On the slow timescale non-linear features are learned using a surrogate loss. On the fast timescale, a value function is estimated as a linear function of those features. It appears to be a single network, where one head drives the representation and the second head learns the values.  They investigate multiple surrogate losses and end up using the MSTDE for its simplicity, even though it provides worse value estimates than MSPBE as detailed in their experiments.  They provide convergence results - regular two-timescale stochastic approximation results from Borkar, for the two-timescale procedure and provide empirical evidence for the benefits of this method compared to other non-linear value function approximation methods.

Clarity and Quality:
The paper is well written in general, the mathematics seems to be sound and the experimental results appear to be thorough. 

Originality:
Using two different heads, one to drive the representation and the second to learn the values appears to be an architectural detail. The surrogate loss to learn the features coupled with a linear policy evaluation algorithm appear to be novel, but does not warrant, in my opinion, the novelty necessary for publication at ICLR. 

The theoretical results appear to be a straightforward application of Borkar’s two-timescale stochastic approximation algorithm to this architecture to get convergence. This therefore, does not appear to be a novel contribution.

You state after equaltion (3) that non-linear function classes do not have a closed form solution. However, it seems that the paper Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation does indeed have a closed form solution for non-linear function approximators when minimizing the MSPBE (albeit making a linearity assumption, which is something your work seems to make as well). 

The work done in the control setting appears to be very similar to the experiments performed in the paper: Shallow Updates for Deep Reinforcement Learning.

Significance:
Overall, I think that the paper is well written and the experimental evaluation is thorough. However, the novelty is lacking as it appears to be training using a multi-headed approach (which exists) and the convergence results appear to be a straightforward application of Borkars two-timescale proof. The novelty therefore appears to be using a surrogate loss function for training the features which does not possess the sufficient novelty in my opinion for ICLR. 

I would suggest the authors' detail why their two-timescale approach is different from that of Borkars. Or additionally add some performance guarantee to the convergence results to extend the theory. This would make for a much stronger paper.","The review is structured as politely critical. The reviewer acknowledges the paper's strengths, such as clarity, mathematical soundness, and thorough experiments. However, they find the novelty lacking, suggesting the contributions are incremental.  Phrases like ""appears to be,"" ""in my opinion,"" and ""I would suggest"" maintain a respectful tone despite the criticism. The reviewer offers concrete suggestions for improvement, further indicating a constructive, rather than hostile, attitude.",-20.0,70.0
"This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data.

Pros
1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end.

2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work.

Cons, and suggestions on experiments
My main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details.

1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice.

2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised.

3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning.

4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling?

Other questions
1. What's the impact of keeping the tree constraint on dependencies during backpropagation?  Have you tried removing the tree constraint like previous work?

2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two?

","The review acknowledges the novelty and potential of the proposed method (positive cues) but expresses strong concerns about the experimental setup and results (negative cues). The reviewer suggests improvements and raises critical questions, indicating a desire to see the work strengthened rather than rejected outright. Overall, the tone is constructive and professional, leaning towards the critical side due to concerns about the experiments' validity. ",20.0,70.0
"This paper proposed a new class of meta-algorithm for reinforcement learning and proved the monotone improvement for a local maximum of the expected reward, which could be used in deep RL setting. The framework seems to be quite general but does not include any specific example, like what non-linear dynamical model in detail could be included and will this framework cover the classical MDP setting? In theory, the dynamical model needs to satisfy L-Lipschitz. So which dynamical model in reality could satisfy this assumption? It seems that the focus of this paper is theoretical side. But the only guarantee is the non-decreasing value function of the policy. In RL, people may be more care about the regret or sample complexity. Previous model-based work with simpler model already can have such strong guarantees, such as linear dynamic (Y. Abbasi-Yadkori and Cs. Szepesvari (2011)), MDP (Agrawal and Jia (2017)). What kind of new insights will this framework give when the model reduces to simpler one (linear model)?

In practical implementation, the authors designed a Stochastic Lower Bound Optimization. Is there any convergence rate guarantee for this stochastic optimization? And also neural network is used for deep RL. So there is also no guarantee for the actual algorithm which is used?

Minor:

1. In (3.2), what norm is considered here?
2. In page 4, the authors mentioned their algorithm can be viewed as an extension of the optimism-in-face-of-uncertainty principle to non-linear parameterized setting. This is a little bit confused. How this algorithm can be viewed as OFU principle? How does it recover the result in linear setting (Y. Abbasi-Yadkori and Cs. Szepesvari (2011))?
3. The organization could be more informative. For example, Section 1 has 13 paragraphs but without any subsection.

Y. Abbasi-Yadkori and Cs. Szepesvari, Regret Bounds for the Adaptive Control of Linear Quadratic Systems, COLT, 2011.
Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. NIPS, 2017","The reviewer acknowledges the potential of the proposed framework, describing it as ""quite general."" However, they also raise several concerns and ask for clarifications, indicating a lack of complete satisfaction with the paper. The reviewer points out the absence of specific examples and questions the practical relevance and novelty of the theoretical contributions. The use of phrases like ""a little bit confused"" and the numerous questions suggest a slightly skeptical and demanding tone, but the language remains overall respectful and constructive.",20.0,60.0
"The authors develop a tree structured extension to the recently proposed recurrent switching linear dynamical systems. Like switching linear dynamical systems (sLDS) the proposed models capture non-linear dynamics by switching between a collection of linear regimes. However, unlike SLDS, the transition between the regimes is a function of a latent tree as well as the preceding continuous latent state. Experiments on synthetic data as well as neural spike train data are presented to demonstrate the utility of the model.

The paper is clearly written and easy to read. The tree structured model (TrSLDS) is a sensible extension to rSLDS. While one wouldn’t expect TrSLDS to necessarily fit the data any better than rSLDS, the potential for recovering multi-scale, possibly more interpretable decompositions of the dynamic process is compelling. 

While the authors do provide some evidence of being able to recover such multi-scale structures, overall the experiments are underwhelming and somewhat sloppy. First, to understand whether the sampler is mixing well, it would be nice to include an experiment where the true dynamics and the entire latent structure (including the discrete states) are known, and then to examine how well this ground-truth structure is recovered. Second, for the results presented in section 5, how many iterations was the sampler run for? In the figures, what is being visualized?, the last sample?, the MAP sample? or something else? I am not sure what to make of the real data experiment in section 5.3. Wouldn’t rSLDS produce nearly identical results? What is TrSLDS buying us in this scenario? Do the higher levels of the tree capture interesting low resolution dynamics that are not shown for some reason? 

My other big concern is scalability. To use larger number of discrete states one would need deeper (or wider if the binary requirement is relaxed) trees. How well does the sampler scale with the number of discrete states? How long did the sampler take for the various 4-state results presented in the paper? 

Minor:
a) There is a missing citation in the first para fo Section 5. 
b) Details of message passing claimed to be in the supplement are missing.

============
There are interesting ideas in this paper. However, experimental section could better highlight the benefits afforded by the model and scalability concerns need to be addressed.

","The reviewer finds the paper interesting and well-written, indicating a positive sentiment. However, they express concerns about the experiments and scalability, which lessens the positivity. The language used is professional and polite, but the criticism is fairly direct. Therefore, the politeness score is closer to neutral.",50.0,20.0
"Overview:
I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the ""pass"" category.

Pros: 
- Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric.

Cons:
- The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior).
- None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years.
- A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount.
- While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility.

Recommendations / Typos

I noted a few typos and omissions that need correction.

- Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission.
- The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity.
- Equation (4): the true posterior has an x as its argument instead of the latent z.
- Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r > d relevant here?

* EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. *","The review is largely positive, acknowledging the paper's potential value and interesting mathematical insights. However, it raises significant concerns about the paper's broad claims and lack of engagement with relevant literature on VAEs beyond Gaussian models. The reviewer finds the paper's core salvageable and provides constructive criticism for improvement. The tone is polite and professional, offering specific recommendations and pointing out typos without resorting to harsh language.",50.0,80.0
"The paper proposes a method to check if a given point is a stationary point or not (if not, it provides a descent direction), and then classify stationary points as either local min or second-order stationary. The method works for a specific non-differentiable loss.  In the worst case, there can be exponentially many flat directions to check (2^L), but usually this is no the case.

Overall, I'm impressed. The analysis seems solid, and a lot of clever ideas are used to get around issues (such as exponential number of regions, and non-convex QPs that cannot be solved by the S-procedure or simple tricks). A wide-variety of techniques are used: non-smooth analysis, recent analysis of non-convex QPs, copositive optimization.

The writing is clear and makes most arguments easy to follow.

There are some limitations:

(1) the technical details are hard to follow, and most are in a lengthy appendix, which I did not check

(2) there was no discussion of robustness. If I find a direction eta for which the directional derivative is zero, what do you mean by ""zero""? This is implemented on a computer, so we don't really expect to find a directional derivative that is exactly zero.  I would have liked to see some discussions with epsilons, and give me a guarantee of an epsilon-SOSP or some kind of notion.  In the experiments, this isn't discussed (though another issue is touched on a little bit: you wanted to find real stationary points to test, but you don't have exactly stationary points, but rather can get arbitrarily close).  To make this practical, I think you need a robust theory.

(3) The numerical simulations mainly provided some evidence that there are usually not too many flat directions, but don't convince us that this is a useful technique on a real problem.  The discussion about possible loss functions at the end was a bit opaque.  Furthermore, if you can't find a dataset/loss, then why is this technique useful?

The paper is interesting and novel enough that despite the limitations, I am supportive of publishing it. It introduces new ideas that I find refreshing. The technique many not ever make it into the state-of-the-art algorithms, but I think the paper has intellectual value regardless of practical value.

In short, quality = high, clarity=high, originality=very high, and significance=hard-to-predict","The reviewer explicitly states ""Overall, I'm impressed."" and finds the paper ""interesting and novel"", with ""high quality"" and ""high clarity"". They also praise the ""originality"" as ""very high"". While they list some limitations, they are generally constructive and acknowledge the intellectual value of the work. Therefore, the sentiment is considered significantly positive. The language used throughout the review is polite and professional, with constructive criticism offered in a respectful manner.",85.0,90.0
"## Overview

This paper proposes a new way to stabilize the training process of GAN by regularizing the Discriminator to be robust to adversarial examples. Specifically, this paper proves that a discriminator which is robust to adversarial attacks also leads to a robust minimax objects. Authors provide theoretical analysis about the how the robustness of the Discriminator affects the properties of the objective function, and the proposed regularization term provides an efficient and effective way to regularize the discriminator to be robust. However, it does not build connection between the robustness of the Discriminator and why it can provide meaningful gradient to the Generator. Experimental results demonstrate the effectiveness of the proposed method. This paper is easy to understand.


## Drawbacks
There are some problems in this paper. First, this paper is not highly motivated and lacks of intuition. I can hardly understand why the robustness can stabilize the training of GAN. Will it solve the problem of gradient vanishing problem or speed up the convergence of GAN? The toy example in Sec. 4.2 shows that it can regularize the Discriminator to provide a meaningful gradient to Generator, but no theoretical analysis is provided. The main gap between them is that the smoothness of D around the generated data points does not imply the effectiveness of gradients. Second, the theoretical analysis is inconsistent with the experimental settings. Theorem 4.3 holds true when f is non-positive, but WGAN’s loss function can be positive and this paper does not give any details about this part. Third, in Sec. 4.2, I can hardly distinguish the difference between robust loss, robust discriminator and regularized objectives.

Besides, there are lots of typos in this paper. In Sec 3, Generative Adversarial Networks part, the notations of x and z are quiet confusing. In Definition 3.2, d which measures the distance between network outputs is not appeared above.

## Summarization
Generally, this paper provides a novel way to stabilize the training of GAN. However, it does not illustrate its motivation clearly and no insight is provided.

## After rebuttal
Some of the issues are addressed. So I change my rating to 6.
","The reviewer acknowledges the paper's novelty and effectiveness but expresses concerns regarding motivation, clarity, and theoretical inconsistencies. While the reviewer finds the paper easy to understand, they point out a lack of clear explanation regarding the connection between discriminator robustness and training stability. The presence of typos and inconsistent terminology further detracts from the review's positivity. However, the reviewer's willingness to increase the rating after rebuttal suggests an openness to the authors' clarifications. Overall, the feedback leans towards the positive side due to the acknowledgement of novelty and effectiveness, but with significant reservations.",50.0,50.0
"PAPER SUMMARY
-------------

The paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. 

Using plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. 

The key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency.
Finding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. 

The paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training.


REVIEW SUMMARY
--------------

I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing.
The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below).


DETAILED COMMENTS
-----------------

- It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem.

- The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems.

- On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.","The review starts with positive statements, highlighting the importance and novelty of the paper. The reviewer finds the experiments convincing. However, the review also points out a potentially serious flaw in the methodology, which could impact its effectiveness. While the reviewer offers suggestions for improvement, the concern regarding the flaw is significant enough to suggest a neutral to cautiously optimistic sentiment. The language used throughout is polite and professional, offering constructive criticism and suggestions.",50.0,80.0
"The paper is well written and easy to read. Exploration is one of the fundamental problems in RL, and the idea of using two agents for better exploration is interesting and novel. However, an explanation of the intuition behind the method would be useful. The experimental results show that the method works well in complex tasks. Since states are compared to each other in L2 distance, the method might not generalize to other domains where L2 distance is not a good distance metric.

Pros:
- well written
- a simple and novel idea tackling a hard problem
- good results on hard tasks

Cons: 
- an explanation of why the method should work is missing
- plot text is too small (what is the unit of X-axis?)

Questions:
- what is the intuition behind the method?
- during training, randomly sampled two states are compared. why it is a good idea? how the replay buffer size will affect it?
- since it is a two-player game, is there anything you can say about its Nash equilibrium? 
- why A is better than B at the task?
- when comparing states, are whole raw observations (including velocity etc.) used?
- section 4.2 doesn't seem to be that relevant or helpful. is it really necessary? 
- fig 4 is missing CER alone results? why is that? it doesn't work by itself on those tasks? ","The review starts with positive remarks, highlighting the paper's clarity, novelty, and strong results. While it points out areas for improvement and poses several questions, the tone remains constructive and inquisitive rather than dismissive. The reviewer acknowledges the paper's strengths and seeks further clarification and justification for certain aspects. ",60.0,70.0
"This paper builds upon recent work by Balestriero and Baraniuk (ICML 2018) that concern max-affine spline opertaor (MASO) interpretation of a substantial class of deep networks. In the new paper a special focus is put on Recurrent Neural Networks (RNNs), and it is highlighted based on theoretical considerations leveraging the MASO and numerical experiments that in the case of a piecewise affine and convex activation function, using noise in initial hidden state acts as regularization.  
Overall I was impressed by the volume of contributions presented throughout the paper and also I very muched like the light shed on important classes of models that turn out to be not as black box as they could seem. My enthouasiasm was somehow tempered when discovering that the MASO modelling here was in fact a special case of Balestriero and Baraniuk (ICML 2018), but it seems that despite this the specific contribution is well motivated and justified, especially regarding application results. Yet, the other thing that has annoyed me and is causing me to only moderately champion the paper so far is that I found the notation heavy, not always well introduced nor explained, and while I believe that the authors have a clear understanding of things, it appears to me that the the opening sections 1 and 2 lack notation and/or conceptual clarity, making the paper hard to accept without additional care. To take a few examples:
a) In equation (3), the exponent (\ell) in A and B is not discussed. On a different level, the term ""S"" is used here but doesn't seem to be employed much in next instances of MASOs...why? 
b) In equation (4), sure you can write a max as a sum with an approxiate indicator (modulo unicity I guess) but then what is called Q^{(\ell)} here becomes a function of A^{(\ell)}, B^{(\ell)}, z^{(\ell-1)}...?
c) In proposition 1, the notation A_sigma is not introduced. Of course, there is a notation table later but this would help (to preserve the flow and sometimes clarify things) to introduce notations upon first usage...
d) Still in prop 1, braket notation not so easy to grasp. What is A[z]z? 
e) Still in prop 1, recall that sigma is assumed piecewise-linear and convex? 
f) In th1, abusive to say that the layer ""is"" a mapping, isn't it?  
g) In Theorem 2, what is f? A generic term for a deterministic function? 
Also, below the Theorem, ""affine"" or ""piecewise affine""? 
h) I found section 4 somehow disconnected and flow-breaking. Put in appendix and use space to better explain the rest? 
i) Section 5 is a strong and original bit, it seems. Should be put more to the fore in abstract/intro/conclusion? ","The reviewer expresses initial enthusiasm for the paper's contributions, highlighting the volume of work and the illumination of seemingly ""black box"" models. However, the reviewer also expresses reservations about the clarity of the paper, particularly the notation and explanations in the early sections. The reviewer provides specific examples of areas where the notation is unclear or poorly introduced. Despite these concerns, the reviewer acknowledges the strength and originality of Section 5. Overall, the sentiment leans towards the positive due to the acknowledgment of the paper's contributions, but the concerns about clarity temper the enthusiasm. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement rather than harsh negativity.",60.0,70.0
"In this paper the authors propose a neural model that, given a logical formula as input, predicts whether the formula is a tautology or not. Showing that a formula is a tautology is important because if we can classify a formula A -> B as a tautology then we can say that B is a logical consequence of A. The structure of the formula is a feedforward neural network built in a top-down manner. The leaves of this network are vectors (each of them represents a particular occurrence of an atom) which, after the construction of the formula, are processed by some recurrent neural networks.

The proposed approach seems interesting. However, my main doubt concerns the model. It seems to outperform the state-of-the-art, but the authors do not give any explanations why. There is no theoretical or intuitive explanation of why the model works. Why we need RNNs and not feedforward NNs? I think this is an big issue.
In conclusion, I think that the paper is a bit borderline. The model should be better explained. However, I think that the approach is compelling and, after a minor revision, the paper could be considered for acceptance.

[Minor comments]
Page 4. 
“The dataset contains train (99876 pairs)”, pairs of what?

Page 5. 
What is the measure of the values reported in Table 1? Precision? 
","The reviewer finds the approach interesting and acknowledges the model's performance but expresses concerns about the lack of explanation for its effectiveness. The reviewer's use of phrases like ""big issue"" and ""a bit borderline"" suggests a leaning towards the negative side, but the overall tone is suggestive of potential for improvement and acceptance after revision. The reviewer also provides constructive feedback and asks specific questions, indicating a polite and professional approach.",20.0,60.0
"update: The authors' feedback has addressed some of my concerns. I update my rating to 6.
=================
original:
This paper provides some new insights into classification bias. On top of the well known unbalanced group size, it shows that a large number of weak but asymmetry weak features also leads to bias. This paper also provides a method to reduces bias and remain the prediction accuracy.

In general, the paper is well written, but some description can be clearer. Some notation seems inconsistent. For example, D in equation (1) denotes the joint distribution (x,y), but it also refers to the marginal distribution of x somewhere else. 

In the high level, I am not totally convinced of how significant the result is.  In particular, the bias this paper defines is on the probability (softmax) scale, but logistic regression is on logit scale--   not even aimed at the unbiasedness in the original scale. So the result in section 2 seems to be expected.  Given the fact that unbiasedness is not invariant under transformation, I am wondering why it should be the main target in the first place.  

In the bias reduction methods in equation 5 and 6, both the objective function and the constraint are empirical estimations. Will it be too noisy to adapt to the high dimensional setting? On the other hand, adding some sparsity regularization improves prediction seems well known in practice.

I would also encourage the authors to have extended work both theoretically and experimentally.  The asymmetry feature is only illustrated by a single logistic regression. Is it a problem of weak features, or indeed a problem of logistic regression? What will happen in a more general case beyond mean-field Gaussian?  I would imagine in this simple case the authors may even derive the closed form expression to verify their heuristics.  

Based on the evaluations above, I would recommend a weak reject. 
","The reviewer starts with a positive note, acknowledging the novelty of the paper's insights. However, the tone becomes increasingly critical, expressing doubts about the significance of the results and raising several methodological concerns. The reviewer also suggests substantial revisions and extensions. While the reviewer ultimately recommends a 'weak reject,' the initial positivity and the constructive feedback suggest a sentiment leaning towards the neutral side. The language used is formal, academic, and devoid of any personal attacks, indicating a polite and professional tone.",-10.0,80.0
"This is in a visual active tracking application. The paper proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.

This is a very interesting problem and I see why their contribution could improve the system performance. 

Clarity: the paper is well-written. I also like how the author provides both formulas and a lot of details on implementation of the end-to-end system. 

Originality: Most of the components are pretty standard, however I value the part that seems pretty novel to me - which is the ""partial zero-sum"" idea.

Evaluation: the result obtained from the simulated environment in 2d and 3d are convincing. However, if 1) real-world test and results  2) a stronger baseline can be used, that would be a stronger acceptance. ","The reviewer clearly states the paper is interesting and well-written. They find the core idea novel and the results convincing. However, they desire further improvements, suggesting stronger baselines and real-world testing. This suggests a positive but not overly enthusiastic sentiment, aiming for constructive feedback.",60.0,80.0
"This paper proposes a new few-shot learning method with class dependencies. To consider the structure in the label space, the authors propose to use conditional batch normalization to help change the embedding based on class-wise statistics. Based on which the final classifier can be learned by the gradient-based meta-learning method, i.e., MAML. Experiments on MiniImageNet show the proposed method can achieve high-performance, and the proposed part can be proved to be effective based on the ablation study.

There are three main concerns about this paper, and the final rating depends on the authors' response.
1. The motivation
The authors claim the label structure is helpful in the few-shot learning. If the reviewer understands correctly, it is the change of embedding network based on class statistics that consider such a label structure. From the objective perspective, there are no terms related to this purpose, and the embedding space learning is also based on the same few-shot objective. Will it introduces more information w.r.t. only using embedding space to do the classification?

2. The novelty.
This paper looks like a MAML version of TADAM. Both of the methods use the conditional batch normalization in the embedding network, while CAML uses MAML to learn another classifier based on the embedding. Although CAML uses the CBN at the example level and considers the class information in a transductive setting, it is not very novel. From the results, the proposed method uses a stronger network but does not improve a lot w.r.t. TADAM.

3. Method details
3.1 Since CBN is example induced, will it prone to overfitting?
3.2 About the model architecture. 
CAML uses a 4*4 skip connection from input to output. It is OK to use this improve the final performance, but the authors also need to show the results without the skip connection to fairly compare with other methods. Is this skip connection very important for this particular model? Most methods use 64 channel in the convNet while 30 channels are used in this paper. Is this computational consideration or to avoid overfitting? It is a bit strange that the main network is just four layers but the conditional network is a larger and stronger resNet.
3.3 About the MAML gradients
How to compute the gradient in the MAML flow? Will the embedding network be updated simultaneously? In other words, will the MAML objective influences the embedding network?
3.4 The training details are not clear. 
The concrete training setting is not clear. For example, does the method need model pre-train? What is the learning rate, and how to adapt it? For the MAML, we also need the inner-update learning rate. How many epochs does CAML need?
3.5 How about build MAML directly on the embedding space?","The reviewer outlines several significant concerns, questioning the novelty and motivation of the paper. They find the improvements marginal and the methodology questionable. The numerous pointed questions about the method's details suggest skepticism and a need for substantial clarification. While the language is professional and not overtly negative, the overall tone conveys significant doubt and criticism.",-50.0,50.0
"This paper provides a visualization framework to understand the generative neural network in GAN models. To achieve this, they first find a group of interpretable units and then quantify the causal effect of interpretable units. Finally, the contextual relationship between these units and their surrounding is examined by inserting the discovered object concepts into new images. Extensive experiments are presented and a video is provided.

Overall, I think this paper is very valuable and well-written. The experiments clearly show the questions proposed in the introduction are answered. Two concerns are as follows.

Cons:
1) The visualization seems to be very heuristic. What I want to know is the theoretical interpretation of the visualization. For example, the Class Activation Maps (CAM) can be directly calculated by the output values of softmax function. How about the visual class for the generative neural networks?
2) I am also very curious, how is the rate of finding the correct sets of units for a particular visual class?
","The reviewer explicitly states that they find the paper valuable and well-written, which points towards a positive sentiment. They do mention two concerns, but these are presented as questions/areas for potential improvement rather than harsh criticisms. The language used is constructive and inquisitive, suggesting a polite and professional tone.",75.0,80.0
"Summary
-------
This paper describes a model for musical timbre transfer.
The proposed method uses constant-Q transform magnitudes as the input representation, transfers between domains (timbres) by a CycleGAN-like architecture, and resynthesizes the generated CQT representation by a modified WaveNET-like decoder. The system is evaluated by human (mechanical turk) listening studies, and the results indicate that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.


High-level comments
-------------------

This paper is extremely well written, and the authors clearly have a great attention to detail in both the audio processing and machine learning domains.  Each of the modifications to prior work was well motivated, and the ablation study at the end, while briefly presented, provides a good sense of the contributions of each piece.

I was unable to listen to the examples provided by the link in section 6, which requires a Microsoft OneDrive login to access.  However, the youtube link provided in the ICLR comments gave a reasonable sample of the results of the system.  Overall, the outputs sound compelling, and match my expectations given the reported results of the listening studies.

On the quantitative side, it would have been nice to see a measurement of phase retrieval of the decoder component, which could be done in isolation from the transfer components by feeding in original CQT magnitudes.  This might help give a sense of how well the model can be expected to perform, particular as it breaks down along target timbres.  I would expect some timbres to be easier to model than others, and having a quantitative handle on that could help put the listener study in a bit more perspective.

Detailed comments
-----------------

The paper contains numerous typos and grammatical quirks, e.g.:
    - page 5: ""GP can stable GAN training""
    - page 7: ""CQT is equivalent to pitch""


The reverse-generation trick in section 3.2 was clever!
","The reviewer explicitly states the paper is ""extremely well written"", that the work is well motivated, and the results compelling. This points towards a very positive sentiment. The reviewer does provide constructive feedback and points out minor issues, but these are framed positively and aim to improve the paper. Therefore, the politeness score is also very high.",85.0,90.0
"This paper provides a new dynamic perspective on deep neural network. Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers. Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system. Local performance around the fixed point is explored. Extensions are provided to include the batch normalization. I believe this paper may stimulate some interesting ideas for other researchers.

Two technical questions:

1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point. How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough? This somewhat conflicts the commonsense of ""the deeper the better?"" 

2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.","The reviewer provides a positive summary of the paper's contributions, stating that it ""may stimulate some interesting ideas for other researchers."" This suggests a positive sentiment. The reviewer also raises relevant technical questions, which is typical of a constructive peer review. The language used is formal and respectful, indicating politeness. Overall, the tone is encouraging and helpful rather than dismissive or overly critical.",60.0,80.0
"The focus on novelty (mentioned in both the abstract, and conclusion as a direct claim) in the presentation hurts the paper overall. Without stronger comparison to other closely related work, and lack of citation to several closely related models, the claim of novelty isn't defined well enough to be useful. Describing what parts of this model are novel compared to e.g. Stochastic WaveNet or the conditional dilated convolutional decoder of ""Improved VAE for Text ..."" (linked below, among many others) would help strengthen the novelty claim, if the claim of novelty is needed or useful at all. Stochastic WaveNet in particular seems very closely related to this work, as does PixelVAE. In addition, use of autoregressive models conditioned on (non-variational, in some sense) latents have been shown in both VQ-VAE and ADA among others, so a discussion would help clarify the novelty claim.

Empirical results are strong, though (related to the novelty issue) there should be greater comparison both quantitatively and qualitatively to further work. In particular, many of the papers linked below show better empirical results on the same datasets. Though the results are not always directly comparable, a discussion of *why* would be useful - similar to how Z-forcing was included.

In the qualitative analysis, it would be good to see a more zoomed out view of the text (as in VRNN), since one of the implicit claims of the improvement from dense STCN is improved global coherence by direct connection to the ""global latents"". As it stands now the text samples are a bit too local to really tell. In addition, the VRNN samples look quite a bit different than what the authors present in their work - what implementation was used for the VRNN samples (they don't appear to be clips from the original paper)? 

On the MNIST setting, there are many missing numbers in the table from related references (some included below), and the >= 60.25 number seems so surprising as to be (possibly) incorrect - more in-depth analysis of this particular result is needed. Overall the MNIST result needs more description and relation to other work, for both sequential and non-sequential models.

The writing is well-done overall, and the presented method and diagrams are clear. My primary concern is in relation to related work, clarification of the novelty claim, and more comparison to existing methods in the results tables. 

Variational Bi-LSTM https://arxiv.org/abs/1711.05717

Stochastic WaveNet https://arxiv.org/abs/1806.06116

PixelVAE https://arxiv.org/abs/1611.05013

Filtering Variational Objectives https://github.com/tensorflow/models/tree/master/research/fivo

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions https://arxiv.org/abs/1702.08139

Temporal Sigmoid Belief Networks for Sequential Modeling http://papers.nips.cc/paper/5655-deep-temporal-sigmoid-belief-networks-for-sequence-modeling

Neural Discrete Representation Learning (VQ-VAE) https://arxiv.org/abs/1711.00937

The challenge of realistic music generation: modelling raw audio at scale (ADA) https://arxiv.org/abs/1806.10474

Learning hierarchical features from Generative Models https://arxiv.org/abs/1702.08396

Avoiding Latent Variable Collapse with Generative Skip Models https://arxiv.org/abs/1807.04863

EDIT: Updated score after second revisions and author responses","The review is critical of the paper's novelty claims and suggests significant improvements are needed in relating the work to existing literature. While it acknowledges the empirical results are strong, it calls for more comprehensive comparisons. The reviewer also provides specific suggestions for improvement and lists relevant references. The tone is direct and critical but professional and helpful, suggesting a desire to see the paper improved rather than rejected.",-20.0,70.0
"The paper propose a framework to deal with large state and action
spaces with sparse rewards in reinforcement learning. In particular,
they propose to use a meta-learner to generate experience to the agent
and to decompose the learning task into simpler sub-tasks. The authors
train a DQN with a novel architecture to navigate the Web.
In addition the authors propose to use several strategies: shallow
encoding (SE), reward shaping (AR) and curriculum learning (CI/CG). 
It is shown how the proposed method outperforms state-of-the-art
systems on several tasks.

In the first set of experiments it is clear the improved performance
of QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not
able to learn in the social-media-all problem. The authors tested only
one of the possible variants (AR) of the proposed approach with good
performance. 

It is not clear in the book-flight-form environment, why the
QWeb+SE+AR obtained 100% success while the MetaQWeb, which includes
one of main components in this paper, has a lower performance.

The proposed method uses a large number of components/methods, but it
is not clear the relevance of each of them. The papers reads like, ""I
have a very complex problem to solve so I try all the methods that I
think will be useful"". The paper will benefit from an individual
assessment of the different components.

The authors should include a section of conclusions and future work.
","The review starts with a neutral summary of the paper's contributions. However, it raises several concerns and questions about the clarity, methodology, and results. The reviewer points out inconsistencies in the results and criticizes the lack of individual assessment for the various components used. The tone is critical, questioning the approach and suggesting improvements. While the language is direct and points out flaws, it maintains a professional and constructive tone overall.",-20.0,60.0
"While most works consider embedding as the problem of mapping an input into a point in an embedding space, paper 1341 considers the problem of mapping an input into a distribution in an embedding space. Computing the matching score of two inputs (e.g. two images) involves the following steps: (i) assuming a Gaussian distribution in the embedding space, computing the mean and standard deviation for each input, (ii) drawing a set of samples from each distribution, (3) computing the normalized distances between the samples and (iv) averaging to obtain a global score.

The proposed approach is validated on a new benchmark built on MNIST.

On the positive side:
-	The topic of injecting uncertainty in neural networks should be of broad interest to the ICLR community.
-	The paper is generally clear.
-	The qualitative evaluation provides intuitive results.

On the negative side:
-	The whole idea of drawing samples to compute the distance between two Gaussian distributions seems unnecessarily complicated. Why not computing directly a distance between distributions? There exist kernels between distributions, such as the Probability Product Kernel (PPK). See Jebara, Kondor, Howard “Probability product kernels”, JMLR’04. The PPK between two distributions p(x) and q(x) writes as: \int_x p^a(x) q^a(x) dx, where a is a parameter. When a=1, it is known as the Expected Likelihood Kernel (ELK). When a=1/2, this is known as the Hellinger or Bhattacharyya kernel (BK). In p and q are Gaussian distributions, then the PPK can be computed in closed form. If p and q are mixtures of Gaussians, then the ELK can be computed in closed form. 
-	The Mixture of Gaussians embedding extension is lacking in details. How does the network generate C Gaussian distributions? By having 2C output branches generating C means and C standard deviation vectors? 
-	It might be useful to provide more details about why the self-similarity measure makes sense as an uncertainty measure. In its current state, the paper does not provide much intuition and it took me some time to understand (I actually understood when I made the connection with the ELK). Also, why not using a simpler measure of uncertainty such as the trace of the covariance matrix?
-	The experiments are lacking in some respects:
o	It would be useful to report results without the VIB regularization.
o	The focus on the cases D=2 and D=3 (embedding in a 2D or 3D space) shades some doubt on the practical usefulness of this framework in a higher-dimensional case.

Miscellaneous:
-	It seems there is a typo between equations (4) and (5). It should write z_1^{(k_1)} \sim p(z_1|x_1)

--- 

In their rebuttal, the authors satisfyingly addressed my concerns. Hence, I am upgrading my overall rating.
","The review is mostly positive. The reviewer finds the paper interesting and well-written. However, the reviewer also raises some valid concerns and suggests several improvements. The reviewer's decision to upgrade the rating after the authors addressed their concerns further indicates a positive sentiment. The language used is formal and polite, typical of scientific peer reviews.",60.0,80.0
"==== Summary ====

This paper proposes a model for learning problems that exhibit compositional and recursive structure, called Compositional Recursive Learner (CRL). The paper approaches the subject by first defining a problem as a transformation of an input representation x from a source domain t_x to a target domain t_y. If t_x = t_y then it is called a recursive problem, and otherwise a translational problem. A composite problem is the composition of such transformations. The key observation of the paper is that many real-world problems can be solved iteratively by either recursively transforming an instance of a problem to a simpler instance, or by translating it to a similar problem which we already know how to solve (e.g., translating a sentence from English to French through Spanish). The CRL model is essentially composed of two parts, a set of differential functions and a controller (policy) for selecting functions. At each step i, the controller observes the last intermediate computation x_i and the target domain t_y, and then selects a function and the subset of x_i to operate on. For each instance, the resulting compositional function is trained via back-propagation, and the controller is trained via policy gradient. Finally, the paper presents experiments on two synthetic datasets, translating an arithmetic expression written in one language to its outcome written in another language, and classifying MNIST digits that were distorted by an unknown random sequence of affine transformations. CRL is compared to RNN on the arithmetic task and shown to be able to generalize both to longer sequences and to unseen language pairs when trained on few examples, while RNN can achieve similar performance only using many more examples. On MNIST, it is qualitatively shown that CRL can usually (but not always) find the sequence of transformations to restore the digit to its canonical form.

==== Detailed Review ====

I generally like this article, as it contains a neat solution to a common problem that builds on and extends prior work. Specifically, the proposed CRL model is a natural evolution of previous attempts at solving problems via compositionally, e.g. Neural Programmer [1] that learns a policy for composing predefined commands, and Neural Module Networks [2] that learns the parameters of shared differential modules connected via deterministically defined structure (found via simple parse tree). The paper contains a careful review of the related works and highlights the similarities and differences from prior approaches. Though the experiments are mostly synthetic, the underlying method seems to be readily applicable to many real-world problems.

However, the true contributions of the paper are somewhat muddied by presenting CRL as more general than what is actually supported by the experiments. More specifically, the paper presents CRL as a general method for learning compositional problems by decomposing them into simpler sub-problems that are automatically discovered, but in practice, a far more limited version of CRL is used in the experiments, and the suggested translational capabilities of CRL, which are important for abstract sub-problem discovery, are not properly validated:

1. In both experiments, the building-block functions are hand-crafted to fit to the prior knowledge on the compositionally of the problem. For the arithmetic task, the functions are limited to operate each step just on a single window of encompassing 3 symbols (e.g., <number> <op> <number>,  <op> <number> <op>) and return a distribution over the possible symbols, which heavily forces the functions to represent simple evaluators for simple expressions of the form <number> <op> <number>. For the distorted MNIST task, the functions are limited to neural networks which choose the parameter of predetermined transformations (scaling, translation, or rotation) of the input. In both cases, CRL did not *found* sub-problems for reducing the complexity of the original instance but just had to *fine tune* loosely predefined sub-problems. Incorporating expert knowledge into the model like so is actually an elegant and useful trick for solving real problems, and it should be emphasized far clearly in the article. The story of “discovering subproblems” should be left for the discussion / future research section, because though it might be a small step towards that goal, it is not quite there yet.
2. The experiments very neatly show how recursive transformations offer a nice framework for simplifying an instance of a problem. However, the translation capabilities of the model are barely tested by the presented experiments, and it can be argued that all transformations used by the model are recursive in both experiments. First, only the arithmetic task has a translation aspect to it, i.e., the task is to read an expression in one language and then output the answer in a different language. Second, this problem is only weakly related to translation because it is possible to translate the symbols independently, word by word, as opposed to written language that has complex dependencies between words. Third, the authors report that in practice proper translation was only used in the very last operation for translating the computed value of the input expression to the requested language, and not as a method to translate one instance that we cannot solve into another that we can. Finally, all functions operate and return on all symbols and not ones limited to a specific language, and so by the paper’s own definition, these are all recursive problems and not translational ones.

In conclusion, I believe this paper should be accepted even with the above issues, mostly because the core method is novel, clearly explained, and appears to be very useful in practice. Nevertheless, I strongly suggest to the authors to revise their article to focus on the core qualities of their method that can be backed by their current experiments, and correctly frame the discussion on possible future capabilities as such.

[1] Reed et al. Neural Programmer-Interpreters. ICLR 2016.
[2] Andreas et al. Neural Module Networks. CVPR 2016.","The reviewer clearly states that they ""generally like this article"" and believe it should be accepted. They highlight the novelty and usefulness of the method. However, they also point out a discrepancy between the claims and the actual experiments regarding the generality of the approach. This suggests a positive sentiment, but not overly enthusiastic due to the identified limitations. The language used is formal, analytical, and provides constructive criticism without resorting to harsh or disrespectful language.",60.0,80.0
"This paper presents a targeted empirical evaluation of generalization in models
for visual reasoning. The paper focuses on the specific problem of recognizing
(object, relation, object) triples in synthetic scenes featuring letters and
numbers, and evaluates models' ability to generalize to the full distribution of
such triples after observing a subset that is sparse in the third argument. It
is found that (1) NMNs with full layout supervision generalize better than other
state-of-the art visual reasoning models (FiLM, MAC, RelNet), but (2) without
supervised layouts, NMNs perform little better than chance, and without
supervised question attentions, NMNs perform better than the other models but
fail to achieve perfect generalization.

STRENGTHS
- thorough analysis with a good set of questions

WEAKNESSES
- some peculiar evaluation and presentation decisions
- introduces *yet another* synthetic visual reasoning dataset rather than
  reusing existing ones

I think this paper would have been stronger if it investigated a slightly
broader notion of generalization and had some additional modeling comparisons.
However, I found it interesting and think it successfully addresses the set of
questions it sets out to answer. If it is accepted, there are a few things that
can be done to improve the experiments.

MODELING AND EVALUATION

- Regarding the dataset: the proliferation of synthetic reasoning datasets is
  annoying because it makes it difficult to compare results without downloading
  and re-running a huge amount of code. (The authors have, to their credit, done
  so for this paper.) I think all the experiments here could have been performed
  successfully using either the CLEVR or ShapeWorld rendering engines: while the
  authors note that they require a ""large number of different objects"", this
  could have been handled by treating e.g. ""red circle"" and ""red square"" as
  distinct atomic primitives in questions---the fact that redness is a useful
  feature in both cases is no different from the fact that a horizontal stroke
  detector is useful for lots of letters.

- I don't understand the motivation behind holding out everything on the
  right-hand side. For models that can't tell that the two are symmetric, why
  not introduce sparsity everwhere---hold out some LHSs and relations?
  
- Table 1 test accuracies: arbitrarily reporting ""best of 3"" for some model /
  dataset pairs and ""confidence interval of 5"" for others is extremely
  unhelpful: it would be best to report (mean / max / stderr) for 5. Also, it's
  never stated which convidence interval is reported.

- Table 1 baselines: why not run Conv+LSTM and RelNet with easier #rhs/lhs data?

- How many MAC cells are used? This can have significant performance
  implications. I think if you used their code out of the box you'll wind up
  with way bigger structures than you need for this task.

- I'm not sure how faithful the `find` module used here is to the one in the
  literature, and one of the interesting claims in this work is that module
  implementation details matter! The various Hu papers use an attentional
  parameterization; the use of a ReLU and full convolution in Eq. 14 suggest
  that that one here can pass around more general feature maps. This is fine but
  the distinction should be made explicit, and it would be interesting to see
  additional comparisons to an NMN with purely attentional bottlenecks.

- Why do all the experiments after 4.3 use #rhs/lhs of 18? If it was 8 it would
  be possible to make more direct comparisons to the other baseline models.

- The comparison to MAC in 4.2 is unfair in the following sense: the NMN
  effectively gets supervised textual attentions if the right parameters are
  always plugged into the right models, while the MAC model has to figure out
  attentions from scratch. A different way of structuring things would be to
  give the MAC model supervised parameterizations in 4.2, and then move the
  current MAC experiment to 4.3 (since it's doing something analogous to
  ""parameterization induction"".
  
- The top-right number in Table 4---particularly the fact that it beats MAC and
  sequential NMNs under the same supervision condition---is one of the most
  interesting results in this paper. Most of the work on relaxing supervision
  for NMNs has focused on (1) inducing new question-specific discrete structures
  from scratch (N2NMN) or (2) finding fixed sequential structures that work well
  in general (SNMN and perhaps MAC). The result this paper suggests an
  alternative, which is finding good fixed tree-shaped structures but continuing
  to do soft parameterization like N2NMN.

- The ""sharpness ratio"" is not super easy to interpret---can't you just report
  something standard like entropy? Fig 4 is unnecessary---just report the means.

- One direction that isn't explored here is the use of Johnson- or Hu-style
  offline learning of a model to map from ""sentences"" to ""logical forms"". To the
  extent that NMNs with ground-truth logical forms get 100% accuracy, this turns
  the generalization problem studied here into a purely symbolic one of the kind
  studied in Lake & Baroni 18. Would be interesting to know whether this makes
  things harder (b/c no grounding signal) or easier (b/c seq2seq learning is
  easier.)

PRESENTATION

- Basically all of the tables in this paper are in the wrong place. Move them
  closer to the first metnion---otherwise they're confusing.

- It's conventional in this conference format to put all figure captions below
  the figures they describe. The mix of above and below here makes it hard to
  attach captions to figures.

- Some of the language about how novel the idea of studying generalization in
  these models is a bit strong. The CoGenT split of the CLEVR dataset is aimed
  at answering similar questions. The original Andreas et al CVPR paper (which btw
  appears to have 2 bib entries) also studied generalization to structurally
  novel inputs, and Hu et al. 17 notes that the latent-variable version of this
  model with no supervision is hard to train.

MISCELLANEOUS

- Last sentence before 4.4: ""NMN-Chain"" should be ""NMN-Tree""?

- Recent paper with a better structure-induction technique:
  https://arxiv.org/abs/1808.09942. Worth citing (or comparing if you have
  time!)","The reviewer provides a generally positive assessment of the paper, stating that it is 'interesting' and 'successfully addresses the set of questions it sets out to answer.' They suggest potential improvements and further research directions, indicating a constructive and engaged attitude. While they point out some weaknesses, these are presented as areas for improvement rather than fundamental flaws. The language used is direct and professional, typical of academic peer reviews. There are no personal attacks or disrespectful remarks.",60.0,70.0
"This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.

The article is well written and I find the contribution simple, but interesting. It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).

My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space. I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?

References
Joulin, A., Cissé, M., Grangier, D. and Jégou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310).","The reviewer explicitly states finding the contribution ""interesting"", ""reasonable"", and ""well supported"". They only offer a single, mildly critical ""philosophical"" question, indicating a positive overall sentiment. The language used is constructive and professional throughout, suggesting a high degree of politeness.",75.0,80.0
"The paper proposes neural networks which are convex on inputs data to control problems. These types of networks, constructed based on either MLP or RNN, are shown to have similar representation power as their non-convex versions, thus are potentially able to better capture the dynamics behind complex systems compared with linear models. On the other hand, convexity on inputs brings much convenience to the later optimization part, because there are no worries on global/local minimum or escaping saddle points. In other words, convex but nonlinear provides not only enough search space, but also fast and tractable optimization. The compromise here is the size of memory, since 1) more weights and biases are needed to connect inputs and hidden layers in such nets and 2) we need to store also the negative parts on a portion of weights. 

Even though the idea of convex networks were not new, this work is novel in extending input convex RNN and applying it into dynamic control problems. As the main theoretical contribution, Theorem 2 shows that to have same representation power, input convex nets use polynomial number of activation functions, compared with exponential from using a set of affine functions. Experiments also show such effectiveness. The paper is clearly and nicely written. These are reasons I suggest accept.


Questions and suggestions:

1) For Lemma 1 and Theorem 1, I wonder whether similar results can be established for non-convex functions. Intuitively, it seems that as long as assuming Lipschiz continuous, we can always approximate a function by a maximum of many affine functions, no matter it is convex or not. Is this right or something is missing?

2) In the main paper, all experiments were aimed to address ICNN and ICRNN have good accuracy, but not they are easier to optimize due to convexity. In the abstract, it is mentioned ""... using 5X less time"", but I can only see this through appendix. A suggestion is at least describing some results on the comparison with training time in the main paper.

3) In Appendix A, it seems the NN is not trained very well as shown in the left figure. Is this because the number of parameters of NN is restricted to be the same as in ICNN? Do training on both spend the same resource, ie, number of epoch? Such descriptions are necessary here.

4) In Table 2 in appendix, why the running time of ICNN increases by a magnitude for large H in Ant case?


Typos:
	Page 1 ""simple control algorithms HAS ...""
	Page 7 paragraph ""Baselines"": ""Such (a) method"".
	In the last line of Table 2, 979.73 should be bold instead of 5577.
	There is a ?? in appendix D.4.
	
","The reviewer explicitly recommends acceptance, praises the clarity and novelty of the work, and their questions appear to be aimed at further improving the paper rather than pointing out flaws. This suggests a positive sentiment. The language used is constructive, professional, and polite throughout.",85.0,90.0
"This paper studies the forgetting behavior of the training examples during SGD. Empirically it shows there are forgettable and unforgettable examples, unforgettable examples are like ""support examples"", one can achieve similar performance by training only on these ""support examples"". The paper also shows this phenomenon is consistent across different network architectures.

Pros:
This paper is written in high quality, clearly presented. It is original in the sense that this is the first empirical study on the forgettability of examples in during neural network training.

Comments and Questions on the experiment details:
1. Is the dataset randomly shuffled after every epoch? One concern is that if the order is fixed, some of the examples will be unforgettable simply because the previous batches have similar examples , and training the model on the previous batches makes it good on some examples in the current batch.
2. It would be more interesting to also include datasets like cifar100, which has more labels. The current datasets all have only 10 categories.
3. An addition figure can be provided which switches the order of training in figure 4b. Namely, start with training on b.2.

Cons:
Lack of insight. Subjectively, I usually expect empirical analysis papers to either come up with unexpected observations or provide guidance for practice. In my opinion, the findings of this work is within expectation, and there is a gap for practice.

Overall this paper is worth publishing for the systematic experiments which empirically verifies that there are support examples in neural networks.","The reviewer acknowledges the paper's originality and quality of writing, indicating a positive sentiment. However, they express a lack of groundbreaking insights and practical guidance, pulling the sentiment towards neutral. The reviewer provides constructive criticism and suggestions, maintaining a polite and professional tone. Therefore, the sentiment leans slightly positive, and the politeness is highly positive.",20.0,80.0
"This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.

Some comments below. 

- The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. 

- Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.

- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. 

- I would consider pushing Table 5 (Appendix) into the main text. 
","The review starts with positive statements about the paper's contribution and impact ('timely', 'impactful resource'). While it raises some constructive suggestions and questions, these are framed as areas for improvement rather than flaws. The overall tone is encouraging and supportive. The language used is formal, academic, and respectful throughout.",75.0,80.0
"This paper proposes to combine fitted value iteration with model predictive control (MPC) to speed up the learning process. The value iteration is the ""Learn offline"" subsystem while MPC is the ""Plan online"" subsystem. In addition, this paper also proposes an exploration technique that increases exploration if the multiple value function estimators disagree. The evaluation is complete and shows nice results.

However, I did not rank this paper high for two reasons. First, it is not clear to me how the model is acquired in MPC. Does the method learn the model? Does the method linearize the dynamics and assume a linear model? I am not sure. I suspect that the method just uses the simulator as the model. If it is the case, the method is not so useful because for complexity systems, such as humanoids, we do not know the model. And the comparisons with model-free learning algorithms are not fair because the paper assumes that the model is given. If this is not the case, I suggest that a more detailed description of MPC should be presented in Section 2.3.

Second, the technical contributions are lean. The three main components, 1) fitted value iteration, 2) MPC and 3) exploration based on multiple value function estimates, are not novel. The combination of them seems straight forward. For example, the H-step Bellman update (Section 2.3) is a blend between Monte-Carlo method and Q learning. It seems to be similar to the TD(\lambda) method. Thus, it is not surprising that it can accelerate convergence of value function.

For the above reasons, I would not recommend accepting this paper at this time.","The reviewer starts with a positive sentiment, acknowledging the completeness and good results of the evaluation. However, they express significant concerns about the clarity and novelty of the work, leading to a negative recommendation. The language used, while direct, is professional and typical of academic peer reviews.",-20.0,50.0
"

=Major Comments=
The prior work on grid cells and deep learning makes it clear that the goal of the work is to demonstrate that a simple learning system equipped with representation learning will produce spatial representations that are grid-like. Finding grid-like representations is important because these representations occur in the mammalian brain. 

Your paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature. Namely, the validation of your proposed representations for position and velocity are mostly validated by the fact that they yield grid-like representations, not that they are useful for downstream tasks.

Furthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training?

This is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn’t need to introduce a latent v. Furthermore, this would mprove your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.


Finally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.

=Additional Comments=
What can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? 

I don’t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search.

I would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don’t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?

","The review is critical of the paper's approach and findings. The reviewer points out several shortcomings, such as lack of strong justification for the proposed model, insufficient connection to neuroscience literature, and the existence of potentially better alternatives. The language, while direct and critical, maintains a professional and constructive tone. ",-40.0,60.0
"The paper proposes a method to prevent posterior collapse, which refers to the phenomenon that VAEs with powerful autoregressive decoders tend to ignore the latent code, i.e., the decoder models the data distribution independently of the code. Specifically, the encoder, decoder, and prior distribution families are chosen such that the KL-term in the ELBO is bounded away from 0, meaning that the encoder output cannot perfectly match the prior. Assuming temporal data, the authors employ a 1-step autoregressive (across) prior with an encoder whose codes are independent conditionally on the input. Furthermore, they propose to use a causal decoder together with an anti-causal or non-causal encoder, which translates into a PixelSNAIL/PixelCNN style decoder and an anti-causal version thereof as encoder in the case of image data. The proposed approach is evaluated on CIFAR10, Imagenet 32x32, and the LM1B data set (text).

Pros:

The method obtains state-of-the-art performance in image generation. The paper features extensive ablation experiments and is well-written. Furthermore, it is demonstrated that the code learns an abstract representation by repeatedly sampling form the decoder conditionally on the code.

Cons:

One question that remains is the relative contribution of 1) lower-bounding the KL-term 2) using causal decoder/anti-causal encoder to the overall result. Is the encoder-decoder structure alone enough to prevent posterior collapse? In this context it would also be interesting to see how the encoder-decoder structure performs without \delta-constraint, but with regularization as in \beta-VAE.

What data set are the ablation experiments performed on? As far as I could see this is not specified.

Also, I suggest toning down the claims that the proposed method works ""without altering the ELBO training objective"" in the introduction and conclusion. After all, the encoding and decoding distributions are chosen such that the KL term in the ELBO is lower-bounded by \delta. In other words the authors impose a constraint to the ELBO.

Minor comments:
- Space missing in the first paragraph of p 5: \kappaas
- ""Auxiliary prior""-paragraph on p 5: marginal posterior -> aggregate posterior?","The review is positive overall. The reviewer acknowledges the strengths of the paper, such as achieving state-of-the-art performance, extensive experiments, and clear writing. While the reviewer raises some questions and suggests minor revisions, these are presented constructively and aim to improve the paper. The language used is formal and polite throughout.",60.0,80.0
"The paper introduces a method for online adaptation of a model that is expected to adapt to changes in the environment the model models. The method is based on a mixture model, where new models are spawned using a Chinese restaurant process, and where each newly spawned model starts with weights that have been trained using meta-learning to quickly adapt to new dynamics. The method is demonstrated on model-based RL for a few simple benchmarks.

The proposed method is well justified, clearly presented, and the experimental results are convincing. The paper is generally clear and well written. The method is clearly most useful for situations where the environment suddenly changes, which is relevant in some real-world problems. As a drawback, using a mixture model (that also grows with time) for such modelling can be considered quite heavy in some situations. Nevertheless, the idea of combining a spawning process with meta-learned priors is neat, and clearly works well.

Minor comments:
- Algorithm 1: is the inequality correct, and is T* supposed to be an argmin instead of argmax?","The review is positive because the reviewer states that the method is ""well justified, clearly presented"", the results are ""convincing"", and the paper is ""well written"". They also call the idea ""neat"" and acknowledge its usefulness in certain situations. While they point out a potential drawback, it is presented neutrally and doesn't detract from the overall positive impression. The language used is polite and professional throughout, with constructive criticism offered in the ""Minor comments"" section.",75.0,100.0
"Update:

I thank the authors for the response. Unfortunately, the response does not mention modifications made to the paper according to the comments. According to pdfdiff, modifications to the paper are very minor, and none of my comments are addressed in the paper. I think the paper shows good results, but it could very much benefit from improved presentation and evaluation. I do recommend acceptance, but if the authors put more work in improving the paper, it could have a larger impact.

------

The paper proposes a learnable planning model based on value iteration. The proposed methods can be seen as modifications of Value Iteration Networks (VIN), with some improvements aimed at improving sample efficiency and generalization to large environment sizes. The method is validated on gridworld-type environments, as well as on a more complex StarCraft-based domain with raw pixel input.

Pros:
1) The topic of the paper is interesting: combining the advantages of learning and planning seems like a promising direction to achieving adaptive and generalizable systems.
2) The presentation is quite good, although some details are missing.
3) The proposed method can be effectively trained with reinforcement learning and generalizes well to much larger environments than trained on. It beats vanilla VIN by a large margin. The MVProp variant of the method is especially successful.

Cons:
1) I would like to see a more complete discussion of the MVProp method. Propagation of only positive rewards seems like somewhat of a hack. Is this a general solution or is it only applicable to gridworld navigation-type tasks? Why? If not, is the area of applicability of MVProp different from VProp? Also, is the area of applicability of VProp different from VIN? It’s important to discuss this in detail.
2) I wonder how would the method behave in more realistic gridworld environments, for instance similar in layout to those used in RL navigation literature (DMLab, ViZDoom, MINOS, etc). The presented environments are quite artificial and seem to basically only require “obstacle avoidance”, not so much deliberate long-distance planning.
3) Some details are missing. For instance, I was not able to find the exact network architectures used in different tasks. 
Related to this, I was confused by the phrase “As these new environments are not static, the agent needs to re-plan at every step, forcing us to train on 8x8 maps to reduce the time spent rolling-out the recurrent modules.” I might be misunderstanding something, but is there any recurrent network in VProp? Isn’t it just predicting the parameters once and then rolling our value iteration forward without any learning? Is this so time-consuming?
4) Why does the performance even of the best method not reach 100% even in the simpler environments in Figure 2? Why is the performance plateauing far from 100% in the more difficult case? It would be interesting to see more analysis of how the method works, when it fails, and which parts still need improvement. On a related topic, it would be good to see more qualitative results both in MazeBaze and StarCraft - in the form of images or videos.
5) Novelty is somewhat limited: the method is conceptually similar to VIN. 

To conclude, I think the paper is interesting and the proposed method seems to perform well in the tested environments. I am quite positive about the paper, and I will gladly raise the rating if my questions are addressed satisfactorily.","The reviewer acknowledges the paper's interesting topic and good presentation, highlighting the strong performance of the proposed method. While the reviewer is positive and recommends acceptance, they also express a desire for improvements in presentation and evaluation. The reviewer poses several questions and requests clarifications, indicating a need for further elaboration and analysis. The tone throughout is professional and constructive, suggesting a willingness to increase the rating if the concerns are addressed.",60.0,80.0
"This paper considers how to augment training data by applying class-preserving transformations to selected datapoints.
It proposes improving random datapoint selection by selection policies based on two metrics: the training loss 
associate with each datapoint (""Loss""), and the influence score (from Koh and Liang that approximates leave-one-one test loss). The authors consider two policies based on these metrics: apply transformations to training points in decreasing 
order of their score, or to training points sampled with probability proportional to score. They also consider two 
refinements: downweighting observations that are selected for transformation, and updating scores everytime 
transformations associated with an observation are added. 

The problem the authors tackle is important and their approach is natural and promising. On the downside, the theoretical 
contribution is moderate, and the empirical studies quite limited. 

The stated goals of the paper are quite modest: ""In this work, we demonstrate that it is possible to significantly reduce the 
number of data points included in data augmentation while realizing the same accuracy and invariance benefits of 
augmenting the entire dataset"". It is not too surprising that carefully choosing observations according suitable policies 
is an improvement over random subsampling, especially, when the test data has been ""poisoned"" to highlight this effect. 
The authors have demonstrated that two intuitive policies do indeed work, have quantified this on 3 datasets. 

However they do not address the important question of whether doing so can improve training time/efficiency. In other words, the authors have not attempted to investigate the computational cost of trying to assign importance scores to each observation. Thus this paper does not really demonstrate the overall usefulness of the proposed methodology.

The experimental setup is also limited to (I think) favor the proposed methodology. Features are precomputed on images using a CNN, and the different methods are compared on a logistic regression layer acting on the frozen features. The existence of such a pretrained model is necessary for the proposed methods, otherwise one cannot assign selection scores to different datapoints. However, this is not needed for random selection, where the transformed inputs can directly be input to the system. A not unreasonable baseline would be to train the entire CNN with the augmented 5%,10%, 25% datasets, rather than just the last layer. Of course this now involves training the entire CNN on the augmented dataset, rather than just the last layer, but how relevant is the two stage training approach that the authors propose?

In short, while I think the proposed methodology is promising, the authors missed a chance to include a more thorough analysis of the trade-offs of their method.

I also think the paper makes only a minimal effort to understand the policies, the experiments could have helped shed some more light on this.

Minor point:
The definition of ""influence"" is terse e.g. I do not see the definition of H anywhere (the Hessian of the empirical loss)","The reviewer acknowledges the importance of the problem and finds the proposed approach ""natural and promising."" They use phrases like ""carefully choosing"" and ""intuitive policies do indeed work,"" indicating a positive view. However, they also express reservations, stating the theoretical contribution is ""moderate"" and the empirical studies ""quite limited."" The reviewer criticizes the paper for not addressing training time/efficiency and for limitations in the experimental setup. Overall, the sentiment leans positive due to the acknowledgment of the approach's potential, but the numerous criticisms pull it back from being overwhelmingly positive.",50.0,70.0
"The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) “end-to-end” always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training.  Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. 

The proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering…), I envision this paper to generate important impacts for practitioners pursuing those ideas. 

Additionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. 

I therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify & improve in revision:

1.	Eqn (7) assumes noise-free case. The author stated “The zero-noise assumption is for simplicity of the proofs.” Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less “simpler” way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTA’s performance under noise?

2.	Section 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA?

3.	The writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, “the x-axes denotes is the indices of layers” should remove “is”. Please make sure more proofreading will be done.

","The reviewer explicitly states a ""very positive attitude"" and ""support for acceptance."" They find the work novel, interesting, and well-aligned with their own intuitions. They also anticipate significant impact. This points to a highly positive sentiment. The reviewer provides constructive criticism and suggestions, but in a collegial and respectful manner, indicating politeness. ",90.0,85.0
"The authors propose a bidirectional model for learning a policy. In particular, a backtracking model was proposed to start from a high-value state and sample back the sequence of actions and states that could lead to the current high-value state. These traces can be used later for learning a good policy. The experiments show the effectiveness of the model in terms of increase the expected rewards in different tasks. However, learning the backtracking model would add some computational efforts to the entire learning phase. I would like to see experiments to show the computational time for these components. 
","The review acknowledges the proposed model's effectiveness (""experiments show the effectiveness"") and its potential value but also points out a concern regarding computational cost.  The reviewer's request for additional experiments suggests a desire to see this concern addressed, indicating the sentiment is not fully positive. The language used is objective and professional.",50.0,75.0
"
This paper shows that with a clever initialization method ResNets can be trained without using batch-norm (and other normalization techniques).  The network can still reach state-of-the-art performance.


The authors propose a new initialization method called ""ZeroInit"" and use it to train very deep ResNets (up to 10000 layers). They also show that the test performance of their method matches the performance of state-of-the-art results on many tasks with the help of strong data augmentation. This paper also indicates that the role of normalization in training deep resnets might not be as important as people thought. In sum, this is a very interesting paper that has novel contribution to the practical side of neural networks and new insights on the theoretical side. 

Pros:
1. The analysis is not complicated and the algorithm for ZeroInit is not complicated.  
2. Many people believe normalization (batch-norm, layer-norm, etc. ) not only improves the trainability of deep NNs but also improves their generalization. This paper provides empirical support that NNs can still generalize well without using normalization. It might be the case that the benefits from the data augmentation (i.e., Mixup + Cutout) strictly contain those from normalization. Thus it is interesting to see if the network can still generalize well (achieving >=95% test accuracy on Cifar10)  without using strong data-augmentation like mixup or cutout. 
3.Theoretical analysis of BatchNorm (and other normalization methods) is quite challenging and often very technical. The empirical results of this paper indicate that such analysis, although very interesting, might not be necessary for the theoretical understanding of ResNets.  


Cons:
1.The analysis works for positively homogeneous activation functions i.e. ReLU, but not for tanh or Swish. 
2.The method works for Residual architectures, but may not be applied to Non-Residual networks (i.e. VGG, Inception)  ","The reviewer clearly states that this is a ""very interesting paper that has novel contribution"". They list several positive aspects of the work, highlighting both practical and theoretical contributions. While they mention some limitations, these are presented constructively and do not detract significantly from the overall positive assessment. ",75.0,90.0
"The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction. The idea is quite simple, simply adding a regularization term so that the output images are sensitive to the input variable that controls the variation of the images. (Note that the variable is not the conditional input to the network.) The paper also shows how the regularization term is related to the gradient penalty term. The most exciting feature about the work is that it can be applied to various conditional synthesis frameworks for various tasks. The paper includes several experiments with comparison to the state-of-the-art. The achieved performance is satisfactory. 

To the authors, wondering if the framework is applicable to unconditional GANs.","The review is positive overall. The reviewer finds the idea ""quite simple"" but appreciates its broad applicability and ""exciting"" potential. They acknowledge the strong points: clear explanation, connection to existing work, diverse experiments, and satisfactory results. The question at the end, while suggesting further exploration, doesn't detract from the positive sentiment. Therefore, the sentiment is scored highly positive, and the politeness remains neutral due to the straightforward academic tone.",75.0,0.0
"
-- Originality --

This paper studies how to use KL-regularization with information asymmetry to speed up and improve reinforcement learning (RL). Compared with existing work, the major novelty in the proposed algorithm is that it uses a default policy learned from data, rather than a fixed default policy. Moreover, the proposed algorithm also limits the amount of information the default policy receives, i.e., there is an ""information asymmetry"" between the agent policy and the default policy. In many applications, the default policy is purposely chosen to be ""goal agnostic"" and hence conducts the ""transfer learning"". To the best of my knowledge, this ""informationally asymmetric"" KL-regularization approach is novel.

-- Clarify --

The paper is well written in general and is easy to follow.

-- Significance --

I think the idea of regularizing RL via an informationally asymmetric default policy is interesting. It might be an efficient way to do transfer learning (generalization) in some RL applications. This paper has also done extensive and rigorous experiments. Some experiment results are thought-provoking.

-- Pros and Cons

Pros:

1)  The idea of regularizing RL via an informationally asymmetric default policy is interesting. To the best of my knowledge, this ""informationally asymmetric"" KL-regularization approach is novel.

2) The experiment results are extensive, rigorous, and thought-provoking.

Cons:

1) My understanding is that this ""informationally asymmetric"" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain.

2) This paper does not have any theoretical results. I fully understand that it is highly non-trivial or even impossible to analyze the proposed algorithm in the general case. However, I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.

3) The experiment results of this paper are interesting, but I think the authors can do a better job of intuitively explaining the experiment results. For instance, the experiment results show that when the reward is ""dense shaping"", the proposed method and the baseline perform similarly. Might the authors provide an intuitive explanation for this observation? I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper. 

","The reviewer finds the paper's core idea novel and interesting, supported by extensive experiments. They praise the writing clarity and significance of potential contributions to transfer learning. However, they desire more justification for the chosen algorithmic combination, theoretical analysis (even in simplified settings), and intuitive explanations for experimental observations. The tone is constructive, suggesting improvements without harsh criticism.",60.0,80.0
"In this paper, authors proposed a so-called FLOWQA for conversational question answering (CoQA). Comparing with machine reading comprehension (MRC),  CoQA includes a conversation history. Thus, FLOWQA makes use of this property of CoQA and adds an additional encoder to handle this. It also includes one classifier to handle with no-answerable questions.

Pros:
The idea is pretty straightforward which makes use of the unique property of CoQA.

Results are strong, e.g., +7.2 improvement over current state-of-the-art on the CoQA dataset. 

The paper is well written.

Cons:
It is lack of detailed analysis how the conversation history affects results and what types of questions the proposed model are handled well.

Limited novelty. The model is very similar to FusionNet (Huang et al, 2018) with an extra history encoder and a no-answerable classifier. 

Questions:
One of simple baseline is to treat this as a MRC task by combining the conversation history with documents. Do you have this result?

The model uses the full history. Have you tried partial history? What's the performance? 
","The review is mostly positive. The reviewer acknowledges the strong results, clear writing, and straightforward idea. However, they also point out limitations like lack of detailed analysis and limited novelty, suggesting areas for improvement. The questions posed are constructive, aiming to further explore the model's capabilities.",60.0,70.0
"The authors formulate planning as sampling from an intractable distribution motivated by control-as-inference, propose to approximately sample from the distribution using a learned model of the environment and SMC, then evaluate their approach on 3 Mujoco tasks. They claim that their method compares favorably to model-free SAC and to CEM and random shooting (RS) planning with model-based RL.

This is an interesting idea and an important problem, but there appear to be several inconsistencies in the proposed algorithm and the experimental results do not provide compelling support for the algorithm. In particular,

Levine 2018 explains that with stochastic transitions, computing the posterior leads to overly optimistic behavior because the transition dynamics are not enforced, whereas the variational bound explicitly enforces that. Is that an issue here?

The value function estimated in SAC is V^\pi the value function of the current policy. The value function needed in Sec 3.2 is a different value function. Can the authors clarify on this discrepancy?

The SMC procedure in Alg 1 appears to be incorrect. It multiplies the weights by exp(V_{t+1}) before resampling. This needs to be accounted for by setting the weights to exp(-V_{t+1}) instead of uniform. See for example auxiliary particle filters.

The experimental section could be significantly improved by addressing the following points: 
* How was the planning horizon h chosen? Is the method sensitive to this choice? What is the model accuracy?
* Does CEM use a value function? If not, it seems like a reasonable baseline to consider CEM w/ a value function to summarize the values beyond the planning horizon. This will evaluate whether SMC or including the value function is important. 
* Comparing to state-of-the-art model-based RL (e.g., one of Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018). 
* How were the task # of steps chosen? They seem arbitrary. What is the performance at 1million and 5million steps?
* Was SAC retuned for this small number of samples/steps?
* Clarify where the error bars come from in Fig 5.2 in the caption.
At the moment, SMCP is within the error bars of a baseline method.

Comments:

In the abstract, the authors claim that the major challenges in planning are: 1) model compounding errors in roll-outs and 2) the exponential search space. Their method only attempts to address 2), is that correct? If so, can the authors state that explicitly.

Recent papers (Chua et al. 2018, Kurutach et al. 2018, Buckman et al. 2018, Ha and Schmidhuber 2018) all show promising model-based results on continuous state/action tasks. These should be mentioned in the intro.

The connection between Gu et al.'s work on SMC and SAC was unclear in the intro, can the authors clarify?

For consistency, ensure that sums go to T instead of \infty.

I found the discussion of SAC at the end of Sec 2.1 confusing. As I understand SAC, it does try to approximate the gradient of the variational bound directly. Can the authors clarify what they mean?

At the end of Sec 2.2, the authors claim that the tackle the particle degeneracy issue (a potentially serious issue) by ""selecting the temperature of the resampling distribution to not be too low."" I could not find further discussion of this anywhere in the paper or appendix.

Sec 3.2, mentions an action prior for the first time. Where does this come from?

Sec 3.3 derives updates assuming a perfect model, but we learn a model. What are the implications of this?

Please ensure the line #'s and the algorithm line #'s match.

Model learning is not described in the main text though it is a key component of the algorithm. The appendix lacks details (e.g., what is the distribution used to model the next state?) and contradicts itself (e.g., one place says 3 layers and another says 2 layers).

In Sec 4.1, a major difference between MCTS and SMC is that MCTS runs serially, whereas SMC runs in parallel. This should be noted and then it's unclear whether SMC-Planning should really be thought of as the maximum entropy tree search equivalent of MCTS.

In Sec 4.1, the authors claim that Alpha-Go and SMCP learn proposals in similar ways. However, SMCP minimizes the KL in the reverse direction (from stated in the text). This is an important distinction.

In Sec 4.3, the authors note that Gu et al. learn the proposal with the reverse KL from SMCP. VSMC (Le et al. 2018, Naesseth et al. 2017, Maddison et al. 2017) is the analogous work to Gu et al. that learn the proposal using the same KL direction as SMCP. The authors should consider citing this work as it directly relates to their algorithm.

In Sec 4.3, the authors claim that their direction of minimizing KL is more appropriate for exploration. Gu et al. suggest the opposite in their work. Can the author's justify their claim?

In Sec 5.1, the authors provide an example of SMCP learning a multimodal policy. This is interesting, but can the authors explain when this will be helpful?

====

11/26
At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance.

12/7
After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed.","The review is critical of the paper, pointing out several inconsistencies and weaknesses in the experimental section. While acknowledging the interesting idea, the reviewer doesn't find strong evidence to support the algorithm's effectiveness due to these issues. The numerous specific questions and suggestions for improvement indicate a rather negative sentiment, although the tone remains professional and polite.",-40.0,60.0
"The idea, transforming the input data to an output space in which the data is distributed uniformly and thus indexing is easier, is interesting. 

My main concerns come from experimental results.

(1) Table 1: where are the results of OPQ and LSQ from? run the codes by the authors of this paper? or from the original paper?

It is not consistent to the LSQ paper (https://www.cs.ubc.ca/~julm/papers/eccv16.pdf). For BigANN1M, from the LSQ paper, the result is >29 recall at 1 for 64 bits. 

(2) Figure 5: similarly, how did you get the results of PQ and OPQ?

(3) There are some other advanced algorithms: e.g.,  additive quantization (Babenko & Lempitsky, 2014) and composite quantization (https://arxiv.org/abs/1712.00955)

The above points make it hard to judge this paper.","The reviewer finds the core idea interesting but has major concerns about the experimental results, questioning their validity and comparability. The reviewer also suggests additional algorithms for comparison. The language is professional and not rude, but points out the issues directly. Therefore, the sentiment is rather negative due to the concerns raised, despite the initial positive note.",-30.0,50.0
"The authors propose a novel method for learning graph convolutional networks. The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian. The authors propose two ways to include the Lanczos algorithm. First, as a preprocessing step where the algorithm is applied once on the input graph and the resulting approximation is fixed during learning. Second, by including a differentiable version of the algorithm into an end-to-end trainable model. 

The proposed method is novel and achieves good results on a set of experiments. 

The authors discuss related work in a thorough and meaningful manner. 

There is not much to criticize. This is a very good paper. The almost 10 pages are perhaps a bit excessive considering there was an (informal) 8 page limit. It might make sense to provide a more accessible discussion of the method and Theorem 1, and move some more detailed/technical parts in pages 4, 5, and 6 to an appendix. 
","The reviewer explicitly states that ""This is a very good paper"", which clearly indicates a positive sentiment. The suggestion to shorten the paper is presented as a suggestion rather than a harsh criticism, contributing to the politeness. There's no use of negative or overly critical language.",85.0,90.0
"PAPER SUMMARY
-------------
This paper proposes an approach to video prediction which autonomously finds an action space encoding differences between subsequent frames. This approach can be used for action-conditioned video prediction and visual servoing. 
Unlike related work, the proposed method is initially trained on video sequences without ground-truth actions. A representation for the action at each time step is inferred in an unsupervised manner. This is achieved by imposing that the representation of this action be as small as possible, while also being composable, i.e. that that several actions can be composed to predict several frames ahead.
Once such a representation is found, a bijective mapping to ground truth actions can be found using only few action-annotated samples. Therefore the proposed approach needs much less annotated data than approaches which directly learn a prediction model using actions and images as inputs.

The approach is evaluated on action-conditioned video prediction and visual servoing. The paper shows that the learned action-space is meaningful in the sense that applying the same action in different initial condition indeed changes the scenes in the same manner, as one would intuitively expect. Furthermore, the paper shows that the approach achieves state of the art results on a action-conditioned video prediction dataset and on a visual servoing task.

POSITIVE POINTS
---------------
The idea of inferring the action space from unlabelled videos is very interesting and relevant.

The paper is well written.

The experimental results are very interesting, it is impressive that the proposed approach manages to learn meaningful actions in an unsupervised manner (see e.g. Figure 3).

NEGATIVE POINTS
---------------
It is not exactly clear to me how the model is trained for the quantitative evaluation. On which sequences is the bijective mapping between inferred actions and true actions learned? Is is a subset of the training set? If yes, how many sequences are used? Or is this mapping directly learned on the test set? This, however, would be an unfair comparison in my opinion, since then the actions would be optimized in order to correctly predict on the tested sequences.

The abstract and introduction are too vague and general. It only becomes clear in the technical and experimental section what problem is addressed in this paper.","The review is positive about the paper's novelty, writing, and results ('very interesting', 'well written', 'impressive'). However, it also raises a significant concern about the training procedure, which impacts the evaluation's fairness. The reviewer doesn't use harsh language and maintains a professional tone.",60.0,80.0
"General:
The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE.

I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019.

Pros:
+ The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating.
+ It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors.
+ I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)).
+ The presented results are fully convincing.

Cons:
- It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented.
- The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique?
- The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered?

Neutral remark:
* Another problem, next to the posterior collapse, is the “hole problem” (see Rezende & Viola, “Taming VAEs”, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors’ opinion on this matter.
* Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit.

--REVISION--
I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.","The reviewer explicitly states ""I believe the paper should be presented at the ICLR 2019"", which is a strong positive sentiment. They also use positive language such as ""very interesting and promising"", ""clearly presented"", ""very appealing"", ""fully convincing"", and ""strongly believe the paper should be accepted"". While the reviewer offers constructive criticism, they are not negatively framed and maintain a polite and professional tone. The reviewer also directly thanks the authors for their comments in the revision section.",90.0,100.0
"This paper considers the problem of inferring unspecified costs in an RL problem (e.g., inferring that vases in a room should not be broken). The primary insight is that the initial state of the environment conveys rich information about such unspecified costs since environments are often optimized for humans. The paper frames the problem of inferring unspecified costs from the initial condition as an inverse reinforcement learning (IRL) problem and applies the Maximum Causal Entropy IRL framework to solve this problem. Two methods are proposed for combining the inferred unspecified costs with specified costs. The efficacy of the proposed approach is demonstrated on a number of simulated examples.

Overall, I was impressed by this paper and I believe that it makes a strong contribution. The paper presents an interesting perspective on a relatively old problem (the frame problem in AI). The primary intuition of the paper (that the initial state conveys information about unspecified costs) and the framing of this problem in terms of IRL is novel. The simulated examples (while relatively simple in terms of the number of states and actions) are informative and demonstrate the strengths of the approach (and also some of the weaknesses; the paper is explicit about the current challenges). The paper is very clearly written and is easy to read.

My concerns are relatively minor:
- Perhaps the weakest bit of the paper is Section 5 (combining the specified reward with the inferred reward). As presented, the Additive method is somewhat hard to justify. However, the simulated results suggest that the Additive method performs slightly better than the Bayesian method. I would suggest either presenting a bit more intuition and justification for the Additive method or getting rid of this method altogether (since the results are not too different from the Bayesian method, which seems a bit more justifiable).
- One practical (and potentially important) question that the paper does not directly address is the problem of choosing the time horizon T (i.e., the time horizon for the past). In the standard IRL setting, it is reasonable to assume that the time horizon is given (since the demonstrations have an associated horizon). However, it is not entirely clear how to choose T in the setting considered in this paper. It is possible that if one chooses T to be too small, the inferred rewards will not be accurate (and one may have to look further back in the past to correctly infer rewards). A discussion of this issue and possible ways to choose T would be helpful.
- In Section 6.1 (baselines), the paper mentions that ""while relative reachability makes use of known dynamics, it does not benefit from our handcoded featurization"". Is it possible to modify the relative reachability method to also take advantage of the handcoded features, perhaps by considering dynamics over the feature space? If not, a sentence explaining that this is not straightforward would be helpful.
- In the related work section (and also in the introduction), I would recommend being more explicit about precisely what the differences are between the presented work and the approaches presented in (Krakovna et al. 2018) and (Turner, 2018). The paper is currently slightly vague about the differences.
- Currently, the title of the paper is a bit uninformative. On first reading the title, I expected a paper on control theory; the title makes no mention of unspecified costs, or reinforcement learning, or humans, etc. I believe that this is a good paper and that the paper would have more readers if the title was more inline with the content of the paper. Of course, this is at the discretion of the authors. My suggestion would be something along the lines of ""Inferring Unspecified Rewards in RL from the Initial State"".

Typos:
- Pg. 1, second paragraph, 3rd line: there is a placeholder for citations.
- Periods are missing at the end of equations.
","The reviewer explicitly states a positive sentiment with ""Overall, I was impressed by this paper and I believe that it makes a strong contribution."" They find the paper's core ideas novel and well-executed. While they list several points for improvement, these are framed as constructive suggestions rather than harsh criticisms. The language remains polite throughout, using professional and considerate phrasing.",85.0,90.0
"The paper proposed a defensive mechanism against adversarial attacks using GANs. The general network structure is very much similar to a standard GANs -- generated perturbations are used as adversarial examples, and a discriminator is used to distinguish between them. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task. 

pros
- the presentation of the approach is clean and easy-to-follow.
- the proposed network structure is simple, but it surprisingly works well in general. 
- descriptions of training details are reasonable, and the experimental results across several datasets are extensive

cons
- the network structure may not be novel, though the performance is very nice. 
- there are algorithms that are carefully crafted to perform the network defense mechanism, such as Samangouei et al, 2018. However, the method described in this paper, despite simple, works very good. It would be great if authors can provide more insights on why it works well (though not the best, but still reasonable), besides only demonstrating the experimental results.
- it would also be nice if authors can visualize the behavior of their design by showing some examples using the dataset they are working on, and provide side-to-side comparisons against other approaches.","The reviewer provides both positive and negative points regarding the paper. While acknowledging the effectiveness, simplicity, and clear presentation, the reviewer also points out a lack of novelty and calls for deeper insights beyond experimental results. The tone is constructive and not overly negative. Therefore, the sentiment leans slightly towards positive, and the politeness is very high.",60.0,80.0
"This paper addresses the generalization of adversarial training by proposing a new domain adaptation method. In order to have robust defense for adversarial examples, they combine supervised and unsupervised learning for domain adaptation. The idea of domain adaptation is to increase the similarity between clear and adversarial examples. For this purpose, in their objective, they are minimizing the domain shift by aligning the covariance matrix and mean vector of the clean and adversarial examples.

From experimental viewpoint, they have lower performance than almost all competitors on clean data, but they are beating them when there is white-box as well as the back-box threats. This means their method gives a good generalization. In CIFAR-100 they do not have this trade-off for accuracy and generalization; they are beating other competitors in clean data as well.

The paper is clear and well-written. The introduction and background give useful information. 

In general, I think the paper has a potential for acceptance, but I have to mention that I am not an expert in Adversarial networks area.

 ","The reviewer provides a generally positive overview of the paper, highlighting its clarity and potential. While they note that they aren't an expert in the specific area, their observations on the paper's contribution and the results lean positive. The use of phrases like ""good generalization,"" ""clear and well-written,"" and ""potential for acceptance"" all point towards a favorable view. The reviewer does point out a performance trade-off on one dataset, but this is presented as an observation rather than a severe flaw.",75.0,100.0
"The paper proposes a subgradient descent method to learn orthogonal, squared /complete n x n  dictionaries under l1 norm regularization. The problem is interesting and relevant, and the paper, or at least the first part, is clear.

The most interesting property is that the solution does not depend on the dictionary initialization, unlike many other competing methods. 

The experiments sections in disappointingly short. Could the authors play with real data? How does sparsity affect the results? How does it change with different sample complexities? Also, it would be nice to have a final conclusion section. I think the paper contains interesting material but, overall, it gives the impression that the authors rushed to submit the paper before the deadline!","The reviewer finds the paper interesting and relevant, highlighting the significance of the proposed method's independence from dictionary initialization. However, they express disappointment towards the short experiments section and suggest several improvements. The concluding sentence, mentioning a rushed submission, suggests a somewhat negative sentiment. Overall, the language is polite and constructive.",40.0,70.0
"The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision. They also show the model performs better on average on bAbI than the original DNC.

The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.

I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC. The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself. The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it. Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.","The reviewer acknowledges the paper's contributions (""improvements to the DNC model"", ""clearly written"", ""interesting insights"") and finds value in its implications beyond just the DNC model. However, they also point out a key weakness: the lack of demonstration that the modified DNC can solve tasks the original couldn't.  The tone is critical but constructive, suggesting acceptance with some reservations.",50.0,70.0
"
In this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully unsupervised way. For doing so, they simultaneously learn a goal-conditioned policy and a goal achievement reward function based on the mutual information between goals sampled from an a priori distribution and states achieved using the goal-conditioned policy. These two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space. A key feature is that the resulting metrics in the visual goal space helps the agent focus on what it can control and ignore distractors, which is critical for open-ended learning.

Overall, the idea looks very original and promissing, but the methods are quite difficult to understand under the current form, the messages from the results are not always clear, and the lack of ablative studies makes it difficult to determine which of the mechanisms are crucial in the system performance and which are not.

* Clarification of the methods:

Given the key features outlined above, I believe the work described in this paper has a lot of potential, but the main issue is that the methods are not easy to get, and the authors could do a better job in that respect. Here is a list of remarks meant to help the authors write a clearer presentation of their method:

- the ""problem formulation"" section contains various things. Part of it could be inserted as a subsection in Section 3, and the last paragraph may rather come into the related work section.

- in Section 3, optimization paragraph, the details given after ""As will be discussed""... might rather come in Section 4 were most of all other details are given.

- in Section 4, I would refer to Algorithm 1 only in the end of the section after all the details have been explained: I went first to the algorithm and could not understand many details that are explained only afterwards.

- in Algorithm 1, shouldn't the two procedures be called ""Imitator"" and ""Teacher"", rather than ""actor"" and ""learner"", to be consistent with the end of Section 3?

- there must be a mathematical relationship between $\xsi_\phi$ and $\hat{q}$, but I could not find this relationship anywhere in the text. What is $\xsi_\phi$ is never introduced clearly...

- p4: we treat h as fixed ... => explain why.

- I don't have a strong background about variational methods, and it is unclear to me why using an expanding set of goals corresponding to already seen states recorded in a buffer makes it that maximizing the log likelihood given in (4) is easier than something else.

More generally, the above are local remarks from a reader who did not succeed in getting a clear picture of what is done exactly and why. Anything you can do to give a more didactic account of the methods is welcome.

* Related work:

The related work section is too poor for a strong paper like this one. Learning to reach goals and learning goal representations are two extremely active domains at the moment and the authors should position themselves with respect to more of these works. Here is a short list in which the authors may find many more relevant papers:

 (Machado and Bowling, 2016), (Machado et al., 2017), GoalGANs (Florensa et al., 2018), RIG (Nair et al., 2018), Many-Goals RL (Veeriah et al., 2018), DAYN (Eysenbach et al., 2018), FUN (Vezhnevets et al., 2017), HierQ, HAC (Levy et al., 2018), HIRO (Nachum et al., 2018), IMGEP (Pere et al., 2018), MUGL IMGEP (Laversanne-Finot et al., 2018).

It would also be useful to position yourself with respect to Sermanet et al. : ""Unsupervised Perceptual Rewards for Imitation Learning"".

About state representation learning, if you consider the topic as relevant for your work, you might have a look at the recent survey from Lesort et al. (2018).

External comments on ICLR web site also point to missing references. The authors should definitely consider doing a much more serious job in positioning their work with respect to the relevant literature.

* Experimental study:

The algorithm comes with a lot of mechanisms and small tricks (at the end of Section 3 and in Section 4) whose importance is never assessed by specific experimental studies. This matters all the more than some of the details do not seem to be much principled. It would be nice to have elements to figure out how important they are with ablative studies putting them aside and comparing performance. Among other things, I would be glad to know how well the system performs without its HER component. Is it critical?

The same about the goal sampling strategy, as mentioned in the discussion: how critical is it in the performance of the algorithms?

- Fig. 1b is not so easy to exploit: it is hard to figure out what the reader should actually extract from these figures

- difficult tasks like cartpole: other papers mention cartpole as a rather easy task.

In the begining of Section 4, the authors mention that the mechanisms of DISCERN naturally induce a form of curriculum (which may be debated), but this aspect is not highlighted clearly enough in the experimental study.

In my opinion, studying fewer environments but giving a more detailed analysis of the performance of DISCERN and its variations in these environment would make the paper stronger.



* typos:

p3: the problem (of) learning a goal achievement reward function

In (3), p_g should most probably be p_{goal}

p4: we treated h(.) ... and did not adapt => treat, do not

p9: needn't => need not
","The reviewer finds the paper's idea original and promising, indicating a positive sentiment. However, they express concerns about the clarity of the methods and results, and the lack of ablative studies. The reviewer suggests numerous improvements without resorting to harsh or disrespectful language, indicating politeness. Overall, the tone is constructive and helpful, suggesting a positive attitude towards the paper's potential.",60.0,70.0
"First, the writing can be better. I had a hard time to understand the paper. It has many symbols, but some of them are not explained. For instance, in  formula (9), what are Q or s? Also, formula (14). I probably can guess them. Is it possible to simplify the notations or use a table to list the symbols? 

Finding good models is a bi-level or tri-level optimization problem. The paper describes a gradient-based hyperparameter optimization method, which finds model parameters, hyperparameter schedules, and network structure (limited) the same time. It is a interesting idea. Comparing random search, grid search and Spearmint, it seems to be better them. The paper rules out the performance gain is from the randomness of the hyperparameters, which is a good thought. 

More evidences are needed to show this method is superior. The paper doesn't explain well why it works, and the experimental results are just ok. The network architecture search part is limited to number of filters in the experiments. Certainly, the results is not as good as  PNASNet or NASNet. 

Evolution algorithm or GA shows good performance in hyperparameter optimization or neural architecture search. Why not compare with them? Random and grid search are not good generally, and Bayesian optimization is expensive and its performance depends on implementation.   

In Table 2 and figure 4, should ""Loss"" be ""Error""? 


","The review starts with a negative point about the writing clarity, which impacts readability. However, it calls the idea interesting and acknowledges its superiority to some existing methods. The reviewer suggests the results are ""just okay"" and desires stronger evidence and comparisons. While it points out limitations, the tone is suggestive rather than harshly critical. Overall, it leans slightly positive due to the interest in the idea, despite the need for improvements.",20.0,60.0
"Summary: This paper aims to find important regions to classify an image. The main algorithm, FIDO, is trained to find a saliency map based on SSR or SDR objective functions. The main novelty of this work is that it uses generative models to in-fill masked out regions by SSR or SDR. As such, compared to existing algorithms, FIDO can synthesize more realistic samples to evaluate.

I like the motivation of this paper since existing algorithms have clear limitations, i.e., using out-of-distribution samples. This issue can be addressed by using a generative network as described in this paper.

However, I think this approach yields another limitation: the performance of the algorithm is bound by the generative network. For example, let’s assume that a head region is important to classify birds. Also assume that the proposed algorithm somehow predicts a mask for the head region during training. If the generative network synthesizes a realistic bird from the mask, then the proposed algorithm will learn that the head region is a supporting region of SSR. In the other case, however, the rendered bird is often not realistic and classified incorrectly. Then, the algorithm will seek for other regions. As a result, the proposed method interprets a classifier network conditioned on the generative network parameters. Authors did not discuss these issues importantly in the paper.

Although the approach has its own limitation, I still believe that the overall direction of the paper is reasonable. It is because I agree that using a generative network to in-fill images to address the motivation of this paper is the best option we have at this current moment. In addition, authors report satisfactory amount of experimental results to support their claim.

Quality: The paper is well written and easy to follow.

Clarify: The explanation of the approach and experiments are clear. Since the method is simple, it also seems that it is easy to reproduce their results.

Originality: Authors apply off-the-shelf algorithms to improve the performance of a known problem. Therefore, I think there is no technical originality except that authors found a reasonable combination of existing algorithms and a problem.

Significance: The paper has a good motivation and deals with an important problem. Experimental results show improvements. Overall, the paper has some amount of impact in this field.

Pros and Cons are discussed above. As a summary,
Pros: 
+ Good motivation.
+ Experiments show qualitative and quantitative improvements.

Cons: 
- Lack of technical novelty and justification of the approach.
","The reviewer acknowledges the good motivation and clear explanation of the paper, indicating a positive sentiment. While they point out a limitation, they also agree that the approach is currently the best option. The language used is analytical and objective, suggesting a neutral to polite tone. Overall, the sentiment leans positive due to the acknowledgment of the paper's strengths, even with the mentioned limitation.",60.0,50.0
"After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. 

The paper is well written and is mostly clear. (1st line on page 4 has a typo, \bar{x}_k in eq (4) should be \bar{x}^l?)

Novelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since:

(i). The fact that ""normalized"" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; 
(ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; 
(iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. 

Technical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them?

Experiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, ""Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors."". How can the authors explain this? 

By using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..","The reviewer acknowledges improvements after author revisions and finds the paper well-written. However, they express uncertainty about a core interpretation and argue the novelty is limited, echoing existing work. While they commend the experiments, they find results contradict the authors' claims and suggest further analysis. The tone is critical, questioning assumptions and interpretations, but remains professional and focused on improving the paper.",40.0,60.0
"The manuscript entitled ""Kernel Change-Point Detection with Auxiliary Deep Generative Models"" describes a novel approach to optimising the choice of kernel towards increased testing power in this challenging machine learning problem.  The proposed method is shown to offer improvements over alternatives on a set of real data problems and the minimax objective identified is well motivated, however, I am not entirely convinced that (a) the performance improvements arise for the hypothesised reasons, and (b) that the test setting is of wide applicability.

A fundamental distinction between parametric and non-parametric tests for CPD in timeseries data is that the adoption of parametric assumptions allows for an easier introduction of strict but meaningful relationships in the temporal structure---e.g. a first order autoregressive model introduces a simple Markov structure---whereas non-parametric kernel tests typically imagine samples to be iid (before and after the change-point).  For this reason, the non-parametric tests may lack robustness to certain realistic types of temporal distributional changes: e.g. in the parameter of an autoregressive timeseries.  On the other hand, it may be prohibitively difficult to design parametric models to well characterise high dimensional data, whereas non-parametric models can typically do well in high dimension when the available data volumes are large.  In the present application it seems that the setting imagined is for low dimensional data of limited size in which there is likely to be non-iid temporal structure (i.e., outside the easy relative advantage of non-parametric methods).  For this reason it seems to me the key advantage offered by the proposed approach with its use of a distributional autoregressive process for the surrogate model may well be to introduce robustness against Type 1 errors due to otherwise unrepresented temporal structure in the base distribution (P).  In summarising the performance results by AUC it is unclear whether it is indeed the desired improvement in test power that offers the advantages or whether it is in fact a decrease in Type 1 errors.

Another side of my concern here is that I disagree with the statement: ""As no prior knowledge of Q ... intuitiviely, we have to make G as close to P as possible"" interpretted as a way to maximise test power; as a way to minimise Type 1 errors, yes.

Across change-point detection methods it is also important to distinguish key aspects of the problem formulation.  One particular specification here is that we have already some labelled instances of data known to come from the P distribution, and perhaps also some fewer instances of data labelled from Q.  This is distinct from fully automated change point detection methods for time series such as automatic scene selection in video data.  Another dissimilarity to that archetypal scenario is that here we suppose the P and Q distributions may have subtle differences that we're interested in; and it would also seem that we assume there is only one change-point to detect.  Or at least the algorithm does not seem to be designed to be applied in a recursive sense as it would be for scene selection.

Finally there is no discussion here of computational complexity and cost?","The reviewer raises several concerns and suggests alternative explanations for the observed results, indicating a critical stance. However, they acknowledge the novelty and well-motivated aspects of the work. The language is formal and academic, lacking overtly negative phrasing. Therefore, the sentiment leans slightly negative, but the politeness remains neutral.",-20.0,0.0
"Summary:
The paper proposes a fast method for generating visual metamers – physically different images that cannot be told apart from an original – via foveated, fast, arbitrary style transfer. The method achieves the same goal as an earlier approach (Freeman & Simoncelli 2011): locally texturizing images in pooling regions that increase with eccentricity, but is orders of magnitude faster. The authors perform a psychophysical evaluation to test how (in)discriminable their synthesized images are amongst each other and compared with originals. Their experiment replicates the result of Freeman & Simoncelli of a V2-like critical scaling in the synth vs. synth condition, but shows that V1-like or smaller scaling is necessary for the original vs. synth condition.

I reviewed an earlier version of this paper for a different venue, where I recommended rejection. The authors have since addressed some of my concerns, which is why I am more positive about the paper now.

Strengths:
+ The motivation for the work is clear and the implementation straightforward, combining existing tools from style transfer in a novel way.
+ It's fast. Rendering speed is indeed a bottleneck in existing methods, so a fast method is useful.
+ The perceptual quality of the rendered images is quantified by psychophysical testing.
+ The role of the scaling factor for the pooling regions is investigated and the key result of Freeman & Simoncelli (pooling regions scale with 0.5*eccentricity) is replicated with the new method. In addition, the result of Wallis et al. (2018) that lower scale factors are required for original vs. synth is replicated as well.


Weaknesses:
- Compared with earlier work, an additional fudge parameter (alpha) is introduced. It is not clear why it is necessary and it complicates interpretation.
- The paper contains a number of sections with obscure mathiness and figures that I can't follow and whose significance is unclear.


Conclusion:
The work is well motivated, the method holds up to its promise of being fast and is empirically validated. However, it feels quite ad-hoc and the writing of the paper is very obscure at various places, which leaves room for improvement.


Details:

- The motivation for introducing alpha not clear to me. Wasn't the idea of F&S that you can reduce the image to its summary statistics within a pooling region whose size scales with eccentricity? Why do you need to retain some content information in the first place? How do images with alpha=1 (i.e. keep only texture) look?

- Related to above, why does alpha need to change with eccentricity? Experiment 1 seems to suggest that changing alpha leads to similar SSIM differences between synths and originals as F&S does, but what's the evidence that SSIM is a useful/important metric here?

- Again related to above, why do you not use the same approach of blending pooling regions like F&S did instead of introducing alpha?

- I would like to know some details about the inference of the critical scaling. It seems surprisingly spot on 0.5 as in F&S for synth vs. synth, but looking at the data in Fig. 12 (rightmost panel), I find the value 0.5 highly surprising given that all the blue points lie more or less on a straight line and the point at a scaling factor of 0.5 is clearly above chance level. Similarly, the fit for original vs. synth does not seem to fit the data all that well and a substantially shallower slope seems equally plausible given the data. How reliable are these estimates, what are the confidence intervals, and was a lapse rate included in the fits (see Wichmann & Hill 2001)?

- I don't get the point of Figs. 4, 13 and 14. I think they could as well be removed without the paper losing anything. Similarly, I don't think sections 2.1 and the lengthy discussion (section 5) are useful at all. Moreover, section 3 seems bogus. I don't understand the arguments made here, especially because the obvious options (alpha=1 or overlapping pooling regions; see above) are not even mentioned.

- How is the model trained? Do the authors use the pre-trained model of Huang & Belongie or is the training different in the context of the proposed method? I could only find the statement that the decoder is trained to invert the encoder, but that doesn't seem to be what Huang & Belongie's model does and the paper does not say anything about how it's trained to invert. Please clarify.

- At various places the writing is somewhat sloppy (missing words, commas, broken sentences), which could have been avoided by carefully proof-reading the paper.","The review starts with positive remarks, highlighting improvements from a previous version and listing the strengths of the work. While it points out weaknesses and areas for improvement, the overall tone is constructive and suggestive rather than dismissive. The reviewer acknowledges the value of the work and provides specific suggestions for improvement. The language, while direct, maintains a professional and respectful tone throughout.",60.0,70.0
"The paper makes a nice contribution to solving Circuit-SAT problem from a Neuro-Symbolic approach, particularly, 1) a novel DAG embedding with a forward layer and a reverse layer that captures the structural information of a circuit-sat input. 2) Compared with Selsam et al.'s work of Neuro-SAT, the proposed model in this paper, DG-DAGRNN, directly produces an assignment of variables, and the method is unsupervised and end-to-end differentiable. 3) Empirical experiments on random k-SAT and random graph k-coloring instances that support the authors' claim on better generalization ability.

The paper is lucid and well written, I would support its acceptance at ICLR. Though I have a few comments and questions for the authors to consider.

- In figure 1 (a), what are x11, x12, etc?

- When comparing the two approaches of Neuro-Symbolic methods, besides the angles of optimality and training cost, it is worth to mention that the first one that based on classical algorithms always has a correctness guarantee, while the second one (learning the entire solution from scratch) usually does not.

- Section 4.1, as a pure decision problem, solving SAT means that giving a yes/no answer (i.e., a classification); while for practical purposes, solving SAT means that producing a model (i.e., a witness) of the formula if it is SAT. This can be misleading for some readers when the authors mentioning ""solving SAT"", and it would be clear if the authors could make a distinction when using such terms.

- Section 4.1, ""without requiring to see the actual SAT solutions during training"", again, what is the meaning of ""solutions"" is not very clear at this point. Readers may realize the experiments in the paper only train with satisfiable formulae from the afterward description, so the ""solutions"" indicates the assignments of variables. But it would be better to make it clear.

- Section 4.1/The Evaluator Network, ""one can show also show that min() < S_min() <= S_max() < max()"", what is the ordering relation (i.e., < and <=) here? It is a bit confusing if a forall quantifier for inputs (a_1, ... a_n) is required here.

- Section 4.1/The Evaluator Network, how does the temperature affect the results of R_G? It would be helpful to show their dynamics.

- Section 4.1/Optimization, ""if the input circuit is UNSAT, one can show that the maximum achievable values for S_\theta is 0.5"", it would be better to provide a brief description of how it is guaranteed. Also, this seems to be suggesting the DG-SAGRNN solver has no false positives, i.e., it will never produce a satisfiable result for unsatisfiable formulae? This would be interesting toward some semi-correctness if the answer is yes.

- Section 5.1, are the testing data all satisfiable formulae? If yes, then the figure 2 shows there is a number of satisfiable formulae but both the models cannot produce correct results -- is that a correct understanding of figure 2? If not, then what is the ground truth?

- I would love to see more experiments on SAT instances with a moderate number of variables but from real-world applications. It would be interesting to see how the model utilizes the rich structural information of instances from real applications (instead of randomly generated formulae).

- The training time and testing time(per instance) are not reported in the experiments.
","The review starts with positive statements, highlighting the contributions of the paper and recommending acceptance. However, the reviewer also provides a substantial list of questions and suggestions for improvement. While these are constructive and typical of peer reviews, the volume suggests the paper is not perfect. Therefore, the sentiment is scored as moderately positive. The language used is polite and professional throughout, indicating respect for the authors' work.",60.0,80.0
"

Summary: This work considers the problem of learning in input-driven environments -- which are characterized by an addition stochastic variable z that can affect the dynamics of the environment and the associated reward the agent might see. The authors show how the PG theorem still applied for a input-aware critic and then they show that the best baseline one can use in conjecture with this critic is a input-dependent one. My main concerns are highlighted in points (3) and (4) in the detailed comments below. 

Clarity: Generally it reads good, although I had to go back-and-forth between the main text and appendix several times to understand the experimental side. Even with the supplementary material, examples in Section 3 and Sections 6.2 could be improved in explanation and discussion.

Originality and Significance: Limited in this version, but could be improved significantly by something like point (3)&(4) in detailed comments. Fairly incremental extension of the PG (and TRPO) with the conditioning on the potentially (unobserved) input variables. The fact that a input-aware critic could benefit from a input-aware baseline is not that surprising. The fact that it reduces variance in the PG update is an interesting result; nevertheless I strongly feel the link or comparison needed is with the standard PG update. 

Disclaimer: I have not checked the proofs in the appendix.

Detailed comments:

1) On learning the input-dependent baselines: Generalising over context via a parametric functional approximation, like UVFAs [1] seems like a more natural first choice. Also these provide a zero-shot generalisation, bypassing the need for a burn-in period of the task. Can you comment on why something like that was not used at least as baseline?

2) Motivating example. The exposition of this example lacks a bit of clarity and can use some more details as it is not a standard MDP example, so it’s harder to grasp the complexity of this task or how standard methods would do on it and where would they struggle. I think it’s meant to be an example of high variance but the performance in Figure 2 seems to suggest this is actually something manageable for something like A2C. It is also not clear in this example how the comparison was done. For instance, are the value functions used, input-dependent? Is the policy input aware? 

3) Input-driven MDP. Case 1/Case 2 : As noted by the authors, in case 1 if both s_t and z_t are observed, this somewhat uninteresting as it recovers a particular structured state variable of a normal MDP. I would argue that the more interesting case here, is where only s_t is observed and z_t is hidden, at least in acting. This might still be information available in hindsight and used in training, but won’t be available ‘online’ -- similar to slack variable, or privileged information at training time.  And in this case it’s not clear to me if this would still result in a variance reduction in the policy update. Case 2 has some of that flavour, but restricts z_t to an iid process. Again, I think the more interesting case is not treated or discussed at all and in my opinion, this might add the best value to this work.
  
4) Now, as mentioned above the interesting case, at least in my opinion, is when z is hidden. From the formulae(eq. (4),(5)), it seems to be that the policy is unaware of the input variables. Thus we are training a policy that should be able to deal with a distribution of inputs z. How does this compare with the normal PG update, that would consider a critic averaged over z-s and a z-independent baseline? Is the variance of the proposed update always smaller than that of the standard PG update when learning a policy that is unaware of z? 

References:
[1] Schaul, T., Horgan, D., Gregor, K. and Silver, D., 2015, June. Universal value function approximators. In International Conference on Machine Learning (pp. 1312-1320).

[POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly.","The review is mostly positive, acknowledging the clarity and potential significance of the work. However, it raises valid concerns about the originality and choice of examples, suggesting areas for improvement. The reviewer finds the core idea interesting but believes exploring cases with hidden input variables would significantly enhance the paper's value. The post-rebuttal comment indicates a positive shift in the reviewer's opinion after clarifications from the authors, leading to an increased score. The language used is polite and professional, offering constructive criticism and suggestions rather than harsh judgments.",50.0,80.0
"This paper presents a study of the community detection problem via graph neural networks. The presented results open the possibility that neural networks are able to discover the optimal algorithm for a given task. This is rather convincingly demonstrated on the example of the stochastic block model, where the optimal performance is known (for 2 symmetric groups) or strongly conjectured (for more groups). The method is rather computationally demanding, and also somewhat unrealistic in the aspect that the training examples might not be available, but for a pioneering study of this kind this is well acceptable.

Despite my overall very positive opinion, I found a couple of claims that are misleading and overall hurt the quality of the paper, and I would strongly suggest to the authors to adjust these claims:

** The method is claimed to ""even improve upon current computational thresholds in hard regimes."" This is misleading, because (as correctly stated in the body of the paper) the computational threshold to which the paper refers apply in the limit of large graph sizes whereas the observed improvements are for finite sizes. It is shown here that for finite sizes the present method is better than belief propagation. But this clearly does not imply that it improves the conjectured computational thresholds that are asymptotic. At best this is an interesting hypothesis for future work, not more. 

** The energy landscape is analyzed ""under certain simplifications and assumptions"". Conclusions state ""an interesting transition from rugged to simple as the size of the graphs increase under appropriate concentration conditions."" This is very vague. It would be great if the paper could offer intuitive explanation of there simplifications and assumptions that is between these unclear remarks and the full statement of the theorem and the proof that I did not find simple to understand. For instance state the intuition on in which region of parameters are those results true and in which they are not. 

** ""multilinear fully connected neural networks whose landscape is well understood (Kawaguchi, 2016)."" this is in my opinion grossly overstated. While surely that paper presents interesting results, they are set in a regime that lets a lot to be still understood about landscape of fully connected neural networks. It is restricted to specific activation functions, and the results for non-linear networks rely on unjustified simplifications, the sample complexity trade-off is not considered, etc. 


Misprint: Page 2: cetain -> certain. 
","The review starts very positively, highlighting the potential of the work. While it points out some misleading claims and areas for improvement, it does so constructively and with suggestions for improvement. The reviewer acknowledges the value of the work, especially for a pioneering study. The language used is formal and professional, without resorting to harsh or disrespectful tones.",60.0,80.0
"Summary: The paper proposes a new smoothness constraint in the original cycle-gan formulation. The cycle-gan formulation minimizes reconstruction error on the input, and there is no criterion other than the adversarial loss function to ensure that it produce a good output (this is in sync with the observations from Gokaslan et al. ECCV'18 and Bansal et al. ECCV'18). A smoothness constraint is defined across random patches in input image and corresponding patches in transformed image. This enables the translation network to preserve edge discontinuities and variation in the output, and leads to better outputs for medical imaging, image to labels task, and horse to zebra and vice versa.

Pros: 

1.  Additional smoothness constraints help in improving the performance over multiple tasks. This constraint is intuitive.

2. Impressive human studies for medical imaging.

3. Improvement in the qualitative results for the shown examples in paper and appendix.

Things not clear from the submission: 

1. The paper is lacking in technical details: 

a. what is the patch-size used for RGB-histogram?

b. what features or conv-layers are used to get the features from VGG (19?) net? 

c. other than medical imaging where there isn't a variation in colors of the two domains, it is not clear why RGB-histogram would work?

d. the current formulation can be thought as a variant of perceptual loss from Johnson et al. ECCV'16 (applied for the patches, or including pair of patches). In my opinion, implementing via perceptual loss formulation would have made the formulation cleaner and simpler? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss. One would hope that a perceptual loss would help improve the performance. Also see, Chen and Koltun, ICCV'17.

2. The proposed approach is highly constrained to the settings where structure in input-output does not change. I am not sure how would this approach work if the settings from Gokaslan et al. ECCV'18 were considered (like cats to dogs where the structure changes while going from input to output)? 

3. Does the proposed approach also provide temporal smoothness in the output? E.g. Figure-6 shows an example of man on horse being zebrafied. My guess is that input is a small video sequence, and I am wondering if it provides temporal smoothness in the output? The failure on human body makes me wonder that smoothness constraints are helping learn the edge discontinuities. What if the edges of the input (using an edge detection algorithm such as HED from Xie and Tu, ICCV'15) were concatenated to the input and used in formulation? This would be similar in spirit to the formulation of deep cascaded bi-networks from Zhu et al . ECCV'16.","The review is mostly positive. The reviewer acknowledges the merits of the paper, highlighting the improvements brought by the proposed method. However, they also raise valid concerns and suggest alternative approaches. The language used is constructive and professional, typical of academic peer reviews.",60.0,70.0
"Summary:
The authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and
achieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   

Strengths:
Solving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. 

Weaknesses:
The method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. 

Questions:
- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? 
- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? 
- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?
- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?
- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?","The reviewer provides both positive and negative points about the paper. They clearly state the strengths, such as outperforming state-of-the-art methods and simplicity of the approach. However, they also point out limitations and express  uncertainties, which are presented as questions rather than harsh criticisms. The questions are specific and show a deep understanding of the subject, aiming for clarification and potential improvements. Overall, the tone is constructive and encouraging further development.",60.0,80.0
"This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. 

Specifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label.

They also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton’s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification.

A question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I’m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually.

I would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton’s method. That variant may require more steps (similar to MAML), but I’m curious in practice how this performs.

A few other minor comments:
- In the related work section, the authors write: “On the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.” Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn’t really present this as a trade-off between accuracy and speed.
- I find the term multinomial classification strange. Why not use multi-class classification?
- In page 8, there is a sentence that is not entirely grammatically correct: ‘Interestingly, increasing the capacity of the other method it is not particularly helpful’.

Overall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting.
","The reviewer finds the paper's core idea ""natural and attractive"" and praises the clarity of writing, thoroughness of the related work section, and well-designed experiments. They commend the approach's effectiveness, particularly the ridge regression variant's competitiveness with more complex models. While the reviewer offers constructive criticism and suggestions for improvement, their overall tone is positive and appreciative. Therefore, the sentiment leans strongly towards the positive side, and the language used is polite and constructive.",85.0,90.0
"The paper addresses a challenging problem of predicting the states of entities over the description of a process. The paper is very well written, and easily understandable. The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system. The approach is novel and well motivated. I will suggest a few improvements: 

1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN. Also, NPN can probably be modified to output spans of a sentence. I will be curious to know how it performs.

2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper. 

3. What are the results when using the whole training set of Recipes ?


","The reviewer explicitly states the paper is ""very well written"" and ""easily understandable"", uses positive language such as ""novel and well motivated"", and their suggestions are framed as potential improvements rather than criticisms. All of this points to a positive sentiment. The language used is constructive and polite throughout, suggesting potential improvements and further avenues of investigation.",75.0,80.0
"UPDATE:

I've read the revised version of this paper, I think the concernings have been clarified.

-------

This paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.

Although I think this paper is a decent paper that deserves an acceptance, there are several concernings:

1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become ""vanishingly small"", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).

2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.

3. In the experimental results, what is the difference between one ""query"" and one ""iteration""? It looks like in one iteration, the Algorithm 2 queries twice?","The review starts with a positive statement indicating the reviewer believes the paper deserves acceptance. However, they also point out several 'concernings'. These concerns are presented as points for the authors to improve, rather than strong criticisms. The language used is formal and typical of academic peer reviews, suggesting a neutral rather than overly positive or negative tone.",60.0,70.0
"In this paper, the authors extend the HER framework to deal with dynamical goals, i.e. goals that change over time.
In order to do so, they first need to learn a model of the dynamics of the goal, and then to select in the replay buffer experience reaching the expected value of the goal at the expected time. Empirical results are based on three (or four, see the appendix) experiments with a Mujoco UR10 simulated environment, and one experiment is successfully transfered to a real robot.

Overall, the addressed problem is relevant (the question being how can you efficiently replay experience when the goal is dynamical?), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below).

Despite some merits, the paper mainly suffers from the fact that the implementation of the approach described above is not explained clearly at all.
Among other things, after reading the paper twice, it is still unclear to me:
- how the agent learns of the goal motion (what substrate for such learning, what architecture, how many repetitions of the goal trajectory, how accurate is the learned model...)
- how the output of this model is taken as input to infer the desired values of the goal in the future: shall the agent address the goal at the next time step or later in time, how does it search in practice in its replay buffer, etc.

These unclarities are partly due to unsufficient structuring of the ""methodology"" section of the paper, but also to unsufficient mastery of scientific english. At many points it is not easy to get what the authors mean, and the paper would definitely benefit from the help of an experienced scientific writer.

Note that Figure 1 helps getting the overall idea, but another Figure showing an architecture diagram with the main model variables would help further.

In Figures 3a and 5, we can see that performance decreases. The explanation of the authors just before 4.3.1 seem to imply that there is a fundamental flaw in the algorithm, as this may happen with any other experiment. This is an important weakness of the approach.

To me, Section 4.5 about transfer to a real robot does not bring much, as the authors did nothing specific to favor this transfer. They just tried and it happens that it works, but I would like to see a discussion why it works, or that the authors show me with an ablation study that if they change something in their approach, it does not work any more.

In Section 4.6, the fact that DHER can outperform HER+ is weird: how can a learn model do better that a model given by hand, unless that given model is wrong? This needs further investigation and discussion.

In more details, a few further remarks:

In related work, twice: you should not replace an accurate enumeration of papers with ""and so on"".

p3: In contrary, => By contrast, 

which is the same to => same as

compare the above with the static goals => please rephrase

In Algorithm 1, line 26: this is not the algorithm A that you optimize, this is its critic network.

line 15: you search for a trajectory that matches the desired goal. Do you take the first that matches? Do you take all that match, and select the ""best"" one? If yes, what is the criterion for being the best?

p5: we find such two failed => two such failed

that borrows from the Ej => please rephrase

we assign certain rules to the goals so that they accordingly move => very unclear. What rules? Specified how? Please give a formal description.

For defining the reward, you use s_{t+1} and g_{t+1}, why not s_t and g_t?

p6: the same cell as the food at a certain time step. Which time step? How do you choose?

The caption of Fig. 6 needs to be improved to be contratsed with Fig. 7.

p8: the performance of DQN and DHER is closed => close?

DHER quickly acheive(s)

Because the law...environment. => This is not a sentence.

Mentioning in the appendix a further experiment (dy-sliding) which is not described in the paper is of little use.
","The reviewer acknowledges the relevance and originality of the paper, indicating a positive sentiment. However, they express significant concerns about the clarity and completeness of the methodology description, which impacts their overall impression. The reviewer also points out potential flaws in the approach and questions the significance of some results. The language used is critical but professional and maintains a level of politeness. Therefore, the sentiment is moderately positive, hampered by clarity issues, and the politeness remains neutral.",40.0,0.0
"Optimization algorithms such as stochastic gradient descent (SGD) and stochastic mirror descent (SMD) have found wide applications in training deep neural networks. In this paper the authors provide some theoretical studies to understand why SGD/SMD can produce a solution with good generalization performance when applied to high-parameterized models. The authors developed a fundamental identity for SGD with least squares loss function, based on which the minimax optimality of SGD is established, meaning that SGD chooses the best estimator that safeguards against the worst-case disturbance. Implicit regularization of SGD is also established in the interpolating case, meaning that SGD iterates converge to the one with minimal distance to the starting point in the set of models with no errors. Results are then extended to SMD with general loss functions.

Comments:

(1) Several results are extended from existing literature. For example, Lemma 1 and Theorem 3 have analogues in (Hassibi et al. 1996). Proposition 8 is recently derived in (Gunasekar et al., 2018). Therefore, it seems that this paper has some incremental nature. I am not sure whether the contribution is sufficient enough.

(2) The authors say that they show the convergence of SMD in Proposition 9, while (Gunasekar et al., 2018) does not. It seems that the convergence may not be surprising since the interpolating case is considered there.

(3) Implicit regularization is only studied in the over-parameterized case. Is it possible to say something in the general setting with noises?

(4) The discussion on the implicit regularization for over-parameterized case is a bit intuitive and based on strong assumptions, e.g., the first iterate is close to the solution set. It would be more interesting to present a more rigorous analysis with relaxed assumptions.","The review acknowledges the technical soundness of the paper by stating ""The authors provide some theoretical studies..."" and does not outright reject the paper. However, it raises concerns about the significance and novelty of the work.  The reviewer points out that some results are extensions of existing work, which makes the contribution seem incremental. The use of ""seems"" and ""I am not sure"" further suggests a lack of strong positive sentiment. The tone is professional and polite throughout, without resorting to harsh or disrespectful language. Therefore, the sentiment is slightly above neutral, reflecting that the reviewer sees potential in the work but has reservations. The politeness is neutral as the reviewer maintains a professional and respectful tone.",20.0,0.0
"The reviewer feels that the paper is hard to follow. The abstract is confusing enough and raises a number of questions.  The paper talks about `""local maxima"" without defining an optimization problem. What is the optimization problem are we talking about? Is it a maximization problem or minimization problem? If we are dealing with a minimization problem, why do we care about maxima?

The first several paragraphs did not make the problem of interest clearer. But at least the fourth paragraph starts talking about training networks (the reviewer guesses this ""network"" refers to neural network, not other types network (e.g., Bayesian network) arising in machine learning). This paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem's local maxima? In addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). It is even not clear if a stationary point can be achieved. So if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example.

The next paragraph brings out a notion of value function, which is hard to follow what it is. A suggestion is to give a much more concrete example to enlighten the readers.

The next two paragraphs seem to be very disconnected. It is not properly defined what is x and how to obtain it. If they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?

Since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the ""related work"" section are really related.

The motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.

Overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. This makes evaluating this work very hard.


========= after author feedback =======
After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications.  The clarity can still be improved, but this might be more of a style matter.  The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ````   accept.","The review is initially highly critical, pointing out a lack of clarity and definition in the paper's core concepts. The reviewer struggles to understand the problem being addressed and finds the examples and related work disconnected as a result. The language used, while direct, is professional and focused on the paper's shortcomings rather than being personally critical of the authors.  However, the post-feedback section shows a significant shift in sentiment. The reviewer acknowledges the authors' efforts to address the concerns, finding the paper now interesting and potentially useful. While some stylistic improvements are still suggested, the overall tone becomes positive, leading to an acceptance recommendation. ",-50.0,50.0
"The paper introduces Neural Logic Machines, a particular way to combine neural networks and first order but finite logic. 

The paper is very well written and structured. However, there are also some downsides.

First of all, Section 2.1 is rather simple from a logical perspective and hence it is not clear what this gets a special term. Moreover, why do mix Boolean logic (propostional logic) and first order logic? Any how to you deal with the free variables, i.e., the variables that are not bounded by a quantifier? The semantics you define later actually assumes that all free variables (in your notation) are bounded by all quantifiers since you apply the same rule to all ground instances. Given that you argue that you want a neural extension of symbolic logic (""NLM is a neural realization of (symbolic) logic machines"") this has to be clarified as it would not be an extension otherwise. 

Furthermore, Section 2.2 argues that we can use a MLP with a sigmoid output to encode any joint distribution. This should be proven. It particular, given that the input to the network are the marginals of the ground atoms. So this is more like a conditional distribution? Moreover, it is not clear how this is different to other approaches that encode the weight of weighted logical rule (e.g. in a MLN) using neural networks, see
e.g. 

Marco Lippi, Paolo Frasconi:
Prediction of protein beta-residue contacts by Markov logic networks with grounding-specific weights. 
Bioinformatics 25(18): 2326-2333 (2009)

Now of course, and this is the nice part of the present paper, by stacking several of the rules, we could directly specify that we may need a certain number of latent predicates. 
This is nice but it is not argued that this is highly novel. Consider again the work by Lippi and Frasconi. We unroll a given NN-parameterized MLN for s fixed number of forward chaining steps. This gives us essentially a computational graph that could also be made differentiable and hence we could also have end2end training. The major difference seems to be that now objects are directly attached with vector encodings, which are not present in Lippi and Frasconi's approach. This is nice but also follows from Rocktaeschel and Riedel's differentiable Prolog work (when combined with Lippi and Frasconi's approach).
Moreover, there have been other combinations of tensors and logic, see e.g. 

Ivan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:
Logic Tensor Networks for Semantic Image Interpretation. 
IJCAI 2017: 1596-1602
 
Here you can also have vector encodings of constants. This also holds for 

Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt:
DeepProbLog: Neural Probabilistic Logic Programming. CoRR abs/1805.10872 (2018)

The authors should really discuss this missing related work. This should also involve
a clarification of the ""ILP systems do not scale"" statement. At least if one views statistical relational learning methods as an extension of ILP, this is not true. Probabilistic ILP aka statistical relational learning has been used to learn models on electronic health records, see e.g., the papers collectively discussed in 

Sriraam Natarajan, Kristian Kersting, Tushar Khot, Jude W. Shavlik:
Boosted Statistical Relational Learners - From Benchmarks to Data-Driven Medicine. Springer Briefs in Computer Science, Springer 2014, ISBN 978-3-319-13643-1, pp. 1-68

So the authors should either discuss SRL and its successes, separating SRL from ILP, or they cannot argue that ILP does not scale. In the related work section, they decided to view both as ILP, and, in turn, the statement that ILP does not scale is not true. Moreover, many of the learning tasks considered have been solved with ILP, too, of course in the ILP setting. Any ILP systems have been shown to scale beyond those toy domains.   
This also includes the blocks world. Here relational MDP solvers can deal e.g. with BW worlds composed of 10 blocks, resulting in MDPs with several million states. And the can compute relational policies that solve e.g. the goal on(a,b) for arbitrary number of blocks. This should be incorporated in the discussion of the introduction in order to avoid the wrong impression that existing methods just work for toy examples. 

Coming back to scaling, the current examples are on rather small datasets, too, namely <12 training instances. Moreover, given that we learn a continuous approximation with a limit depth of reasoning, it is also very likely that the models to not generate well to larger test instances. So the scaling issue has to be qualified to avoid to give the wrong impression that the present paper solves this issue. 

Finally, the BW experiments should indicate some more information on the goal configuration. This would help to understand whether an average number of moves of 84 is good or bad. Moreover, some hints about the MDP formulation should be provided, given that there have been relational MDPs that solve many of the probabilistic planning competition tasks. And, given that the conclusions argue that NLMs can learn the ""underlying logical rules"", the learned rules should actually be shown. 

Nevertheless, the direction is really interesting but there several downsides that have to be addressed. ","The review acknowledges the paper's strengths, such as good writing and structure, and the interesting direction of combining neural networks and logic. However, it raises several concerns and criticisms, questioning the novelty and significance of the contributions. The reviewer points out potential flaws in the logic and assumptions, missing related work, and limitations in the experimental evaluation. The overall tone suggests that while the research direction is promising, the paper needs significant revisions and further work to address the raised issues.",-20.0,50.0
"imho, this manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues. 
reading the other comments online, the authors seem to have addressed those concerns as well.","The reviewer uses positive language such as ""clearly written"", ""addresses a confusing point"", ""clarifies some issues"", ""novel and useful approach"".  There is no negative language. The reviewer also acknowledges that the authors have addressed concerns from other reviewers, further indicating a positive sentiment. Therefore, the sentiment is highly positive, but not overly enthusiastic, leading to a score closer to 80 than 100. The language is polite and professional throughout.",80.0,80.0
"Summary:

This paper introduces a new supervised dimensionality reduction model. Supervision is provided in the form of class probabilities and the learning algorithm learns low-dimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities. The representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lower-dimensional space. The authors provide an extensive theoretical analysis of the proposed method and evaluate it on dimensionality reduction, visualization, and zero-shot learning tasks.

Review:

Overall, I thought this was an excellent paper. The idea is well-motivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (I will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials). I was honestly impressed by the shear volume of content in this paper, particularly since I found none of it to be superfluous. Frankly, this paper might be better served as two papers or a longer journal paper, but that is hardly a reason not to accept it. I strongly recommend acceptance and have only a couple of comments on presentation.

Comments:

- When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing. 

- The authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup.","The reviewer clearly states that this is ""an excellent paper"" with a ""well-motivated idea"", ""clear presentation"", and ""comprehensive"" evaluations. They were impressed by the content and strongly recommend acceptance.  The suggestions made are constructive and framed as suggestions, not demands. Thus, the sentiment is very positive and the language is polite.",90.0,100.0
"This work adds to a growing literature on biologically plausible (BP) learning algorithms. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. This seemingly runs counter to the conclusions of Bartunov et al.; while the authors state that their results are ""complementary"", they also state that the findings “directly conflict” with the results of Bartunov, concluding that BP algorithms remain viable options for both learning in artificial networks and the brain.

To reach these conclusions the authors report results on a number of experiments. First, they show successful training of a ResNet-18 architecture on ImageNet using sign-symmetry, with their model performing nearly as well as one trained with backpropagation. Next, they demonstrate decent performance on MS COCO object detection using RetinaNet. Finally, they end with a discussion that seeks to explain the differences in their approach and the approach of Batunov et al, and with a potential biological implementation of sign symmetry.

Overall the clarity of the writing is sufficient. The algorithm is properly explained, and there are sufficient citations to reference prior work. The results are generally clear (though there is an incomplete experiment, I agree with the authors that it is unlikely for the preliminary results to change). I believe that there is enough detail for this work to be reproducible. The work is also sufficiently novel in that experiments using sign-symmetry on difficult datasets have not been undertaken, to my knowledge.

Unfortunately, the clarity and rigor of the *scientific argument* is insufficient for a number of reasons. These will be enumerated below.

First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author’s conclusions highlight precisely these two points). Unfortunately, the experiments and underlying experimental logic push towards addressing the first question, and use this as evidence towards a conclusion to the second question. More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn’t seem to be an important detail with sign-symmetry, for reasons explained below). This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry’s merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a “direct conflict”. To this end, though the authors claim that the conditions on which Bartunov et al tested are “somewhat restrictive”, this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry’s usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments.

Second, the work does not sufficiently weigh the “degree” of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. Of course, one doesn’t want to go down the road of declaring that “algorithm A is more plausible than algorithm B!”, but the nuances should at least be seriously discussed if the algorithms are to be properly compared. In backpropagation the feedback connections must be similar in sign and magnitude. Sign-symmetry eliminates the requirement that the connections be similar in magnitude. However, this factor is arguably the least important of the two (the direction of the gradient is more important than the magnitudes), and we are still left with feedback weights that somehow have to tie their sign to their feedforward counterparts, which is not an issue in target propagation or feedback alignment. The authors try to explain away this difficulty with an appeal to molecular biology, which leads into my third point.

Third, the appeal to molecular mechanisms to explain how sign-symmetry can arise is not rigorous. There is a plethora of molecular mechanisms at play in our cells; indeed, there are enough mechanisms to hand-craft *any* sort of circuit one likes. Thus, it is somewhat vacuous to conclude that a particular circuit can be “easily implemented” in the brain simply by appealing to a hand-crafted circuit. For this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons X, Y, Z. Unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal. But perhaps most problematic, the argument leaves the problem of sign-switching in the feedforward network to “future work”. This is perhaps *the most* important problem at play here, and until it is answered, these arguments don’t have sufficient impact.

Altogether the scientific argument of this work needs tightening. The tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms. The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms.
","The review acknowledges the paper's contributions and clarity but raises significant concerns about its scientific rigor, particularly regarding the comparison to Bartunov et al.'s work and the biological plausibility arguments. The reviewer finds the paper's interpretation of Bartunov et al. misrepresented and the biological implementation arguments insufficiently rigorous. While the language is critical, it maintains a professional and objective tone, focusing on the scientific shortcomings rather than resorting to personal attacks.",-20.0,60.0
"# Summary

The paper proposes training generative models that produce multi-agent trajectories using heuristic functions that label variables that would otherwise be latent in training data. The generative models are hierarchical, and these latent variables correspond to higher level goals in agent behavior. The paper focuses on basketball offenses as a motivating scenario in which multiple agents have coordinated high-level behavior. The generative models are RNNs where each output is fed into the decoder of a variational autoencoder to produce observed states. The authors add an intermediate layer to capture the latent variables, called macro-intents. The parameters are learned by maximizing an evidence lower bound.

Experiments qualitatively and quantitatively show that the hierarchical model produces realistic multi-agent traces.

# Comments

The paper presents a sensible solution for heuristically labeling latent variables. It is not particularly surprising that the model then learns useful behavior because it no longer has to maximize the marginal likelihood over all possible macro-intents. What is more interesting is that a heuristic labeling function is sufficient to label macro-intents that lead to learning realistic basketball offenses and swarm behavior.

Are any of the baselines (VRNN-single, VRNN-indep, and VRNN-mi) equivalent to training the hierarchical model by maximizing an ELBO on the marginal likelihood? I do not think this comparison is done, which might be interesting to quantify how much of a difference heuristic labeling makes. Of course, the potentially poor fit of a variational distribution would confound the results.

# Minor things

1) In the caption of Table 1, it says ""Our hierarchical model achieves higher log-likelihoods than baselines for both datasets."" Are not the reported scores evidence lower-bounds? So it achieves a higher evidence lower bound, but without actually computing the true likelihood, could not the other models have higher likelihoods?

2) Under ""Human preference study"" it says ""All judges preferred our model over the baselines with 98% statistical significance."" I am not familiar with this terminology. Does that mean that a p value for some null hypothesis is .02?

3) Something is wrong with the citation commands. Perhaps \citep should be used.","The review is mostly positive. The reviewer finds the approach sensible and the results interesting. They do not express strong excitement but acknowledge the value of the work. The reviewer also provides constructive feedback and suggestions for improvement, indicating a willingness to see the work published with revisions. The language is polite and professional throughout, with no signs of harsh criticism or disrespect.",60.0,80.0
"This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node.

I find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range.

Question: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs.
","The reviewer explicitly states that they find the idea interesting and praise the comprehensiveness of the experiments. They also highlight two interesting findings from the paper. While they pose a question, it is framed constructively as an opportunity for improvement rather than a criticism. Overall, the tone is positive and encouraging.",75.0,80.0
"The submission presents a reinforcement learning method for exploring/probing the environment to determine an environment’s properties and exploit these during later tasks. The method relies on jointly learning an embedding that simplifies prediction of future states and a policy that maximises a curiosity/intrinsic motivation like reward to learn to explore areas where the prediction model underperforms. In particular, the reward is based on the difference between prediction based on the learned embedding and prediction based on a prior collected dataset, such that the reward optimises to collect data with a large difference between the prediction accuracy of both models. The subsequently frozen policy and embedding are then used in other domains in a system identification like manner with the embedding utilised as input for a final task policy. The method is evaluated on a striker and hopper environment with varying dynamics parameters and shown to outperform a broad set of baselines. 

In particular the broad set of baselines and small performed ablation study on the proposed method are quite interesting and beneficial for understanding the approach. However, the ablation study could be in more detail with respect to the additional training variations (Section 4.1.3; e.g. without all training tricks). Additionally, information about the baselines should be extended in the appendix as e.g. different capacities alone could have an impact where the performances of diff. algorithms are comparably similar. In particular, additional information about the training procedure for the UP-OSI (Yu et al 2017) baseline is required as the original approach relies on iterative training and it is unclear if the baseline implementation follows the original implementation (similar to Section 4.1.3.). 

Overall the submission provides an interesting new direction on learning system identification approaches, that while quite similar to existing work (Yu et al 2017), provides increased performance on two benchmark tasks. The contribution of the paper focuses on detailed evaluation and, overall, beneficial details of the proposed method. The novelty of the submission is however limited and highly similar to current methods.

Minor issues:
- Related work on learning system identification:
Learning to Perform Physics Experiments via Deep Reinforcement Learning
Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter Battaglia, Nando de Freitas
","The reviewer provides constructive criticism, acknowledges the strengths (broad baselines, ablation study, performance), and points out weaknesses (limited novelty, need for more detailed analysis and information). The language is formal and suggestive, not demanding, indicating a positive but discerning view.",60.0,80.0
"Summary:
The paper reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead it is trained to generate frames that happen at any point in the future. The motivation for such approach is that there might be future states that are highly uncertain – and thus, difficult to predict – that might not be useful for other tasks involving video prediction such as planning. The authors derive different loss functions for such Time-Agnostic Prediction (TAP), including extensions to the Variational AutoEncoders (VAE) and Generative Adversarial Networks (GAN) frameworks, and conduct experiments that suggest that the frames predicted by TAP models correspond to ‘subgoal’ states useful for planning.

Strenghts:
[+] The idea of TAP is novel and intuitively makes sense. 
It is clear that there are frames in video prediction that might not be interesting/useful yet are difficult to predict, TAP allows to skip such frames.
[+] The formulation of the TAP losses is clear and well justified. 
The authors do a good job at showing a first version of a TAP loss, generalizing it to express preferences, and its extension to VAE and GAN models, showing that 

Weaknesses:
[-] The claim that the model discovers meaningful planning subgoals might be overstated. 
The hierarchical planning evaluation experiment seems like it would clearly favor TAP compared to a fixed model (why would the middle prediction in time of the fixed model correspond to reasonable planning goals?). Furthermore, for certain tasks and environments it seems like the uncertain frames might be the ones that correspond to important subgoals. For example, for the BAIR Push Dataset, usually the harder frames to predict are the arm-object interactions, which probably would correspond to the relevant subgoals.

Overall I believe that the idea in this paper is a meaningful novel contribution. The paper is well-written and the experiments support the fact that TAP might be a better choice for training frame predictors for certain tasks.","The reviewer provides both positive and negative points, but the overall tone is encouraging, suggesting this is a ""meaningful novel contribution."" The reviewer acknowledges the strengths and contributions while pointing out potential limitations and areas for improvement. The language is constructive and objective.",60.0,80.0
"This paper presents a method to reconstruct images using only noisy measurements. This problem is practically interesting, since the noiseless signal may be unavailable in many applications. The approach combines ideas from recent development in compressed sensing and GANs. However, the model’s presentation is confusing, and many important details of the experiments are missing.

Pros:

* The problem is interesting and important
* The combination of compressed sensing and GANs for image reconstruction is novel

Cons:

* The model structure is unclear: for example, what is the role of the variable \theta? Section 2.1 says it is known, but the algorithm samples from its prior(?). Since there is no further explanation with respect to the experiments, I am not sure how the values of \theta or its distributions were determined. Although \theta is formally similar to the \theta parameters of the measurement function in ambientGANs, this interpretation is at odds with the example given in the paper (below eq.1, saying \theta can be positions or sizes).
* A few important details of the model are missing. For example, what is the exact structure of the measurement function F?
* The baseline models are a bit confusing. More detail about unpaired vs paired supervision would also be helpful for understanding how these baseline models use the additional information.
* Although the paper mentioned parameters are obtained from cross-validation, it would still be helpful to describe a few important ones (e.g., neural network size, weight \lambda) for comparison with other models.The experiments on only CelebA dataset are too limited.","The review acknowledges the importance and novelty of the paper's proposed method ('interesting and important', 'novel'). However, it also raises several concerns regarding the clarity of the model presentation and missing experimental details. The reviewer points out specific areas of confusion and suggests improvements. The tone is critical but professional, focusing on constructive feedback for improvement. Therefore, the sentiment is slightly leaning towards the negative side due to the significant concerns raised, despite acknowledging the paper's positive aspects. The language used is polite and professional throughout the review.",-20.0,80.0
"The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc. The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP. Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can’t fit random noise (and thus maybe better suited for denoising).

The paper is clearly written, and the proposed architecture has too cool properties: it’s compact enough to be used for image compression; and it doesn’t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).

I have two main concerns about this paper.
First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture. Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).

My second concern is about the theoretical contribution. On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising. On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters. Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)). This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.
Also, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.

Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.

Some less important points:

Fig 4 is very confusing.
First, it doesn’t label the X axis.
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.
Third, I don’t get what is plotted on different subplots. The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models? Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it’s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.
Also, in this quote “In Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + η (i.e., FORMULA ...” the formula doesn’t correspond to the text.
And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.

I don’t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?

The authors claim that the model is not convolutional. But first, it’s not obvious why this would be a good thing (or a bad thing for that matter). Second, it’s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.

> The deep decoder is a deep image model G: R N → R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).
I think it should be vice versa, N >> n

The following footnote
> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512×512×3, and choose k = 64 and k = 128 for the respective compression ratios.
Uses unintroduced (at that point) notation and is very confusing.

It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).

I’m also wondering, is it harder to optimize the proposed architecture compared to DIP? The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance.
","The reviewer acknowledges the paper's contributions (novel architecture, theoretical analysis) and writing quality. However, they express concerns about the framing of contributions and the limited applicability of the theoretical results. The reviewer also points out several areas for improvement in clarity, presentation, and experimental validation. The tone is generally constructive and not overly negative.",40.0,70.0
"Thanks the authors for an interesting work!
The paper studies the differences between human and DNN vision via means of minimal images (i.e. smallest image crops that can be correctly classified).

There are a few notable take-away messages:
1. DNNs are not invariant to even tiny (1-2 px) translations of small image crops.
    - It would be more insightful if authors added comparison between DNN sensitivity to tiny translations of small image crops vs. full-size images (i.e. translation-based adversarial examples https://openreview.net/forum?id=BJfvknCqFQ ).
2. The smaller the image crops, the more sensitive DNNs become (here, more FRIs)
3. DNNs and human vision misclassify the image crops differently: (1) DNNs have almost twice more FRI(s) and (2) FRIs of human and DNNs differ in location.

Questions:

- ""After extracting the region from the image, the region is resized to be of the size required by the network.""
Would upscaling say a small 28x28 crop into 224x224 image here naturally negatively impact the DNN predictive performance?
That is, because typically image classifiers are trained on one (or a few) fixed resolution(s) of images.

One hypothesis here is that fragile recognition may be because the test image resolution does not match the training image resolution.
Human on the other hands, have been trained on images of variable resolutions.

An alternative to upscaling here is to zero-pad the crop region. Can you help us understand your choice of upscaling here?

+ Originality
The originality is limited as it is a close extenstion work of Ullman et al. 2016

+ Clarity
The paper is well written and presented.

+ Significance
This work extends our understanding of the differences between DNNs and human vision.
However, given what we learned from the adversarial example research area, the contribution of this work is low because results might not be too surprising.","The reviewer provides generally positive feedback, highlighting interesting takeaways from the paper. They also validate the clarity and significance of the work, although with a reservation about the significance being potentially limited. The reviewer does not use any harsh language and maintains a respectful and constructive tone throughout.",60.0,80.0
"This paper proposed a novel approach for learning disentangled representation from supervised data (x as the input image, y as different attributes), by learning an encoder E and a decoder D so that (1) D(E(x)) reconstructs the image, (2) E(D(x)) reconstruct the latent vector, in particular for the vectors that are constructed by mingling different portion of the latent vectors extracted from two training samples, (3) the Jacobian matrix matches and (4) the predicted latent vector matches with the provided attributes. In addition, the work also proposes to progressively add latent nodes to the network for training. The claim is that using this framework, one avoid GAN-style training (e.g., Fader network) which could be unstable and hard to tune. 

Although the idea is interesting, the experiments are lacking. While previous works (e.g., Fader network) has both qualitative (e.g., image quality when changing attribute values) and quantitative results (e.g., classification results of generated image with novel combination of attributes), this paper only shows visual comparison (Fig. 4 and Fig. 5), and its comparison with Fader network is a bit vague (e.g., it is not clear to me why Fig. 5(e) generated by proposed approach is “more natural” than Fig. 5(d), even if I check the updated version mentioned by the authors' comments). Also in the paper there are five hyperparameters (Eqn. 14) and the center claim is that using Jacobian loss is better. However, there is no ablation study to support the claim and/or the design choice. From my opinion, the paper should show the performance of supervised training of attributes, the effects of using Jacobian loss and/or cycle loss, the inception score of generated images, etc. 

I acknowledge the authors for their honesty in raising the issues of Fig. 4, and providing an updated version. ","The reviewer acknowledges the interesting idea but raises significant concerns about the lack of experimental validation. They point out missing quantitative results, vague comparisons, and the absence of ablation studies. The tone is critical but professional, suggesting improvements rather than outright rejection.",20.0,60.0
"Mode collapse in the context of GANs occurs when the generator only learns one of the 
multiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.

This paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.

The authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. 

For the inference, they adapt the Stochastic Gradient HMC used by Saatsci & Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.

Overall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.

Some typos:

- The authors argue that compare to point mass...
+ The authors argue that, compared to point mass...

- Theorem 1 states that any the ideal generator
+ Theorem 1 states that any ideal generator

- Assume the GAN objective and the discriminator space are symmetry
+ Assume the GAN objective and the discriminator space have symmetry

- Eqn. 8 will degenerated as a Gibbs sampling
+ Eqn. 8 will degenerate as a Gibbs sampling","The reviewer explicitly states ""Overall, I enjoyed reading this paper"" and describes it with positive adjectives such as ""well-written"", ""clear"", ""significant"", ""convincing"", ""interesting"", and ""easy"". The reviewer also points out only minor issues and typos, further indicating a positive sentiment. Thus, the sentiment is scored highly. The language used is polite and constructive throughout the review, without any negative or disrespectful language. Therefore, the politeness score is also high.",85.0,90.0
"* Description

The work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.  Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case. Offline gradient descent and stochastic gradient descent cases are considered.

* Strengths

The work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations. The technical results obtained are non-trivial and detailed proofs are presented. Also I did not verify the proofs the paper appears technically correct and technically clear. The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages. Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum). 

* Criticism
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.
-- Concerns regarding the clarity of presentation and interpretation of the results.
 
The properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations. In contrast, the studied learning rates are asymptotic and there is a big discrepancy. SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and this is with or without normalization. In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability. There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven. It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated. 

As seen from the formal construction, the theoretical results apply equally well to all normalization methods. It occludes the clarity that BN is emphasized amongst them. 

Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let’s consider the following cases.
1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector. In this case, the paper proves that no careful selection of the learning rate is necessary. This result is beyond machine learning and unfortunately I cannot evaluate its merit. Is it known / not known in optimization?

2. The case of data-independent normalization (such as weight normalization).
Without normalization, we have to tune learning rate to achieve the optimal convergence. With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.

3. The case of Batch Normalization. Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches. In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches. The theory tells the same as in case 2 above but with an additional price of optimizing a different function.

These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate. 


-- Difference from Wu et al. 2018

This works is cited as a source of inspiration in several places in the paper. As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work. Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?
 

* Side Notes (not affecting the review recommendation)

I believe that the claim that “BN reduces covariate shift” (actively discussed in the intro) was an imprecise statement in the original work. Instead, BN should be able to quickly adapt to the covariate shift when it occurs. It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.

* Revision
I took into account the discussion and the newly added experiments and increased the score. The experiments verify the proven effect and make the paper more substantial. Some additional comments about experiments follow.
Training loss plots would be more clear in the log scale.
Comparison to ""SGD BN removed"" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017). Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.
The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm. Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?

","The review is mostly positive, acknowledging the technical soundness of the paper and the non-trivial results. However, the reviewer raises significant concerns about the paper's practical relevance and clarity, particularly regarding the applicability of the theoretical findings to real-world scenarios. The reviewer finds the connection to Batch Normalization's empirical success somewhat tenuous and calls for further clarification. The language used is polite and professional, employing constructive criticism and suggestions for improvement.",40.0,80.0
"This paper discusses the effect of weight decay on the training of deep network models with and without batch normalization and when using first/second order optimization methods. 

First, it is discussed how weight decay affects the learning dynamics in networks with batch normalization when trained with SGD. The dominant generalization benefit due to weight decay comes from increasing the effective learning rate of parameters on which batch normalization is applied. The authors therefore hypothesize that a larger learning rate has a regularization effect.

Second, the role of weight decay is discussed when training with second order methods without batch normalization. Under the approximation of not differentiating the curvature matrix used in second order method, it is shown that using weight decay is equivalent to adding to the loss an L2 regularization in the metric space of the curvature matrix considered. It is then shown that if the curvature matrix is the Gauss-Newton matrix, this L2 regularization (and hence the weight decay) is equivalent to the Frobenius norm of the input-output Jacobian when the input has a spherical Gaussian distribution. Similar arguments are made about KFAC with Gauss-Newton norm. The generalization benefit due to weight decay in this case is claimed based on the recent paper by Novak et al 2018 which empirically shows a strong correlation between input-output Jacobian norm and generalization error.


Finally, the role of weight decay is discussed for second order methods when using batch normalization. In this case it is discussed for Gauss-Newton KFAC that the benefit mostly comes from the application of weight decay on the softmax layer and the effect of weight decay on other weights cancel out due to batch normalization. A comparison between Gauss-Newton KFAC and Fischer KFAC is also made. Thus the generalization benefit is presumably attributed to the second order properties of KFAC and a smaller norm of softmax layer weights.

Comments:
The paper is technically correct and proofs look good.

I have mixed comments about this paper. I find the analysis in section 4.2 and 4.3 which discuss about the role of weight decay for second order methods (with and without batch-norm) to be novel and insightful (described above). 

But on the other hand, I feel section 4.1 is more of a discussion on existing work rather than novel contribution. Most of what is said, both analytically and experimentally, is a repetition of van Laarhoven 2017, except for a few details. It would have been interesting to carefully study the effect of weight decay on the gamma parameter of batch-norm which controls the complexity of the network along with the softmax layer weights as it was left for future work in van Laarhoven 2017. But instead the authors brush it under the carpet by saying they did not find the gamma and beta parameters to have significant impact on performance, and fixed them during training.  I also find the claim of section 4.1 to be a bit mis-leading because it is claimed that weight decay applied with SGD and batch normalization only has benefits due to batch-norm dynamics, and not due to complexity control even though in Fig 2 and 4, there is a noticeable difference between training without weight decay, and training with weight decay only on last layer. Furthermore, when hypothesizing the regularization effect of large learning rate in section 4.1, a large body of literature that has studied this effect has not been cited. Examples are [1], [2], [3]. 

I have other concerns which mainly stem from lack of clarity in writing:

1. In the line right above remark 1, it is not clear what “assumption” refer to. I am guessing the distribution of the input being spherical Gaussian?
2. In remark 1, regarding the claim about the equivalence of L2 norm of theta under Gauss-Newton metric and the Frobenius norm of input-output Jacobian, why does f_theta need to be a linear function without any non-linearity? I think the linearity part is only needed for the KFAC result.
3. In remark 1, what does it mean by “Furthermore, if G is approximated by KFAC”? For linear f_theta, given lemma 1 and theorem 1, the claimed equivalence always holds true, no?
4. In the 1st line of last paragraph of page 6, what are the general conditions under which the connection between Gauss-Newton norm and Jacobian norm does not hold true?
5. In figure 5, how are the different points in the plots achieved? By varying hyper-parameters?

A minor suggestion: in theorem 1 (and lemma 1), instead of assuming network has no bias, it can be said that the L2 regularization term does not have bias terms. This is more reasonable because bias terms have no effect on complexity and so it is reasonable to not apply weight decay on bias.

Overall I think the paper is good *if* section 4.1 is sorted out and writing (especially in section 4.2) is improved. For these reasons, I am currently giving a score of 6, but I will increase it if my concerns are addressed.

[1] a bayesian perspective on generalization and stochastic gradient descent
[2] Train longer, generalize better: closing the generalization gap in large batch training of neural networks
[3] Three Factors Influencing Minima in SGD","The reviewer provides a generally positive assessment, acknowledging the novelty and insights of sections 4.2 and 4.3. While they find section 4.1 less novel and raise concerns about clarity and completeness, they acknowledge the paper's technical correctness and potential. The reviewer's willingness to increase the score if concerns are addressed suggests openness and a positive outlook. The language used is professional and constructive, focusing on specific areas for improvement without resorting to harsh or disrespectful language.",60.0,80.0
"This is an experimental paper that investigates how spatial ordering of patches influences the classification performances of CNNs. To do so, the authors design CNNs close to ResNets that almost only consist in a simple cascade of 1x1 convolutions, obtaining relatively small receptive field. It is an interesting read, and I recommend it as a valuable contribution to ICLR, that might lead to nice future works.

I have however several comments and questions, that I would like to be addressed.

1) First I think a reference is missing. Indeed, to my knowledge, it is not the first work to use this kind of techniques. Cf [1]. This does not alterate however the novelty of the approach.

2) « We construct a linear DNN-based BoF » : I do not like this formulation. Here, you assume that you build a ResNet-50 with 1x1 as a representation and have a last final linear layer as a classifier. One could also claim it is a ResNet-48 as a representation followed by 2 layers of 1x1 as a classifier.

3) « our proposed model architecture is simpler » this is very subjective because for instance the FV models are learned in a layer-wise fashion, which makes their learning procedure more interpretable because each layer objective is specified. Furthermore, analyzing these models is now equivalent to analyze a cascade of fully connected layers, which is not simple at all.

4) Again, the interpretability mentioned in Sec. 3  is in term of spatial localization, not mapping. I think it is important to make clear this consideration. Indeed, this work basically leaves the problem of understanding general CNNs to the problem of understanding MLPs.

5) The graphic of the Appendix A is a bit misleading : it seems 13 downsampling are performed whereas it is not the case, because the first element of each group of block is actually only done once.(if I understood correctly)

6) I think the word feature is sometimes mis-used: sometimes it seems it can refer to a patch, sometimes to the code for a patch. (« Surprisingly, feature sizes assmall as 17 × 17 pixels »)

I got also few questions:
Q1 : I was wondering if you did try manifold learning on the patches ? Do you expect it to work ?
Q2 : Is there a batch normalization in the FC or a normalization? Did you try to threshold the heat maps before feeding them to the linear layer? I'm wondering indeed if the amplitude of those heatmaps is really key.
Q3 : do you think it would be easy to exploit the non-overlapping patches for a better parallelization of computations ?

Finally, I find very positive the amount of experiments to test the similarity with standard CNNs. Of course, it’s far from being a formal proof, but I think it is a very nice first step.

[1] Oyallon, Edouard, Eugene Belilovsky, and Sergey Zagoruyko. ""Scaling the scattering transform: Deep hybrid networks."" International Conference on Computer Vision (ICCV). 2017.","The reviewer provides generally positive feedback, stating that the paper is ""an interesting read"" and a ""valuable contribution."" They recommend the paper for acceptance while providing constructive criticism and suggestions for improvement. The language is formal and academic, but not overly critical. ",60.0,70.0
"This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good.

I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature.

One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry.

Even with this reservation, I support accepting the paper.

Minor: In Section 3.4, ""existing a room"" -> ""exiting a room""","The reviewer clearly states they ""like this paper a lot"" and find it a ""good addition to the literature."" They have one major reservation, but it is presented as a suggestion for improvement rather than a damning flaw. The language is constructive and professional throughout.",75.0,100.0
"The fundamental idea proposed in this paper is a sensible one:  design the functional form of a policy so that there is an initial parameterized stage that operates on perceptual input and outputs some ""symbolic"" (I'd be happier if we could just call them ""discrete"") characterization of the input, and then an arbitrary program that operates on the symbolic output of the first stage.

My fundamental problem is with equation 3.  If you want to talk about the factoring of the probability distribution p(a | s) that's fine, but, to do it in fine detail, it should be:
P(a | s) = \sum_sigma P(a, sigma | s) = \sum_sigma P(a | sigma, s) * P(sigma | s)
And then by conditional independence of a from s given sigma
 = \sum_sigma P(a | sigma) * P(sigma | s)
But, critically, there needs to be a sum over sigma!  Now, it could be that I am misunderstanding your notation and you mean for p(a | sigma) to stand for a whole factor and for the operation in (3) to be factor multiplication, but I don't think that's what is going on.

Then, I think, you go on to assume, that p(a | sigma) is a delta distribution.  That's fine.

But then equation 5 in Theorem 1 again seems to mention delta without summing over it, which still seems incorrect to me.

And, ultimately, I think the theorem doesn't make sense because the transformation that the program performs on its input is not included in the gradient computation.  Consider the case where the program always outputs action 0 no matter what its symbolic input is.   Then the gradient of the log prob of a trajectory with respect to theta should be 0, but instead you end up with the gradient of the log prob of the symbol trajectory with respect to theta.

I got so hung up here that I didn't feel I could evaluate the rest of the paper.  

One other point is that there is a lot of work that is closely related to this at the high level, including papers about Value Iteration Networks, QMDP Networks, Particle Filter Networks, etc.  They all combine a fixed program with a parametric part and differentiate the whole transformation to do gradient updates.  It would be important in any revision of this paper to connect with that literature.","The reviewer raises serious concerns about the core mathematical formulation of the paper, particularly Equation 3 and its implications. They find the lack of summation over sigma and the handling of the delta distribution problematic. The reviewer also points out a potential flaw in Theorem 1, suggesting the gradient computation might be incorrect. They admit to not evaluating the rest of the paper due to these concerns. Despite the criticism, the reviewer's tone remains professional and focused on the technical aspects. They offer constructive feedback, suggesting a connection to related literature. The initial acknowledgment of the paper's idea as 'sensible' also suggests a somewhat positive initial disposition. ",-50.0,50.0
"The paper proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The proposed algorithm is based on matching the geodesic distances on the product manifold to graph distances. I find the proposed method quite interesting and think that it might be promising in data analysis problems. Here are a few issues that would be good to clarify:

- Could you please formally define K in page 3?

- I find the estimation of the signature very interesting. However, I am confused about how the curvature calculation process is (or can be) integrated into the embedding method proposed in Algorithm 1. How exactly does the sectional curvature estimation find use in the current results? Is the “Best model” reported in Table 2 determined via the sectional curvature estimation method? If yes, it would be good to see also the Davg and mAP figures of the best model in Table 2 for comparison.

- I think it would also be good to compare the results in Table 2 to some standard dimensionality reduction algorithms like ISOMAP, for instance in terms of Davg. Does the proposed approach bring advantage over such algorithms that try to match the distances in the learnt domain with the geodesic distances in the original graph?

- As a general comment, my feeling about this paper is that the link between the different contributions does not stand out so clearly. In particular, how are the embedding algorithm in Section 3.1, the signature estimation algorithm in Section 3.2, and the Karcher mean discussed in Section 3.3 related? Can all these ideas find use in an overall representation learning framework? 

- In the experimental results in page 7, it is argued that the product space does not perform worse than the optimal single constant curvature spaces. The figures in the experimental results seem to support this. However, I am wondering whether the complexity of learning the product space should also play a role in deciding in what kind of space the data should be embedded in. In particular, in a setting with limited availability of data samples, I guess the sample error might get too high if one tries to learn a very high dimensional product space.  


Typos: 

Page 3: Note the “analogy” to Euclidean products
Page 7 and Table 1: I guess “ring of cycles” should have been “ring of trees” instead
Page 13: Ganea et al formulates “basic basic” machine learning tools …","The reviewer clearly states that they find the proposed method ""quite interesting"" and ""promising"". They provide constructive criticism and suggestions for improvement rather than harsh negative feedback. The language is formal, respectful, and suggestive (e.g., ""it would be good to clarify"" rather than demanding changes). Therefore, the sentiment is positive, and the language is polite.",75.0,80.0
"This paper proposes fine-tuning an out-of-distribution detector using an Outlier Exposure (OE) dataset. The novelty is in proposing a model-specific rather than dataset-specific fine-tuning. Their modifications are referred to as Outlier Exposure. OE includes the choice of an OE dataset for fine-tuning and a regularization term evaluated on the OE dataset. It is a comprehensive study that explores multiple datasets and improves dataset-specific baselines.

Suggestions and clarification requests:
- The structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works. I suggest moving the details of model-specific OE regularization terms to section 3 and review the details of the baseline models. Then present the other set of novelties in proposing OE datasets in a new section before presenting the results. Clearly presenting two sets of novelties in this work and then the results. If constrained in space, I suggest squeezing the discussion, conclusion, and 4.1.
- In the related work section Radford et al., 2016 is references when mentioning GAN. Why not the original reference for GAN?
- Maybe define BPP, BPC, and BPW in the paragraphs on PixelCNN++ and language modeling or add a reference.
- Numbers in Table 3 column MSP should match the numbers in Table 1, right? Or am I missing something?","The review is positive overall. The reviewer acknowledges the novelty and comprehensiveness of the work. While they have suggestions for improvement, these are presented constructively and aim to enhance clarity and presentation. The reviewer focuses on suggesting better structuring and providing specific, actionable points for improvement, indicating a positive view of the paper's potential.",60.0,80.0
"I enjoyed reading this manuscript. The paper is based on a simple idea used by others as well (i.e., the image has two components, one  that encode content which is shared across domains and another one characterizing the domain specific style). The other important idea is the use of feature masks that steer the translation process without requiring semantic labels. This is similar to attention models used by others but I think it is novel when applying to this specific application domain. I was a bit disappointed by the evaluation part. The authors decided to perform ablation and to show the importance of each component using only the MNIST-Single dataset. While this is good as a toy example I would have expected to see such analysis on a more complex example, e.g., street-view translation. This is also surprising considering that it is not even present in the supplementary material. Overall, this is a solid submission with interesting ideas and good implementation.   ","The reviewer explicitly states enjoying the manuscript and finds the idea interesting and the implementation good. They call it a ""solid submission."" This points towards a positive sentiment. However, the reviewer also expresses disappointment about the evaluation part, which lowers the positivity. Overall, the language is polite and professional, using constructive criticism when pointing out the weakness. Therefore, the politeness score is close to fully positive.",75.0,90.0
"Overall:
This paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.
The problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising.

Clarity:
The paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand.

Significance:
I think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score.
The reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.

Question and minor comments:
In the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians.
In this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL.
Faced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method?  
To clarify the usefulness of this method, I think the additional experimental comparisons are needed.

About the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased ""experimentally"".

Followings are minor comments.
In experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance.
I think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator.
However, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper.

In Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure?
(Same for Figure 4)","The reviewer acknowledges the significance of the work and finds the proposed solution promising. However, they raise valid concerns and questions about the method's effectiveness compared to existing approaches, indicating a need for further clarification and experiments. This suggests a slightly positive sentiment leaning towards neutral due to the uncertainties. The language used is polite and professional, maintaining a constructive and respectful tone throughout the review.",30.0,80.0
"This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters and leveraged recent progress on meta-learning for optimizing the adversarial attacks. Different from some recent work on adversarial attacks for graph neural networks (Zuigner et al. 2018; Dai et al. 2018), which focus on attacking specific nodes, this paper focuses on attacking the  overall performance of graph neural networks. Experiments on a few data sets prove the effectiveness of the proposed approach. 

Strength:
- the studied problem is very important and recently attracting increasing attention
- Experiments show that the proposed method is effective.

Weakness:
- the complexity of the proposed method seems to be very high
- the data sets used in the experiments are too small
Details:
-- the complexity of the proposed method seems to be very high. The authors should explicitly discuss the complexity of the proposed method. 
-- the data sets in the experiments are too small. Some large data sets would be much more compelling.
-- Are the adversarial examples identified by the proposed method transferrable to other graph embedding algorithms (e.g., the unsupervised node embedding methods, DeepWalk, LINE, and node2vec)?
-- I like Figure 3, though some concrete examples would be more intuitive. ","The review acknowledges the importance and effectiveness of the proposed method, indicating a positive sentiment. However, it also raises concerns about complexity and limited dataset size, which slightly reduces the positivity. The language used is constructive and professional, suggesting a polite tone. There are no personal attacks or disrespectful remarks.",60.0,80.0
"*Summary :
The paper explores variants of popular adaptive optimization methods.
The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates.
The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments.


*Significance:
-There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.

-Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound.
Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well.

-The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea.

*Clarity:
The idea and motivation are very clear and so are the experiments.


*Presentation:
The presentation is mostly good.

Summary of review:
The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method.

","The reviewer acknowledges the impressive empirical results (""The experimental part on the other hand is impressive"") and clear presentation. However, they find the theoretical contribution lacking, stating it doesn't demonstrate a significant advantage over existing methods.  The criticism, while direct, is presented in a professional and analytical manner, focusing on the scientific content rather than personalizing the critique.",50.0,70.0
"This paper describes a novel method for solving inverse problems in imaging.

The basic idea of this approach is use the following steps:
1. initialize with nonnegative least squares solution to inverse problem (x0)
2. compute m different projections of x0
3. estimate x from the m different projections by solving ""reformuated"" inverse problem using TV regularization.

The learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a ""good"" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be.

More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. 

Empirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem.

The core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the ""inverse-ness"" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned.

At the heart of this paper is the idea that for an L-Lipschitz function f : R^k → R the sample complexity
is O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided.
This isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we
really get the benefits.

The authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares.

Practically I'm quite concerned about their method requiring training 130 separate convolutional neural
nets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is
it possible that any network at all would be okay? Can we just reconstruct the image from regression
on 130 randomly-initialized convolutional networks? 

The proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. 
",The reviewer acknowledges the novelty of the approach and its empirical success but expresses several concerns and doubts about the method's theoretical grounding and practical aspects. They find some of the justifications unconvincing and call for more comprehensive comparisons and ablation studies. The language is critical and questioning but remains within the bounds of professional academic discourse.,20.0,60.0
"This is a paper of the verification of neural networks, i.e. check their robustness, 
and the main contribution here is to tackle it as a statistical problem adressed with 
multi-level splitting Monte Carlo approach. I found the paper well motivated and original, 
resulting in a publishable piece of research up to a few necessary adjustments. These 
concern principally notation issues and some potential improvements in the writing. 
Let me list below some main remarks along the text, including also some typos. 

* In the introduction, ""the classical approach"" is mentioned but to be the latter is 
insufficiently covered. Some more detail would be welcome. 

* page 2, ""predict the probability"": rather employ ""estimate"" in such context? 

* ""linear piecewise"": ""piecewise linear""? 

* what is ""an exact upper bound""? 

* In related work, no reference to previous work on ""statistical"" approaches to NN 
verification. Is it actually the case that this angle has never been explored so far?

* I am not an expert but to me ""the density of adversarial examples"" calls for further 
explanation. 

* From page 3 onwards: I was truly confused by the use of [x] throughought the text 
(e.g. in Equation (4)). x is already present within the indicator, no need to add yet 
another instance of it. Here and later I suffered from what seems to be like an awkward 
attempts to stress dependency on variables that already appear or should otherwise 
appear in a less convoluted way. 

* In Section 4, it took me some time to understand that the considered metrics do not 
require actual observations but rather concern coherence properties of the NN per se. 
While this follows from the current framework, the paper might benefit from some more 
explanation in words regarding this important aspect. 

* In page 6, what is meant by ""more perceptually similar to the datapoint""? 

* In the discussion: is it really ""a new measure"" that is introduced here? 

* In the appendix: the MH acronym should better be introduced, as should the notation 
g(x,|x') if not done elsewhere (in which case a cross-reference would be welcome). 
Besides this, writing ""the last samples"" requires disambiguation (using ""respective""?). 


 

 ","The reviewer provides constructive criticism, acknowledges the paper's strengths (well-motivated, original), and suggests specific improvements. The language is formal and objective, typical of academic peer reviews. While there are points of contention and areas for improvement, the overall tone suggests the reviewer sees merit in the work.",60.0,70.0
"In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning.
The main idea is that two states should be distinguished *functionally* in terms of the actions that are needed to reach them,
in contrast with generative methods which try to capture all aspects of the state dynamics, even those which are not relevant for the task at hand.
The method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance
between policies conditioned by these two states as the loss that the representation learning algorithm should minimize.
The experimental study is based on 6 simulated environments and outlines various properties of the framework.

Overall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like ICLR.

The first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning ""downstream tasks"" in a second step. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further.

A second point is about unsufficiently clear thoughts about the way to intuitively advocate for the approach. The authors first claim that two states are functionally different if they are reached from different actions. Thinking further about what ""functionally"" means, I would rather have said that two states are functionally different if different goals can be reached from them. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). But thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would end-up with a different intuitive presentation of their framework. Shouldn't finally D_{act} be a distance between goals rather than between states?

Section 4 lists the properties that can be expected from the framework. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as ""state abstraction (or clustering?) from actionable representation"". And the corresponding properties should come with their own questions and subsection in the experimental study (more about this below).

About the related work, a few remarks:
- The authors do not refer to papers about using auxiliary tasks. Though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.
- The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. To me, they must read it.
- The authors should also read Laversanne-Finot et al. (2018, CoRL) who learn goal space representations and show an ability to extract independently controllable features from that.

A positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. Also, the results described in Fig. 5 are interesting. A side note is that the authors address in this Figure a problem pointed in Penedones et al (2018) about ""The Leakage Propagation problem"" and that their solution seems more convincing than in the original paper, maybe they should have a look.
But there are also several weaknesses:
- for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. This definitely hampers reproducibility of the work. A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest.
- most importantly, in Section 6.4, 6.5 and 6.6, much too few details are given. Particularly in 6.6, the task is hardly described with a few words. The message a reader can get from this section is not much more than ""we are doing something that works, believe us!"". So the authors should choose between two options:
* either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than ""with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either"".
* or add a huge appendix with all the missing details.
I'm clearly in favor of the first option.

Some more detailed points or questions about the experimental section:
- not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used.
- in Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible?
- in Section 6.3, about the pushing experiment, I would like to argue against the fact that the block position is the important factor and the end-effector position is secundary. Indeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint?
- Globally, although it is interesting, Fig.6 only conveys a quite indirect message about the quality of the learned representation.
- Still in Fig. 6, what is described as ""blue"" appears as violet in the figures and pink in the caption, this does not help when reading for the first time.
- In Section 6.4, Fig.7 a, ARC happens to do better than the oracle. The authors should describe the oracle in more details and discuss why it does not provide a ""perfect"" representation.
- Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. Why insist on this one? And a deeper discussion of the performance of each method would be much more valuable than just showing these curves.
- Section 6.5 is so short that I do not find it useful at all.
- Section 6.6 should be split into the HRL question and the clustering question, as mentioned above. But this only makes sense if the experiments are properly described, as is it is not useful.

Finally, the discussion is rather empty, and would be much more interesting if the experiments had been analyzed in more details.

typos:

p1: that can knows => know
p7: euclidean => Euclidean
","The reviewer finds the core idea interesting but identifies significant weaknesses in both the framework and the experimental validation. They provide constructive criticism and suggestions for improvement, indicating a willingness to see the work published after major revisions. The tone, while direct, maintains a professional and constructive approach. Specific examples like ""The idea is interesting, but..."" or ""A positive side of the experimental study is..."" illustrate this balance between critique and encouragement.",-20.0,60.0
"Authors propose a parameter sharing scheme by allowing parameters to be reused across layers. It further makes connection between traditional CNNs with RNNs by adding additional regularization and using hard sharing scheme.

The way of parameter sharing is similar to the filter prediction method proposed in Rebuff et al’s work, where they model a convolutional layer’s parameters as a linear combination of a bank of filters and use that to address difference among multiple domains.

Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi, Learning multiple visual domains with residual adapters, NIPS 2017.

The discussion on the connection between coefficients for different layers and a network’s structure and visualization of layer similarity matrix is interesting. Additional regularization can further encourage a recurrent neural network to be learned. 

However, they only experiment with one or two templates and advantage on accuracy and model size  over other methods is not very clear.","The review starts with a neutral summary of the paper's contributions. While it acknowledges the interesting aspects of the work ('The discussion... is interesting.'), it also points out a significant limitation ('However,...advantage...is not very clear.'). The lack of strong positive statements and the presence of a notable concern suggest a slightly negative sentiment overall. The language used is objective and professional throughout the review.",-20.0,50.0
"This paper presents the following approach to domain adaptation. Train a source domain RNN. While doing inference on the target domain, first you run the source domain RNN on the sequence. Then while running the target domain RNN, set the hidden state at time step i, h^t_i, to be a function 'f' of  h^t_{i-1} and information from source domain \psi_i; \psi_i is computed as a convex combination of the state of the source domain RNN, h^s_{i}, and an attention-weighted average of all the states h^s{1...n}. So in effect, the paper transfers information from each of source domain cells -- the cell at time step i and all the ""collocated"" cells (collocation being defined in terms of attention). This idea is then extended in a straightforward way to LSTMs as well. 
 
Doing ""cell-level"" transfer enables more information to be transferred according to the authors, but it comes at a higher computation since we need to do O(n^2) computations for each cell.

The authors show that this beats a variety of baselines for classification tasks (sentiment), and for sequence tagging task (POS tagging over twitter.)

Pros:
1. The idea makes sense and the experimental results show solid 

Cons:
1. Some questions around generalization are not clearly answered. E.g. how are the transfer parameters of function 'f' (that controls how much source information is transferred to target) trained? If the function 'f' and the target RNN is trained on target data, why does 'f' not overfit to only selecting information from the target domain? Would something like dropping information from target domain help?

2. Why not also compare with a simple algorithm of transferring parameters from source to target domain? Another simple baseline is to just train final prediction function (softmax or sigmoid) on the concatenated source and target hidden states. Why are these not compared with? Also, including the performance of simple baselines like word2vec/bow is always a good idea, especially on the sentiment data which is very commonly used and widely cited. 

3. Experiments: the authors cite the hierarchical attention transfer work of Li et al (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments. Why?

Writing:
The writing is quite confusing at places and is the biggest problem with this paper. E.g.

1. The authors use the word ""collocated"" everywhere, but it is not clear at all what they mean. This makes the introduction quite confusing to understand. I assumed it to mean words in the target sentences that are strongly attended to. Is this correct? However, on page 4, they claim ""The model needs to be evaluated O(n^2) times for each sentence pair."" -- what is meant by sentence pair here? It almost leads me to think that they consider all source sentence and target sentences? This is quite confusing. 

2. The authors keep claiming that ""layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain"", but it is not clear exactly what do they mean by layer-wise here. Do they mean transferring the information from source cell i to target cell i as it is? In the experiments section on LWT, the authors claim that ""More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification."" Why is it not possible to train a softmax over both the source hidden state and the target hidden state for POS tagging? 

nits:
page 4 line 1: ""i'th cell in the source domain"" -> ""i'th cell in the target domain"". ""j'th cell in target"" -> ""j'th cell in sourcE"".


Revised: increased score after author response.
","The review starts with a summary of the paper's approach, which helps to establish a neutral starting point. The reviewer acknowledges the merits of the paper, stating that the ""idea makes sense"" and the results are ""solid."" This suggests a somewhat positive sentiment. However, the reviewer also raises several valid concerns and questions, indicating that the paper is not without its flaws. The reviewer points out potential issues with generalization, missing baselines, and lack of comparison with a relevant work. The reviewer also criticizes the writing clarity, particularly the use of the term ""collocated."" The tone remains professional and respectful throughout, even when expressing criticism. Overall, the sentiment leans slightly towards the positive side due to the acknowledgment of the paper's merits, but the numerous concerns balance it out. ",20.0,60.0
"Summary:
The authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios. They propose a way to adaptively update the value for the bêta hyper-parameter to respect the constraint on I(X,Z). Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck. It can also be considered as an adaptive version of instance noise, which serves the same goal. The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods. Best results for ‘classical’ adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case).


Pros :
- This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance. The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges. 

- The proposition of a principled way of adaptively varying the value of Bêta to actually respect more closely the constraint I(X,Z) < I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed Bêtas and does also bring the KL divergence to the desired I_c.

- The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator.


Cons:

- In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive Bêta.

- I think the Bêta-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task.

- I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde “being a mixture of the target distribution and the generator” (Section 4) can let the implementation details be ambiguous. I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold. This makes me think batches are separated? Either way, this should be more clearly stated in the text.

- The last results for  the ‘traditional’ GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results. I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well. In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish. Was GP tested in Imitation Learning/Inverse RL ? Was it better? Could it still be combined with VIB for better results? 

- In the saliency map of Figure 5, I’m unclear as to what the colors represent (especially on the GAIL side). I doubt that this is simply due to the colormap used, but this colormap should be presented.

Overall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented. I think the limited novelty is counterbalanced by the quality of empirical analysis. Some clarity issues and missing citations should be easy to correct. I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance.

[1] Deep Variational Information Bottleneck, (Alemi et al. 2017)
[2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017)
","The reviewer acknowledges the merits of the paper, highlighting its strong empirical analysis and the value of the experiments. While they point out a limited novelty and suggest some areas for improvement (clarity, missing citations, additional comparisons), their overall tone is positive, suggesting acceptance and potential for the work to be useful to others in the field.",65.0,75.0
"This paper proposes a new approach to use more informative signals (than only class labels), specifically, regions humans deem important on images, to improve deep convolutional neural networks. They collected a large dataset by implementing a game on clickme.ai and showed that using this information results in both i) improved classification accuracy and ii) more interpretable features. 

I think this is good work and should be accepted. The main contribution is three fold: i) a publicly available dataset that many researchers can use, ii) a network module to incorporate this human information that might be inserted into many networks to improve performance, and iii) some insights on the effect of such human supervision and the relation between features that humans deem important to those that neural nets deem important. 

Some suggestions on how to improve the paper:
1. I find Sections 3 & 4 hard to track - some missing details and notation issues. Several variables are introduced without detailing the proper dimensions, e.g., the global feature attention vector g (which is shown in the figure actually). The relation between U and u_k isn't clear. Also, it will help to put a one-sentence summary of what this module does at the beginning of Section 3, like the last half-sentence in the caption of Figure 3. I was quite lost until I see that. Some more intuition is needed, on W_expand and W_shrink; maybe moving some of the ""neuroscience motivation"" paragraph up into the main text will help. Bold letters are used to denote many different things - in  Section 4 as a set of layers, in other places a matrix/tensor, and even an operation (F). 

2. Is there any explanation on why you add the regularization term to every layer in a network? This setup seems to make it easy to explain what happens in Figure 4. One interesting observation is that after your regularization, the GALA features with ClickMe maps exhibit minimal variation across layers (those shown). But without this supervision the features are highly different. What does this mean? Is this caused entirely by the regularization? Or there's something else going on, e.g., this is evidence suggesting that with proper supervision like human attention regions, one might be able to use a much shallower network to achieve the same performance as a very deep one?

3. Using a set of 10 images to compute the correlation between ClickMe and Clicktionary maps isn't ideal - this is even less than the number of categories among the images. I'm also not entirely convinced that ""game outcomes from the first and second half are roughly equal"" says much about humans not using a neural net-specific strategy, since you can't rule out the case that they learned to play the game very quickly (in the first 10 of the total 380 rounds). 

4. Title - this paper sound more ""human feedback"" to me than ""humans-in-the-loop"", because the loop has only 1 iteration.  Because you are collecting feedback from humans but not yet giving anything back to them. Maybe change the title?","The reviewer explicitly states ""this is good work and should be accepted."" They provide constructive criticism and suggestions for improvement, indicating a positive stance. The language used is formal, respectful, and suggestive rather than demanding, indicating politeness.",80.0,85.0
"General comment
==============
The authors describe two loss functions for learning embeddings of protein amino acids based on i) predicting the global structural similarity of two proteins, and ii) predicting amino acid contacts within proteins. As far as I know, these loss functions are novel and the authors show clear improvements when using the learned embeddings in downstream tasks. The paper is well motivated and mostly clearly written. However, the evaluation must be strengthened and some aspects of it clarified. Provided that the authors address my comments below, I think it is a good ICLR application paper.

Major comments
=============
1. The authors should describe how they optimized hyperparameters such as the learning, lambda (loss section 3.3), or the smoothing factor (section 3.4). These should be optimized on an evaluation set, but the authors only mentioned that they split the dataset into training and holdout (test) set (section 4.1).

2. The way the authors present results in table 1 and table 2 is unclear. Both table 1 and table 2 contain results of the structural similarity tasks but with different baselines. ‘SSA w/ contact predictions’ is also undefined and can be interpreted as ‘with’ or ‘without’ contacts predictions. I therefore strongly recommend to show structural similarity results in table 1 and secondary structure results in table 2 and include in both tables i) ‘SSA full’, ‘SSA without contact predictions’, and ‘SSA without language model’.

3. The authors should compare SSA to the current state-of-the art in structure prediction in addition to baseline models.

4. The authors should evaluate how well their method predicts amino acid contact maps.

5. The authors should describe how they were dealing with variable-length protein sequences. Are sequences truncated and embedded to a fixed length? What is the mean and variance in protein sequence lengths in the considered datasets? The authors should point out that their method is limited to fixed length sequences.

6. The authors should briefly describe the training and inference time on a single GPU and CPU. How much memory is required for training with a certain sequence length, e.g. 400 amino acids per sequence? Does the model fit on a single GPU?

7. The authors should discuss limitations of their method, e.g. that it cannot handle variable length sequences and that the memory scales quadratically by the the sequence length.

8. CRF (SSA) (table 3) includes a biLSTM layer between SSA and the CRF. However, the biLSTM can learn a non-linear projection of embeddings learned by SSA such that it is unclear if improvements are due to the embeddings learned by SSA or the biLSTM+CRF architecture. The authors should therefore train a biLSTM+CRF model on one-hot encoded amino-acids and include it as baseline in table 3.


Minor comments
=============
9. The way the similarity score s’ is computed (section 3.2.1) should be motivated more clearly. Why do the authors compute the score s’ manually instead of predicting it, e.g. using a model that takes the embeddings z of both proteins as input and predicts a single scalar s’? 

10. How does ordinal regression (section 3.2.2) perform compared with a softmax layer? Why do the authors compute s’ and then train logistic regression classifiers on s’ to predict the similarity level, instead of predicting the similarity level directly based on the embeddings z?

11. Why do the authors use a distance threshold of 8A (section 3.3)? Is this common practice in the field?

12. Why do the authors use the not product and the absolute difference as features instead of the embeddings z directly? Which activation function is used to predict contact probabilities (sigmoid, softmax, …)?

13. The authors should reference and describe the results presented in table 1 more clearly.

14. Optional: the authors should analyze if learned embeddings are correlated with amino acid and structural properties such as their size, charge, or solvent accessibility. Do embeddings clusters by certain properties? This can be analyzed, e.g., using a tSNE plot. 

15. How does TMalign perform when using the maximum or geometric average instead of the arithmetic average of the two scores (section 4.1)","The review is positive overall. The reviewer finds the paper to be well-motivated, clearly written, and novel. They see the work as a valuable contribution to the field. While the reviewer lists several major and minor comments, these are constructive suggestions for improvement rather than harsh criticisms. The language used is formal, professional, and polite.",75.0,90.0
"I have no major complaints with this work.  It is well presented and easily understandable. I agree with the claim that the largest gains are largely syntactic, but this leads me to wonder about more tail phenomena.   PP attachment is a classic example of a syntactic decision requiring semantics, but one could also imagine doing a CCG supertagging analysis to see how well the model captures specific long-tail phenomena.  Though a very different task Vaswani et al 16, for example, showed how bi-LSTMs were necessary for certain constructions (presumably current models would perform much better and may capture this information already).

An important caveat of these results is that the evaluation (by necessity) is occurring in English.  Discourse in a pro-drop language would presumably require longer contexts than many of these approaches currently handle.","The reviewer starts with ""I have no major complaints with this work"" and uses generally positive language throughout. They offer constructive suggestions and point out potential limitations without being overly critical. Therefore, the sentiment is positive and the language is polite.",75.0,80.0
"
The authors make a case that deep networks are biased
toward fitting data with simple functions.

The start by examining the priors on classifiers obtained by sampling
the weights of a neural network according to different distributions.  They do this
in two ways.  First, they examine properties of the distribution
on binary-valued functions on seven boolean inputs obtained by
sampling the weights of a small neural network.  They also empirically compare
the labelings obtained by sampling the weights of a network with
labelings obtained from a Gaussian process model arising from earlier
work.

Next, they analyze the complexity of the functions produced, using
different measures of the complexity of boolean functions.  A
favorite of theirs is something that they call Lempel-Ziv complexity,
which is measured by choosing an arbitrarily ordering of the
domain, writing the outputs of the function in that ordering,
and looking at how well the Lempel-Ziv algorithm compresses this
sequence.  I am not convinced that this is the most meaningful
and fundamental measure of the complexity of functions.
(In the supplementary material, they examine some others.
They show plots relating the different measures in the body
of the paper.  None of the measures is specified in detail in the
body of the paper. They provide plots relating these complexity
measures, but they don't demonstrate a very close connection.)

The authors then evaluate the generalization bound obtained by
applying a PAC Bayes bound, together with the assumption that
the training process produces weights sampled from the distribution
obtained by conditioning weights chosen according to the random
initialization on the event that they fit they fit the training
data perfectly.  They do this for small networks and simple datasets.
They bounds are loose, but not vacuous, and follow the same order
of difficulty on a handful of datasets as the true generalization
error.

In all of their experiments, they stop training when the training
accuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf
have found that continuing training past this point further improves test
accuracy.  The experiments all use architectures that are
quite dissimilar to what is commonly used in practice, and
achieve much worse accuracy, so that a reader is concerned
that the results differ qualitatively in other respects.

I do not find it surprising that randomly sampling parameters
of deep networks leads to simple functions.

Papers like the Soudry, et al paper cited in this submission are
inconsistent with the assumption in the paper that SGD samples
parameters uniformly.

It is not clear to me how many hidden layers were used for the
results in Table 1 (is it four?).  

I did find it interesting to see exactly how concentrated the
distribution of functions obtained in their 7-input experiment
was, and also found results on the agreement of the Gaussian process
models with the randomly sampled weight interesting, as far as they
went.  Overall, I am not sure that this paper provided enough
fundamental new insight to be published in ICLR.
","The reviewer finds some aspects of the work interesting, such as the concentration of functions in the 7-input experiment and the agreement with Gaussian process models. However, they also express several concerns and reservations. They are not convinced by the choice of Lempel-Ziv complexity as a measure, find the experimental setup unrealistic, and believe the paper lacks fundamental new insights. The tone is critical but professional and not overtly negative.",-20.0,60.0
"Adversarial attacks and defences are of growing popularity now a days. As AI starts to be present everywhere, more and more people can start to try to attack those systems. Critical systems such as security systems are the ones that can suffer more from those attacks. In this paper the case of vehicles that attack an object detection system by trying to not be detected are tackled.

The proposed system is trained and evaluated in a simulation environment. A set of possible camouflage patterns are proposed and the system learns how to setup those in the cars to reduce the performance of the detection system. Two methods are proposed. Those methods are based on Expectation over transformation method. This method requires the simulator to be differentiable which is not the case with Unity/Unreal environments. The methods proposed skip the need of the simulator to be differentiable by approximating it with a neural network.

The obtained results reduce the effectivity of the detection system. The methods are compared with two trivial baselines. Isn't there any other state of the art methods to compare with?

The paper is well written, the results are ok, the related work is comprehensive and the formulation is correct. The method is simply but effective. Some minor comments:
 - Is the simulator used CARLA? Or is a new one? Where are the 3D assets extracted from?
 - Two methods are proposed but I only find results for one","The review starts with a neutral statement about the field of study, not directly praising or criticizing the paper. It then summarizes the paper's content without any strong positive or negative remarks. While it acknowledges the paper's merits (well-written, decent results, comprehensive related work, correct formulation, simple but effective method), it also raises several questions and points out the lack of comparison with state-of-the-art methods, which suggests room for improvement. Overall, the feedback is constructive and leans slightly towards the positive side due to the positive points mentioned.",40.0,70.0
"In this paper the authors present mutual posterior divergence regularization, a data-dependent regularization for the ELBO that enforces diversity and smoothness of the variational posteriors. The experiments show the effectiveness of the model for density estimation and representation learning.
This is an interesting paper dealing with the important issues of fully exploiting the stochastic part of VAE models and avoiding inactive latent units in the presence of very expressive decoders. The paper reads well and is well motivated. 

The authors claim that their method is ""encouraging the learned variational posteriors to be diverse"". While it is important to have models that can use well the latent space, the constraints that are encoded seem too strong. If two data points are very similar, why should there be a term encouraging their posterior approximation to be different? In this case, their true posteriors will be in fact be similar, so it seems counter-intuitive to force their approximations to be different.

The numerical results seem promising, but I think they could be further improved and made more convincing.
- For the density estimation experiments, while there is an improvement in terms of NLL thanks to the new regularizer, it is not clear which is the additional computational burden. How much longer does it takes to train the model when computing all the regularization terms in the experiments with batch size 100? 
- I am not completely convinced by the claims on the ability of the regularizer to improve the learned representations. K-means implicitly assumes that the data manifold is Euclidean. However, as shown for example by [Arvanitidis et al. Latent space oddity: on the curvature of deep generative models, ICLR 2018] and other authors, the latent manifold of VAEs is not Euclidean, and curved riemannian manifolds should be used when computing distances and performing clustering. Applying k-means in the high dimensional latent spaces of ResNet VAE and VLAE does not seem therefore a good idea.
One possible reason why your MAE model may perform better in the unsupervised clustering of table 2 is that the terms added to the elbo by the regularizer may force the space to be more Euclidean (e.g. the squared difference term in the Gaussian KL) and therefore more suitable for k-means. 
- The semi-supervised classification experiment is definitely better to assess the representation learning capabilities, but KNN suffers with the same issues with the Euclidean distance as in the k-means experiments, and the linear classifier may not be flexible enough for non-euclidean and non-linear manifolds. Have you tried any other non-linear classifiers?
- Comparisons with other methods that aim at making the model learn better representation (such as the kl-annealing of the beta-vae) would be useful.
- The lack of improvements on the natural image task is a bit concerning for the generalizability of the results.

Typos and minor comments:
- devergence -> divergence in introduction
- assistant -> assistance in 2.3
- the items (1) and (2) in 3.1 are not very clear
- set -> sets in 3.2
- achieving -> achieve below theorem 1
- cluatering -> clustering in table 2","The review starts with positive remarks, highlighting interesting aspects of the paper and good presentation. However, it then raises significant concerns about the core idea and methodology, suggesting the constraints might be too strong and the evaluation not entirely convincing. The reviewer also provides constructive suggestions for improvement, indicating a desire to see the work strengthened rather than rejected. The tone remains professional and polite throughout, offering specific and actionable feedback. Therefore, the sentiment leans slightly towards the positive side due to the initial praise and constructive criticism, but not overly positive due to the significant concerns raised.",55.0,75.0
"Summary: 
 
The paper provides the convergence analysis at linear rate of gradient descent to global minima for deep linear neural networks – the fully-connected neural networks with linear activation with l2 loss. The convergence only works under two necessary assumptions on initialization: “weight matrices at initialization are approximately balanced” and “the initial loss is smaller than the loss of any rank-deficient solution”. The result of this work is similar to that of Barlett et al. 2018, but the difference is that, in Barlett et al. 2018, they consider a subclass of linear neural networks (linear residual networks – a subclass of linear neural networks which the input, output and all hidden layers are the same dimensions). 
 
Comments: 
 
This paper focuses on theoretical aspect of Deep Learning. Yes, theoretical study of gradient-based optimization in deep learning is still open and needs to spread more. I have the following comments and questions to the author(s) and hope to discuss further during the rebuttal period: 
 
1) Most of the deep learning applications are well-known used the neural networks with non-linear activation (specifically ReLU). Could you please provide any successful applications that linear neural networks could achieve better performance over the “non-linear” one? Yes, more layers may lead to better performance since we have more parameters. However, it is still not clear that which one is better between “linear” and “non-linear” with the same size of networks. I am not sure if this linear neural networks could generalize well. 
 
2) For N=1, the problem should become linear regression with strongly convex loss, which means that there exists a unique W: y = W*x in order to minimize the loss. Hence, if W = W_N*....*W_1, the problem becomes non-convex w.r.t parameters W_N, ...., W_1 but all the minima could be global. Can you please provide some intuitions why the loss function could have saddle points? Also, is not easier to just solve the minimization problem on W?

3) Similar with l2 loss, it seems that the problem needs to be restricted on l2 loss. In understand that it could have in some applications. Do you try to think of different loss for example in binary classification problems? 
 
4) I wonder about the constant “c > 0” in the definition 2 and it would use it to determine the learning rate. Do you think that in order to satisfy the definition 2 for the most cases, constant c would be (arbitrarily) small or may be very close to 0? If so, the convergence rate may be affected in this case. 
 
5) The result of Theorem 2 is nice and seems new in term of probabilistic bound.  I did not see the similar result in the existing literature for neural networks. 
 
6)  It would be nice if the author(s) could provide some experiments to verify the theory. I am also curious to know what performance it could achieve for this kind of networks. 
 
I would love to discuss with the author(s) during the rebuttal period. ","The reviewer acknowledges the paper's focus on a relevant theoretical aspect of deep learning and finds the result in Theorem 2 novel. However, they raise several questions and concerns, particularly about the practical applicability of the findings (e.g., questioning the use of linear neural networks and the l2 loss). The reviewer also suggests additional experiments. Overall, the feedback is constructive and given in a questioning rather than accusatory tone.",40.0,70.0
"Strengths:
- Good coverage of related work
- Clear presentation of the methods
- Evaluation using established SemEval datasets


Weaknesses:
1. It is not entirely clear what is the connection between fuzzy bag of words and DynaMax. In principle DynaMax can work with other methods too. This point should be elaborated a bit more.
2. It is claimed that the this paper shows that max-pooled word vectors are a special case of fuzzy bag of words. This is not correct. The paper shows how to ""convert"" one to the other. 
3. It is also claimed that point 2 above implies that max-pooled vectors should be compared with the fuzzy Jaccard index instead of cosine similarity. There is no proof or substantial justification to support this. 
4. Some relevant work that is missing:
- De Boom, C., Van Canneyt, S., Demeester, T., Dhoedt, B.: Representation learning for very
short texts using weighted word embedding aggregation. Pattern Recognition Letters 80,
150–156 (2016)
- Kenter, T., De Rijke, M.: Short text similarity with word embeddings. In: International on
Conference on Information and Knowledge Management. pp. 1411–1420. ACM (2015)","The review acknowledges the strengths of the paper, such as good coverage of related work and clear presentation. However, it also points out several weaknesses and misconceptions in the paper's claims. The reviewer provides concrete examples and suggestions for improvement. The tone is direct and factual, but not disrespectful.",20.0,60.0
"It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. This might explain why in some cases overparametrized models have better generalization properties.

This paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better. First, the concepts of \textit{capacity} and \textit{impact} of a hidden unit are introduced. Then, {\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \textit{capacity} and \textit{impact}. Next, {\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. Finally {\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.

## Strengths
- The paper is theoretically sound, the statement of the theorems
    are clear and the authors seem knowledgeable when bounding the
    generalization error via Rademacher complexity estimation.

- The paper is readable and the notation is consistent throughout.

- The experimental section is well described, provides enough empirical
    evidence for the claims made, and the plots are readable and well
    presented, although they are best viewed on a screen.

- The appendix provides proofs for the theoretical claims in the
    paper. However, I cannot certify that they are correct.

- The problem studied is not new, but to my knowledge the
    presented bounds are novel and the concepts of capacity and
    impact are new. Theorem 3 improves substantially over
    previous results.

- The ideas presented in the paper might be useful for other researchers
    that could build upon them, and attempt to extend and generalize
    the results to different network architectures.

- The authors acknowledge that there might be other reasons
    that could also explain the better generalization properties in the
    over-parameterized regime, and tone down their claims accordingly.

## Weaknesses
\begin{itemize}
- The abstract reads ""Our capacity bound correlates with the behavior
    of test error with increasing network sizes ..."", it should
    be pointed out that the actual bound increases with increasing
    network size (because of a sqrt(h/m) term), and that such claim
    holds only in practice.

- In page 8 (discussion following Theorem 3) the claim
    ""... all the previous capacity lower bounds for spectral
        norm bounded classes of neural networks (...) correspond to
        the Lipschitz constant of the network. Our lower bound strictly
    improves over this ..."", is not clear. Perhaps a more concise
    presentation of the argument is needed. In particular it is not clear
    how a lower bound for the Rademacher complexity of F_W translates into a
    lower bound for the rademacher complexity of l_\gamma F_W. This makes the claim of tightness of Theorem 1 not clear. Also this makes
    the initial claim about the tightness of Theorem 2 not clear.
","The review is overall quite positive. The reviewer lists significantly more strengths than weaknesses, and the weaknesses are presented as opportunities for improvement and clarification rather than harsh criticisms. The reviewer acknowledges the novelty and potential impact of the work. The language used is formal and respectful, typical of academic peer reviews.",75.0,80.0
"The main theory points out two scenarios causing Adam-type optimizers to diverge, which extends Reddi et al's results. 

The theorem in this paper applies to all Adam-type algorithms, which combine momentum with adaptive learning rates and thus are more general as compared to the recent papers, such as Zhou et al's. The relationship between optimizers' effective step size, step size oscillation and convergence is well demonstrated and is interesting.

Remarks:
1. The main theorem and proof are based on the non-convex settings while the examples to demonstrate the convergence condition are simple convex functions.

2. The message delivered by MNIST experiment is limited, is not clear and is not very relevant to the main part of the paper. It would be better to compare these algorithms in larger deep learning tasks.

Typo:
Page 5, section 3.1: Term A is a generalization of term alpha^2 g^2 (instead of just alpha^2) for SGD.","The review starts by highlighting the strengths of the paper, such as the generalizability of the theorem and the interesting relationship demonstrated. The reviewer uses positive language like ""well demonstrated"" and ""interesting."" However, the review also points out areas for improvement, suggesting the MNIST experiment is limited and encouraging the authors to consider larger tasks. The tone remains constructive and polite throughout, using phrases like ""It would be better"" to offer suggestions. Overall, the sentiment leans positive due to the acknowledgment of the paper's contributions.",60.0,70.0
"Summary:
This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data. For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned. For the logistic loss, this paper further shows the linear function is the maximum margin solution.

Comments:
This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect. These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks. The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results. 
There are two weaknesses. First, there is no convergence rate. Second, the step size assumption (Assumption 5) is unnatural. If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption? 

Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar.
However, I don't think this is a strong theory people due to the two weakness I mentioned.","The reviewer highlights the ""interesting properties"" and ""new analysis"" as positive aspects, indicating they find merit in the work. They use phrases like ""good job"" and ""interesting results,"" suggesting a positive view. However, they also point out weaknesses, such as the lack of convergence rate and an unnatural assumption. Despite these, they conclude that the paper is ""above the bar"" but not a ""strong theory."" This suggests a somewhat positive but not overwhelmingly enthusiastic sentiment. The language used is balanced and professional, directly addressing the paper's strengths and weaknesses without resorting to harsh or impolite language.",60.0,80.0
"This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers.
From what I can tell, you train K randomly initialized models to maximize displacement (optionally with a periodic implementation).
This ensemble of low-level models is then presented to a high-level controller, that can use them for actions.
When you do this, the resultant algorithm performs well on a selection of deep RL tasks.

There are several things to like about this paper:
- Hierarchical RL is an important area of research, and this algorithm appears to make progress beyond the state of the art.

- The ideas of using ensemble of low-level policies is intuitive and appealing.

- The authors provide a reasonable explanation of their ""periodicity"" ideas, together with evidence that it can be beneficial, but is not always essential to the algorithm.

- Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out.


There are several places this paper could be improved:
- First, the statement of the *main* algorithm needs to be brought together so that people can follow it clearly. I understand one of the main reasons this is complicated is because the authors have tried to make this ""general"" or to be used with DQN/PPO/A3C... but if you present a clear implementation for *one* of them (PPO?) then I think this will be a huge improvement.

- Something *feels* a little hacky about this... why are there only two timescales? Is this a general procedure that we should always expect to work? *why* are we doing this... and what can its downsides be? The ablation studies are good, but I think a little more thought/discussion on how this fits in with a bigger picture of RL/control would be good.

Overall, I hope that I understood the main idea correctly... and if so, I generally like it.
I think it will be possible to make this much clearer even with some simple amendments.","The reviewer finds the core idea of the paper interesting and promising, praising its novelty and potential impact on hierarchical RL. They also commend the clarity of the writing and the inclusion of ablation studies. However, they express concerns about the clarity of the algorithm's presentation and raise questions about its generalizability and potential downsides. The tone throughout is constructive and professional, suggesting ways to improve the paper rather than outright criticizing it.",60.0,80.0
"This paper presents an approach to creating word representations that operate at both the sub-word level and generalise across languages. The paper presents soft decoupled encoding as a method to learn word representations from weighted bags of character-n grams, a language specific transformation layer, and a ""latent semantic embedding"" layer. The experiments are conducted over low-resource languages from the multilingual TED corpus. The experiments show consistent improvements compared to existing approaches to training translation models with sub-word representations. The ablation studies in Section 4.4 are informative about the relative importance of different parts of the proposed model.

Can you comment on how your model is related to the character-level CNN of Lee et al. (TACL 2017)?

In the experiments, do you co-train the LRLs with the HRLs? This wasn't completely clear to me from the paper. In Section 4.2 you use phrases like ""concatenated bilingual data"" but I couldn't find an explicit statement that you were co-training on both language pairs.

What does it mean for the latent embedding to have a size of 10,000? Does that mean that W_s is a 10,000 x D matrix?

Is Eq (4) actually a residual connection, as per He et al. (CVPR 2016)? It looks more like a skip connection to me.

Why do you not present results for all languages in Section 4.6?

What is the total number of parameters in the SDE section of the encoder? The paper states that you encode 1--5 character n-grams, and presumably the larger the value of N, the sparser the data, and the larger the number of parameters that you need to estimate.

For which other tasks do you think this model would be useful?","The review is predominantly positive. The reviewer acknowledges the merits of the paper, such as its contribution to sub-word representation learning and its consistent improvements over existing approaches. The questions posed are constructive, seeking clarification and further elaboration on specific aspects of the research. The reviewer's use of phrases like ""Can you comment..."" and ""This wasn't completely clear to me..."" indicates a polite and constructive approach rather than a critical tone.",60.0,80.0
"This work presents an extension of the MAML framework for ""learning to learn."" This extension changes the space in which ""inner-loop"" gradient steps are taken to adapt the model to a new task, and also introduces stochasticity. The authors validate their proposed method with regression experiments in a toy setting and few-shot classification experiments on mini- and tiered-Imagenet. The latter are well known and competitive benchmarks in few-shot learning.

The primary innovations that distinguish this work from previous gradient-based approaches to meta-learning (namely MAML) are that (i) the initial set of parameters is data-dependent and drawn from a generative distribution; and (ii) the adaptation of model parameters proceeds in a lower-dimensional latent space rather than in the higher-dimensional parameter space. Specifically, model parameters are generated from a distribution parameterized by an adapted latent code at each adaptation step. I find both of these innovations novel.

The experimental results, in which LEO outperforms the state of the art on two benchmarks derived from ImageNet by ""comfortable margins,"" and the ablation study demonstrate convincingly that these innovations are also significant. I also found the curvature analysis and embedding visualization illuminating of the model's function. My one suggestion would be to test the model on realistic data from beyond the image domain, perhaps on something sequential like language (consider the few-shot PTB setting from Vinyals et al. (2016)). I'm aware anecdotally that MAML struggles with adapting RNNs and I wonder if LEO overcomes that weakness.

The paper is clearly written and I had little difficulty in following the algorithmic details, although I'm sure it helped to be familiar with the convoluted meta-learning and inner-/outer- loop frameworks. I recommend it for publication.

Pros:
- Natural, novel extension to gradient-based meta-learning
- state of the art results on two competitive few-shot benchmarks
- good analysis
- clear writing

Cons:
- realistic, high-dim data is only from the image domain

Minor questions for the authors:
- Relation Networks are computationally intensive, although in few-shot learning the sets encoded are fairly small. Can you discuss the computational cost and training time of the full framework?
- What happens empirically when you generate parameters for more than just the output layer in, eg, your convolutional networks?
- What happens if you try to learn networks from scratch through the meta-learning process rather than pre-training and fine-tuning them? Some of the methods you compare against do so, to my understanding.","The reviewer explicitly states ""I recommend it for publication."" and lists several positive aspects like ""novel innovations"", ""convincing"" results, ""illuminating analysis"" and ""clear writing."" They have minor suggestions and questions, but their overall sentiment is very positive. The language used is polite and professional throughout the review.",85.0,90.0
"This work proposes to use semantic knowledge about the relationships and functionality of different objects, to help in navigation tasks, in both familiar and unfamiliar situations. The paper is very well written and it is clear what the authors did. The approach seems sound, and while it combines two existing approaches (actor-critic reinforcement learning for navigation, and belief propagation using graph convolution networks) is sufficiently novel to be of interest to at least some members of the community. The experimental evaluation is good, and the proposed method outperforms Mnih 2016 by a significant margin, especially in the more interesting settings. A good ablation study is provided. 

My main concern is that there seems to be a larger pool of work in semantic navigation than what the evaluation includes. Anderson 2018, Zhu 2017 and Gupta 2017 seem relevant. While none of these use knowledge graphs, some of these show they outperform Mnih 2016 so would be stronger baselines. 

I am also curious whether the proposed work generalizes across scene type categories (e.g. if it learns on kitchens but it tested on living rooms). This would be an experiment in the spirit of unknown object/scene but even more challenging. ","The review starts with positive statements about the paper's clarity, novelty, and experimental evaluation. The reviewer finds the approach sound and the results promising, particularly the significant outperformance of the baseline. However, the reviewer also raises a valid concern about the limited scope of the evaluation, suggesting additional baselines for a more comprehensive comparison. The reviewer's tone is constructive and inquisitive, suggesting further research directions rather than outright criticism. Overall, the sentiment leans towards the positive side due to the initial praise, but the concern slightly balances it, leading to a score closer to neutral than extremely positive. The language used is polite and professional throughout.",60.0,80.0
"This manuscript contributes a new online gradient descent algorithm with adaptation to local curvature, in the style of the Adam optimizer, ie with a diagonal reweighting of the gradient that serves as an adaptive step size. First the authors identify a limitation of Adam: the adaptive step size decreases with the gradient magnitude. The paper is well written.

The strengths of the paper are a interesting theoretical analysis of convergence difficulties in ADAM, a proposal for an improvement, and nice empirical results that shows good benefits. In my eyes, the limitations of the paper are that the example studied is a bit contrived and as a results, I am not sure how general the improvements.

# Specific comments and suggestions

Under the ambitious term ""theorem"", the results of theorem 2 and 3 limited to the example of failure given in eq 6. I would have been more humble, and called such analyses ""lemma"". Similarly, theorem 4 is an extension of this example to stochastic online settings. More generally, I am worried that the theoretical results and the intuitions backing the improvements are built only on one pathological example. Are there arguments to claim that this example is a prototype for a more general behavior?


Ali Rahimi presented a very simple example of poor perform of the Adam optimizer in his test-of-time award speech at NIPS this year (https://www.youtube.com/watch?v=Qi1Yry33TQE): a very ill-conditioned factorized linear model (product of two matrices that correspond to two different layers) with a square loss. It seems like an excellent test for any optimizer that tries to be robust to ill-conditioning (as with Adam), though I suspect that the problem solved here is a different one than the problem raised by Rahimi's example.


With regards to the solution proposed, temporal decorrelation, I wonder how it interacts with mini-batch side. With only a light understanding of the problem, it seems to me that large mini-batches will decrease the variance of the gradient estimates and hence increase the correlation of successive samples, breaking the assumptions of the method.


Using a shared scalar across the multiple dimensions implies that the direction of the step is now the same as that of the gradient. This is a strong departure compared to ADAM. It would be interesting to illustrate the two behaviors to optimize an ill-conditioned quadratic function, for which the gradient direction is not a very good choice.


The performance gain compared to ADAM seems consistent. It would have been interesting to see Nadam in the comparisons.



I would like to congratulate the authors for sharing code.

There is a typo on the y label of figure 4 right.
","The review is overall positive. The reviewer finds the paper well-written, the theoretical analysis interesting, and the empirical results promising. However, the reviewer also raises concerns about the generalizability of the findings, suggesting the studied example might be contrived. The reviewer remains polite throughout, offering constructive criticism and suggestions for improvement rather than harsh judgments.",60.0,80.0
"This paper presents an extension of Wasserstein autoencoder (WAE) by modifying the regularization term in learning objective of variational autoencoder. This term measures the divergence between the distribution of the encoded training samples and the samplable prior distribution. The modification is based on the sliced-Wasserstein distance where the distance between two distributions is measured through slicing or projecting the high-dimensional distributions into one-dimensional marginal distributions. As a result, a closed-form solution to the integral in Eq. (9) is obtained via a numerical method. The adversarial learning in WAE, designed to fulfill the calculation of high-dimensional distance, can be avoided. In general, this is an interesting work by introducing new idea of sliced-Wasserstein distance.

Remarks:
1. A theoretical paper which addresses how and why the sliced-Wasserstein distance between p_z and q_z is reasonable to build a new variant of variational auto-encoder.
2.  Reformulating the Wasserstein distance into Monge primal formulation with the assumption based on the property of diffeomorphic mapping.
3. As a result, the implementation based on the unstable adversarial training or the maximum mean discrepancy (MMD) training can be avoided. Computational attractiveness is assured. MMD needs the choice of kernel function which is basically a data-dependent design parameter.
4. Provide an empirical numerical solution which is compatible with SGD optimization.
5. The key idea of this paper is shown in Eq. (14). Learning objective is expressed in a deterministic way. However, the style of objective in Eq. (14) involves the stochastic learning.
6. This paper is not actually doubly-blind reviewed. Authors have exposed their identities in arXiv.
","The review starts with positive pointers like ""interesting work"" and ""new idea"". While it lists remarks as points for improvement, these are framed constructively and suggest directions for strengthening the paper, not outright flaws. The tone is academic and objective throughout.",60.0,80.0
"The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging. For sufficiently short intervals between synchronization, the algorithm achieves the same convergence rate in terms of the number of gradient evaluations as parallel minibatch SGD, but with the advantage that communication can be significantly reduced.

The algorithm is simple and practical, and the analysis is concise and seems like it could be applicable more generally to other parallel SGD variants.

I am curious about what happens for the analysis of the algorithm when $H$ becomes large. As the authors point out, when $H=T$, this is one-shot averaging which is known to converge. The authors mention not working too hard to optimize the bounds for extreme values of $H$, which is fine, but I wonder if this is possible using their analysis technique, or whether new tools would be necessary.","The review is positive overall. The reviewer finds the algorithm simple and practical, the analysis concise, and believes it could be applicable more generally. They also pose a thoughtful question, suggesting potential future research directions, which indicates engagement with the work. The language used is neutral and professional throughout.",75.0,100.0
"The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information.

The authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization.

My main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model.

It would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).

More specific comments:
--
The abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature.

Using existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.)

I'm confused by one aspect of the DocNADE model: ""the topic assigned ...equally depends all the other words appearing in the same document"". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? 

Related to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't.

Relation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about ""distributed compositional priors"" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these ""priors"" are compositional, but that's a separate question.)

Equation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM?

It would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM?

Similar question: how can the DocNADE model provide a *global* context if the model is generative?

Perplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact?

I don't understand why lambda has to be zero ""to compute the exact log-likelihood"".

The first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that.

Minor presentational comments:
--
Some of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings?

Strictly speaking, I'm not sure that 'bear' in the example is a proper noun.

""orderless sets of words"": bags, not sets, since the counts matter, no?

The tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us.
","The review is quite critical of the paper's novelty and clarity, but it does acknowledge the thorough evaluation. The reviewer raises several concerns and questions, indicating a lack of enthusiasm for the work. However, the language remains professional and polite throughout, focusing on constructive criticism and suggestions for improvement.",20.0,70.0
"Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present.

This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug.

Unfortunatley this leads to a number of weaknesses:
 - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.
 - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range)
 - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on.
 - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines?

Strengths of the paper are:
 - Well-written and easy to follow and understand.
 - Evaluation on several datasets.
 - Interesting architecture for bug-localization if the idea really works.

[1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection","The review starts with a neutral tone, explaining the context and the paper's proposal. However, it quickly transitions into a negative sentiment by using terms like ""Unfortunately,"" ""weaknesses,"" and highlighting the significant performance gap compared to state-of-the-art methods. While it lists some strengths, the overall tone and emphasis on the weaknesses suggest a negative sentiment. The language used is formal and professional, avoiding any personal attacks or disrespectful remarks.",-40.0,60.0
"This manuscript describes a deep convolutional neural network for
assigning proteins to subcellular compartments on the basis of
microscopy images.

Positive points:

- This is an important, well-studied problem.

- The results appear to improve significantly on the state of the art.

- The experimental comparison is quite extensive, including
  reimplementations of four, competing state-of-the-art methods, and
  lots of details about how the comparisons were carried out.

- The manuscript also includes a human-computer competition, which the
  computer soundly wins.

- The manuscript is written very clearly.

Concerns:

There is not much here in the way of new machine learning methods.
The authors describe a particular neural network architecture
(""GapNet-PL"") and show empirical evidence that it performs well on a
particular dataset.  No claims are made about the generalizability of
the particular model architecture used here to other datasets or other
tasks.

A significant concern is one that is common to much of the deep
learning literature these days, namely, that the manuscript fails to
separate model development from model validation. We are told only
about the final model that the authors propose here, with no
discussion of how the model was arrived at.  The concern here is that,
in all likelihood, the authors had to try various model topologies,
training strategies, etc., before settling on this particular setup.
If all of this was done on the same train/validation/test split, then
there is a risk of overfitting.

The dataset used here is not new; it was the basis for a competition
carried out previously.  It is therefore somewhat strange that the
authors chose to report only the results from their reimplementations
of competing methods.  There is a risk that the authors'
reimplementations involve some suboptimal choices, relative to the
methods used by the originators of those methods.

Another concern is the potential circularity of the labels.  At one
point, we are told that ""Most importantly, these labels have not been
derived from the given microscopy images, but from other
biotechnologies such as microarrays or from literature.""  However,
earlier we are told that the labels come from ""a large battery of
biotechnologies and approaches, such as microarrays, confocal
microscopy, knowledge from literature, bioinformatics predictions and
additional experimental evidence, such as western blots, or small
interfering RNA knockdowns.""  The concern is that, to the extent that
the labels are due to bioinformatics predictions, then we may simply
be learning to re-create some other image processing tool.

The manuscript contains a fair amount of biology jargon (western
blots, small interfering RNA knockdowns, antibodies, Hoechst staining,
etc.) that will not be understandable to a typical ICLR reader.

At the end, I think it would be instructive to show some examples
where the human expert and the network disagreed.

Minor:

p. 2: ""automatic detection of malaria"" -- from images of what?

p. 2: Put a semicolon before ""however"" and a comma after.

p. 2: Change ""Linear Discriminant"" to ""linear discriminant."" Also, remove
the abbreviations (SVM and LDA), since they are never used again in
this manuscript.

p. 5: Delete comma in ""assumption, that.""

p. 8: ""nearly perfect"" -> ""nearly perfectly""

The confusion matrices in Figure 5 should not be row normalized --
just report raw counts.  Also, it would be better to order the classes
so that confusable ones are nearby in the list.
","The review acknowledges the positive aspects of the work, such as its significance, improved results, and clear writing. However, it raises significant concerns about the lack of novelty in machine learning methods, potential overfitting due to the non-separation of model development and validation, and the circularity of labels. The reviewer also points out the use of biology jargon and suggests improvements in presenting the results. Overall, the sentiment leans towards the negative side due to these concerns. The language used is polite and professional, offering constructive criticism and suggestions for improvement.",-20.0,80.0
"Summary:

This paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like ""go to the cup"". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). 

The really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. 

In order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. 

Experiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. 

Comments and Questions:

- The paper is generally well-written and easy to understand. Thanks!

- The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning.

- One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in ""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. 

- The following papers are relevant and should be cited and discussed:
""Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments"" by Anderson et al, CVPR 2018.

""Embodied Question Answering"", Das et al, CVPR 2018.

Update:
------------
After looking at other reviews and author rebuttals to all reviews I am raising my grade. 
","The review is overwhelmingly positive. The reviewer explicitly states the paper is ""well-written and easy to understand"" and that the ""idea of using IRL to learn generalizable reward functions... is interesting and clearly shows benefits"". They do offer constructive criticism and suggestions for improvement, but their overall sentiment is positive, especially considering they would raise their score after reading the author's response to the reviews. ",85.0,90.0
"Paper summary:

The paper presents two distinct contributions in text-to-speech systems:
a) It describes a method for distilling a Gaussian WaveNet into a Gaussian Inverse Autoregressive Flow that uses an analytically computed KL between their conditionals.
b) It presents a text-to-speech system that is trained end-to-end from text to waveforms.

Technical quality:

The distillation method presented in the paper is technically correct. The evaluation is based on Mean Opinion Score and seems to follow good practices.

The paper makes three claims:
a) A WaveNet with Gaussian conditionals can model speech waveforms equally well as WaveNets with other types of conditionals.
b) Analytically computing KL divergence stabilizes distillation.
c) A text-to-speech system trained end-to-end from text to waveforms outperforms one that has separately trained text-to-spectrogram and spectrogram-to-waveform subsystems.

Claims (a) and (c) are clearly demonstrated in the experiments. However, there is nothing in the paper that substantiates claim (b). I think the paper would be strengthened if the performance of sample-based KL distillation was added into Table 2, and if learning curves were reported that evaluate the amount of stabilization that an analytical KL may offer vs a sample-based KL.

Further points about the experiments:
- It wasn't clear to me whether distillation happens at the same time as the autoregressive WaveNet is trained on data, or after it has been fully trained. I think the paper should make this clear.
- The paper says that distillation makes generation three orders of magnitude faster. I think it would be good if actual generation times (e.g. in seconds) were reported.

Clarity:

The paper is generally well-written. Sections 1 and 2 in particular are excellent.

However, section 3 contains several notational errors and technical inaccuracies, that makes it rather confusing to read. In particular:
- q(x_t | z_{<=t}) is used in several places to mean the Gaussian conditional q(x_t | z_{<t}) (e.g. in Eqs (6) and (7), and elsewhere). This is confusing, as q(x_t | z_{<=t}) is actually a delta distribution.
- q(x | z) is used in several places to mean q(x) (e.g. in Eq. (7), in Alg. 1 and elsewhere). This is confusing, as q(x | z) is also a delta distribution.
I believe that section 3, especially subsections 3.2 and 3.3.1, should be reworked to be made clearer, and the notation should be carefully revised.

I don't think the paper needs to span 9 pages. Section 3 is rather wordy, and should be compressed to the important points.

Originality:

Distilling a Gaussian autoregressive model to another Gaussian autoregressive model by matching their Gaussian conditionals with an analytical KL is rather straightforward, and, methodologically speaking, I wouldn't consider it an original contribution on its own. However, I think its application and demonstration in text-to-speech constitutes an original contribution.

Significance:

The paper contains a substantial amount of significant work that I think is important to be communicated to the ICLR community, especially the text-to-speech community.

Review summary:

Pros:
+ Substantial amount of good work.
+ Significant improvement in text-to-speech end-to-end software.
+ Generally well-written (with the exception of section 3 which needs work).

Cons:
- Some more experiments would be good to substantiate the claim that analytical KL is better.
- Notational errors and confusion in section 3.
- Too wordy, no need for 9 pages.

Nitpicks:
- As I said above, I wouldn't consider distillation of models with Gaussian conditionals using analytical KLs methodologically novel, so I think the phrase ""novel regularized KL divergence"" should be moderated.
- Eq. (1) should contain theta on the left hand side too.
- Page 3: ""at Appendix B"" --> ""in Appendix B"".
- Page 4: In flows we don't just ""suppose z has the same dimension as x""; rather, it's a necessary condition that must hold.
- Footnote 5: It's unclear to me what it means to ""make the loss less sensitive"".
- References: Real NVP, Fourier, Bayes, PixelCNN, WaveNet, VoiceLoop should be properly capitalized.","The reviewer acknowledges the paper's strengths, including its significant contributions to text-to-speech systems and generally clear writing. However, they also point out areas for improvement, such as the need for further experiments to support a claim and the presence of notational errors in a specific section. Overall, the feedback is constructive and suggestive of improvements, indicating a positive sentiment. The language used is polite and professional, focusing on the technical aspects and offering concrete suggestions.",60.0,80.0
"The goal of this paper is to use deep generative models for missing data imputation. This paper proposes learning a latent variable deep generative model over every randomly sampled subset of observed features. First, a masking variable is sampled from a chosen prior distribution. The mask determines which features are observed. Then, the likelihood of the observed features is maximized via a lower bound. Inference in this latent variable model is achieved through the use of an inference network which conditions on the set of ""missing"" (to the generative model) features.

Novelty:
Generative models have a long history of being used to impute missing data. e.g. http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2011.pdf, https://arxiv.org/pdf/1610.04167.pdf,
https://arxiv.org/pdf/1808.01684.pdf, https://arxiv.org/pdf/1401.4082.pdf [Appendix F]
It is a little difficult to guage what the novelty of this work is.

Clarity
This is a poorly written paper. Distilling the proposed methodology down to one paragraph was challenging since the text meanders through several concepts whose relevance to the overarching goal is questionable. For example, it is not clear what Section 3.2 adds to the discussion. The text describes a heuristic used in learning GSNNs only to say that the loss function used by GSNNs is not used in the experimental section for this paper -- this renders most of 4.3.2 redundant. There are issues like awkward grammar, sloppy notation, and spelling mistakes (please run spell check!) throughout the manuscript. Please use a different notation when referring to the variational distributions (do not re-use ""p"").

Experimental Results
The model is evaluated against MICE and MissForest on UCI datasets. RMSE and accuracy of classification (from imputed data is compared). The complexity of data considered is simplistic (and may not make use of the expressivity of the deep generative model). Why not run these experiments on datasets like MNIST and Omniglot?
Beyond that:
(a) was there any comparison to how classification performance behaves when using another neural network based imputation baseline (e.g. the method in Yoon et. al)?
(b) the *kind* of missingness considered here appears to be MCAR (the easiest kind to tackle) -- did you consider experiments with other kinds of missingess?

The qualitative results presented in this work are interesting. The method does appear to produce more diverse in-paintings than the method from Yeh et. al (though the examples considered are not aligned).

Table 5 claims negative log-likelihood numbers on MNIST as low as 61 and 41 (I assume nats...). These numbers do not make sense. How were they computed?


Priors on b:
What kind of priors on b did you experiment with? ","The review starts with a neutral tone, pointing out the goal of the paper. However, it quickly becomes negative, criticizing the novelty, clarity, experimental setup, and results. The reviewer uses phrases like ""poorly written,"" ""awkward grammar,"" ""sloppy notation,"" and ""do not make sense,"" indicating a negative sentiment. While the reviewer provides constructive feedback, the language is quite direct and critical.",-50.0,-20.0
"I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so I’ll focus on them:

K-ary dependencies
Functions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that you’re explicitly modelling higher-order interactions that improves performance? Or is it that you’re doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? 

These two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance that’s the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? 

SGD approaches:
I think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I don’t follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \pi-SGD to converge, but we aren’t provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don’t seem to be specific to \pi-SGD - any SGD algorithm with “slightly biased” gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isn’t evaluated so we’re left with theory that doesn’t provide guidance and isn’t evaluated.

Summary:
There are two ways to read this paper:
 1. Janossy pooling as a framework & proposed pooling approach implemented in one of the two ways discussed above.
 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.

I liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that I’m arguing for it’s acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. 

[Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep Sets
[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.
[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works
[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs
[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets","The reviewer explicitly states enjoying the paper and arguing for its acceptance, which indicates a positive sentiment. They provide constructive criticism and suggestions for improvement, but this is presented in a professional and respectful manner. The reviewer acknowledges the strengths of the paper, such as its identification of limitations in existing methods, while also pointing out areas where further investigation or clarification is needed. The language used is objective and avoids any personal attacks or disrespectful remarks.",75.0,80.0
"This paper considers learning informative priors for convolutional neural network models based on fits to data sets from similar problem domains.  For trained networks on related datasets the authors use autoencoders to obtain an expressive prior on the filter weights, with independence assumed between different layers.  The resulting prior is generative and its density has no closed form expression, and a novel variational method for dealing with this is described.  Some empirical comparisons of the deep weight prior with alternative priors is considered, as well as a comparison of deep weight samples for initialization with alternative initialization schemes.  

This is an interesting paper.  It is mostly clearly written, but there is a lack of detail in Section 4 that makes it hard for me, at least, to understand exactly what was done there.  I think the originality level of the paper is high.  The issue of informative priors in these complex models seems wide open and the authors provide an interesting approach both conceptually and computationally.  I did wonder whether there was any link between the suggested priors and the idea of modelling the current and related data sets used in constructing the prior jointly, with data set specific parameters given an exchangeable prior?  This would be a standard hierarchical modelling approach.  Such an approach would not be computationally attractive, I just wondered if there is some conceptual link with the current method being an approximation of that approach in some sense.  In Section 4.1, it seems that for the trained networks on the source datasets, point estimates of the filter weights are treated as data for learning the variational autoencoder - is that correct?  Could you model dataset heterogeneity here as well?  Presumably the p_l(z) density is N(0,I)?  Details of the inference and reconstruction networks are sketchy.  In Section 4.2, you say that the number of filters is proportional to the scale parameter k and that you vary k.  What scale parameter do you mean?  


","The reviewer explicitly states ""This is an interesting paper"" and that it has ""high originality"". They also provide constructive criticism and ask questions for clarification, indicating a positive attitude towards the paper. The language used is formal and polite throughout the review.",75.0,90.0
"This paper studies the problem of *learning* online combinatorial algorithms via Reinforcement Learning. In particular, the paper studies three different well-studied problems, namely AdWords/Online Matching, Online Knapsack, and Secretary Problem. The common thread to all three problems is that they are special cases of Online Packing problem and that there exist optimal algorithms with theoretical guarantees. Moreover, all these problems have an algorithm based on the unifying Primal-Dual framework (*). The paper runs extensive experiments and shows that the learned RL policies resemble the algorithms created in theory by comparing many properties of the returned policy. Overall I think this paper tackles a very interesting question, is well-written and has extensive experiments. 

I will detail my technical comments and questions to authors. I would especially appreciate detailed answers to (1), (2), (3). (4) and (5) are more open-ended questions and/or beyond the scope of the current paper. It can be viewed as feedback for some future work.

(1) (*) The paper starts with the claim that one of the key insights to this work is the primal-dual framework. Yet this has not been exploited in the paper; at least I can't see where it is used! Can the authors give more details? For example, one way I could see this being used is as follows. In the AdWords problem, the evolution of the Dual variable is well understood (See [Devanur, Kleinberg, Jain '13]). One can experiment to see how the dual values change over the run of the algorithm for the learned policy and compare that with how it changes in the analysis. If they are similar, then that is yet another evidence that the two algorithms are similar.

(2) One key point to note is that all three algorithms include only the ""primal"" observations in the state. This strengthens this work since all these algorithms, in theory, are analyzed by realizing that the primal algorithm can be interpreted as an appropriate process on the evolution of the dual variables.  Thus it seems like the RL policy is actually learning the optimal way to set the dual variables in the online phase. Is this true? I guess the experiments above can indeed verify this. If this is true, it implies that the key message of this paper is that RL algorithms can be used to learn algorithms that can be analyzed via the primal-dual framework. Right now, the authors stop short of this by saying this work is inspired from it. It would be good to see this taken to completion.

(3) It seems like there has been some work on using RL to learn algorithms in combinatorial optimization (see [Dai et al., NIPS 2017]). Can the authors discuss both in the rebuttal and in the paper on how their work compares and differs from this work? 

(4) I wonder if the authors experimented with the i.i.d. arrival process for Knapsacks and/or Online Matching/Adwords. It is known that the theoretical algorithms for both these problems do much better than the pessimistic adversarial arrival order. It will be interesting to see if the RL policies also find this. On a related note, did the authors try this on b-matching with b=1? The problem tends to get easier as b is large and/or when bid/budget ratio is small in Adwords. However even when b=1, in theory, we can get 1-1/e [KVV '90].

(5) Finally, I am curious if the authors tested this on problems that are not packing but covering and/or mixed packing and covering problems. Online Set Cover is a candidate example. The other direction is also to test Online Minimization problems. Note that Online Min-cost matching is significantly harder than Online maximum weight matching. Moreover, Online Min-cost matching does not have a primal-dual analysis (to the best of my knowledge). The latter helps because if the RL policy fails to learn the best-known algorithm, then it is further evidence that it is indeed learning through the primal-dual framework.

Some minor comments:

Styling is not consistent throughout the paper. For example, there are places with the header followed by a colon sometimes, a period other times and sometimes no punctuation. Please make these consistent throughout the paper.

The fonts on the figures are very small to read. It would be easy on the reader if the sizes were made larger.


","The reviewer provides generally positive feedback, noting the paper tackles an interesting question, is well-written, and includes extensive experiments. While they have pointed questions and suggestions for improvement, these are framed constructively and with the aim of enhancing the paper's contribution. The language is formal, academic, and overall polite.",75.0,80.0
"DOM-Q-NET: GROUNDED RL ON STRUCTURED LANGUAGE 

This paper presents a somewhat novel graph-based Q-learning method for web navigation benchmark tasks. The authors show that multi-task learning helps in this case and their method is able to learn without BC as previous works have needed. While this work is interesting and to my knowledge somewhat novel. I concerns with one aspect of the evaluation. In some part it was stated that they show the highest success rate for testing on 100 episodes, if this is indeed the maximum success rate, it is unclear if these results are misleading or not. It is possible that there was a lucky seed in those 100 episodes leading to a higher max that is not representative of the algorithm performance. Also, please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which are pointed out below. I am rating marginally below due mainly to the potentially misleading results from the comment on using the highest success rate to report results and to a minor extent due to the novelty aspect (though this is an interesting application).


Comments:

- “Evaluation metric: we plot the moving average of the reward for last 100 episodes, and report the highest success rate for testing on 100 episodes.” —> This is unclear, do you mean you only displayed the maximum success rate out of all 100 episodes? So if the success rates are [0, 100, 0, 0, 0], Figure 2 shows 100% success? If so, this is somewhat misleading and a better metric may have been the average success rate with confidence intervals. Otherwise you may have just gotten a lucky random seed potentially.
- I would’ve liked to see if this is the only method which benefits from multitask learning or do DOMNETs also benefit. This however, is just a nice to have.
- I appreciate the inclusion of hyper parameters and commitment to releasing the code in an effort to promote reproducibility! Great job there. 
- I really like the idea of using graph networks with RL, though I’m not sure if it’s novel to this work. Interesting line of work!
- While this is an interesting application, I’m not sure about the novelty. I suggest spending a bit more time discussing how this work contrasts with methods like Wang et al., or others cited here.

Typos:

“MiniWoB(Shi et al., 2017) benchmark tasks. “ —> missing space between citation
“Q network architecture with graph neural network” —> with a graph neural network
""MiniWoB(Shi et al., 2017)” —> MiniWoB (Shi et al., 2017) (missing space)
“achieved the state” —> achieved state of the art 
“2016; Wang et al., 2018)as main” —> missing space
“series of attentions between DOM elements and goal” —> series of attention (modules?) between the DOM elements and the goal (?)
“constrained action set” —> constrained action sets
“In appendix, we define our criteria for difficulties of different tasks.” —> In the appendix","The reviewer finds the work interesting and novel in its application of graph networks to RL for web navigation. They praise the authors for promoting reproducibility. However, they express concerns about the evaluation metric, suggesting it might be misleading and recommending alternative approaches. The reviewer also points out areas for improvement in novelty discussion and writing clarity. Overall, the feedback is constructive and suggestive of improvements, leaning towards the positive side due to the interest in the work.",60.0,80.0
"This paper proposes a well-designed instance level unsupervised image-to-image translation method which can handle the arbitrary number of instances in a permutation-invariant way. The idea is interesting and the results on various translation datasets are reasonable.  

Pros:
* The proposed method process each instance separately to handle multiple instances. The summarization operation is a simple but effective way to achieve the permutation-invariant property. The context preserving loss is suitable for preserving the background information.
* The paper is well written and easy to follow.

Cons:
* My main concern is about the comparisons with CycleGAN in Figure 4 to 6. Although the CycleGAN+Seg results are shown in Figure 9 indicating that the proposed method can handle multiple instances better. I think there should also be CycleGAN+Seg results in Figure 4 to 6, since the instance segmentation is an extra information. And in my opinion, the CycleGAN+Seg can handle the situation where there are only a few instances (also can be observed in the 1st row in Figure 9). Besides, CycleGAN+Seg can naturally handle the arbitrary number of instances without extra computation cost.

Questions:
*  I wonder what will happen if the network does not permutation-invariant. Except that the results will vary for different the input order, will the generated quality decrease? Since the order information may be useful for some applications.

Overall, I think the proposed method is interesting but the comparison should be fairer in Figure 4 to 6. 
","The reviewer acknowledges the paper's merits (well-designed method, interesting idea, reasonable results, well-written) which points towards a positive sentiment. However, they raise a significant concern regarding the fairness of comparisons, suggesting the authors may have omitted results that could challenge their method's superiority. This casts a slightly less positive light on the review. Overall, the tone is professional and constructive, suggesting a neutral-to-positive politeness.",60.0,70.0
"This paper presents a system that infers programs describing 3D scenes composed of simple primitives. The system consists of three stages each of which is trained separately. First, the perceptual module extracts object masks and their attributes. The objects are then are split into several groups. Finally, each group is mapped to a corresponding DSL program using a sequence-to-sequence network similar to the ones typically employed in neural machine translation.

Pros:
+ The paper is written clearly and easy to read.
+ Visual program synthesis is very exciting and important direction both for image understanding and generation.
+ The results on synthetic datasets are good. The authors also demonstrate the applicability of the approach to real-world data (albeit significantly constrained).
+ I find it surprising that a seq2seq is good at producing an accurate program for a group of objects.
+ Visual analogy making experiments are impressive.

Cons:
- The proposed model requires rich annotation of training data since all the components of the systems are trained in a supervised fashion. It’s not clear how to use the method on the in-the-wild data without such annotation.
- Related to the previous point, even when it’s possible to synthesize data, it is non-trivial to obtain the ground-truth grouping of objects. Judging by Table 2, it seems that the system breaks in absence of the grouping information.
- The data used in the paper is quite simplistic (limited number of primitives located in a regular grid). I’m wondering if there is a natural way to extend the approach to more complex settings. My guess is that the performance will drop significantly.

Notes/questions:
* Section 2, paragraph 1: The paper by [Ganin et al., 2018] presents both a system for reproducing an image as well as for sampling from a distribution; moreover, it presents experiments on 3D data (i.e., not limited to drawing).
* Section 3.4, paragraph 2: I’m not sure I understand the last sentence. How can we know that we successfully recovered the scene at test time? Could the authors elaborate on the stopping criterion for sampling?
* Section 4.2, paragraph 2: Do I understand correctly that the main difference between the test set and the generalization set is the number of groups? (i.e., 2 vs 3). If so, it’s a fairly limited demonstration of generalization capabilities of the system.
* Section 4.2, paragraph 4: “we search top 3 proposals ...” – How do we decide which one is better? Do we somehow have an access to the ground truth program at test time?
* Could the authors explain the representation of a program more clearly? How are loops handled? How can one subtract/add programs in the analogy making experiment?

Overall, I think it is a interesting paper and can be potentially accepted on the condition that the authors address my questions and concerns.","The reviewer provides both positive and negative points about the paper, ultimately stating that it is interesting and could be accepted with some revisions. This suggests a slightly positive sentiment overall. The language used is constructive and professional, suggesting a neutral to polite tone.",60.0,70.0
"Given an additively decomposable function F(X, Q) = sum_over_x_in_X cost(x, Q), one can approximate it using either random sampling of x in X (unbiased, possibly high variance), or using importance sampling and replace the sum_over_x with a sum_over_coreset importance_of_a_point * cost(x, Q) which if properly defined can be both unbiased and have low variance [1]. In this work the authors consider the weighted sum of activations as F and suggest that for each neuron we can subsample the incoming edges. To construct the importance sampling strategy the authors adapt the classic notion of sensitivity from the coreset literature. Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points. The performed analysis is sound (up to my knowledge).

Pro:
- I commend the authors for a clean and polished writeup.
- The analysis seems to be sound (apart from the issues discussed below)
- The experimental results look promising, at least in the limited setup.

Con:
- There exists competing work with rigorous guarantees, for example [2].
- The analysis hinges on two assumptions which, in my opinion, make the problem feasible: having (sub) exponential tails allows for strong concentration results, and with proper analysis (as done by the authors), the fact that the additively decomposable function can be approximated given well-behaving summands is not surprising. The analysis is definitely non-trivial and I commend the authors for a clean writeup.
- While rigorous guarantees are lacking for some previous work, previously introduced techniques were shown to be extremely effective in practice and across a spectrum of tasks. As the guarantees arguably stem from the assumptions 1 and 2, I feel that it’s unfair to not compare to those results empirically. Hence, failing to compare to results of at least [2, 3] is a major drawback of this work.
- The result holds for n points drawn from P. However, in practice the network might receive essentially arbitrary input from P at inference time. Given that we need to decide on the number of edges to preserve apriori, what are the implications?
- The presented bounds should be discussed on an intuitive level (i.e. the number of non zero entries is approximately cubic in L).

I consider this to be a well-executed paper which brings together the main ideas from the coreset literature and shows one avenue of establishing provable results. However, given that no comparison to the state-of-the-art techniques is given I'm not confident that the community will apply these techniques in practice. On the other hand, the main strength -- the theoretical guarantees -- hinge on the introduced assumptions. As such, without additional empirical results demonstrating the utility with respect to the state-of-the-art methods (for the same capacity in terms of NNZ) I cannot recommend acceptance.

[1] https://arxiv.org/abs/1601.00617
[2] papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee
[3] https://arxiv.org/abs/1510.00149


========
Thank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6. ","The review starts with positive remarks, highlighting the paper's clarity and sound analysis. However, it then raises significant concerns about the assumptions made and the lack of comparison with state-of-the-art methods. While acknowledging the theoretical contributions, the reviewer expresses doubts about the practical utility and doesn't recommend acceptance without further empirical evidence. The update from 5 to 6 suggests a slight improvement in the reviewer's opinion, likely due to the authors addressing some of the concerns, but not enough for a strong acceptance.",40.0,70.0
"The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.

The introduction is vague and involves undefined terms such as ""useful habits"". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?

The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.","The reviewer provides a negative review, stating that the paper's motivation and experimental results are insufficient. They find the introduction vague and the method presentation unclear. The reviewer also criticizes the experiments for lacking comparison to relevant baselines. However, the language used, while direct, maintains a professional and critical tone without resorting to personal attacks or disrespectful language.",-50.0,50.0
"This paper proposes an unsupervised style transfer method uses two-pathway encoder and a decoder for both domains. The loss function can be written using reconstruction losses and the confusion term. Experimental results are very promising comparing to state of the art methods. 

The methodology presented in this paper is simple yet powerful according to the experimental results. However I do have a few concerns:   

1. The writing can certainly be improved.  I had a difficult time understanding Section 2. For example the function Q is upper cased but later the f and g are all lower cased. Why domains A and B are defined using the space and the probability measure? ""our framework assumes that the distribution of persons with sunglasses and that of persons without them is the same,"" The ""distribution of persons"" is not a rigorous definition and is hard to infer what does it actually mean. ""f"" does not appear in the loss terms although it appears under ""min"". 

2. I like the simplicity of the objective function, but it is hard for me to understand that why the algorithm does not pick up spurious differences between A and B. For example, what if there are lighting differences and glasses/no-glasses differences between A and B? See 3rd row of figure 2 for an example. 

3. Given the huge differences in performance between the proposed method and MUNIT and DRIT, some analysis/discussion on the reason of success/failure should be given.

--------------------------------------------------------

I have read authors' response. ","The review starts with positive remarks, highlighting the promising results and the simplicity of the method. However, it then raises several concerns about clarity, rigor, and potential limitations, suggesting areas for improvement. The reviewer acknowledges the authors' response, indicating an ongoing dialogue. Overall, the tone is constructive and polite, aiming to guide the authors towards a stronger paper.",50.0,70.0
"The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?","The review acknowledges the merit of the proposed approach (""These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately"") but raises significant concerns about the theoretical contributions and experimental validation. The numerous questions indicate a lack of clarity and completeness in the presented work. Overall, the tone is critical but professional, suggesting areas for improvement rather than outright rejection.",-20.0,60.0
"Paper overview: Model ensembling techniques aim at improving machine learning model prediction results by i) executing several different algorithms on the same task and ii) solving the discrepancies in the responses of all the algorithms, for each task. Some common methods are voting and averaging (arithmetic or geometric average) on the results provided by the different algorithms. 
Since averaging amounts to computing barycenters with different distance functions, this paper proposes to use the Wassertein barycenter instead of the L2 barycenter (arithmetic average) or the extended KL barycenter (geometric mean). 

Remarks, typos and experiences that would be interesting to add: 
     1) Please define the acronyms before using them, for instance DNN (in first page, 4th line), KL (also first page), NLP, etc. 
    2) In practice, when ensembling different methods, the geometric and arithmetic mean are not computed with equal weights ($\lambda_l$ in Definition 1). Instead, these weights are computed as the optimal values for a given small dev-set. It would be interesting to see how well does the method compare to these optimal weighted averages, and also if it improves is we also compute the optimal $\lambda_l$ for the Wasserstein barycenter. 
    3) How computationally expensive are these methods? 
    4) So the output of the ensembling method is a point in the word embedding space, but we know that not all points in this space have an associated word, thus, how are the words chosen?
    5) The image captioning example of Fig.4 is very interesting (although the original image should be added to understand better the different results), can you show also some negative examples? That is to say, when is the Wassertein method is failing but not the other methods.


Points in favor: 
     1)Better results: The proposed model is not only theoretically interesting, but it also improves the arithmetic and geometric mean baselines.
    2) Interesting theoretical and practical properties: semantic accuracy, diversity and robustness (see Proposition 1). 

Points against: The paper is not easy to read. Ensembling methods are normally applied to the output of a classifier or a regression method, so it is not evident to understand why the 'underlying geometry' is in the word embedding space (page 2 after the Definition 1). I think this is explained in the second paragraph of the paper, but that paragraph is really not clear. I assume that is makes sense to use the word-embedding space for the image caption generation or other ML tasks where the output is a word, but I am not sure how this is used in other cases. 

Conclusion: The paper proposes a new method for model assembling by rethinking other popular methods such as the arithmetic and geometric average. It also shows that it improves the current methods. Therefore, I think it presents enough novelties to be accepted in the conference.","The reviewer provides constructive criticism, acknowledges the merits of the paper (improved results, interesting theoretical and practical properties), and ultimately recommends acceptance. While they point out areas for improvement (clarity, additional experiments), their tone remains professional and encouraging. Therefore, the sentiment is positive, but not overly enthusiastic, and the politeness is high.",65.0,80.0
"This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims.

Pros:
* The problem is interesting and well explained
* The proposed method is clearly motivated
* The proposal looks theoretically solid

Cons:

* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.

* Fig. 3 needs more explanation. The horizontal axes are unlabelled, and ""margin normalization"" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.

* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?

* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.

A typo in page 6, last line: wth -> with","The review starts with positive remarks, highlighting interesting aspects of the paper. However, it then raises several significant concerns about the clarity, methodology, and ultimately the strength of the results. The reviewer remains professional and constructive throughout, but the extent of the issues pointed out suggests a lukewarm reception overall.",20.0,70.0
"Overview:

This paper proposes a new approach to do unsupervised phoneme recognition by learning from unlabelled speech in combination with a trained phoneme language model. The proposed loss function is a combination of a term encouraging the language model of predicted phonemes to match the given language model distribution, and a term to encourage adjacent speech frames to be assigned to the same phoneme class. Phoneme boundaries are iteratively refined using a separate model. Experiments where a hidden Markov model is applied on top of the predicted phonemes are also performed.


Main strengths:

The paper is clear and addresses a very important research problem. The approach and losses proposed in Section 2 have also not been proposed before, and given that an external language model is available, are very natural choices.


Main weaknesses:

The main weakness of this paper is that it does not situate itself within the rich body of literature on this problem.  I give several references below, but I think the authors can include even more studies -- there are several studies around ""zero-resource"" speech processing, and I would encourage the authors to work through the review papers [1, 6].

Concretely, I do not think the authors can claim that ""this is the first fully unsupervised speech recognition method that does not use any oracle segmentation or labels."" I think it could be argued that the system of [3] is doing this, and there are even earlier studies. I also don't think this claim is actually necessary since the paper has enough merit to stand on its own, as long as the related work is discussed properly.

For instance, the proposed approach shares commonalities with several other approaches: [2] also used two separate steps for acoustic modelling and boundary segmentation; [4, 7, 8] builds towards the setting where non-matching text data is available (for language model training) together with untranscribed speech for model development; the approach of [5] uses a very similar refinement step to the one described in Section 3, where an HMM model is initialised and retrained on noisy predicted labels.

In the experiments (Section 4), it would also be useful to report more fine-grained metrics. [6] gives an overview of several of the standard metrics used in this area, but at a minimum phoneme boundary recall, precision and F-scores should be reported in order to allow comparisons to other studies.


Overall feedback:

Given that this paper is situated within the broader context of this research area, which already has a small community around it, I think the novelty in the approach is strong enough to warrant publication given that the additional metrics are reported in the experiments.


Papers/links that should be reviewed and cited:

1. E. Dunbar et al., ""The Zero Resource Speech Challenge 2017,"" in Proc. ASRU, 2017.
2. H. Kamper, K. Livescu, and S. Goldwater. An embedded segmental k-means model for unsupervised segmentation and clustering of speech. in Proc. ASRU, 2017.
3. Lee, C.-y. and Glass, J. R. A nonparametric Bayesian approach to acoustic model discovery. ACL, 2012.
4. Ondel, Lucas, Lukaš Burget, Jan Černocký, and Santosh Kesiraju. ""Bayesian phonotactic language model for acoustic unit discovery."" In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pp. 5750-5754. IEEE, 2017.
5. Walter, O., Korthals, T., Haeb-Umbach, R., and Raj, B. (2013). A hierarchical system for word discovery exploiting DTW-based initialization. ASRU, 2013.
6. M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, ""The Zero Resource Speech Challenge 2015: Proposed approaches and results,” in Proc. SLTU, 2016.
7. https://www.clsp.jhu.edu/wp-content/uploads/sites/75/2018/05/jsalt2016-burget-building-speech-recognition.pdf
8. https://www.clsp.jhu.edu/workshops/16-workshop/building-speech-recognition-system-from-untranscribed-data/

","The review is positive in its assessment of the paper, highlighting its clarity, relevance, and novelty. While it points out a significant weakness regarding the positioning against existing literature, it acknowledges the paper's merit and suggests improvements for a stronger submission. The language used is constructive and objective, typical of academic peer reviews.",60.0,70.0
"This work is an extension to the work of Sukbaatar et al. (2016) with two main differences:
1) Selective communication: agents are able to decide whether they want to communicate.
2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal.
These two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. 
The paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following:
 
Method:

In a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and  more realistic (e.g., something like the work of Mordatch & Abbeel, 2017)

Experiment:

The experiment ""StarCraft explore"" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing.
 
Authors provide 3 baselines:
1) no communication, but IR
2) no communication, no IR
3) global communication, no IR (commNet)

I think having a baseline that has global communication with IR can show the effect of selective communication better. 

There are some questions in the experiment section that have not been addressed very well. For example:
 Is there any difference between the results of table 1, if we look at the cooperative setup? 
Does their model outperform a model which has global communication with IR? 
Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? 
Why is CommNet work worse than IRIC and IC in table 2?","The reviewer provides a generally positive overview of the paper, praising its clarity, experimental setup, and the interesting nature of the extensions. However, they also raise valid concerns and questions about the methodology and results, suggesting areas for improvement and further investigation. The tone is constructive and professional throughout.",50.0,75.0
"After the rebuttal:

1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is ""using soft instead of hard constraint"" for BNN. 

2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].

---------------------------
This paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited.

1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example,
- A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training
- Training Ternary Neural Networks with Exact Proximal Operator
- Loss-aware Binarization of Deep Networks

All above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). 

2. Could authors perform experiments with ImageNet?

3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda","The reviewer acknowledges the paper's clarity and interesting connections but expresses concerns about limited novelty. They point out existing works that diminish the claimed contribution and suggest the main difference is a technical detail. The reviewer's questions about experiments and parameter impact further indicate a desire for stronger validation. The tone is critical but professional, focusing on content rather than personal criticism.",-20.0,60.0
"The paper considers a shortcoming of sequence to sequence models trained using maximum likelihood estimation. In particular, a model trained in this way can be biased in the sense that training sequences typically have different sets of prefixes compared to test sequences. As a result, at the prediction time the model does not generalize well and for a given input sequence the decoder constructs a label sequence which reflects the training label sequences rather than the actual target label.

To address this shortcoming, the authors propose an approach based on edit distances and the implicit use of given label sequences during training. The main idea is to generate a label sequence with respect to the current parameter vector of a conditional probabilistic model (see Eqs. 2 & 3, as well as the objective in Eq. 6) and then based on the edit distance find the best possible completions for any prefix of that model-based label sequence. The training objective is then defined in Eq. (6): to each element in the output sequence the objective assigns the KL-divergence between a conditional distribution of the next element in the label sequence given a label prefix generated using the current model and the exponential family model based on edit distances given by the prefixes and optimal completions after the position of interest in the label sequence. The objective and the corresponding gradient can be computed efficiently using dynamic programming.

Intuitively, the approach tries to find a parameter vector such that the decoder at a particular instance is likely to construct a label sequence with a small edit distance to the target label. As the training objective now considers all possible elements of the vocabulary given a prefix sequence, it is reasonable to expect that it performs better than MLE which only considers target vocabulary elements given target prefix sequences (e.g., compare Eqs. 3 & 6).

The experiments were conducted on the `Wall Street Journal' and `Librispeech' datasets and the reported results are a significant improvement over the state-of-the-art. I am not an expert in the field and cannot judge the related work objectively but can say that the context for their approach is set appropriately. I would, however, prefer more clarity in the presentation of the approach. This especially applies to the presentation of the approach around Eq. (6). It might not be straightforward for a reader to figure out how the tilde-sequences are obtained. As the objective is non-convex, in order to be able to reproduce the results it would be useful to provide some heuristics for choosing the initial solutions for the parameter vector. In Section 3, please also provide a reference to the appendix so that a reader can understand the conditional probabilistic model.","The reviewer provides a generally positive overview of the paper, acknowledging the significance of the research and the improvement over state-of-the-art. While they have some reservations about the clarity of the presentation, particularly around Equation 6, and suggest improvements for reproducibility, the overall tone is constructive and encouraging. The reviewer's request for more clarity and reproducibility suggestions point to areas of improvement rather than fundamental flaws.",75.0,75.0
"This paper proposes a novel approach with the hypothesis that the reliable features can guide the less reliable ones. This approach is applied to the object detection task and show consistent performance improvements.

pros)
(+) This paper is well-written and easy to follow.
(+) The base idea that divides the learned features into two sets; the reliable feature set and the less reliable one is very interesting and looks novel. Plus, the hypothesis, which is that reliable features can guide the features in the less reliable set is also interesting.
(+) The performance improvements are quite large.
(+) Extensive ablative studies are provided to support the proposed method well.

cons)
(-) The method of obtaining the representative in buffer B is not clearly presented.
(-) The overall training and inference procedure are not clearly presented. 
(-) Some notations and descriptions are vague and confusing.
(-) More than two datasets are necessary to show the effectiveness of the methods

comments)
- What is the higher level feature map P_m? and How did you choose the higher level feature map at the m-th level in option (b) and (c) in Section 3.3.
- What is the meaning of the ""past"" features in Section 3.2?
- It is better to show the exact architecture of the make-up module and the critic module.
- Can this method apply to the other backbones such as VGG or ResNets without FPN?
- The sentences at the bottom of p.4 starting with ""Note that only~"" looks ambiguous. 
- f_critic^j may be the j-th element of F_critic, please denote what f_critic^j stands for.

Even if the paper needs to be revised for better readability, I think this paper is above the standard of ICLR because the idea is interesting and novel. Furthermore, the experimental studies are properly designed and well support the main idea. I am leaning toward acceptance, but I would like to see the other reviewers' comments.","The reviewer provides both positive and negative points, but leans towards acceptance and finds the paper above standard. They highlight the novelty and interesting aspects, as well as the well-designed experiments. While they point out areas for improvement like clarity and additional datasets, the overall tone suggests a positive disposition towards the paper.",65.0,75.0
"The major contribution of this work is extending routing networks (Rosenbaum et al., ICLR 2018) to use diverse architectures across routed modules. I view this as an important contribution and am very impressed by the experiment on Omniglot where it shows big performance gain on a split with very few examples. This idea of incorporating in architectural bias and not just parameter bias for small data problems is very compelling and intuitive to me on the surface. The ablation study was also very interesting in this regard. I really like the discourse and found it to be filled with interesting insights throughout ranging from the connection between routing networks and neural architecture search to the heuristic for selecting k.  However, after the great discourse, I was quite disappointed by the breadth of the experiments. 

The paper is positioned as exploring two parallel ideas that are independently interesting 1) diversity in the architecture of modules in routing models 2) the effect of increasing depth in routing models. For the first idea, this is shown very well by the Omniglot experiment but is not evaluated in any other setting. Showing this in a few other experiments would have really driven this point home in my opinion.  The second idea is not really executed in a convincing way to me. The authors call it a ‘negative result’ in the end, but I’m not sure I really feel like I learned anything from this experiment. I wonder about statistical significance. I also feel like the authors are trying to turn it into a commentary that this is a pain point for all variants of routing models while they only actually tried it for their proposed architecture which makes quite a few decisions along the way. I would have liked to see more model variants and datasets before really feeling like I can make any empirical determinations about the fundamental limitations of all routing models in this regard.  Additionally, if there were such a fundamental scaling limitation, you would imagine that an experiment could be constructed that really highlighted this fact where all routing models do way worse.

In short, I think there are some really good idea in this paper and vote for acceptance on that basis. Had the authors provided more empirical evidence about architectural diversity, I would have given it a very high score. The analysis of depth is also a very interesting topic, but it could possibly even serve as another paper considering that the current results don’t really come to concrete conclusions for the community. 
","The reviewer explicitly mentions ""really good ideas"" and ""vote for acceptance"", which indicates positive sentiment. However, the reviewer also expresses disappointment with some aspects, such as the breadth of experiments, calling them ""quite disappointed"" and wanting more evidence. This suggests the sentiment is positive but with reservations. Therefore, a score of 50 seems appropriate. The language used is polite and professional throughout, with constructive criticism and suggestions for improvement. There's no use of harsh language or personal attacks, maintaining a respectful tone. Hence, a politeness score of 100 is justified.",50.0,100.0
"The paper proposes a method for learning regression models through evolutionary
algorithms that promise to be more interpretable than other models while
achieving similar or higher performance. The authors evaluate their approach on
99 datasets from OpenML, demonstrating very promising performance.

The authors take a very interesting approach to modeling regression problems by
constructing complex algebraic expressions from simple building blocks with
genetic programming. In particular, they aim to keep the constructed expression
as small as possible to be able to interpret it easier. The evaluation is
thorough and convincing, demonstrating very good results.

The presented results show that the new method beats the performance of existing
methods; however, as only very limited hyperparameter tuning for the other
methods was performed, it is unclear to what extent this will hold true in
general. As the main focus of the paper is on the increased interpretability of
the learned models, this is only a minor flaw though.

The interpretability of the final models is measured in terms of their size.
While this is a reasonable proxy that is easy to measure, the question remains
to what extent the models are really interpretable by humans. This is definitely
something that should be explored in future work, as a small-size model does not
necessarily imply that humans can understand it easily, especially as the
generated algebraic expressions can be complex even for small trees.

The description of the proposed method could be improved; in particular it was
unclear to this reviewer why the features needed to be differentiable and what
the benefit of this was (i.e. why was this the most appropriate way of adjusting
weights).

In summary, the paper should be accepted.","The review is overwhelmingly positive. The reviewer highlights the novelty and promising performance of the proposed method. While pointing out areas for improvement, such as the need for further investigation into the interpretability of the models and a clearer explanation of certain aspects of the method, these are presented as constructive suggestions rather than harsh criticisms. The language used is formal and respectful throughout.",85.0,90.0
"Quality: This submission claims to present a model that can control non-annotated attributes such as speaking style, accent, background noise, etc. Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. For example a reliable numerical evidence is needed on page 4 following ""We also found..."", page 5 following ""We discovered...."", page 5 following ""It clearly presents..."", page 5 following ""Drawing samples..."" evidence is given only for 1 dimension, page 6 following ""Figure 7(b)..."". 

Clarity: The model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. What are different modes using the proposed model? Why both negative results are in the appendix? 

Originality: moderately

Significance: moderately
","The review is lukewarm, noting that while the model is simple and moderately original and significant, it requires more evidence to support its claims. The reviewer points out several instances where further evidence is needed. The language used is generally neutral and professional, directly addressing the authors and providing constructive feedback.",20.0,60.0
"This paper introduces a generative model for question answering.  Instead of modeling p(a|q,c), the authors propose to model p(q,a|c), factorized as p(a|c) * p(q|a,c).  This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working.                                                       
                                                                                                     
Reactions:                                                                                           
- Section 2.1: Is there a bias problem here, where you're only ever training with the correct answer?  Oh, I see you covered that in section 2.6.  Great.
- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?
- Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?
- Section 3.1 (and section 2.6): Can you back up your claim of ""modeling more complex dependencies"" in the generative case?  Is that really what's going on?  How can we know?  What does ""modeling more complex dependencies"" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.
- Section 3.3: Your goal here seems similar to the goal of Clark and Gardner (2018), trying to correctly calibrate confidence scores in the face of SQuAD-like data, and similar to the goals of adding unanswerable questions in SQuAD 2.0.  I know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.
                                                                                                     
Clarity issues:                                                                                      
- Bottom of page 2, ""sum with a vector of size d"" - it's not clear to me what this means.            
- Top of page 3, ""Answer Encoder"", something is off with the sentence ""For each word representation"" 
- Section 2.5, ""we first embed words independently of the question"" - did you mean ""of the _context_""?
- Section 2.5.2 - it's not clear to me how that particular bias mechanism ""allows the model to easily filter out parts of the context which are irrelevant to the question"".  The bias mechanism is independent of the question.
- Section 2.7 - when you said ""beam search"", I was expecting a beam over the question words, or something.  I suppose a two-step beam search is still a beam search, it just conjured the wrong image for me, and I wonder if there's another way you can describe it that better evokes what you're actually doing.
- Section 3.1 - ""and are results..."" - missing ""competitive with""?                                   
- Last sentence: ""we believe their is"" -> ""we believe there is"" ","The reviewer explicitly states ""This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working."" This indicates a positive sentiment. While the review contains numerous questions and suggestions for improvement, these are constructive and presented in a collegial manner. The reviewer aims to help the authors improve their work rather than criticize it.",80.0,75.0
"This paper first identifies an inequivalence between L2 regularization and the original weight decay in adaptive stochastic gradient methods, e.g., the Adam method, and then proposes two decoupled variants, SGDW and AdamW, respective. The authors also cited a recent work to provide a justification of their proposed update rules from the perspective of Bayesian filtering. To demonstrate the effectiveness of both methods, experiments on CIFAR10 and ImageNet32x32 are conducted to compare with the original methods. Results show that the proposed methods consistently lead to faster convergence. Overall the paper is well written and easy to follow, with enough details describing the experimental settings. 

First of all I appreciate the authors pointing out that weight decay is not equal to L2 regularization in general. This is evident once the original definition of weight decay is given. The main motivation comes from the argument that instead of using L2 regularization, weight decay should be used in adaptive gradient methods. The Bayesian filtering interpretation helps to justify the proposed method. But it is not clear to me why the hyperparameters w and \alpha are decoupled in the proposed methods? For example, in Line 6 of Alg. 1, g_t is a function of w, and later in Line 8, g_t is coupled with \alpha which naturally introduces a term w \alpha into m_t. So both w and \alpha are still coupled together in the proposed algorithm. If this is the case why the authors still call w and \alpha decoupled? 

To me the most interesting result is Proposition 3 where the authors show that weight decay actually corresponds to preconditioned L2 regularization. This helps to explain what's the algorithmic difference between these two methods in adaptive gradient methods, and provides an intuitive insight on why weight decay may lead to better results compared with the vanilla L2 regularization. 

Experiments on image recognition tasks basically confirm the authors' claims. However, as the authors have already pointed out, it is better to have more thorough experiments on other kinds of tasks, e.g., in text classification, etc. If the improvement does come from the difference between weight decay vs L2, then I would also expect the same improvement on other tasks. It would be great to see more experimental results on other tasks to have a better understanding of this problem. So far it is not clear whether the same improvement holds in general or not. 
","The reviewer provides a generally positive assessment of the paper, acknowledging its clarity, contribution to understanding the difference between weight decay and L2 regularization, and the promising experimental results. However, they also raise a valid concern about the decoupling claim and suggest further experiments for stronger validation.  The language is constructive and professional throughout.",60.0,80.0
"The paper introduces a decentralized training method for multi-agent reinforcement learning, where the agents infer the policies of other agents and use the inferred models for decision making. The method is intuitively straightforward and the paper provides some justification for convergence. I think the underlying theory is okay (new but not too surprising, a lot of the connections can be made with single agent RL), but the paper would be much stronger with experiments that have more than two players, one state and one dimensional actions.

(
Update: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. 

However, I wonder if the reasoning for PR2 is limited to ""self-play"", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.

Also, maybe explain self-play mathematically to make the paper self contained?
)

1. From the abstract, "" PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario"". Theorem 2 only shows that under relatively strong assumptions (e.g. single Nash equilibrium), the soft value iteration operator is a contraction. This seems to have little to do with the actual convergence of PR2-Q and PR2-AC, especially AC which uses gradient-based approach. Also here the ""convergence"" in the abstract seem to imply convergence to the (single) global optimal solution (as is shown in the experiments), for which I thought you cannot prove even for single agent AC -- the best you can do is to show that gradient norm converges to zero, which gives you a local optima. Maybe things are different with the presence of the (concave) entropy regularization?

2. Theorem 2 also assumes that the opponent model $\rho$ will find the global optimal solution (i.e. (11, 12) can be computed tractably). However, the paper does not discuss the case where $\rho$ or $Q_\theta$ in question is imperfect (similar to humans over/underestimate its opponents), which might cause the actual solution to deviate significantly from the (single) NE. This would definitely be a problem in more high-dimensional MARL scenarios. I wonder if one could extend the convergence arguments by extending Prop 2.

3. The experiments mostly demonstrates almost the simplest non-trivial Markov games, where it could be possible that (11, 12) are true for PR2. However, the effectiveness of the method have not been demonstrated in other (slightly higher-dimensional) environments, such as the particle environments in the MADDPG paper. It does not seem to be very hard to implement this, and I wonder if this is related to the approximation error in (11, 12). The success in such environments would make the arguments much stronger, and provide sound empirical guidance to MARL practitioners.

Minor points:
- Are the policies in question stationary? How is PR2 different from the case of single agent RL (conditioned on perfect knowledge of a stationary opponent policy)?
- I have a hard time understanding why PR2 would have different behavior than IGA even with full knowledge of the opponent policy, assuming each policy is updated with infinitesimally small (but same) learning rates. What is the shape of the PR2 optimization function wrt agent 1?
- I wonder if using 2 layer neural networks with 100 units each on a 1 dimensional problem is overkill.
- Figure 4(a): what are the blue dots?
- Does (11) depend on the amount of data collected from the opponents? If so, how?
- I would recommend combining Prop 1 and prop 2 to save space. Both results are straightforward to prove, but the importance sampling perspective might be useful.
- Have you tried to compare with SGA (Balduzzi et al) or Optimistic mirror descent?
- I am also curious about an ablation study over the components used to infer opponent policies. A much simpler case would be action-dependent baselines, which seem to implicitly use some information about the opponents.","The review is mostly positive. The reviewer acknowledges the theoretical contribution of the paper and finds the method interesting. The reviewer also mentions that the revised version of the paper addresses most of their concerns. However, the reviewer also raises several valid questions and suggests several experiments, indicating that there is still room for improvement. Overall, the tone of the review is polite and professional.",60.0,80.0
"The main contributions of the paper are an edit encoder model similar to (Guu et al. 2017 http://aclweb.org/anthology/Q18-1031), a new dataset of tree-structured source code edits, and thorough and well thought-out analysis of the edit encodings. The paper is clearly written, and provides clear support for each of their main claims.

I think this would be of interest to NLP researchers and others working on sequence- and graph-transduction models, but I think the authors could have gone further to demonstrate the robustness of their edit encodings and their applicability to other tasks. This would also benefit greatly from a more direct comparison to Guu et al. 2017, which presents a very similar ""neural editor"" model.

Some more specific points:

- I really like the idea of transferring edits from one context to another. The one-shot experiment is well-designed, however it would benefit from also having a lower bound to get a better sense of how good the encodings are.

- If I'm reading it correctly, the edit encoder has access to the full sequences x- and x+, in addition to the alignment symbols. I wonder if this hurts the quality of the representations, since it's possible (albeit not efficient) to memorize the output sequence x+ and decode it directly from the 512-dimensional vector. Have you explored more constrained versions of the edit encoder (such as the bag-of-edits from Guu et al. 2017) or alternate learning objectives to control for this?

- The WikiAtomicEdits corpus has 13.7 million English insertions - why did you subsample this to only 1M? There is also a human-annotated subset of that you might use as evaluation data, similar to the C#Fixers set.

- On the human evaluation: Who were the annotators? The categories ""similar edit"", and ""semantically or syntactically same edit"" seem to leave a lot to interpretation; were more specific instructions given? It also might be interesting, if possible, to separately classify syntactically similar and semantically similar edits.

- On the automatic evaluation: accuracy seems brittle for evaluating sequence output. Did you consider reporting BLEU, ROUGE, or another ""soft"" sequence metric?

- It would be worth citing existing literature on classification of Wikipedia edits, for example Yang et al. 2017 (https://www.cs.cmu.edu/~diyiy/docs/emnlp17.pdf). An interesting experiment would be to correlate your edit encodings with their taxonomy.","The review starts with positive remarks, highlighting the contributions and clarity of the paper. While it suggests areas for improvement and further investigation, the overall tone remains constructive and encouraging. The reviewer provides specific, actionable suggestions without resorting to harsh language.",60.0,80.0
"I like the idea of the paper and I believe it addressing a very relevant problem. While the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper. It is surely good to present the toy example with the MNIST dataset but the ethnicity domain is less difficult than what the authors claim. This is also pretty evident from the results presented (e.g., in Table 3). The proposed approach provides maybe slightly better results than the state of the art but the results do not seem to be statistically significant. This is probable also due to the fact that the problem itself is made simpler by the cropped faces, no background, etc. I would have preferred to see an application domain where the improvement would be more substantial. Nevertheless, I think the theoretical presentation is good and I believe the manuscript has very good potential. ","The reviewer starts with positive feedback, highlighting the relevance and good formalization. However, they express concerns about the evaluation, suggesting more challenging experiments are needed. While acknowledging the theoretical strength, they find the empirical results not entirely convincing due to the simplicity of the chosen domain. The language is critical but constructive, suggesting improvements rather than outright rejection.",40.0,60.0
"This paper extends the recent results concerning GP equivalence of infinitely wide FC nets to the convolutional case. This paper is generally of a high quality (notwithstanding the lack of keys on figures) and provides insights to an important class of model. I recommend that this paper be accepted, but I think it could be improved in a few ways. 

Firstly, and rather mundanely: the figures. Fig 1 is not easy to read due to the density of plotting, and as there is no key it isn’t possible to tell what it shows. Figure 2 is rather is called a ‘graphical model’ but the variables (weights and biases) are not shown. It should be specified that this is the graphical model of the infinite limit, in which case the K variables should not be random. Also, the caption on this figure refers to variables that aren’t in the figure, and is grammatically incorrect (perhaps something like ‘the limit of an infinitely wide convolutional’ is missing?). Figure 3 has a caption which seems to be inconsistent with the coloring (for example green is center pixel in the text, but blue in the key). Figure 6 is also missing a key. In Figure 5, what does the tick symbol denote? Finally, the value some of Table 1 is questionable as so many entries are missing. For example, the Fashion-MNIST column has only two values, which seems to me of little use. [I would have given the paper a rating of 7 were it not for these issues]

Regarding the presentation of the content, I found this paper generally easy to follow and the arguments sound. Here are few points:

There is an important distinction between finite width Bayesian-CNNs and the infinite limit, and this distinction is indeed made in the paper but not clearly enough in my view. I would anticipate that some readers might come away after a cursory reading thinking that Bayesian-CNNs are fundamentally worse than their parametric counterparts, but this is emphatically not the message of the paper. It seems that the infinite limit that is the cause of two problems. The first problem (or perhaps benefit) is that the infinite limit gives Gaussian inner layers, just as in the fully connected case. The second problem (and I’d say this is definitely a problem this time) is that the infinite limit loses the covariance between the pixels, at least with a fully connected final layer. I would recall [Matthews 2018, long version] section 7, which discusses that point that taking the infinite limit in the fully connected is actually potentially undesirable. To quote Matthews 2018, “MacKay (2002, p. 547) famously reflected on what is lost when taking the Gaussian process limit of a single hidden layer network, remarking that Gaussian processes will not learn hidden features”. Some discussion of this would enhance the presented paper, in my view. 

The discussion of eq (7) could be made more clear. Eq (7) is only defined on K, and not in composition with A. It is important that the alpha dependency is preserved by the A operation, and while I suppose this is obvious I would welcome a bit more detail. It would help to demonstrate the application of the results of [Cho and Saul 2009] to the convolution case explicitly (i.e. for C o A), in my view. 

Regarding results, effort has clearly gone to keep the comparisons as fair as possible, but with these large datasets it is difficult to disentangle the many factors that might effect performance (as acknowledged on p9). It is a weakness of the paper that there is no toy example. An example demonstrating a situation which can only be solved with hierarchical features (e.g. features that are larger than the receptive field of a single layer) would be particularly interesting, as in this case I think the GP-CNN would fail, even with the average pooling, whereas the finite Bayesian-CNN would succeed (with a sufficiently accurate inference method).  

It would improve readability to stress the 1D notation in the main text rather than in a footnote. On first reading I missed this detail and was confused as I was trying to interpret everything as a 2D convolution. On reflection I think notation is used in the paper is good, but I think the generalization to 2D should be elevated to something more than the footnote. Perhaps a paragraph explaining how the 2D case works would be appropriate, especially as all the experiments are in 2D cases. 

Some further smaller points on specific [section, paragraph, line]s

1,2,4 I think ‘easily’ is a bit of an overstatement. In this work the kernel is itself defined via a recursive convolutional operation, which doesn’t seem to me much more interpretable than the parametric convolution. At least the filters can be examined in parametric case, which isn’t the case here. I do agree with the sentiment that a function prior is better than an implicit weight prior, however.

1,2,-1 This seems too vague to me, as at least to some extent, Matthew 2018 did indeed consider using NN-GPs to gain insight about equivalent NN models (e.g. section 5.3)

1.1,:,: I find it very surprising that there are no references to Cho and Saul 2009 in this section (one does appear in 2.2.2, however). 

1.1,3,-2:-1 ‘Our work differs from all of these in that our GP corresponds exactly to a fully Bayesian CNN in the many channel limit’ I do not think this is completely true, as the deep convolution GP does correspond to an infinite limit of a Bayesian CNN, just not the same limit as the one taken in this paper. Similarly a DGP following the Danianou and Lawrence 2013 is an infinite limit of a NN, but one with bottlenecks between layers. It is important that readers appreciate that infinite limits can be taken in different ways, and the resulting models may be very different. This certain limit taken in this work has desirable computational properties, but arguably undesirable modelling implications.

1.1,-1,-2 It should be made more clear here that the SGD trained models are non-Bayesian. 

Figure 3 The MC-CNN-GP appears to have performance that is nearly independent of the depth, even including 1 layer. Could this be explained?

2.2,2,: The z^l variables are zero mean Gaussian with a fixed covariance, not delta functions, as I understand it. They are independent of each other due to the deterministic K^l, certainly, but they are not themselves deterministic. Could this be clarified? 
","The review is positive overall, recommending acceptance but with revisions. The reviewer praises the paper's quality and insights. However, they dedicate significant space to constructive criticism, mainly concerning the figures and clarity. While the tone is direct, it's professional and aims to improve the paper. The reviewer even acknowledges the fairness of comparisons and effort put into the research.",60.0,70.0
"This paper proposes a method for estimating non-linear similarities between items using Gramian estimation. This is achieved by having two separate neural networks defined for each item to be compared, which are then combined via a dot product. The proposed innovation in this paper is to use Gramian estimation for the penalty parameter of the optimization which allows for the non-linear case. Two algorithms are proposed which allow for estimation in the stochastic / online setting. Experiments are presented which appear to show good performance on some standard benchmark tasks. 

Overall, I think this is an interesting set of ideas for an important problem. I have two reservations. First, the organization of the paper needs to be addressed in order to aid user readability. The paper often jumps across sections without giving motivation or connecting language. This will limit the audience of the paper and the work. Second (and more importantly), I found the experiments to be slightly underwhelming. The hyperparameters (batch size, learning rate) and architecture don’t have any rationale attached to them. It is also not entirely clear whether the chosen comparison methods fully constitute the current state of the art. Nonetheless, I think this is an interesting idea and strong work with compelling results. 

Editorial comments:

The organization of this paper leaves something to be desired. The introductions ends very abruptly, and then appears to begin again after the related work section. From what I can tell the first three sections all constitute the introduction and should be merged with appropriate edits to make the narrative clear.

“where x and y are nodes in a graph and the similarity is wheter an edge” → typo and sentence ends prematurely. 
","The reviewer explicitly states ""Overall, I think this is an interesting set of ideas for an important problem"" and ""Nonetheless, I think this is an interesting idea and strong work with compelling results."" These statements indicate a positive sentiment. However, the reviewer also lists two major reservations, indicating that it's not purely positive. Hence, a sentiment closer to neutral but leaning towards the positive side is more appropriate. The language used throughout the review is constructive, professional, and respectful, indicating a high level of politeness.",60.0,80.0
"# [Updated after author response]
Thank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. 

In its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.

------------------------------------------

# Summary
The authors present an empirical study of catastrophic forgetting (CF) in deep neural networks. Eight models are tested against nine datasets with 10 classes each but a varying number of samples. The authors construct a number of sequential learning tasks to test the model performances in different scenarios. The main conclusion is that CF is still a problem in all models, despite claims in other papers.

# Quality
The paper shows healthy criticism of the methods used to evaluate CF in previous works. I very much like this.

While I like the different experimental set-ups and the attention to realistic scenarios outlined in section 1.2, I find the analysis of the experiments somewhat superficial. The accuracies of each model for each task and dataset are reported, but there is little insight into what causes CF. For instance, do some choices of hyperparameters consistently cause a higher/lower degree of CF across models? I also think the metrics proposed by Kemker et al. (2018) are more informative than just reporting the last and best accuracy, and that including these metrics would improve the quality of the paper.

# Clarity
The paper is generally clearly written and distinct paragraphs are often highlighted, which makes reading and getting an overview much easier. In particular, I like the summary given in sections 1.3 and 1.4.

Section 2.4 describing the experimental setup could be clearer. It takes a bit of time to decipher Table 2, and it would have been good with a few short comments on what the different types of tasks (D5-5, D9-1, DP10-10) will tell us about the model performances. E.g. what do you expect to see from the experiments of D5-5 that is not covered by D9-1 and vice versa? And why are the number of tasks in each category so different (8 vs 3 vs 1)?

I am not a huge fan of 3D plots, and I don't think they do anything good in section 4. The perspective can make it tricky to compare models, and the different graphs overshadow each other. I would prefer 2D plots in the supplementary, with a few representative ones shown in the main paper. I would also experiment with turning Table 3 into a heat map.

# Originality
To my knowledge, the paper presents the largest evaluation of CF in terms of evaluated datasets. Kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the CF even clearer. I think it would be good to cite this paper and briefly discuss it in connection with the current work.

# Significance
The paper is mostly a report of the outcome of a substantial experiment on CF, showing that all tested models suffer from CF to some extent. While this is interesting and useful to know, there is not much to learn in terms of what can cause or prevent CF in DNNs. The paper's significance lies in showing that CF is still a problem, but there is room for improvement in the analysis of the outcome of the experiments.

# Other notes
The first sentence of the second paragraph in section 5 seems to be missing something.

# References
Kemker, R., McClure, M., Abitino, A., Hayes, T., & Kanan, C. (2018). In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410","The review is generally positive, acknowledging the paper's strengths such as its criticism of previous CF evaluation methods, the realistic experimental setups, and the clear writing. However, it also points out areas for improvement, mainly the superficial analysis of the experimental results and the lack of insights into the causes of CF. The reviewer suggests using additional metrics and improving the visualization of the results. The reviewer also notes the significance of the paper in highlighting CF as a persistent problem but sees room for enhancing the analysis to provide a deeper understanding of the issue. The language used is constructive and polite, focusing on suggesting improvements rather than simply criticizing. ",60.0,80.0
"This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training.  Specifically, a lower-triangular random binary matrix c is introduced, where c_{i,j} = 1 for i>j, indicates that z_i depends on z_j, where z is the latent vector.  Each element of c is separately parametrized by a Bernoulli distribution whose means are optimized for during training, using the target \mathbb{E}_{p(c)}[\mathcal{L}_c] where \mathcal{L}_c indicates the ELBO for a particular instance of c.  The resulting ""Graph-VAE"" scheme is shown to train models with improved marginal likelihood than a number of baselines for MNIST, Omniglot, and CIFAR-10.

The core concept for this paper is good, the results are impressive, and the paper is, for the most part, easy to follow.  Though I think a lot of people have been thinking about how to learn dependency structures in VAEs, I think this work is the first to clearly lay out a concrete approach for doing so.  I thus think that even though this is not the most novel of papers, it is work which will be of significant interest to the ICLR community.  However, the paper has a number of technical issues and I do not believe the paper is suitable for publication unless they are addressed, or at the vest least acknowledged. I further have some misgivings with the experiments and the explanations of some key elements of the method.  Because of these issues, I think the paper falls below the acceptance threshold in its current form, but I think they could potentially be correctable during the rebuttal period and I will be very happy to substantially increase my score if they are; I feel this has the potential to be a very good paper that I would ultimately like to see published.

%%% Lower bound %%%

My first major concern is in the justification of the final approach (Eq 8), namely using a lower bound argument to move the p(c) term outside of the log.  A target being a lower bound on something we care about is never in itself a justification for that target -- it just says that the resulting estimator is provably negatively biased.  The arguments behind the use of lower bounds in conventional ELBOs are based on much more subtle arguments in terms of the bound becoming tight if we have good posterior approximations and implicit assumptions that the bound will behave similarly to the true marginal.  The bound derived in A.1 of the current paper is instead almost completely useless and serves little purpose other than adding ""mathiness"" of the type discussed in https://arxiv.org/abs/1807.0334. Eq 8 is not a variational end-to-end target like you claim.  It is never tight and will demonstrably behave very differently to the original target.

To see why it will behave very differently, consider how the original and bound would combine two instances of c for the MNIST experiment, one corresponding to the MAP values of c in the final trained system, the other a value of c that has an ELBO which is, say, 10 nats lower.  Using Eq 8, these will have similar contributions to the overall expectation and so a good network setup (i.e. theta and phi) is one which produces a decent ELBO for both.  Under the original expectation, on the other hand, the MAP value of c corresponds to a setup that has many orders of magnitude higher probability and so the best network setup is the one that does well for the MAP value of c, with the other instance being of little importance.  We thus see that the original target and the lower bound behave very differently for a given p(c).

Thankfully, the target in Eq 8 is a potentially reasonable thing to do in its own right (maybe actually more so that the original formulation), because the averaging over c is somewhat spurious given you are optimizing its mean parameters anyway.  It is easy to show that the ""optimum"" p(c) for a given (\theta,\phi) is always a delta function on the value of c which has the highest ELBO_c.  As Fig 3 shows, the optimization of the parameters of p(c) practically leads to such a collapse.  This is effectively desirable behavior given the overall aims and so averaging over values of c is from a modeling perspective actually a complete red herring anyway.  It is very much possible that the training procedure represented by Eq 8 is (almost by chance) a good approach in terms of learning the optimal configuration for c, but if this is the case it needs to be presented as such, instead of using the current argument about putting a prior on c and constructing a second lower bound, which is a best dubious and misleading, and at worst complete rubbish.  Ideally, the current explanations would be replaced by a more principled justification, but even just saying you tried Eq 8 and it worked well empirically would be a lot better than what is there at the moment.

%%% Encoder dependency structure does not match the generative model %%%

My second major concern is that the dependency structure used for the encoder is incorrect from the point of view of the generative model.  Namely, a dependency structure on the prior does not induce the same dependency structure on the posterior.  In general, just because z_1 and z_2 are independent, doesn't mean that z_1 and z_2 are independent given x (see e.g. Bishop).  Consequently, the encoder in your setup will be incapable of correctly representing the posterior implied by the generative model.  This has a number of serious practical and theoretical knock-on effects, such as prohibiting the bound becoming tight, causing the encoder to indirectly impact the expressivity of the generative model etc.  Note that this problem is not shared with the Ladder VAE, as there the Markovian dependency structure means produces a special case where the posterior and prior dependency structure is shared.

As shown in https://arxiv.org/abs/1712.00287 (a critical missing reference more generally), it is actually possible to derive the dependency structure of the posterior from that of the prior.  I think in your case their results imply that the encoder needs to be fully connected as the decoder can induce arbitrary dependencies between the latent variables.  I am somewhat surprised that this has not had more of an apparent negative impact on the empirical results and I think at the very very least the paper needs to acknowledge this issue.  I would recommend the authors run experiments using a fully connected encoder and the Graph-VAE decoder (and potentially also vice verse).  Should this approach perform well, it would represent a more principled approach to replace the old on from a generative model perspective.  Should it not, it would provide an empirical justification for what is, in essence, a different restriction to that of the learned prior structure: it is conceivably actually the case that these encoder restrictions induce the desired decoder behavior, but this is distinct to learning a particular dependency structure in the generative model.

%%% Specifics of model and experiments %%%

Though the paper is generally very easy to read, there as some key areas where the explanations are overly terse.  In particular, the explanation surrounding the encoding was difficult to follow and it took me a while to establish exactly what was going on; I am still unsure how \tilde{\psi} and \hat{\psi} are combined.  I think a more careful explanation here and a section giving more detail in the appendices would both help massively.

I was not clear on exactly what was meant by the FC-VAE.  I do not completely agree with the assertion that a standard VAE has independent latents.  Though the typical choice that the prior is N(0,I) obviously causes the prior to have independent latents, as explained earlier, this does not mean the latents are independent in the posterior.  Furthermore, the encoder implicitly incorporates these dependencies through its mean vector, even if it uses a diagonal covariance (which is usually rather small anyway).  What is actually changed from this by the FC-VAE?  Are you doing some kind of normalizing flow approach here?  If so this needs proper explanation.

Relatedly, I am also far from convinced by the arguments presented about why the FC-VAE does worse at the end of the experiments.  VAEs attempt to maximize a marginal likelihood (through a surrogate target) and a model which makes no structural assumptions will generally have a lower marginal likelihood than one which makes the correct structural assumptions.  It is thus perfectly reasonable that when you learn dependency structures, you will get a higher marginal likelihood than if you presume none.  I thus find your arguments about local optima somewhat speculative and further investigation is required.

%%% Experiments %%%

Though certainly not terrible, I felt that the experimental evaluation of the work could have been better.  The biggest issue I have is that no error bars are given for the results, so it is difficult to assess the robustness of the Graph-VAE.  I think it would be good to add convergence plots with error bars to see how the performance varies with time and provide an idea of variability.  More generally, the experiment section overall feels more terse and rushed than the rest of the paper, with some details difficult to find or potentially even straight up missing.

Though Fig 3 is very nice, it would be nice to have additional plots seeing qualitatively what happens with the latent space.  E.g. on average what proportion of the c tend to zero?  Is the same dependency structure always learned?  What do the dataset encodings look like?  Are there noticeable qualitative changes in samples generated from the learned models?  I would be perfectly happy for the paper to extend over the 8 pages to allow more results addressing these questions.","The reviewer acknowledges the paper's strong points: good core concept, impressive results, and clear presentation. They believe it would be of significant interest to the ICLR community. However, they also point out several technical issues, a lack of clarity in explanations, and shortcomings in the experimental evaluation. The reviewer's tone is critical but constructive, aiming to improve the paper rather than simply rejecting it. They even express a desire to see the paper published after addressing the concerns. Therefore, the sentiment is more positive than neutral, but the numerous and significant concerns prevent it from being overwhelmingly positive. The language is formal and academic, typical of peer reviews, and avoids any personal attacks or disrespectful remarks.",60.0,80.0
"- Summary: This paper proposes verification algorithms for a class of convex-relaxable specifications to evaluate the robustness of the network under adversarial examples. Experimental results are shown for semantic specifications for CIFAR, errors in predicting sum of two digits and conservation of energy in a simple pendulum. 

- Clarity and correctness: It is a well-written and well-organized paper. Notations and expressions are clear. The math seems to be correct. 

- Significance: The paper claims to have introduced a class of convex-relaxable specifications which constitute specifications that can be verified using a convex relaxation. However, as described later in the paper, it is limited to feed-forward neural networks with ReLU and softmax activation functions and quadratic parts (it would be better to tone down the claims in the abstract and introduction parts.)

- Novelty: The idea of accounting for label semantics and quadratic expressions when training a robust neural network is important and very practical. This paper introduces some nice ideas to generalize linear verification functions to a larger class of convex-relaxable functions, however, it seems to be more limited in practice than it claims and falls short in presenting justifying experimental results.

** More detailed comments:

** The idea of generalizing verifications to a convex-relaxable set is interesting, however, applying it in general is not very clear -- as the authors worked on a case by case basis in section 3.1. 

** One of my main concerns is regarding the relaxation step. There is no discussion on the effects of the tightness of the relaxation on the actual results of the models; when in reality, there is an infinite pool of candidates for 'convexifying' the verification functions. It would be nice to see that analysis as well as a discussion on how much are we willing to lose w.r.t. to the tightness of the bounds -- especially when there is a trade-off between better approximation to the verification function and tightness of the bound. 

** I barely found the experimental results satisfying. To find ""reasonable"" inputs to the model, authors considered perturbing points in the test set. However, I am not sure if this is a reasonable assumption when there would be no access to test data points when training a neural network with robustness to adversarial examples. And if bounding them is a very hard task, I am wondering if that is a reasonable assumption to begin with.

** It is hard to have a sense of how good the results are in Figure 1 due to lack of benchmark results (I could not find them in the Appendix either.)

** The experimental results in section 4.4 are very limited. I suggest that the authors consider running more experiments on more data sets and re-running them with more settings (N=2 for digit sums looks very limited, and if increasing N has some effects, it would be nice to see them or discuss those effects.)

** Page 2, ""if they do a find a proof"" should be --> ""if they do find a proof"" 
** Page 5, ""(as described in Section (Bunel et al., 2017; Dvijotham et al., 2018)"", ""Section"" should be omitted.

******************************************************
After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring.
One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.","The review starts with positive remarks, highlighting clarity, correctness, and interesting ideas. However, it raises significant concerns about the paper's claims, limited practical application, and lack of strong experimental validation. The reviewer finds the relaxation step inadequately discussed and the experimental results unsatisfactory. While acknowledging the interesting aspects, the reviewer leans towards rejection due to these limitations. The final paragraph reveals a shift in sentiment after the authors' response, leading to an acceptance. The reviewer acknowledges the broader applicability clarified by the authors and finds the added experimental results reassuring.  The initial skepticism and detailed critique point to a score around -20, but the final acceptance warrants a more positive score, settling around 60. The language is polite throughout, using constructive criticism and avoiding harsh language, justifying a politeness score of 80.",60.0,80.0
"This paper propose to add an OT-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences. Indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.
The main issue with this computation is that it provides a distance between a set of words but not a sequence of words. The ordering is then not taken into account. Authors should discuss this point in the paper.
Experiments show an improvement of the method w.r.t. not penalized loss.

Minor comments:
- in Figure 1, the OT matching as described in the text is not the solution of eq (2) but rather the solution of eq. (3) or the entropic regularization (the set of ""edges"" is higher than the admissible highest number of edges).
- Introduction ""OT [...] providing a natural measure of distance for sequences comparisons"": it is not clear why this statement is true. OT allows comparing distributions, with no notion of ordering (see above).  
- Table 1: what is NMT?
- first paragraph, p7: how do you define a ""substantial"" improvement of the scores?
- how do you set parameter $\gamma$ in the experiments? Why did you choose \beta=0.5 for the ipot algorithm?

","The review is mostly positive. The reviewer acknowledges the paper's contribution (using OT for semantic meaning in seq-2-seq) and notes the experimental improvements. However, they raise a valid concern about the method's disregard for word order, suggesting this requires attention. The tone is direct and professional, typical of academic reviews, without resorting to harsh language.",60.0,50.0
"The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. 

There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. 

Comments: 
-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. 
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization 




","The review acknowledges the lack of groundbreaking novelty but points out the value in exploring a novel setup. It finds the paper well-written and the experiments sound. While it raises relevant questions and suggestions, the tone remains constructive and objective. Therefore, the sentiment leans slightly towards the positive side due to the positive points outweighing the critique, and the politeness remains neutral.",25.0,0.0
"The authors made several claims and provide suggestions on training binary networks, however, they are not proved or theoretically analyzed.  The empirical verification of the proposed hypothesis was viewed as weak as the only two datasets used are small datasets MNIST and CIFAR-10, and the used network architectures are also limited. Much more rigorous and thorough testing is required for an empirical paper which proposes new claims. 

Take the first claim ""end-to-end training of binary networks crucially relies on the optimiser taking advantage of second moment gradient estimates"" as an example. As it is known that choice of optimizer is highly dependent on the specific dataset and network structure, it is not convincing to jump to this conclusion using the observations on two small datasets and limited network architectures.  E.g, many binarization papers use momentum for ImageNet dataset with residual networks. Does Adam also outperforms momentum in this case? Similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.

Some minor issues are:
1. In Figure 4, different methods are not run to convergence, and the comparison may not be fair.
2. The second paragraph in section 4: ""It can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in Figure 5)."" However, in figure 5, the red curve is ""Clipping gradients"", which one is correct?
3. The authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart? ","The reviewer points out significant weaknesses in the paper's claims and methodology, indicating a negative sentiment. However, the language remains professional and focused on the technical aspects, suggesting a neutral politeness level.",-50.0,0.0
"In distributed optimisation, it is well known that asynchronous methods outperform synchronous methods in many cases. However, the questions as to whether (and when) asynchronous methods can be shown to have any speed-up, as the number of nodes increases, has been open. The paper under review answers the question in the affirmative and does so very elegantly.

I have only a few minor quibbles and a question. There are some recent papers that could be cited:
http://proceedings.mlr.press/v80/zhou18b.html
http://proceedings.mlr.press/v80/lian18a.html
https://nips.cc/Conferences/2018/Schedule?showEvent=11368
and the formatting of the bibliography needs to be improved. 

In the synchronous case, some of the analyses extend to partially separable functions, e.g.:
https://arxiv.org/abs/1406.0238
and citations thereof. Would it be possible to extend the present work in that direction?","The review starts with very positive framing, stating that the paper solves an open question in an elegant way. This alone indicates a positive sentiment. While the reviewer provides recommendations, they are framed as ""minor quibbles"" and open-ended questions rather than harsh criticisms, suggesting a polite and constructive approach.",80.0,75.0
"The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule. In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces. Experiments are performaed on 2 simple toy datasets and a simple language modeling task. A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this. Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.

Overall I like the motivation, provided background information and simplicity of the approach. Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation. However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short. Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.


Strengths:
- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step
- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation
- maze navigation shows incremental benefits over non-modulated plasticity
- thorough experimentation
- clipping-trick is a neat observation 


Weaknesses:
- evaluation: only on toy tasks (which includes PTB), no real world tasks
- very incremental improvements on PTB over a very simple baseline (far from SotA)
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures
- no qualitative analysis on how modulation is actually use by the systems. E.g., when is modulation strong and when is it not used 


Comments:
- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called ""significant"". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements. Furthermore PTB is not a ""challenging"" LM benchmark.

","The reviewer finds the paper's motivation and approach promising, highlighting the well-designed cue-reward experiment. However, they express concerns about the limited evaluation on simple tasks with marginal improvements and the lack of qualitative analysis. While acknowledging the paper's strengths, the reviewer leans towards considering it more suitable for a workshop due to the need for further experiments and analysis. The language used is professional and constructive, suggesting specific areas for improvement without resorting to harsh criticism.",40.0,80.0
"The paper is well written and structured, presenting the problem clearly and accurately. It contains considerable relevant references and enough background knowledge. It nicely motivates the proposed approach, locates the contributions in the state-of-the-art and reviews related work. It is also very honest in terms of how it differs on the technical level from existing approaches. 
The paper presents interesting and novel findings to further state-of-the-art’s understanding on how language concepts are represented in the intermediate layers of deep convolutional neural networks, showing that channels in convolutional representations are selectively sensitive to specific natural language concepts. It also nicely discusses how concepts granularity evolves with layers’ deepness in the case of natural language tasks.
What I am missing, however, is an empirical study of concepts coverage over multiple layers, studying the multiple occurrences of single concepts at different layers, and a deeper dive on the rather noisy elements of natural language and the layers’ activation dynamics towards such elements.
Overall, however, the ideas presented in the paper are interesting and original, and the experimental section is convincing. My recommendation is to accept this submission.
","The review starts with positive remarks about the paper's clarity, structure, and contribution to the field. The reviewer uses phrases like ""well written,"" ""interesting and novel findings,"" and ""convincing experimental section."" This suggests a positive sentiment. While the reviewer mentions areas for improvement, they are constructive and framed as suggestions rather than harsh criticisms. The language remains professional and courteous throughout, indicating politeness. Therefore, the sentiment is scored as positive, and the politeness is also scored highly.",75.0,80.0
"This paper proposed a framework that can improve the performances of reinforcement learning algorithms in tasks that involve long time horizons and sparse rewards. The proposed method is a hierarchical reinforcement learning framework that can use policy hierarchies with an arbitrary number of levels. To improve the sample efficiency in the learning process, the authors proposed to apply the hindsight experience replay mechanism at each level. Also, in order to avoid the actor function to output an unrealistic subgoal, the authors proposed the subgoal testing technique. 

The proposed framework is interesting. And the example in Section 3.5 clearly demonstrate how this framework works. The authors proposed to solve a UMDP by solving a hierarchy of k UMDPs, where k is a hyperparameter. Each level (except for the bottom most level) will output subgoal states for the next level to achieve. This hierarchy is reasonable and easy to understand. However, from the definition on Page 3, it seems that all of the intermediate levels i (the case where 0 < i < k - 1) has the same state and action spaces. They are all equal to the state set of the original UMDP. Under this setting, will adding more intermediate levels help improve the performance a lot? We only see results with at most one intermediate level in the experiment. It will be better if the authors can show results on more levels (i.e. at least 4 levels in total). 

Moreover, the proposed framework has a policy limit parameter T, meaning that we only consider if a goal can be achieved within T steps or not, at each level. Is this parameter necessary to be the same for all levels? Also, it will be better if the author can show some results on the performances of the proposed method according to different values for T. The authors also proposed the subgoal testing technique. It is also better if the authors can show some performance comparisons on the cases with and without this technique.

The authors claimed that their method has the advantage over some existing HRL methods (e.g. the Option-Critic Architecture [1]) that their method can use policy hierarchies with an arbitrary number of levels while these methods can only use policy hierarchies with two levels. In the experiments, the authors also showed that, in some of their experiments, the 3-layer agent (with 2 subgoal layers) outperforms the 2-layer agent (with 1 subgoal layer), under their framework. However, the authors did not compare their 2-layer agent's performance with these existing HRL methods, which means that we do not know if their 3-layer agent's performance is better than that of some of the existing 2-layer agent methods. In addition to that, as I mentioned before, it is better if the authors can show experiment results on more levels (e.g. 4 levels and more) to show that their method can perform well in practice for policy hierarchies with many levels.


References:

[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. CoRR, abs/1609.05140, 2016.

","The review starts with positive remarks, highlighting the interesting aspects of the proposed framework and acknowledging its clarity. However, it then raises several concerns and suggestions for improvement, indicating a desire for more comprehensive analysis and comparisons. The overall tone is constructive and suggestive rather than dismissive, pointing towards a moderately positive sentiment. The language used is polite and professional throughout, employing respectful phrasing and avoiding harsh criticism.",50.0,80.0
"The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. 
Thought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC.

Quality: strong, clarity:strong, originality:strong, significance: strong,

Pros:
- The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow.
- The proposed method is able to improve in sample complexity, speed and convergence over past methods.
- The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update.
- Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory.  
Cons:
- It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour)

","The reviewer provides a positive assessment of the paper, highlighting its strengths such as strong theoretical results, improved sample complexity, speed and convergence, strong empirical results, and interesting exploration patterns. While the reviewer mentions a suggestion for improvement (adding more comparison and analysis on variance), it is presented as constructive feedback rather than a harsh criticism. The language used is objective and professional throughout the review.",75.0,80.0
"
========
Summary
========

The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a ""complementary entropy"" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.

The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.

Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit
that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.

===========================
 Main comments and questions
===========================

End of page 1: ""the model behavior for classes other than the ground  truth stays unharnessed and not well-defined"". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?

Page 3, sec 2.1: ""optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)"". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g.

This indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? 

For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:
Suppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 
After step 2:  0.1 0.3 0.3 0.3
Then step 1: 0.5 0.3 0.1 0.1
Then step 2: 0.1 0.3 0.3 0.3
And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?

Sec 3.1:
""additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance"": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?

Sec 3.2:
The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.

Sec 3.4:
As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?

===========================
Secondary comments and typos
===========================

Page 3, sec 2.1: ""...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities..."", using maximizes instead of optimizes would be clearer.

In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \hat_y as an argument in this case?

Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}
(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?

Sec 3:
""We perform extensive experiments to evaluate COT on the tasks"" --> COT on tasks

""compare it with the baseline algorithms that achieve state-of-the-art in the respective domain."" --> domainS

""to evaluate the model’s robustness trained by COT when attacked"" needs reformulation.

""we select a state- of-the-art model that has the open-source implementation"" --> an open-source implementation

Sec 3.2:
Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?

Table 3 and 4: why is it the validation error that is reported and not the test error?

Sec 3.3:
""Neural machine translation (NMT) has populated the use of neural sequence models"": populated has not the intended meaning.

""We apply the same pre-processing steps as shown in the model"" --> in the paper?

Sec 3.4:
""We believe that the models trained using COT are generalized better"" --> ""..using COT generalize better""

""using both FGSM and I-FGSM method"" --> methodS

""The baseline models are the same as Section 3.2."" --> as in Section 3.2.

""the number of iteration is set at 10."" --> to 10

""using complement objective may help defend adversarial attacks."" --> defend against

""Studying on COT and adversarial attacks.."" --> could be better formulated

References: there are some inconsistencies (e.g.: initials versus first name)


Pros
====
- Paper is clear and well-written
- It seems to me that it is a new original idea
- Wide applicability
- Extensive convincing experimental results

Cons
====
- No theoretical guarantee that the procedure should converge
- The training time may be twice longer (to clarify)
- The adversarial section, as it is,  does not seem relevant for me

","The reviewer acknowledges the paper's novelty, clarity, and strong experimental results, all of which contribute to a positive sentiment. However, they raise valid concerns about the convergence of the proposed method and the lack of theoretical support. The reviewer also points out the increased training time and the somewhat irrelevant adversarial section. While the reviewer poses critical questions, the language remains polite and constructive throughout, suggesting areas for improvement and further investigation. Therefore, the sentiment leans towards the positive side, but not overly so due to the mentioned limitations.",60.0,80.0
"Summary

The authors introduce BabyAI, a platform with the aim to study grounded language learning with a human in the loop. The platform includes a *simulated* human expert (bot) that teaches a neural learner. The current domain used in a 2D gridworld and the synthetic instructions require the agent to navigate the world (including unlocking doors) and move objects to specified locations. They also introduce ""Baby Language"" to give instructions to the agent as well as to automatically verify their execution.

The paper includes a detailed description of the minigrid env with the included tasks and instruction language set. 

Authors trained small and large LSTM models on the tasks on a variety of standard learning approaches, using pure exploration (RL) and imitation from a synthetic bot (IL). They show IL is much more data efficient than RL in this domain as well. Also, a curriculum approach is evaluated (pre-train on task 1-N, then train on task N+1).  

Pro
- Human-in-the-loop research is an exciting direction.
- The language instruction set is a starting point for high-level human instructions. 

Con
- It is still unclear how to effectively learn with human-in-the-loop. The authors don't actually evaluate 
1) how well the bot imitates a human, or 
2) how an actual human would interact and speed up learning. 
All experiments are done with standard learning approaches with a synthetic bot. 
- The authors assume that human feedback comes as instructions or demonstrations. These are not the only forms of feedback possible (e.g., preferences). (Does the platform easily support those?)

Reproducibility
- Open-sourcing the platform is a good contribution to the community.
","The review acknowledges the importance of the research direction and the platform's potential. However, it points out significant limitations in the current study regarding the lack of actual human-in-the-loop evaluation and the limited types of feedback considered. The language is critical but remains professional and within the bounds of constructive feedback.",40.0,60.0
"Overall impression: 
I think that this is a well written interesting paper with strong results. One thing I’d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I’ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. 


Abstract:
We demonstrate its effectiveness on a number of challenging tasks. -> be more specific.

The term single-timestep optimization is not very clear. Can this be clarified?

they are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task.

Related works: 
Guided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. 

“we learn shaped, dense rewards”-> too early in the paper for this to make sense. can provide some contextt

Section 2.2:
fully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit.

Small typos in appendix 5.1 (r should be replaced by the density ratio)

The update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from “self” experiences. 

How is the priority list threshold and size chosen?
 Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards?

Appendices are very clear and very informative while being succinct!

I would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm 

What is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification

Experiments. 
Only 1 question to answer in this section is labelled? Put 2) and 3) appropriately. 

Can a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes?

Can the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ","The reviewer explicitly recommends accepting the paper and praises the work. They find the paper well-written, interesting, and with strong results. While they provide constructive feedback and suggestions for improvement, these are framed as ways to enhance an already good paper. The language used is formal, respectful, and encouraging, typical of academic peer reviews.",85.0,90.0
"This paper proposes to accelerate architecture search by replacing the expensive inner loop (wherein candidate architectures are trained to completion) with a HyperNetwork which predicts the weights of candidate architectures, as in SMASH. Contrary to SMASH, this work employs a Graph neural network to allow for the use of any feedforward architecture, enabling fast architecture search through parameter prediction using highly performant search spaces. The authors test their system and show that performance using Graph HyperNet-generated weights correlates with performance when trained normally. The authors benchmark their method against competing approaches (""traditional"" NAS techniques which incur the full expense of the inner loop, and one-shot techniques which learn a large model then select architectures by searching for paths in said model) and show competitive performance.

This is a solid technical contribution with a well-designed set of experiments. While the novelty is not especially high, the paper does a good job of synthesizing existing tools and achieves reasonably strong results with much less compute, making for a strong entry into the growing table of fast architecture search methods. I argue in favor of acceptance.

Notes:

-Whereas SMASH is limited to architectures which can be described with its proposed encoding scheme, GHNs only requires that the architecture be represented as a graph (which, to my knowledge, means it can handle any feedforward architecture). 

-Section 4.2: It's not entirely clear how this setup allows for variable sized kernels or variable #channels. Is the output of H simply as large as the largest allowable parameter tensor, and sliced as necessary? A snippet of code might be more illuminating here than a set of equations. Additionally (I may have missed this in the text) is the #channels in each node held fixed with a predfined pattern, or also searched for? Are the channels for each node within a block allowed to vary relative to one another?

-Do you sample a new, random architecture at every SGD step during training of the GHN?

-I have no expertise in graph neural networks, and I cannot judge the efficacy of this scheme wrt other GNN techniques, nor can I judge the forward-backward message passing scheme of section 4.4. If another reviewer has expertise in this area and can provide an evaluation that would be great.
 
-GPU-days is an okay metric, but it's also problematic, since it will of course depend on the choice of GPU (e.g. you can achieve a 10x speedup just from switching from a 600-series to a V100! How does using 4 GPUS for 1 hour compare to 1 GPU for 4 hours? How does this change if you have more CPU power and can load data faster? What if you're using a DL framework which is faster than your competitor's?) Given that the difference here is an order of magnitude, I don't think it matters, but if authors begin to optimize for GPU-milliseconds then it will need to be better standardized.
 
-Further empirical evidence showing the correlation between approximate performance and true performance is also strong. I very much like that this study has been run for a method based on finding paths in a larger model (ENAS) and shows that ENAS' performance does indeed correlate with true performance, *but* not perfectly, something which (if I recall correctly) is not addressed in the original paper.
 
-It is worth noting that for ImageNet-Mobile and CIFAR-10 they perform on par with the top methods but tend to use more parameters.  

-I like figures 3 and 4, the comparisons against MSDNet and random networks as a function of op budget is good to see.

-Table 4 shows that the correlation is weaker (regardless of method) for the top architectures, which I don't find surprising as I would expect the variation in performance amongst top architectures to be lower. It would be interesting to also see what the range of error rates are; I would expect that the correlation is higher when the range of error rates across the population of architectures is large, as it is easier to distinguish very bad architectures from very good architectures. Distinguishing among a set of good-to-very-good architectures is likely to be more difficult.

-For Section 5.3, I found the choice to use unseen architectures a little bit confusing. I think that even for this study, there's no reason to use a held-out set, as we seek to scrutinize the ability of the system to approximate performance even with architectures it *does* see during training. 

-How much does the accuracy drop when using GHN weights? I would like to see a plot showing true accuracy vs. accuracy with GHN weights for the random-100 networks, as using approximations like this typically results in the approximated weights being substantially worse. I am curious to see just how much of a drop there is.

-Section 5.4: it's interesting that performance is stronger when the GHN only sees a few (7) nodes during training, even though it sees 17 nodes during testing. I would expect that the best performance is attained with training-testing parity. Again, as I do not have any expertise in graph neural networks, I'm not sure if this is common (to train on smaller graphs and generalize to larger ones), so if the authors or another reviewer would like to comment and further illuminate this behavior, that would be helpful.

Some typos:

Abstract: ""prematured""  should be ""premature""

Introducton, last paragraph: ""CNN networks."" CNN already stands for Convolutional Neural Network.","The review is overwhelmingly positive. The reviewer explicitly recommends acceptance, finds the contribution solid, the experiments well-designed, and the results reasonably strong. They also praise specific aspects like Figures 3 and 4. While they point out limitations like the novelty not being particularly high and some aspects needing clarification, these are presented constructively and are outweighed by the positive feedback. The tone throughout is professional and courteous.",85.0,90.0
"This paper presents a very interesting interpretation of the neural network architecture.

I think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation.
Also, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. 
So I think this work has significant value for the community.

The paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.)

\questions
In the experiment when you mention about ""embed solvers as a replacement to their corresponding blocks of layers"", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss.

In the experiment, the author mentioned that  ""what happens if the algorithm is applied for multiple iterations?"". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm?

\minor comments
The definition of \lambda_max below Eq(3) are not shown, thus should be added.","The reviewer explicitly states that the paper has ""significant value for the community."" They find the paper ""clearly written and easy to follow."" While they do have questions and recommendations, these are presented constructively and suggest ways to improve the paper rather than being outright criticisms. The reviewer's tone remains professional and respectful throughout.",85.0,90.0
"For binary layers, how to calculate and backpropagate gradients is a big problem, particularly for the binary neural networks. To solve the problem, this paper proposes an unbiased and low variance augment-REINFORCE-merge (ARM) estimator. With the help of an appropriate reparameterization, the antithetic sampling in an augmented space can be used to drive a variance-reduction mechanism. The experimental results show that ARM estimator converges fast, has low computational complexity, and provides advanced prediction performance.

This paper is well-organized. The motivation of the proposed model is well-driven and algorithm is articulated clearly. Meanwhile, the derivations and analysis of the proposed algorithm are correct. The experimental results show that the proposed model is better than the other existing methods.

A few minor revision are list below.
1) In figure 1, it seems difficult to decide which one is better from the trace plots of the true/estimated gradients. Also, why the author choose to compare the REINFORCE instead of REBAR and RELAX, since REBAR and RELAX improve on REINFORCE by introducing stochastically estimated control variates. Also, about trace plots of the loss functions, I am curious why REINFORCE has a big vibration during 1500~2000 iterations. 
2) About Table 2, are all compared methods in the same experimental settings?
","The review starts with positive statements, highlighting the paper's strengths in addressing a relevant problem, clear articulation, correct analysis, and strong experimental results. This suggests a positive sentiment. The reviewer then provides constructive criticism and suggestions for improvement, which is typical of peer reviews and indicates a polite and professional tone. The suggestions are specific, actionable, and aim to further strengthen the paper.",75.0,80.0
"### post rebuttal### authors addressed most of my concerns and greatly improved the manuscript and hence I am increasing my score. 
 
Summary: 

The paper introduces a static formulation for unbalanced optimal transport by learning simultaneously a transport map T and scaling factor xi .

Some theory is given to relate this formulation to unbalanced transport metrics such as Wasserstein Fisher Rao metrics  for e.g. Chizat et al 2018.  

The paper proposes to  relax the constraint in the proposed static formulation using a divergence.  furthermore using a bound on the divergence , the final discrepancy proposed  is written as a min max problem between the witness function f of the divergence and the transport map T , and scaling factor xi. 

An algorithm is given to find the optimal map T as a generator in GAN and to learn the scaling factor  and the witness function of the divergence with a neural network paramterization , the whole optimized with stochastic gradient. 

Small experimentation on image to image transportation with unbalance in the classes is given and show how the scaling factor behaves wrt to this kind of unbalance. 


Novelty and  Originality:

The paper claims that there are no known static formulations known with a scaling factor and a transport map learned simultaneously. We refer the authors to Unbalanced optimal Transport: Geometry and Kantrovich Formulation Chizat et al 2015. In page 19 in this paper Equation 2.33 a similar formulation to Equation 4 in this paper is given. (Note that phi corresponds to T and lambda to xi). This is known as the monge formulation of unbalanced optimal transport. The main difference is that the authors here introduce a stochastic map T and an additional probabilty space Z. Assuming that the mapping is deterministic those two formulations are equivalent. 

Correctness: 

The metric defined in this paper can be written as follow and corresponds to a generalization of the monge formulation in chizat 2015 :
L(mu,nu)= inf_{T, xi}  int   c_1(x,T_x(z) ) xi(x) lambda(z) dmu(x)  + int c_2(x_i(x)) dmu(x)
                        		 s.t T_# (xi mu)=nu
In order to get a kantorovich formulation out of this chizat et al 2015 defines semi couplings and the formulation is given in Equations 3.1 page 20. 

This paper proposes to relax  T_# (xi mu)=nu with D_psi (xi \mu, \nu) and hence proposes to use:

L(mu,nu)= inf_{T, xi} int   c_1(x,T_x(z) ) xi(x) lambda(z) dmu(x)  + int c_2(x_i(x)) dmu(x)+  D_psi (xi \mu, \nu)

Lemma 3.2 of the paper claims that the formulation above corresponds to the Kantrovich formulation of unbalanced transport. I doubt the correctness of this:

Inspecting the proof of Lemma 3.2 L \geq W seems correct to me, but it is unclear what is going on in the proof of the other direction? The existence of T_x is not well supported by rigorous proof or citation? Where does xi come from in the third line of the equalities in the end of page 14? I don’t follow the equalities written at the end of page 14. 

Another concern is the space Z, how does the metric depend on this space? should there be an inf on all Z?

Other comments:

- Appendix A is good wish you baselined your experiments with those algorithms. 

- The experiments don’t show any benefit for learning the scaling factor, are there any applications in biology that would make a better case for this method?

- What was the architecture used to model T, xi, and f?

- Improved training dynamics in the appendix, it seems you are ignoring the weighting while optimizing on theta? than how would the weighing be beneficial ?","The review starts with a positive note acknowledging the authors' efforts in addressing the reviewer's previous concerns. However, the reviewer still raises several questions and doubts about the correctness of a lemma, indicating a critical stance. The reviewer also provides constructive feedback and suggestions for improvement. Overall, the sentiment leans towards the positive side due to the initial appreciation and constructive criticism, but the remaining concerns hold it back from being overwhelmingly positive. The language used is formal, respectful, and focused on the scientific content, indicating a high degree of politeness.",60.0,80.0
"This paper presents a method for learning about the parts and motion dynamics of a video by trying to predict future frames.  Specifically, a model based on optical flow is defined, noting that the motion of hierarchically related parts are additive.  Flow fields are represented using an encoder/decoder architecture and a binary structural matrix encodes the representations between parts.  This matrix is predicted given the previous frame and flow field.  This is then used to estimate a new flow field and generate a possible future frame.  The system is trained to predict future frames using an L2 loss on the predicted image and motion field and regularized to prefer more compact and parsimonious representations.

The method is applied to synthetic datasets generated by moving shapes or MNIST digits and shown to work well compared to some recent baseline methods for part segmentation and hierarchy representation.  It is also applied and qualitatively evaluated for future frame prediction on an atari video game and human motion sequences.  The qualitative evaluation shows that part prediction is plausible but the results for future frame prediction are somewhat unclear as there are no baseline comparisons for this aspect of the task.

Overall the approach seems very interesting and well motivated.   However, the experimental comparisons are limited and baselines are lacking.  Further, some relevant related work is missing.

Specific concerns:
- Motion segmentation has been studied for a long time in computer vision, a comparison against some of these methods may be warranted.  See, e.g., Mangas-Flores and Jepson, CVPR 2013.
- There is some missing related work on learning part relations.  See, e.g., Ross, et al IJCV 2010 and Ross and Zemel JMLR 2006.
- There is also some missing work on future frame prediction.  In particular, PredNet seems relevant to discuss in the context of this work and as a baseline comparison method.  See Lotter et al ICLR 2017.
- A reasonable baseline might be simply to apply the previous frames motion field to generate the next frame.  This would be a good comparison to include.
- The ""Human Studies"" section is very unclear.  How is ""same tree structure"" defined exactly and how were humans asked to annotate the tree structure?  If it's about the hierarchical relationship, then I would expect humans to always be pretty consistent with the hierarchy of body parts and suggests that the model is doing relatively poorly.  If it's some other way, then this needs to be clarified.  Further, how was this study performed?  If this section can't be thoroughly explained it should be removed from the paper as it is at best confusing and potentially very misleading.
- The system only considers a single frame and flow-field for part prediction.  From this perspective, the effectiveness of the method seems somewhat surprising.
- The system takes as input both a frame and a flow field.  I assume that flow field is computed between I0 and I1 and not I1 and I2, however this is never specified anywhere I can find in the manuscript.  If this is not the case, then the problem setup is (almost) trivial.

","The reviewer provides constructive criticism, acknowledges the paper's merits ('interesting and well motivated'), but also points out significant limitations ('experimental comparisons are limited and baselines are lacking'). The tone is direct and professional, suggesting areas for improvement without resorting to harsh language.",40.0,70.0
"As a reviewer I am expert in learning in structured data domains. 
The paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. 
Overall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to ""how many"" nodes have been generated before, i.e. we do not want to have a high probability to ""go back"" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. 
The complexity of the proposed system is actually an issue since the author(s) do not attempt (except for  the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). 
Reference to previous relevant work seems to be complete.
I think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees.

Minor issues:
- Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed.
- eq.(6): \mathbb{u}^d is not defined.
- Section 3.3:
   - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles.
  - the definition of f(G_i) involves  \mathbb{x}_u. I guess they should be  \mathbb{x}_u^G.
  - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ?
- Table I: please provide an explanation of why using a larger value for \delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal.
- diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.","The reviewer provides a generally positive assessment of the paper, acknowledging its relevance and interest to the ICLR audience. While they point out areas for improvement, such as missing details and lack of analysis on certain components, their critique is constructive and suggestive rather than dismissive. The reviewer acknowledges the complexity of the system as a potential limitation but doesn't view it as a fatal flaw. The use of phrases like ""I guess"" and ""it is not clear"" suggests a collaborative tone rather than a harshly critical one. The reviewer also offers specific suggestions for improvement, indicating a willingness to see the paper published with revisions.",60.0,70.0
"The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator’s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo.

The paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results.

Section specific comments:
- The loss of modality of reconstruction loss (section 3.2) seems like something which doesn’t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss.

- The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help.

The paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea.

=== Update after rebuttal:
Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.

The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision.


References:
[1] Kendall, Alex, and Yarin Gal. ""What uncertainties do we need in bayesian deep learning for computer vision?."" Advances in neural information processing systems. 2017.
[2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018.
[3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",The reviewer acknowledges the paper's clarity and experimental validation but expresses significant concerns about the novelty of the core idea and the strength of the theoretical arguments. They find the analysis in sections 3.2 and 4.4 to be unconvincing and ultimately do not find the rebuttal compelling. The language used throughout is critical but maintains a professional and academic tone.,-50.0,60.0
"This paper used a complex-valued network to learn the modified complex ratio mask with a weighted SDR loss for the speech enhancement task. It can get good enhancement performance.

For me, the complex-valued network is already there and weighted SDR loss is not difficult to think. The modified complex ratio mask is a bit interesting. However, I think it better to compare with [Donald S Williamson et al] where the hyperbolic tangent compression is used.

Apart from the objective metrics, a human listening test using MOS or preference score should be conducted.

On Fig 3, the unbounded complex mask might suffer from the infinity problem leading to training failure. However, on table 2, the performance of the unbounded mask is quite close to your method. It is a bit strange for me.

The total idea is good, but the novelty is not much.

","The reviewer acknowledges the good performance of the paper's method ('It can get good enhancement performance') but expresses concerns about its novelty ('The complex-valued network is already there and weighted SDR loss is not difficult to think', 'The total idea is good, but the novelty is not much'). They also raise some questions and suggest further experiments. The language is quite direct and critical, but not disrespectful.",20.0,40.0
"In this paper, authors analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. They give an almost-tight theoretical analysis of the performance and verify them with simulations.
 
Authors motivated the theoretical analysis from typical applications, for which the desired function can be only important to be approximated well on the relevant part of domains. Instead of formalizing the above problem, authors tackle a particular simple question. However, it is not easy to understand the relationships between the two problems.
 
A regression task is studied where the data has a sparser latent structure. Authors measure the performance of estimators via the expected reconstruction error from theoretical perspectives for both two-layer ReLU network and polynomial kernel. Empirical experiments will be even better to show the performance of some applications consistent with the theoretical results.","The review acknowledges the paper's strengths, such as the ""almost-tight theoretical analysis"" and the motivation from typical applications. However, it also points out a significant weakness: the disconnect between the studied problem and its practical relevance. The reviewer finds it ""not easy to understand the relationships between the two problems,"" which drags the overall sentiment down. The language used is formal and devoid of harsh or overly critical terms, suggesting a neutral to slightly positive politeness.",20.0,60.0
"This paper considers the problem of transferring motor skills from multiple experts to a student policy. To this end, the paper proposes two approaches: (1) an approach for policy cloning that learns to mimic the (local) linear feedback behavior of an expert (where the expert takes the form of a neural network), and (2) an approach that learns to compress a large number of experts via a latent space model. The approaches are applied to the problem of one-shot imitation from motion capture data (using the CMU motion capture database). The paper also considers an extension of the proposed approach to the problem of high-level planning; this is done by treating the learned latent space as a new action space and training a high-level policy that operates in this space. 

Strengths:
S1. The supplementary video was clear and helpful in understanding the setup.
S2. The paper is written in a generally readable fashion.
S3. The related work section does a thorough job of describing the context of the work.  

However, I have some significant concerns with the paper. These are described below. 

Significant concerns:
C1. My biggest concern is that the paper does not make a strong case for the benefits of LPFC over simpler strategies. The results in Figure 3 demonstrate that a linear feedback policy computed along the expert's nominal trajectory performs as well as (and occasionally even better than) LPFC. This is quite concerning.
C2. Moreover, as the authors themselves admit, ""while LPFC did not work quite as well in the full-scale model as cloning from noisy rollouts, we believe it holds promise insofar as it may be useful in rollout-limited settings..."". However, the paper does not present any theoretical/experimental evidence that would suggest this.
C3. Another concern has to do with the two-step procedure for LPFC (Section 2.2), where the first step is to learn an expert policy (in the form of a neural network) and the second step is to perform behavior cloning by finding a policy that tries to match the local behavior of the expert (i.e., finding a policy that attempts to produce similar actions as the expert policy linearized about the nominal trajectory). This two-step procedure seems unnecessary; the paper does not make a case for why the expert policies are not chosen as linear feedback controllers (along nominal trajectories) in the first place.
C4. The linearization of the expert policy produced in (1) may not lead to a stabilizing feedback controller and could easily destabilize the system. It is easy to imagine cases where the expert neural network policy maintains trajectories of the system in a tube around the nominal trajectory, but whose linearization does not lead to a stabilizing feedback controller. Do you see this in practice? If not, is there any intuition for why this doesn't occur? If this doesn't occur in practice, this would suggest that the expert policies are not highly nonlinear in the neighborhood of states under consideration (in which case, why learn neural network experts in the first place instead of directly learning a linear feedback controller as the expert policy as suggested in C3?)
C5. I would have liked to have seen more implementation details in Section 3. In particular, how exactly was the linear feedback policy along the expert's nominal trajectory computed? Is this the same as (2)? Or did you estimate a linear dynamical model (along the expert's nominal trajectory) and then compute an LQR controller? More details on the architecture used for the behavioral cloning baseline would also have been helpful (was this a MLP? How many layers?)

Minor comments:
- There are some periods missing at the end of equations (eqs. (1), (2), (6), (8), (9)).","The review starts with positive sentiment, highlighting strengths like the video, readability, and related work section. However, it quickly shifts to a critical tone, expressing ""significant concerns"" that carry more weight than the initial praise. The concerns are serious, questioning the core approach and results of the paper. While the language is direct and critical, it maintains a professional and academic tone, avoiding personal attacks or disrespectful language. The reviewer provides constructive feedback with specific examples and suggestions for improvement.",-30.0,50.0
"This is a strong theory paper and I recommend to accept.

Paper Summary:
This paper studies the problem of learning a two-layer fully connected neural network where both the output layer and the first layer are unknown. In contrast to previous papers in this line which require the input distribution being standard Gaussian, this paper only requires the input distribution is symmetric. This paper proposes an algorithm which only uses polynomial samples and runs in polynomial time. 
The algorithm proposed in this paper is based on the method-of-moments framework and several new techniques that are specially designed to exploit this two-layer architecture and the symmetric input assumption.
This paper also presents experiments to illustrate the effectiveness of the proposed approach (though in experiments, the algorithm is slightly modified).

Novelty:
1. This paper extends the key observation by Goel et al. 2018 to higher orders (Lemma 6). I believe this is an important generalization as it is very useful in studying multi-neuron neural networks.
2. This paper proposes the notation, distinguishing matrix, which is a natural concept to study multi-neuron neural networks in the population level.
3. The “Pure Neuron Detector” procedure is very interesting, as it reduces the problem of learning a group of weights to a much easier problem, learning a single weight vector. 

Clarity:
This paper is well written.

Major comments:
My major concern is on the requirement of the output dimension. In the main text, this paper assumes the output dimension is the same as the number of neurons and in the appendix, the authors show this condition can be relaxed to the output dimension being larger than the number of neurons. This is a strong assumption, as in practice, the output dimension is usually 1 for many regression problems or the number of classes for classification problems. 
Furthermore, this assumption is actually crucial for the algorithm proposed in this paper. If the output dimension is small, then the “Pure Neuron Detection” step does work. Please clarify if I understand incorrectly. If this is indeed the case, I suggest discussing this strong assumption in the main text and listing the problem of relaxing it as an open problem. 


Minor comments:
1. I suggest adding the following papers to the related work section in the final version:
https://arxiv.org/abs/1805.06523
https://arxiv.org/abs/1810.02054
https://arxiv.org/abs/1810.04133
https://arxiv.org/abs/1712.00779
These paper are relatively new but very relevant. 

2. There are many typos in the references. For example, “relu” should be ReLU.




","The reviewer explicitly recommends acceptance and praises the paper's contributions as ""strong"" and ""important."" They do present a major concern, but it's framed as a point of clarification and a suggestion for future work, not a reason to reject. The language is constructive and not dismissive of the paper's value.",75.0,90.0
"In this paper, the authors proposed a unified framework which computes spectral decompositions by stochastic gradient descent. This allows learning eigenfunctions over high-dimensional spaces and generating to new data without Nystrom approximation. From technical perspective, the paper is good. Nevertheless, I feel the paper is quite weak from the perspective of presentation. There are a couple of aspects the presentation can be improved from. 

(1) I feel the authors should formally define what a Spectral inference network is, especially what the network is composed of, what are the nodes, what are the edges, and the semantics of the network and what's motivation of this type of network.

(2) In Section 3, the paper derives a sequence of formulas, and many of the relevant results were given without being proven or a reference. Although I know the results are most likely to be correct, it does not hurt to make them rigorous. There are also places in the paper, the claim or statement is inclusive. For example, in the end of Section 2.3, ""if the distribution p(x) is unknown, then constructing an explicitly orthonormal function basis may not be possible"". I feel the authors should avoid this type of handwaving claims.  

(3) The authors may consider summarize all the technical contribution in the paper. 

One specific question:

What's Omega above formula (6)? Is it the support of x? Is it continuous or discrete? Above formula (8), the authors said ""If omega is a graph"". It is a little bit confusing there. ","The reviewer provides a mixed sentiment, acknowledging the technical strength of the paper but expressing concerns about the presentation. Phrases like ""the paper is good"" and ""the results are most likely to be correct"" suggest a positive leaning. However, the repeated use of ""I feel"" and the listing of multiple areas for improvement indicate a lack of enthusiasm and a considerable amount of required revisions. The reviewer's tone is direct and professional, focusing on constructive criticism without resorting to harsh language.",20.0,60.0
"The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance. The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z. Performance is measured in terms of Fréchet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet). The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings. An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.

I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation. Here are the questions I would like the authors to discuss further:

- The proposed approach is a fairly specific form of self-modulation. In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks. In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves. This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator. Can you clarify how you view the relationship between the approaches mentioned above?
- It’s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the “information” contained in the noise vector to better propagate to and influence different parts of the generator. ResNets also have this ability to “propagate” the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial. I’m curious to hear the authors’ thoughts in this.
- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?
","The reviewer explicitly states a positive sentiment towards the paper, using phrases like ""I am overall positive about the paper"" and ""well-explained and backed by rigorous evaluation."" The reviewer also provides constructive feedback and asks for further clarification, indicating a positive attitude. The language used is polite and professional throughout the review, with no negative or condescending remarks.",75.0,90.0
"This paper addresses a novel variant of AutoML, to automatically learn and generate optimization schedules for iterative alternate optimization problems. The problem is formulated as a RL problem, and comprehensive experiments on four various applications have demonstrated that the optimization schedule produced can guide the task model to achieve better quality of convergence, more sample-efficient, and the trained controller is transferable between datasets and models. Overall, the writing is quite clear, the problem is interesting and important, and the results are promising. 

Some suggestions:

1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ? More discussions on these questions can be very helpful to further understand the proposed method.  

2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility. 

3. Any plan for open source ? ","The review starts with positive statements, highlighting the novelty, clarity, interesting problem choice, and promising results. This clearly indicates positive sentiment. The suggestions are framed as questions and requests for further details, using phrases like ""can be very helpful"" and ""More details...can be very helpful."" This suggests a polite and constructive tone aimed at improving the paper.",75.0,80.0
"The authors propose in this paper an approach for learning models with tractable approximate posterior inference. The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described. From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).
Concerning the experimental section:
- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution. However, I do not understand how are the *discrete* output y is handled. Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice? 
- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.
- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn’t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better. I’m not sure I understand what we are supposed to learn from the astrophysics experiments.
The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method. However, the real-world experiments are not necessarily the easiest to read. 
EDIT: the concerns were mostly addressed in the revision. ","The review starts with positive remarks, highlighting the paper's motivation, clarity, and compatibility with other methods. While the reviewer raises specific questions and points for improvement, they also acknowledge the method's interest and the effectiveness demonstrated in toy experiments. The concluding statement, even before the edit, suggests a positive view overall with minor reservations about the clarity of real-world experiments. The 'EDIT' remark further reinforces the positive sentiment as it indicates that the authors addressed the reviewer's concerns. Therefore, the sentiment leans towards the positive side. The language used is constructive, professional, and inquisitive, indicating a polite tone.",65.0,80.0
"RNNs are difficult to explain, understand and analyze due to the continuous-valued memory vectors and observations features they use. Thus, this paper attempts to extract finite representation from RNNs so as to better interpret or understand RNNs. They introduce a new technique called Quantized Bottleneck Insertion to extract Moore Machines (MM). The extracted MM can be analyzed to improve the understanding of memory use and general behavior on the policies. The experiments on synthetic datasets and six Atari games validate the effectiveness of the proposal.

Here are my detailed comments:
Interpreting or understanding RNNs is a very interesting and important topic since RNNs and their variants like LSTM, GRU are widely used in different domains such as reinforcement learning, sentiment analysis, stock market prediction, natural language processing, etc. The more understandable on RNNs, the more trustful on them. In this paper, the authors try to extract more interpretable representation of RNNs, namely Moore Machines (MM). MM is actually a classical finite state automaton. The authors mention that (Zeng et al., 1993) is the most similar work to theirs. In fact a series of works have been proposed to extract finite state automaton, which is similar to (Zeng et al., 1993) such as [1], [2], [3], etc. I think the authors could make the related works more complete by incorporating these literatures I mentioned.
 
Besides, I think this work is a good application of the idea of extraction of RNNs on reinforcement learning since no works have introduced this idea into this domain as far as I know. The authors use the autoencoder named as QBN to quantize the space of hidden states. This is a good operation of clustering or quantizing the space of hidden states since it can be tuned to make the final performance better. The authors also incorporate the minimization of MM to show the probability of shrinking memory which can also make the extracted MM more interpretable. As a result, the policy represented by MM is intuitive and vivid.
 
Nevertheless, there is an obvious weak point in this paper. Specifically, the authors claim that the main contribution of this paper is to introduce an approach for transforming RNNs to finite state representations. But I do not see any comparisons between the proposed methods and other relative methods such as the method proposed by (Zeng et al., 1993) to show the effectiveness or improvement of the proposed method. I suggest the authors could incorporate comparisons to make the results more convincing.
 
[1] C. W. Omlin and C. L. Giles, ""Extraction of rules from discrete-time recurrent neural networks,"" Neural Networks, vol. 9, no. 1, pp. 41–52, 1996.
[2] C. W. Omlin and C. L. Giles, ""Constructing deterministic finite-state automata in recurrent neural networks,"" Journal of the ACM, vol. 43, no. 6, pp. 937-972, 1996.
[3] A. Cleeremans, D. Servan-Schreiber, and J. L. McClelland. ""Finite state automata and simple recurrent networks."" Neural computation, vol. 1, no. 3, pp. 372-381, 1989.","The reviewer acknowledges the importance of the paper's topic and recognizes the strengths of the approach, such as the use of QBN and the minimization of MM. This points towards a positive sentiment. However, the reviewer also points out a significant weakness - the lack of comparison with other methods, which impacts the paper's claim and makes the results less convincing. This suggests the sentiment is not overly positive. Therefore, a moderately positive score seems appropriate. The language used is formal, respectful, and constructive, indicating a high level of politeness.",60.0,80.0
"This paper proposes a method for functional regularization for training neural nets, such that the sequence of neural nets during training is stable in function space. Specifically, the authors define a L2 norm (i.e., a Hilbert norm), which can be used to measure distances in this space between two functions. The authors argue that this can aid in preventing catastrophic forgetting, which is demonstrated in a synthetic multi-task variant of MNIST.   The authors also show how to regularize the gradient updates to be conservative in function space in standard stochastic gradient style learning, but with rather inconclusive empirical results.  The authors also draw upon a connection to the natural gradient.


***Clarity***

The paper is reasonably well written.  I think the logical flow could be improved at places.   I think the major issue with clarity is the title.  The authors use the term ""regularizing"" in a fairly narrow sense, in particular regularizing the training trajectory to be stable in function space.  However, the more dominant usage for regularizing is to regularize the final learned function to some prior, which is not studied or even really discussed in the paper.

Detailed comments:

-- The notation in Section 2 could be cleaned up.  The use of \mu is a bit disconnected from the rest of the notation.  

-- Computing the empirical L2 distance accurately can also be NP hard.  There's no stated guarantee of how large N needs to be to have a good empirical estimate.  Figure 3 is nice, but I think a more thorough discussion on this point could be useful.

-- L2-Space was never formally defined.  

-- Section 2.1 isn't explained clearly.  For instance, in the last paragraph, the first sentence states ""the networks are initialized at very different point"", and halfway into the paragraph a sentence states ""all three initializations begin at approximately the same point in function space."".  The upshot is that Figure 1 doesn't crisply capture the intuition the authors aim to convey.


***Originality***

Strictly speaking, the proposed formulation is novel as far as I am aware.  However, the basic idea has been the air for a while.  For instance, there are some related work in RL/IL on functional regularization:
-- https://arxiv.org/abs/1606.00968

The proposed formulation is, in some sense, the obvious thing to try (which is a good thing).  The detailed connection to the natural gradient is nice.  I do wish that the authors made stronger use of properties of a Hilbert space, as the usage of Hilbert spaces is fairly superficial.  For instance, one can apply operators in a Hilbert space, or utilize an inner product.  It just feels like there was a lost opportunity to really explore the implications.


***Significance***

This is the place where the contributions of this paper are most questionable.  While the multi-task MNIST experiments are nice in demonstrating resilience against catastrophic forgetting, the experiments are pretty synthetic.  What about a more ""real"" multi-task learning problem?

More broadly, it feels like this paper is suffering from a bit of an identity crisis.  It uses regularizing in a narrow sense to generate conservative updates.  It argues that this can help in catastrophic forgetting.  It also shows how to employ this to construct the standard bounded-update gradient descent rules, although without much rigorous discussion for the implications.  There are some nice empirical results on a synthetic multi-task learning task, and inconclusive results otherwise.  There's a nice little discussion on the connection to the natural gradient.  It argues that that this form of regularization lives in a Hilbert space, but the usage of a Hilbert space is fairly superficial.  All in all, there are some nice pieces of work here and there, but it's all together neither here or there in terms of an overall contribution.    


***Overall Quality***

I think if the authors really pushed one of the angles to a more meaningful contribution, this paper would've been much stronger.  As it stands, the paper just feels too scattered in its focus, without a truly compelling result, either theoretically or empirically.","The reviewer acknowledges some positive aspects of the paper, such as the novelty of the formulation and the connection to the natural gradient. However, they also express several concerns, including the clarity of the paper, the lack of strong empirical results, and the overall scattered focus. They find the contributions questionable and suggest the paper would be stronger if it focused on one angle more deeply. The language used is critical but professional and not overtly rude.",-20.0,60.0
"This paper tests a number of untrained sentence representation models - based on random embedding projections, randomly-initialized LSTMs, and echo state networks - and compares the outputs of these models against influential trained sentence encoders (SkipThought, InferSent) on transfer and probing tasks. The paper finds that using the trained encoders yields only marginal improvement over the fully untrained models.

I think this is a strong paper, with a valuable contribution. The paper sheds important light on weaknesses of current methods of sentence encoding, as well as weaknesses of the standard evaluations used for sentence representation models - specifically, on currently-available metrics, most of the performance achievements observed in sentence encoders can apparently be accomplished without any encoder training at all, casting doubt on the capacity of these encoders - or existing downstream tasks - to tap into meaningful information about language. The paper establishes stronger and more appropriate baselines for sentence encoders, which I believe will be valuable for assessment of sentence representation models moving forward. 

The paper is clearly written and well-organized, and to my knowledge the contribution is novel. I appreciate the care that has been taken to implement fair and well-controlled comparisons between models. Overall, I am happy with this paper, and I would like to see it accepted. 

Additional comments:

-A useful addition to the reported results would be confidence intervals of some kind, to get a sense of the extent to which the small improvements for the trained encoders are statistically significant.

-I wonder about how the embedding projection method would compare to simply training higher-dimensional word embeddings from the start. Do we expect substantial differences between these two options?","The reviewer explicitly states ""I think this is a strong paper, with a valuable contribution"" and ""Overall, I am happy with this paper, and I would like to see it accepted."" These statements clearly indicate a positive sentiment. The reviewer provides constructive criticism and suggestions, but these are framed politely and positively. The language throughout is professional and respectful.",90.0,90.0
"This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function. They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost. Their experimental results showed competitive performance to SGD/Adam on the same network architectures. 

1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function. While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case. An appendix with more numerical comparisons on other loss functions might also be insightful. 
2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2). In Table 2 I saw some optimizers end up with much lower test accuracy. Can the authors show the convergence plots of these methods (similar to Figure 2)?","The review starts with a positive summary of the paper's contributions, highlighting its novelty and experimental results. The reviewer acknowledges the importance of Figure 1 and its role in the algorithm design. While the reviewer suggests improvements like analytical comparisons and additional experiments, the tone is constructive and suggestive rather than critical. The reviewer is suggesting further analysis and comparisons to strengthen the paper, which is typical in peer reviews. The language used is formal, objective, and respectful throughout the review.",60.0,80.0
"The authors introduce a class of quasi-hyperbolic algorithms that mix SGD with SGDM (or similar with Adam) and show improved empirical results. They also prove theoretical convergence of the methods and motivate the design well. The paper is well-written and contained the necessary references. Although I did feel that the authors could have better compared their method against the recent AggMom (Aggregated Momentum: Stability Through Passive Damping by Lucas et al.). Seems like there are a few similarities there. 

I enjoyed reading this paper and endorse it for acceptance. The theoretical results presented and easy to follow and state the assumptions clearly. I appreciated the fact that the authors aimed to keep the paper self-contained in its theory. The numerical experiments are thorough and fair. The authors test  the algorithms on an extremely wide set of problems ranging from image recognition (including CIFAR and ImageNet), natural language processing (including the state-of-the-art machine translation model), and reinforcement learning (including MuJoCo). I have not seen such a wide comparison in any paper proposing training algorithms before. Further, the numerical experiments are well-designed and also fair. The hyperparameters are chosen carefully, and both training and validation errors are presented. I also appreciate that the authors made the code available during the reviewing phase. Out of curiosity, I ran the code on some of my workflows and found that there was some improvement in performance as well. 


","The reviewer explicitly states ""I enjoyed reading this paper and endorse it for acceptance.""  They also use phrases like ""well-written"", ""necessary references"", ""easy to follow"", ""clearly"", ""self-contained"", ""thorough"", ""fair"", ""extremely wide"", ""state-of-the-art"", ""well-designed"", and ""carefully"". All of these are very positive statements. The suggestion for comparison with AggMom is presented as an opportunity for improvement, not a flaw. Therefore, the sentiment is overwhelmingly positive. The language used is polite and professional throughout.",90.0,100.0
"This paper proposes to parameterize the weight matrices of neural nets using the SVD, with approximate orthogonality enforced on the singular vectors using Orthogonal Regularization (as opposed to e.g. the Cayley transform or optimizing on the Stiefel manifold), allowing for direct, efficient control over the spectra. The method is applied to GAN discriminators to stabilize training as a natural extension of Spectral Normalization. This method incurs a slight memory and compute cost and achieves a minor performance improvement over Spectral Normalization on two benchmark image generation tasks.

I'm a bit back and forth on this paper. On the one hand, I think the ideas this paper proposes are very interesting and could provide a strong basis off which future work can be built--the extension of spectral normalization to further study and manipulation of the spectra is natural and very promising. However, the results obtained are not particularly strong, and as they stand do not, in my opinion, justify the increased compute and memory cost of the proposed methods. The paper's presentation also wavers between being strong (there were some sections I read and immediately understood) and impenetrable (there were other sections which I had to read 5-10 times just to try and grip what was going on).

Ultimately, my vote is for acceptance. I think that we should not throw out a work with interesting and potentially useful ideas just because it does not set a new SOTA, especially when the current trend with GANs seems to suggest that top performance comes at a compute cost that all but a few groups do not have access to. With another editing pass to improve language and presentation this would be a strong, relevant paper worthy of the attention of the ICLR community.

My notes:

-The key idea of parameterizing matrices as the SVD by construction, but using a regularizer to properly constrain U and V (instead of the expensive Cayley transform, or trying to pin the matrices to the Stifel manifold) is very intriguing, and I think there is a lot of potential here.

-This paper suffers from a high degree of mathiness, substituting dense notation in places where verbal explanation would be more appropriate. There are several spots where explaining the intuition behind a given idea (particularly when proposing the various spectrum regularizers) would be far more effective than the huge amount of notation. In the author's defense, the notation is generally used as effectively as it could be. My issue is that it often is just insufficient, and communication would be better served with more illustrative figures and/or language.

-I found the way the paper references Figure 1 confusing. The decays are substantially different for each layer--are these *all* supposed to be examples of slow decay? Layer 6 appears to have 90% of its singular values below 0.5, while layer 0 has more than 50%. If this is slow decay, what does an undesirable fast decay look like? Isn't the fast decay as shown in figure 2 almost exactly what we see for Layer 6 in figure 1? What is the significance of the sharp drop that occurs after some set number of singular values? The figure itself is easy to understand, but the way the authors repeatedly refer to it as an example of smooth singular decays is confusing.

-what is D-optimal design? This is not something commonly known in the ML literature. The authors should explain what exactly that D-optimal regularizer does, and elucidate its backward dynamics (in an appendix if space does not permit it in the main body). Does it encourage all singular values to have similar values? Does it push them all towards 1? I found the brief explanation (""encourages a slow singular value decay"") to be too brief--consider adding  a plot of the D-optimal spectrum to Figure 1, so that the reader can easily see how it would compare to the observed spectra. Ideally, the authors would show an example of the target spectra for each of the proposed regularizers in Figure 1. This might also help elucidate what the authors consider a desirable singular value decay, and mollify some of the issues I take with the way the paper references figure 1.

-The explanation of the Divergence Regularizer is similarly confusing and suffers from mathiness, a fact which I believe is further exacerbated by its somewhat odd motivation. Why, if the end result is a reference curve toward which the spectra will be regularized, do the authors propose (1) a random variable which is a transformation of a gaussian (2) to take the PDF of that random variable (3) discretize the PDF  (4) take the KL between a uniform discrete distribution and the discretized PMF and (5) ignore the normalization term? If the authors were actually working with random variables and proposing a divergence this might make sense, but the items under consideration are singular values which are non-stochastic parameters of a model, so treating them this way seems very odd. Based on figure 2 it looks like the resulting reference curves are fine, but the explanation of how to arrive there is quite convoluted--I would honestly have been more satisfied if the authors had simply designed a function (a polynomial logarithmic function perhaps) with a hyperparameter or two to control the curvature.

-""Our experimental results show that both combinations achieve an impressive results on CIFAR10 and STL-10 datasets""
Please do not use subjective adjectives like ""impressive."" A 6.5% improvement is okay, but not very impressive, and when you use subjective language you run the risk of readers and reviewers subjectively disagreeing with you, as is the case with this reviewer. Please also fix the typo in this sentence, it should at least be ""...achieve [impressive] results"" or ""achieve an [impressive] improvement on..."" 

Section 3:
-What is generalization supposed to mean in this context? It's unclear to me why this is at all relevant--is this supposed to be indicating the bounds for which the Discriminator will correctly distinguish real vs generated images? Or is there some other definition of generalization which is relevant? Does it actually matter for what we care about (training implicit generative models)? 

-What exactly is the use of this generalization bound? What does it tell us? What are the actual situations in which it holds? Is it possible that it will ever be relevant to training GANs or to developing new methods for training GANs?

Experiments:
-I appreciate that results are taken over 10 different random seeds.

-If the choice of gamma is unimportant then why is it different for one experiment? I found footnote 4 confusing and contradictory.  

-For figure 3, I do not think that the margin is ""significant""--it constitutes a relative 6.5% improvement, which I do not believe really justifies the increased complexity and compute cost of the method.

-I appreciate Table 1 and Figure 4 for elucidating (a) how orthogonal the U and V matrices end up and (b) the observed decay of the spectra.

Appendix:
-Please change table 7 to be more readable, with captions underneath each figure rather than listed at the top and forcing readers to count the rows and match them to the caption. What is the difference between SN-GAN and Spectral Norm in this table? Or is that a typo, and it should be spectral-constraint?

-I Would like to see a discussion of table 7 / interpretation of why the spectra look that way (and why they evolve that way over training) for each regularizer.  

Minor:
-Typos and grammatical mistakes throughout.
-As per the CIFAR-10/100 website (https://www.cs.toronto.edu/~kriz/cifar.html) the Torralba citation is not the proper one for the CIFAR datasets, despite several recent papers which have used it.
-Intro, last paragraph, ""Generation bound"" should be generalization bound?
-Page 4, paragraph 2, last sentence, problem is misspelled.","The reviewer finds the ideas interesting and potentially useful, leading to a positive sentiment. However, the lack of strong results and concerns about presentation slightly dampen the enthusiasm. The language is primarily professional and polite, with constructive criticism offered throughout.",60.0,75.0
"The big-little module is an extension of the multi-scale module. Different scales takes different complexities: higher complexity for low-scale, and lower complexity for high scale. Two schemes of merging two branches are also discussed, and the linear combination is empirically better. 

As expected, the results are better than ResNets, ResNexts, SEResNexts. I do not have  comments except ablation study is needed to show the results for more choices of alpha, beta, e.g., alpha =1, beta =1.","The review is short and does not contain strong positive or negative language. The reviewer states the results are ""as expected"" and ""better"" than other models, which leans slightly positive. However, the request for an ablation study suggests the reviewer sees room for improvement, bringing the sentiment closer to neutral. The language used is direct and professional, without being overly positive or negative.",20.0,50.0
"I like the simplicity of the approach in this paper (especially compared to very computationally hungry methods such as Deepmind's ""Graph Networks as Learnable Physics Engines for Inference and Control""). The fact that the approach allows for online learning is also interesting. I very much appreciate that you tested your approach on a real robot arm!

I have a number of questions, which I believe could help strengthen this paper:
- The decomposition of H into L^TL ensures H is positive definite, however there are no constraints on g (gravity/external forces). How do you ensure the model doesn't degenerate into only using g and ignoring H? In the current formulation g only depends on q, however this seems insufficient to model velocity dependent external forces (e.g. contact dynamics). Please elaborate.
- How would you handle partial observability of states? Have you tried this? 
- How would you extend this approach to soft robots or robots for which the dimensionality of the state space is unknown?
- Have you tested your method on systems that are not kinematic chains? How would complex contact dynamics be handled (e.g. legged robots)?
- It would be interesting to see more comparisons with recent work (e.g. Deepmind's).

Some figures (e.g. Figure 6) are missing units on the axes. Please fix.","The reviewer starts with positive remarks, praising the simplicity and online learning capability of the approach, and the fact it was tested on a real robot. They also use phrases like ""very much appreciate"" which indicates positivity. However, they also raise a significant number of questions which suggests the paper might need substantial improvements. Overall, the tone is constructive and the suggestions are presented politely. Therefore, the sentiment is moderately positive and the politeness is highly positive.",60.0,80.0
"

*Pros:*
-	Easily accessible paper with good illustrations and a mostly fair presentation of the results (see suggestions below).
-	It is a first attempt to generate audio with GANs which results in an efficient scheme for generating short, fixed-length audio segments of reasonable (but not high) quality.
-	Human evaluations (using crowdsourcing) provides empirical evidence that the approach has merit.
-	The paper appears reproducible and comes with data and code.

*Cons*:
-	Potentially a missing comparison with existing generative methods (e.g. WaveNet). See comments/questions below ** 
-	The underlying idea is relatively straightforward in that the proposed methods is a non-trivial application of already known techniques from ML and audio signal processing.

*Significance*: The proposed GAN-based audio generator is an interesting step in the development of more efficient audio generation and it is of interest to a subcommunity of ICLR as it provides a number of concrete techniques for applying GANs to audio.

*Further comments/ questions:*
-	Abstract/introduction: I’d suggest being more explicit about the limitations of the method, i.e. you are currently able to generate short and fixed-length audio.
-	SpecGAN (p 4): I’d suggest including some justification of the chosen pre-processing of spectrograms (p. 4, last paragraph). 
-	** Evaluation:  The paper dismisses existing generative methods early in the evaluation phase but the justification for doing so is not entirely clear to me: Firstly, if the inception score is used as an objective criterion it would seem reasonable to include the values in the paper. Secondly, as inception scores are based on spectrograms it could potentially favour methods using spectrograms directly (SpecGAN) or indirectly (WaveGAN, via early stopping) thus putting the purely sample based methods (e.g. WaveNet) at a disadvantage. It would seem fair to pre-screen the audio before dismissing competitors instead of solely relying on potentially biased inception scores (which was probably also done in this work, but not clearly stated…)? Finally, while not the aim of the paper, it would have been beneficial to discuss and understand the failures of existing methods in more detail to convince the reader that a fair attempt has been made to getting competitors to work before leaving them out entirely. 
-	Results/analysis: It is unclear to me how many people annotated the individual samples? What is the standard deviation over the human responses (perhaps include in tab 1)? Consider including a reflection on (or perhaps even test statistically) the alignment between the qualitative diversity/quality scores and the subjective ratings to justify the use of the objective scores in the training/selection process.
-	Related work: I think it would provide a better narrative if the existing techniques are outlined earlier on in the paper.
","The review is overall positive, highlighting the merits of the paper such as its accessibility, novelty, and reproducibility. While it does point out some limitations and areas for improvement, it does so in a constructive and respectful manner. The reviewer acknowledges the value of the work and provides specific suggestions for enhancing the paper. The language used is professional and polite throughout, employing a questioning tone rather than accusatory when raising concerns.",60.0,80.0
"Authors case the problem of finding informative measurements by using a maximum likelihood formulation and show how a data-driven dimensionality reduction protocol is built for sensing signals using convolutional architectures. A novel parallelization scheme is discussed and analyzed for speeding up the signal recovery process.
 
Previous works have been proposed to jointly learn the signal sensing and reconstruction algorithm using convolutional networks. Authors do not consider them as the baseline methods due to the fact that the blocky reconstruction approach is unrealistic such as MRI. However, there is no empirical result to support his conclusion.  In addition, the comparisons to these methods can further convince the readers about the advantage of the proposed method.
 
It is not clear where the maximum deviation from isometry in Algorithm 1 is discussed since the MSE is used as a loss function.
 
Authors provided theoretical insights for the proposed algorithm. It indicates that the lower-bound of the mutual information is maximized and minimizing the mean squared error is a special case, but it is unclear why this can provide theoretical guarantee for the proposed method. More details are good for the connections between the theory and the proposed algorithm.
 
One of the contributions in this paper is the speed, so the results on the speed should be put in the main paper.","The review acknowledges the authors' contributions but raises several concerns regarding clarity, lack of empirical support for certain claims, and missing details on important aspects like speed results. The language, while direct and critical, maintains a professional and respectful tone. There's a clear expectation for the authors to address the raised issues, indicating a desire to see the work improved rather than outright rejection.",40.0,60.0
"The authors propose a Laplacian in the context of reinforcement learning, together with learning the representations. Overall the authors make a nice contribution. The insight of defining rho to be the stationary distribution of the Markov chain P^pi and connecting this to eq (1) is interesting. Also the definition of the reward function on p.7 in terms of the distance between phi(s_{t+1}) and phi(z_g) looks original. The method is also well illustrated and compared with other methods, showing the efficiency of the proposed method.

On the other hand I also have further comments and suggestions:

- it would be good if the authors could comment on the choice of d. This is in fact a model selection problem. According to which criterion is this selected?

- the authors define D(u,v) in eq (4). Why this choice? Is there some intuition or interpretation possible related to this expression?

- in (6) beta is called a Lagrange multiplier. Given that a soft constraint (not a hard constraint) is added for the orthonormality constraint it is not a Lagrange multiplier.

How sensitive are the results with respect to the choice of beta in (6) (or epsilon in the eq above)? The orthonormality constraint will only be approximately satisfied. Isn't this a problem?

Wouldn't it be better in this case to rely on optimization algorithm on Grassmann and Stiefel manifolds?

- The authors provide a scalable approach related to section 2 by stochastic optimization. Other scalable methods related to kernel spectral clustering (related to subsets/subgraphs and making out-of-sample extensions) were proposed in literature, e.g.

Multiway Spectral Clustering with Out-of-Sample Extensions through Weighted Kernel PCA, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(2), 335-347, 2010.

Kernel Spectral Clustering for Big Data Networks, Entropy, Special Issue: Big Data, 15(5), 1567-1586, 2013.


","The reviewer starts by highlighting the positive aspects of the paper, such as the novelty, interesting insights, good illustration of the method, and efficiency. This suggests a positive sentiment. However, the review also includes a significant number of  constructive comments and suggestions for improvement. While these are presented politely and professionally, the sheer volume of points raised suggests the reviewer sees room for improvement. Therefore, the sentiment is likely positive but with reservations. The language used throughout is polite and professional, focusing on the scientific content and offering constructive feedback.",60.0,80.0
"Updated rating after author response from 8 to 7 because I agree that Figure 1 and some discussions were confusing in the original manuscript.
--------------------------------------------------------------------------

This paper investigates the relationship between the eigenvectors of the Hessian. This paper investigates characteristics of Hessian of the empirical losses of DNNs through comprehensive experiments. These experiments showed many important insights, 1) the top-K eigenvalues become bigger in the early stage, and decrease in later stage. 2) Bigger SGD steps and smaller batch-size leads to smaller and earlier peak of eigenvalues. 3) The sharpest direction update does not contribute to the loss value decrease in the normal step size (or bigger). From these analyses, this paper proposes to decrease the SGD step length on top-K eigenvectors for speeding up the convergence. Experimental results showed that the proposed method could converge to local minima in a fewer epoch and obtain better result, which means higher test accuracy.

This paper is well-written and well-organized. Findings about eigenvalues and these relationship between the SGD step length are very impressive. Although the step length adjustment on the top-K eigenvector directions are not realistic solution for improving the current SGD-based optimization on DNNs due to heavy computational cost, I think these findings and insights are very helpful to ICLR and other ML communities.","The review starts with a neutral statement and then highlights the positive aspects of the paper such as ""well-written"", ""well-organized"", ""impressive findings"", and ""helpful to ICLR and other ML communities."" The reviewer acknowledges the limitations of the proposed method but doesn't deem them detrimental. The slightly lowered rating after the author response doesn't significantly impact the overall positive sentiment. ",75.0,100.0
"This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed ""Discriminator-Actor-Critic"". They key point here being that they propose a replay buffer to sample transitions from. 

It is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. 

Pros:
	* Well written
	* Motivation is clear
	* Example on biased reward functions 
	* Experiments are carefully designed and thorough
Cons:
	* The analysis of the results in section 5.1 is a bit short

Questions:
	* You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning?

	* What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend

	* In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments?

	* Is it possible to show results of the effect of absorbing states on the Mujoco environments?

Minor suggestions:
In Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.","The review is positive overall. The reviewer states that the paper is ""well written and easy to follow"", the authors ""position their work well"", and the experiments are ""carefully designed and thorough."" While the review does list some cons and questions, these are presented constructively and suggest areas for improvement rather than major flaws. ",75.0,100.0
"The paper focuses on adversarial vulnerability of neural networks, and more specifically on perturbation-based versus invariance-based adversarial examples and how using bijective networks (with so-called metameric sampling) may help overcoming issues related to invariance. The approach is used to get around insufficiencies of cross-entropy-based information-maximization, as illustrated on experiments where the proposed variation on CE outperforms CE. 

While I am not a neural network expert, I felt that the ideas developed in the paper are worthwhile and should eventally lead to useful contributions and be published. This being said, I did not find the paper in its present form to be fit for publication in a high-tier conference or journal. The main reason for this is the disbalance between the somehow heavy and overly commented first four pages (especially in Section 2) contrasting with the surprisingly moderate level of detail when it comes to bijective networks, supposedly the heart of the actual original contribution. To me this is severely affecting the overall quality of the paper. The contents of sections 3 and 4 seem relevant, but I struggled find out what precisely is the main contribution in the end, probably because of the lack of detail on bijective networks mentioned before. Again, I am not an expert, and I will indicate that in the system of course, but while I cannot completely judge all aspects of the technical relevance and the originality of the approach, I am fairly convinced that the paper deserves to be substantially revised before it can be accepted for publication.   

Edit: After paper additions I am changing my score to a 6. ","The reviewer finds the ideas worthwhile and potentially useful, leading to a moderately positive sentiment. However, the criticism regarding the paper's structure and lack of clarity lowers the positivity. The reviewer's willingness to revise the score upwards after additions indicates openness and a constructive approach. The language used is formal, respectful, and provides concrete feedback without resorting to harsh or offensive language.",40.0,80.0
"This paper describes approximation and estimation error bounds for functions in Besov spaces using estimators corresponding to deep ReLU networks. The general idea of connecting network parameters such as depth, width, and sparsity to classical function spaces is interesting and could lead to novel insights into how and why these networks work and under what settings. The authors carefully define Besov spaces and related literature, and overall the paper is clearly written. 

Despite these strengths, I'm left with several questions about the results. The most critical is this: piecewise polynomials are members of the Besov spaces of interest, and ReLU networks produce piecewise linear functions. How can piecewise linear approximations of piecewise polynomial functions lead to minimax optimal rates? The authors' analysis is based on cardinal B-spline approximations, which generally makes sense, but it seems like you would need more terms in a superposition of B-splines of order 2 (piecewise linear) than higher orders to approximate a piecewise polynomial to within a given accuracy. The larger number of terms should lead to worse estimation errors, which is contrary to the main result of the paper. I don't see how to reconcile these ideas. 

A second question is about the context of some broad claims, such as that the rates achieved by neural networks cannot be attained by any linear or nonadaptive method. Regarding linear methods, I agree with the author, but I feel like this aspect is given undue emphasis. The key paper cited for rates for linear methods is the Donoho and Johnstone Wavelet Shrinkage paper, in which they clearly show that nonlinear, nonadaptive wavelet shrinkage estimators do indeed achieve minimax rates (within a log factor) for Besov spaces. Given this, how should I interpret claims like ""any linear/non-linear approximator
with fixed N -bases does not achieve the approximation error ... in some parameter settings such as 0 < p < 2 < r ""?
Wavelets provide a fixed N-basis and achieve optimal rates for Besov spaces. Is the constraint on p and r a setting in which wavelet optimality breaks down? If not, then I don't think the claim is correct. If so, then it would be helpful to understand how relevant this regime for p and r is to practical settings (as opposed to being an edge case). 

The work on mixed Besov spaces (e.g. tensor product space of 1-d Besov spaces) is a fine result but not surprising.

A minor note: some of the references are strange, like citing a 2015 paper for minimax rates for Besov spaces that have been known for far longer or a 2003 paper that describes interpolation spaces that were beautifully described in DeVore '98. It would be appropriate to cite these earlier sources. ","The review starts with positive remarks, highlighting the relevance and clarity of the paper. However, it then raises serious concerns and questions about the core results, suggesting potential flaws in the arguments. While the reviewer acknowledges the potential of the approach, their doubts about the validity of the findings point towards a rather critical stance. The language used is professional and respectful, directly addressing the authors and their arguments without resorting to personal attacks or dismissive language.",20.0,80.0
"This is an interesting paper which proposes a novel angle on the problem of learning long-term dependencies in recurrent nets. The authors argue that most of the action should be in the imaginary part of the eigenvalues of the Jacobian J=F' of the new_state = old_state + epsilon F(old_state, input) incremental type of recurrence, while the real part should be slightly negative. If they were 0 the discrete time updates would still not be stable, so slightly negative (which leads to exponential loss of information) leads to stability while making it possible for the information decay to be pretty slow. They also propose a gated variant which sometimes works better. 

This is similar to earlier work based on orthogonal or unitary Jacobians of new_state = H(old_state,input) updates, since the Jacobian of H(old_state,input) = old_state + epsilon F( old_state,input) is I + epsilon F'. In this light, it is not clear why the proposed architecture would be better than the partially orthogonal / unitary variants previously proposed. My general concern with this this type of architecture is that they can store information in 'cycles' (like in fig 1g, 1h) but this is a pretty strong constraint. For example, in the experiments, the authors did not apparently vary the length of the sequences (which would break the trick of using periodic attractors to store information). In practical applications this is very important. Also, all of the experiments are with classification tasks with few categories (10), i.e., requiring only storing 4 bits of information. Memorization tasks requiring to store many more bits, and with randomly varying sequence lengths, would better test the abilities of the proposed architecture.

","The review starts with positive wording, highlighting the novelty of the paper's approach. However, the reviewer raises several concerns and suggests additional experiments, indicating a more critical stance overall. Therefore, the sentiment is moderately positive. The language used is polite and professional, employing constructive criticism and avoiding harsh or disrespectful language.",40.0,80.0
"This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling (DRS), to help filter ‘good’ samples from GANs’ generator. More specifically, after training GANs’ generator and discriminator are fixed; GANs’ discriminator is further exploited to design a rejection sampler, which is used to reject the ‘bad’ samples generated from the fixed generator; accordingly, the accepted generated samples have good quality (better IS and FID results). Experiments of SAGAN model on GMM toys and ImageNet dataset show that DRS helps further increases the IS and reduces the FID.

The paper is easy to follow, and the experimental results are convincing. However, I am curious about the follow questions.

(1)	Besides helping generate better samples, could you list several other applications where the proposed technique is useful? 

(2)	In the last paragraph of Page 4, I don’t think the presented Discriminator Rejection Sampling “addresses” the issues in Sec 3.2, especially the first paragraph of Page 5.

(3)	The hyperparameter gamma in Eq. (8) is of vital importance for the proposed DRS. Actually, it is believed the key to determining whether DRS works or not. Detailed analysis/experiments about hyperparameter gamma are considered missing. 
","The review starts with a positive note, acknowledging the paper's clarity and convincing results. This suggests a positive sentiment. However, the reviewer then poses several ""questions,"" which are actually concerns and requests for clarification/improvement. These concerns, particularly the statement about the hyperparameter gamma being ""the key to determining whether DRS works or not,"" indicate that the reviewer sees significant potential issues. Therefore, the sentiment is likely positive but with reservations. 

The language used is polite throughout. The reviewer phrases their concerns as questions and refrains from harsh or accusatory language. The tone remains professional and constructive.",60.0,80.0
"This paper proposes to construct multiple classification tasks from unsupervised data.

Quality:
The detail of the proposed method is not mathematically presented and its performance is not theoretically analyzed.
Although the proposed method is empirically shown to be superior to other approaches, the motivation is not clearly presented.
Hence the overall quality of this paper is not high.

Clarity:
The readability of this paper is not high as it is redundant or unclear at several points.
For example, Sections 2.1, 2.3 and Sections 2.2, 2.4 can be integrated, respectively, and more mathematical details can be included instead.

Originality:
The proposal of constructing meta-learning based on unsupervised learning seems to be original.

Significance:
- The motivation is not clear. The proposed method artificially generates a number of classification tasks. But how to use such classifiers for artificially generated labels in real-world applications is not motivated.
  It is better to give a representative application, to which the proposed method fits.
- There is no theoretical analysis on the proposed method.
  For example, why is the first embedding step required? Clustering can be directly performed on the give dataset D = {x_i}.
- Although the paper discusses using unsupervised learning for meta-learning, only k-means is considered in the proposed method.
  There are a number of types of unsupervised learning, including other clustering algorithms and other tasks such as outlier detection, hence analyzing them is also interesting.
- The proposed method includes several hyper-parameters. But how to set them in practice it not clear.

Pros:
- An interesting approach to meta-learning is presented.

Cons:
- Motivation is not clear.
- There is no theoretical analysis.
","The reviewer highlights both positive and negative aspects of the paper. While acknowledging the originality of the approach, the reviewer expresses concerns about the clarity, motivation, lack of theoretical analysis, and practical considerations. The language used, while direct, maintains a professional and constructive tone.",-10.0,50.0
"In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (“burn-in” period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab.

The proposed technique (dubbed R2D2) is not particularly original (it is essentially “just” using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method.

I do have a couple of important concerns though. The first one is that a few potentially important changes were made to the “traditional” settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from “Meta-Gradient Reinforcement Learning” (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the “despite this” above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more “standard” Atari settings, even if it would mean using different hyper-parameters on DMLab).

The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it.

Besides the above, I have a few additional small questions:
1. “We also found no benefit from using the importance weighting that has been typically applied with prioritized replay”: this is potentially surprising since this could be “wrong”, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab)
2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the “deep experts” version of IMPALA — this is not clear in Fig. 3 — in which case this last question would be irrelevant)
3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can’t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don’t you report their results on Atari?
4. Table 4’s caption mentions “30 no-op starts” but you actually used the standard “random starts” setting, right? (not a fixed number of 30 no-ops)

And finally a few minor comments / suggestions:
- In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula)
- At top of p. 3 I guess \bar{delta}_i is the mean of the delta_i’s, but then the index i should be removed
- In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?)
- p. 4, “the true stored recurrent states at each step”: “true” is a bit misleading here as it can be interpreted as “the states one would obtain by re-processing the whole episode from scratch with the current network” => I would suggest to remove it, or to change it (e.g. “previously”). By the way, I think it would have been interesting to also compare to these states recomputed “from scratch”, since they are the actual ground truth.
- I think you should mention in Table 1’s caption that the PBT IMPALA is a single network trained to solve all tasks
- Typo at bottom of p. 7, “Indeed, Table 1 that even...”

Update: score updated to 8 (from 7) following discussion below","The review is overwhelmingly positive. The reviewer praises the thorough experiments and impressive results, particularly on the Atari benchmark. They acknowledge the significance of the findings for researchers in the field. While they raise valid concerns about reproducibility and certain experimental choices, they also offer constructive suggestions for improvement. The tone throughout is professional and respectful, indicating a positive disposition towards the paper.",80.0,90.0
"This paper addresses black-box classifier attacks in the “hard-label” setting, meaning that the only information the attacker has access to is single top-1 label predictions. Relative to even the standard black-box setting where the attacker has access to the per-class logits or probabilities, this setting is difficult as it makes the optimization landscape non-smooth. The proposed approach reformulates the optimization problem such that the outer-loop optimizes the direction using approximate gradients, and the inner-loop estimates the distance to the nearest attack in a given direction. The results show that the proposed approach successfully finds both untargeted and targeted adversarial examples for classifiers of various image datasets (including ImageNet), usually with substantially better query-efficiency and better final results (lower distance and/or higher success rate) than competing methods.

=====================================

Pros:

Very well-written and readable paper with good background and context for those (like me) who don’t closely follow the literature on adversarial attacks. Figs. 1-3 are nice visual aids for understanding the problem and optimization landscape.

Novel formulation and approach that appears to be well-motivated from the literature on randomized gradient-free search methods. Novel theoretical analysis in Appendix that generalizes prior work to approximations (although, see notes below).

Good empirical results showing that the method is capable of query-efficiently finding attacks of classifiers on real-world datasets including ImageNet. Also shows that the model needn’t be differentiable to be subject to such attacks by demonstrating the approach on a decision-tree based classifier. Appears to compare to and usually outperform appropriate baselines from prior work (though I’m not very familiar with the literature here).

=====================================

Cons/questions/suggestions/nitpicks:

Eq 4-5: why \texttt argmin? Inconsistent with other min/maxes.

Eq 4-5: Though I understood the intention, I think the equations are incorrect as written: argmin_{\lambda} { F(\lambda) } of a binary-valued function F would produce the set of all \lambdas that make F=0, rather than the smallest \lambda that makes F=1. I think it should be something like:

min_{\lambda>0} {\lambda}
s.t. f(x_0+\lambda \theta/||\theta||) != y_0

Sec 3.1: why call the first search “fine-grained”? Isn’t the binary search more fine-grained? I’d suggest changing it to “coarse-grained” unless I’m misunderstanding something.

Algorithm 2: it would be nice if this included all the tricks described as “implementation details” in the paragraph right before Sec. 4 -- e.g. averaging over multiple sampled directions u_t and line search to choose the step size \eta. These seem important and not necessarily obvious to me.

Algorithm 2: it could be interesting to show how performance varies with number of sampled directions per step u_t.

Sec: 4.1.2: why might your algorithm perform worse than boundary-attack on targeted attacks for CIFAR classifiers? Would like to have seen at least a hypothesis on this.

Sec 6.3 Theorem 1: I think the theorem statement is a bit imprecise. There is an abuse of big-O notation here -- O(f(n)) is a set, not a quantity, so statements such as \epsilon ~ O(...) and \beta <= O(...) and “at most O(...)” are not well-defined (though common in informal settings) and the latter two are redundant given the meaning of O as an upper bound. The original theorem from [Nesterov & Spokoiny 2017] that this Theorem 1 would generalize doesn’t rely on big-O notation -- I think following the same conventions here might improve the theorem and proof.

=====================================

Overall, this is a good paper with nice exposition, addressing a challenging but practically useful problem setting and proposing a novel and well-motivated approach with strong empirical results.","The review is overwhelmingly positive. The reviewer highlights the paper's strengths, including its clarity, novelty, theoretical contributions, and empirical results. While the reviewer provides constructive criticism and suggestions for improvement, these are presented in a collegial and helpful manner. The reviewer explicitly states that this is a 'good paper' with 'strong empirical results'. ",85.0,90.0
"The work is a special case of density estimation problems in Statistics, with a use of conditional independence assumptions to learn the joint distribution of nodes. While the work appears to be impressive, such ideas have typically been used in Statistics and machine learning very widely over the years(Belief Propagation,  Topic modeling with anchor words assumptions etc...). This work could be easily extended to multi-class classifications where each node belongs to multiple classes. It would be interesting to know the authors' thoughts on that. The hard classification rule in the paper seems to be too restrictive to be of use in practical scenarios, and soft classification would be a useful pragmatic alternative. 
","The review starts with a backhanded compliment, acknowledging the work as impressive but quickly diminishing its novelty by pointing out similar pre-existing ideas. The reviewer suggests extensions and points out limitations (hard classification) with suggestions for improvement (soft classification). Overall, the tone is critical but not dismissive. The language is academic and professional.",20.0,60.0
"
=================
Updated Thoughts
=================

I was primarily concerned about a lack of analysis regarding the technical contributions moving from AQM to AQM+. The revisions and author comments here have addressed the specific experiments I've asked for and more generally clarified the contributions made as part of AQM+. I've increased my rating to reflect my increased confidence in this paper. Overall, I think this is a good paper and will be interesting to the community.

I also thank the authors for their substantial efforts to revise the paper and address these concerns.


===========
Strengths:
===========

The approach is a sensible application of AQM to the GuessWhich setting and results in significant improvements over existing approaches both in terms of quantitative results and qualitative examples. 

===========
Concerns:
===========

[A] Technical Novelty is Limited Compared to AQM 
The major departures from the AQM approach claimed in the paper (Section 3.3) are:
	[1] the generation of candidate questions through beam search rather than predefined set 
	[2.1] The approximate answerer being an RNN generating free-form language instead of a binary classifier. 
	[2.2] Dropping the assumption that \tilde p(a_t | c, q_t) = \tilde p (a_t | c, q_t, h_{t-1}). 
	[3] Estimate approximate information gain using subsets of the class and answer space corresponding to the beam-search generated question set and their corresponding answers.

I have some concerns about these:

For [1], the original AQM paper explores this exact setting for GuessWhat in Section 5.2 -- generating the top-100 questions from a pretrained RNN question generator via beam search and ranking them based on information gain. From my understand, this aspect of the approach is not novel.

For [2.1] I disagree that this is a departure from the AQM approach, instead simply an artifact of the experimental setting. The original AQM paper was based in the GuessWhat game in which the answerer could only reply with yes/no/na; however, the method itself is agnostic of this choice. In fact, the detailed algorithm explanation in Appendix A of the AQM paper explicitly discusses the possibility of the answer generator being an RNN model. 

Generally, the modifications to AQM largely seem like necessary, straight-forward adjustments to the problem setting of GuessWhich and not algorithmic advances. That said, the changes make sense and do adapt the method to this more complex setting where it performs quite well!


[B] Design decisions are not well justified experimentally
Given that the proposed changes seem rather minor, it would be good to see strong analysis of their effect. Looking back at the claimed difference from AQM, there appear to be a few ablations missing:
- How useful is generating questions? I would have liked to see a comparison to a Q_fix set samples from training. (This corresponds to difference [1] above.)
- How important is dialog history to the aprxAns model? (This corresponds to difference [2.2] above).
- How important is the choice to restrict to |C| classes? Figure 4b begins to study this question but conflates the experiment by simultaneously increasing |Q| and |A|. (This correspond to difference [3] above.)

[C] No evaluation of Visual Dialog metrics
It would be useful to the community to see if this marked improvement in GuessWhich performance also results in improved ability to predict human response to novel dialogs. I (and I imagine many others) would like to see evaluation on the standard Visual Dialog test metrics. If this introspective inference process improves these metrics, it would significantly strengthen the paper!

[D] No discussion of inference time
It would be useful to include discussion of relative inference time. The AQM framework requires substantially more computation than an non-introspective model. Could authors report this relative increase in inference efficiency (say at K=20)? 


[E] Lack of Comparison to Base AQM
I would expect explicit comparison to AQM for a model named AQM+ or a discussion on why this is not possible.


===========
Minor Things:
===========

- I don't understand the 2nd claimed contribution from the introduction ""At every turn, AQM+ generates a question considering the context of the previous dialog, which is desirable in practice."" Is this claim because the aprxAns module uses history? 

- Review versions of papers often lack polished writing. I encourage the authors to review their manuscript for future versions with an eye for clarity of terminology, even if it means a departure from established notation in prior work. 

- The RL-QA qualitative results, are these from non-delta or delta? Is there a difference between the two in terms of interpretability? 

===========
Overview:
===========

The modifications made to adapt AQM to the GuessWhich setting presented here as AQM+ seem to be somewhat minor technical contributions. Further, where these difference could be explored in greater detail, there is a lack of analysis. That said, the proposed approach does make significant qualitative and quantitative improvements in the target problem. I'm fairly on the fence for this paper and look forward to seeing additional analysis and the opinions of other reviewers.



","The review starts with a positive note, acknowledging improvements since the last iteration and stating that the paper is 'good' and 'interesting'. However, the reviewer then raises several concerns, indicating that the technical novelty is limited and some design decisions lack justification. They also suggest additional experiments and comparisons. While the reviewer acknowledges the paper's strengths in improving performance, the numerous concerns suggest a more cautious than enthusiastic endorsement. Therefore, the sentiment is scored as moderately positive.",50.0,75.0
"This is a work regarding the alignment of word embedding for multiple languages.Though there are existing works similar to this one, most of them are only considering a pair of two languages, resulting in the composition issue mentioned in this work. The authors proposed a way of using a regularization term to reduce such degraded accuracy and demonstrate the validity of the proposed algorithm via experiments. I find the work to be interesting and well written. Several points that I want to bring up:

1. The language tree at the end of section 5 is very interesting. Does it change if the initialization/parameter is different?

2. The matrix P in (1) is simply a standard permutation matrix. I think the definitions are redundant.

3. The experiment results are expected since the algorithms are designed for better composition quality. An additional experiment, e.g. classification of instances in multiple languages, could further help demonstrate the strength of the proposed technic.

4. How to choose the regularization parameter \mu and what's the effect of \mu?

5. Some written issues like the notation of orthogonal matrix set, both \mathcal{O} and \mathbb{O} are used.","The reviewer explicitly states that they find the work ""interesting and well-written."" They also offer constructive criticism and suggestions for improvement, which indicates a positive attitude towards the paper. The language used is formal, respectful, and focused on the scientific content, suggesting a polite tone.",75.0,80.0
"This paper proposes a method for learning skills in absence of a reward function. These skills are learned so that the diversity of the trajectories produced by each skill is maximised. This is achieved by having a discriminator attempting to tell these skills apart. The agent is rewarded for visiting states that are easy to distinguish and the discriminator is trained to better infer the skills from states visited by the agent. Furthermore, a maximum entropy policy is used to force the skills to be diverse. The proposed method is general and any RL algorithm with entropy maximisation in the objective can be used, the implementation in the paper uses the Soft Actor Critic method.

The problem that they are tackling is interesting and is of clear value for obtaining more generalisable RL algorithms. The paper is overall clear and easy to follow, the results are interesting and potentially useful, although I have some reservations regarding how they assess this usefulness in the current version of this paper.
Structure-wise, I would say that the choice of writing the paper in the form of a Q&A, with very brief explanations and details was more distracting and at times unnecessary than I liked (e.g. Question 7 could move to Appendix as it is quite trivial).

I really appreciated how much care has been taken to discuss differences with the closest prior work, Variational Intrinsic Control (VIC) by Gregor et al. 
One such difference is that their prior distribution over skills is not learnt. While there are good arguments by the authors about why this is appealing (e.g. it prevents collapsing to sampling only a few skills), I feel this could be also quite a limitation of their method. This assumes that you have a good a-priori knowledge and assumptions regarding how many skills are useful or needed in the environment. This is unlikely to be the case in complex environments, where you first need to learn simple skills in order to explore the environment, and later learn to form new more complex skills. During this process, you might want to prune simplistic skills after you learnt more abstract and complex ones, for instance in the context of continual learning. I understand this could be investigated in future work, but I feel they take a rather optimistic take on this problem.

Overall, the use case for the proposed method is slightly unclear to me. While the paper claims to allow diverse set of skills to be learnt, it is highly dependent on learning varied action sequences that help you visit different part of state space, regardless of their usefulness. This means there could be learn a lot of skills that capture part of the state space that is not useful or desirable for downstream tasks. While there is a case made for DIAYN being a stepping stone for imitation learning and hierarchical RL, I don’t find the reported experiments for imitation learning and HRL convincing. In the imitation learning experiment, the distance (KL divergence) between all skills and the expert data is computed and the closest skill is then chosen as the policy imitating the expert. The results are weak and no comparisons with any LfD baselines are reported. The HRL experiments also lack comparisons to any other HRL baseline. I feel that this section is rather weak, especially compared to the rest of the paper, and I am not sure it achieves much.

As a general comment, the choice of reporting the training progress using “hours spent training” is an peculiar choice which is never discussed. I understand that for methods with varying computational costs this might be a fairer comparison but it would be perhaps good to also report progress against number of required environment interactions (including pre-training).
Another assumption made is that the method is valuable in situations where the reward function is expensive to compute and the unsupervised pre-training is free (somewhat easing the large amount of pre-training required). However, it would have been interesting to see examples of such environments in their experiments supporting these claims, as this assumption is not valid for the chosen MuJoCo environments.

Despite these comments, I still feel this is valuable work, that can clearly inspire further relevant work and deserves to be presented at ICLR.
It presents a solid contribution, given its technical novelty, proposed applications and its overall generality. 
However, the paper could use more convincing experiments to support its claims.

Additional comments and typos:
- Figure 5 lack error bars across the 5 random seeds and are crucial to assess whether this performance difference is indeed significant given the amount of pre-training required.
- Figure 7’s title and caption is missing...
- typo: page 3, last paragraph “...mutual information between skills and states, **I(S; Z )**” not I(A; Z)
- typo: page 7 paragraph next to Figure 6 “...whereas DIAYN explicitly **learns** skills that effectively partition the state space”
- typo: page 7 above Figure 8 “...make them **exceedingly** difficult for non- hierarchical RL algorithms.”
","The reviewer provides a generally positive overview of the paper, highlighting its interesting problem, clear presentation, and potential usefulness. They acknowledge the value of the technical novelty and the work's ability to inspire further research. However, the reviewer also expresses reservations, particularly regarding the clarity of the use case, the lack of strong baselines in the imitation learning and HRL experiments, and the choice of reporting training progress in hours. The language used is constructive and professional throughout.",60.0,80.0
"This paper introduces a deep RL algorithm to solve the Rubik's cube. The particularity of this algorithm is to handle the huge state space and very sparse reward of the Rubik's cube. To do so, a) it ensures each training batch contains states close to the reward by scrambling the solution; b) it computes an approximate value and policy for that state using the current model and c) it weights data points based by the inverse of the number of random moves from the solution used to generate that training point. The resulting model is compared to two non-ML algorithms and shown to be competitive either on computational speed or on the quality of the solution.  

This paper is well written and clear. To the best of my knowledge, this is the first RL-based approach to handle the Rubik's cube problem so well. The specificities of this problem make it interesting. While the idea of starting from the solution seemed straightforward at first, the paper describes more advanced tricks claimed to be necessary to make the algorithm work. The algorithm seems to be quite successful and competitive with expert algorithms, which I find very nice. Overall, I found the proposed approach interesting and sparsity of reward is an important problem so I would rather be in favor of accepting this paper. 

On the negative side, I am slightly disappointed that the paper does not link to a repository with the code. Is this something the authors are considering in the future? While it does not seem difficult to code, it is still nice to have the experimental setup.

There has been (unsuccessful) attempts to solve the Rubik's cube using deep RL before. I found some of them here: https://github.com/jasonrute/puzzle_cube . I am not sure whether these can be considered prior art as I could not find associated accepted papers but some are quite detailed. Some could also provide additional baselines for the proposed methods and highlight the challenges of the Rubik's cube.

I am also curious whether/how redundant positions are handled by the proposed approach and wished this would be discussed a bit. Considering the nature of the state space and the dynamics, I would have expected this to be a significant problem, unlike in Go or chess. Does the algorithm forbid the reverse of the last action? Is the learned value/policy function good enough that backwards moves are seldom explored? Since the paper mention that BFS is interesting to remove cycles, I assume identical states are not duplicated. Is this correct?","The reviewer clearly states that they are ""in favor of accepting this paper"" and finds the work interesting and well-executed. They commend the paper's clarity, novelty, and the success of the algorithm. While they raise valid questions and suggestions, these are framed constructively and aim to improve the paper. Therefore, the sentiment is positive, but not overly enthusiastic, as the reviewer also points out areas for improvement. The language is polite and professional throughout, with constructive criticism and suggestions for improvement.",75.0,90.0
"Summary: 

This paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. 

My detailed comments are as follows.


Strengths:

1. The motivation for this paper is reasonable and very important. 

2. The authors proposed a new method for dynamic channel pruning.

Weaknesses:

1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming?

2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read.

3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue.

4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments.

5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet?

6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments.

7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018)
[1] Channel pruning for accelerating very deep neural networks. CVPR 2017.
[2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017.
[3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018.
","The review acknowledges the importance of the paper's subject and the novelty of the proposed method as strengths. However, it raises several significant concerns regarding the novelty, writing quality, technical details, limited experimental evaluation, and lack of comparison with state-of-the-art methods. The reviewer also points out potential flaws in the methodology. The tone, while direct and critical, maintains a professional and constructive approach by providing specific recommendations for improvement. ",-10.0,60.0
"Summary:
This work presents a method to generate adversary examples capable of fooling a neural network classifier. Szegedy et al. (2013) were the first to expose the weakness of neural networks against adversarial attacks, by adding a human-imperceptible noise to images to induce misclassification. Since then, several works tackled this problem by modifying the image directly in the pixel space: the norm-balls convention. The authors argue that this leads to non-realistic attacks and that a network would not benefit from training with these adversarial images when performing in the real world. Their solution and contributions are parametric norm-balls: unlike state-of-the-art methods, they perform perturbations in the image formation space, namely the geometry and the lighting, which are indeed perturbations that could happen in real life. For that, they defined a differentiable renderer by making some assumptions to simplify its expression compared to solving a light transport equation. The main simplifications are the direct illumination to gain computation efficiency and the distant illumination and diffuse material assumptions to represent lighting in terms of spherical harmonics as in Ramamoorthi et al. (2001), which require only 9 parameters to approximate lighting. This allows them to analytically derivate their loss function according to the geometry and lighting and therefore generate their adversary examples via gradient descent. They show that their adversary images generalize to other classifiers than the one used (ResNet). They then show that injecting these images into the training set increase the robustness of WideResNet against real attacks. These real attack images were taken by the authors in a laboratory with varying illumination.

Strength:
- The proposed perturbations in the image formation space simulate the real life scenario attacks.
- The presented results show that the generated adversary images do fool the classifier (used to compute the loss) but also new classifiers (different than the one used to compute the loss). As a consequence the generated adversary images increase the robustness of the considered classifier. 
- Flexibility in their cost function allows for diverse types of attacks: the same modified geometry can fool a classifier in several views, either into detecting the same object or detecting different false objects under different views. 

Major comments:
- Method can only compute synthetic adversary examples, unlike state-of-the-art.
- The main contribution claimed by the author is that their perturbations are realistic and that it would help better increase the robustness of classifiers against real attacks. However, they do not give any comparison to the state-of-the-art methods as is expected. 

Minor comments:
- Even if the paper is well written, they are still some typos. 
","The review is mostly positive. It highlights the strengths of the paper, such as the novelty and realism of the proposed method, and the promising results. The reviewer also acknowledges the paper is well-written. However, it also points out a major weakness: the lack of comparison with state-of-the-art methods. This omission prevents the reviewer from fully assessing the claimed contribution of the paper. The language used is neutral and professional throughout the review.",60.0,50.0
"The authors demonstrate the generalization bound for deep neural networks using the PAC-Bayesian approach. They adopt the idea of noise resilience in the analysis and obtain a result that has improved dependence in terms of the network dimensions, but involves parameters (e.g., pre-activation) that may be large potentially. 

My major concern is also regarding the dependence on the pre-activation that can be very large in practice. This is also shown in the numerical experiments. Therefore, the overall generalization bound can be larger than existing results, though the later have stronger dependence on the network sizes. By examining the analysis for the main result, it seems to me that the reason the authors can induce weaker dependence on network sizes is essentially they involved the pre-activation parameters. This can be viewed as a trade-off how strong the generalization bound depend on the network sizes and other related parameters (like the pre-activation here) rather than strictly tighten the error bound from a more refined/structured way. I also suggest that the authors provide the comparison of their bound and existing ones to see the quantitative difference of the results. 

Regarding the noise resilience, it is not clear to where the noise resilience shows up from the analysis or the result. From the proof of the main result, the analysis seems to be standard as in the PAC-Bayesian analysis, which is based on bounding the difference of the network before and after injecting randomness into the parameters. The difference with respect to the previous result due to the different way of bounding such a gap, where the Jacobian, the pre-activation and function output pop up. But this does not explain how well a network can tolerate the noise, either in the parameter space of the data space. This is different with the previous analysis based on the noise resilience, such as [1]. So, the title and the way the authors explain as noise resilience is somewhat misleading. More detailed explanation will help.

[1] Arora et al. Stronger generalization bounds for deep nets via a compression approach. 
","The reviewer raises valid concerns about the paper's approach and findings, particularly the dependence on pre-activation parameters, which weakens the claimed improvement in generalization bounds. While acknowledging the authors' attempt, the reviewer finds the explanation of noise resilience unclear and potentially misleading. The language used is professional and suggestive, not overly negative or rude.",-20.0,60.0
"This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice. The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\tilde f_i - f_i| should be minimized where \tilde f_i is the estimate of the true frequency f_i.

Pros:
-- Interesting topic of using machine learned advice to speed up frequency estimation is considered
-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice
-- Experiments are given to justify claimed improvements in performance

Cons:

-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.

-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves. While in some applications this might be natural, this is certainly very restrictive in situations where f_i’s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.

-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.

-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.

-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.


Other comments:
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.



","The review acknowledges the paper's positive aspects (interesting topic, new bounds, experimental justification) but raises several significant concerns. These concerns, particularly regarding the error model's limitations and the algorithm's simplicity, suggest that the reviewer finds the paper's contributions somewhat limited. The tone is critical but professional and not disrespectful. Therefore, the sentiment leans slightly negative, and the politeness remains neutral.",-20.0,0.0
"Interesting work, extending previous work by Balestriero and Baraniuk in a relevant and non-trivial direction. The presentation could be cleaner and clearer, 

The paper contains solid work and contributes to an interesting perspective/interpretation of deep networks. The presentation is reasonably clear, although somewhat cluttered by a large number of subscripts and superscripts, which could be avoided by using a more modular formulation; e.g., in equation (1), when referring to a specific layer l, the superscript l can be dropped as it adds no useful information. By the way, when l is first used, just before equation (1), it is undefined, although the reader can guess what it stands for.

It is not clear why $[\pi^{(l)}]_{k,t}$ is defined after equation (5), as these quantities are not mentioned in Theorem 2. Another confusion issue is that it is not clear if the assumption made in Proposition 1 concerning is only valid there of if it is assued to hold elsewhere in the paper.

Proposition 2 is simply a statement of the well-known relationship between between soft-max (a.k.a. logistic regression) and the maximum entropy principle (see, for example, http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf).

","The review starts with positive remarks, acknowledging the work's value and contribution ('interesting work', 'relevant and non-trivial', 'solid work', 'interesting perspective'). However, it also points out areas for improvement regarding clarity and presentation. While the reviewer provides constructive criticism and suggestions, the tone remains professional and respectful throughout. The mention of 'confusion issues' and stating that Proposition 2 is 'simply a statement of the well-known relationship' slightly detracts from the overall positivity. ",60.0,70.0
"The authors propose an exploration bonus that is aimed to aid in sparse reward RL problems. The bonus is given by an auxillary network which tries to score whether a candidate observation is difficult to reach with respect to all previously observed novel observations which are stored in a memory buffer. The paper considers many experiments on complex 3D environments. 

The paper is well written and very well illustrated. The method can be clearly understood from the 3 figures and the examples are nice. I think the method is interesting and novel and it is evaluated on a realistic and challenging problem.

It would be good if the authors could further elaborate on the scalability of the method in terms of compute/memory requirements and related to that if the implementation is cumbersome. I didn’t understand well how the method avoids the issue of old memories leaving the buffer. It seems for a large enough environment important observations will eventually become discarded causing a poor approximation of the curiosity bonus? For the large scale experiments I would like to know more rough details of the number of the compute time needed for the method relative to the PPO baseline and the other baseline (e.g. number of nodes for example and how long they run approximately)

Are there any potential issues with adapting the method on 2D environments like Atari? this could permit direct comparisons with several other recently proposed techniques in this area.

The Grid-Oracle result is very interesting and a contribution on it’s own if similar results for complex 3D environments are not published anywhere else. It demonstrates well that exploration bonuses can help drastically in these tasks. I think if possible it would be interesting to have an idea how fast this method converges (number of training steps) and not just the final reward as reported in the tables. Indeed as a general problem the current number of training steps of any methods shown seem to indicate these techniques are too data hungry for non-simulated environments. For some applications (e.g. aimed at sim-to-real transfer) the grid-oracle approach might be a good alternative to consider. I would be interested to know if the authors had some thoughts on this.

Overall I lean towards accept, the method is shown to work on relatively very complex problems in DMLab and VizDoom while most sparse reward solutions proposed are typically evaluated on relatively simpler and unrealistic tasks. I would consider to further increase my score if the authors can address some of the comments. 
","The reviewer explicitly states ""Overall I lean towards accept"" and provides positive feedback such as ""well written"", ""clearly understood"", ""interesting and novel"", ""realistic and challenging"", ""very interesting and a contribution on its own"". The suggestions for improvement are constructive and aim to further strengthen the paper rather than pointing out flaws. Therefore, the sentiment is assessed as positive. The language used is polite and professional throughout the review, without any negative or condescending remarks.",75.0,100.0
"The paper presents an application of Bayesian neural networks in predicting 
future street scenes. The inference is done by using variational approximation 
to the posterior. Moreover, the authors propose to using a synthetic (approximate)
likelihood and the optimization step in variational approxiation is based on a regularization.
These modifications are claimed by the authors that it yields a better results in practice 
(more stable, capture the multi-modal nature). Numerical parts in the paper support
the authors' claims: their method outperforms some other state-of-the-art methods.

The presentation is not too hard to follow.
I think this is a nice applied piece, although I have never worked on this applied side.

Minor comment:
In the second sentence, in Section 3.1, page 3, 
$f: x \mapsto y$    NOT $f: x \rightarrow y$. 
We use the ""\rightarrow"" for spaces X,Y not for variables.  


","The reviewer provides a positive summary of the paper, highlighting its strengths like the application of Bayesian neural networks, the use of synthetic likelihood, and the effectiveness of the proposed modifications. The reviewer acknowledges the paper's clarity and finds the numerical results convincing. While there's a minor comment on mathematical notation, it doesn't detract from the overall positive sentiment. ",75.0,90.0
"This paper studies convergence of gradient descent on a two-layer fully connected ReLU network with binary output and square loss. The main result is that if the number of hidden units is polynomially large in terms of the number of training samples, then under suitable randomly initialization conditions and given that the output weights are fixed, gradient descent necessarily converge to zero training loss.

Pros:
The paper is presented clearly enough, but I still urge the authors to carefully check for typos and grammatical mistakes as they revise the paper. As far as I have checked, the proofs are correct. The analysis is quite simple and elegant. This is one thing that I really like about this paper compared to previous work. 

Cons:
The current setting and conditions for the main result to hold are quite a bit limited. If one has polynomially large number of neurons (i.e. on the order of n^6 where n is number of training samples) as stated in the paper, then the weights of the hidden layer can be easily chosen so that the outputs of all training samples become linearly independent in the hidden layer (see e.g. [1] for the construction, which requires only n neurons even with weight sharing) , and thus fixing these weights and optimizing for the output weights would lead directly to a convex problem with the same theoretical guarantee. At this point, it would be good to explain why this paper is focusing on the opposite setting, namely fixing the output weights and learning just the hidden layer weights, because it seems that this just makes the problem become more non-trivial compared to the previous case while yielding almost the same results . Either way, this is not the way how practical neural networks are trained as only a subset of the weights are optimized. Thus it's hard to conclude from here why the commonly used GD w.r.t. all variables converges to zero loss as stated in the abstract.

The condition on the Gram matrix H_infty in Theorem 3.1 seems to be critical. I would like to see the proof that this condition can be fulfilled under certain conditions on the training data.

In Lemma 3.1, it seems that ""log^2(n/delta)"" should be ""log(n^2/delta)""? 

Despite the above limitations, I think that the analysis in this paper is still interesting (mainly due to its simplicity) from a theoretical perspective. Given the difficulty of the problem, I'm happy to vote for its acceptance.

[1] Optimization landscape and expressivity of deep CNNs","The reviewer acknowledges the paper's clarity, correctness, and elegant analysis as positive aspects. They find the paper interesting from a theoretical perspective and lean towards acceptance. However, they also point out significant limitations regarding the practicality of the setting, the limited generalizability of the findings, and the dependence on a specific condition without proof of its fulfillment. The reviewer also points out a potential error in the paper. The language used is polite and professional, offering constructive criticism and suggestions for improvement.",50.0,80.0
"Evaluation:
This is a solid paper: The idea is clear, it is well communicated and put into context of the existing literature, and the results are promising. The experiments are well chosen and illustrate the method well. The connection between the chosen setting (BAMDPs) to POMDPs is explained well and explored in the empirical evaluation as well. I think that the methods section could go into a bit more detail, and the underlying assumptions that the authors make could be discussed more critically.

Summary:
This paper looks at Bayes-Adaptive MDPs (BAMDPs) in which the latent parameter space is either
- a discrete finite set or
- a bounded continuous set that can be approximated via discretization.
Consequently, the authors choose to represent the belief as a categorical distribution, which can be represented by a vector of weights.
They further assume that the environment model is known. Hence, the posterior belief can be computed exactly.
If I understand correctly, the main contribution is that the authors represent the policy as a neural network and train it using a policy gradient algorithm.
This is a good first step towards scalable Bayesian policy optimisation.

Main Feedback:
- In the Introduction, first paragraph, you say one of the aspects of real-world robotics is that there's ""(1) an underlying dynamical system with unknown latent parameters"". I would argue that the dynamic system itself is typically also unknown, including how it is parametrized by these latent parameters. I think it is important to point this out more explicitly in the introduction (it is mentioned in sec 2 and 5, but maybe it's worth mentioning it in 4 again as well): for the problems that you look at, you assume that the form of the transition function is known (just not its parameters phi). 
- In the main methods section (4), it would be nice to see some more detail about the Bayes filter. Can you write out the distribution over the latent parameters, and write out how the filtering is done? Explain how to compute the normalising constant (and mention explicitly why this is possible for your set-up, and why it would be infeasible if the latent space cannot be discretized). How exactly is the posterior distribution represented and fed to the policy? Seeing this done explicitly in Section 4 (even if it repeats some things that are explained in 2) would help someone that is interested in (re-)implementing the proposed method.
- I would like to see a more critical discussion in Section 7 about the assumptions that the authors make: that the environment models are known, and that the latent space can be discretized. How realistic are those assumptions (and in which kind of real-world problems can we make them), and what are ways forward to drop these assumptions?

Other Comments:
- Introduction: Using an encoder for the state/belief is an implementation choice, and (as I see it) not part of the main contribution. I would focus on explaining the intuition behind BPO in the introduction, and only mention the architecture choice as a side note.
- Related Work: The authors might be interested in the recent work of Igl et al. (ICML 2018, ""Deep Variational RL for POMDPs""), who approximate the belief in a POMDP using variational inference and a particle filter.

Significance for ICLR:
- In the light-dark experiment, the authors visualise the belief that the agent has at every time step. It would have been nice to see an analysis of how exactly the belief looks also for maybe 1-2 other experiments, and how (when) the agent makes a decision based on this. This could replace Table 2 (which I guess should be called Figure 2?), which I did not find very insightful.","The review is positive overall. The reviewer finds the paper to be ""solid"" with a ""clear idea"" and ""promising results."" They also praise the choice of experiments and the clarity of the writing. While the reviewer provides constructive feedback and suggestions for improvement, they are presented politely and with the aim of enhancing the paper. There are no harsh criticisms or negative remarks. ",75.0,90.0
"

[clarity]
This paper is basically well written. 
The motivation is clear and reasonable.
However, I have some points that I need to confirm for review (Please see the significance part).


[originality]
The idea of taking advantage of von Mises-Fisher distributions is not novel in the context of DL/DNN research community.
E.g.,
von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification.

However, as described in the paper, the incorporation of von Mises-Fisher for calculating loss function seems to be novel, to the best of my knowledge.


[significance]
Unfortunately, the experiments in this paper do not fully support the effectiveness of the proposed method. 
See below for more detailed comments.


*weak baseline (comparison)
As an anonymous reviewer pointed out, the author should run baseline method with beam search if the authors aim to convince readers (including reviewers) for the effectiveness of the proposed method.
I understand that it is important to investigate the effectiveness of the proposed method in the identical settings. However, it is also important to compare the proposed method with strong baseline to reveal the relative effectiveness of the proposed method comparing with the current state-of-the-art methods. 


* open vocabulary setting
I am confused whether the experimental setting for the proposed method is really in an open vocabulary setting or not.
If my understanding is correct, the vocabulary sizes used for the proposed method were 50,000 (iwslt2016) and 300,000 (wmt16), which cannot be an open vocabulary setting. 
If this is correct, the applicability of the proposed method is potentially limited comparing with the subword-based approach.
Is there any comment for this question?


* convergence speed
I think the claim of faster convergence of the proposed method in terms of iteration may be misleading. This might be true, but it is empirically proven only by single dataset and single run. The authors should show more empirical results on several datasets or provide a theoretical justification for this claim.


Overall, basically I like the idea of the proposed method. 
I also aim to remove the large computational cost of softmax in neural encoder-decoder approach.
In my feeling, the proposed method should be a bit more improved for a recommendation of clear acceptance.
","The reviewer finds the idea interesting and well-presented (""This paper is basically well written."", ""The motivation is clear and reasonable."", ""Overall, basically I like the idea of the proposed method.""). However, they have major concerns about the significance of the work, mainly due to weak baseline comparisons, potential limitations in the experimental setup, and lack of strong evidence for some claims. This suggests a sentiment leaning towards skepticism and a need for major revisions. The language used is polite throughout, with constructive criticism and suggestions for improvement.",30.0,70.0
"This paper investigates sophistical exploration approaches for reinforcement learning. Motivated by the fact that most of bandit algorithms do not handle heteroscedasticity of noise, the authors built on Information Direct Sampling and on Distributional Reinforcement Learning to propose a new exploration algorithm family. Two versions of the exploration strategy are evaluated against the state-of-the-art on Atari games: DQN-IDS for homoscedatic noise and C51-IDS for heteroscedastic noise. 

The paper is well-written. The background section provides the clues to understand the approach. In IDS, the selected action is the one that minimizes the ratio between a squared conservative estimate of the regret and the information gain. Following (Ktischner and Krause 2018), the authors propose to use \log(1+\sigma^2_t(a)/\rho^2(a)) as the information gain function, which corresponds to a Gaussian prior, where \sigma^2_t is the variance of the parametric estimate of E[R(a)] and \rho^2(a) is the variance of R(a). \sigma^2_t is evaluated by bootstrap (Boostrapped DQN). Where the paper becomes very interesting is that recent works on distributional RL allow to evaluate \rho^2(a). This is the main input of this paper: combining two recent approaches for handling heteroscedasticity of noise in Reinforcement Learning.

Major concern:
While the approach is appealing for handling heteroscedastic noise, the use of a normalized variance (eq 9) and a lower bound of variance (page 7) reveal that the approach needs some tuning which is not theoretically founded. 
This is problematic since in reinforcement learning, the environment is usually assumed to be unknown. What are the results when the lower bound of the variance is not used? When the variance of Z(a) is low, the variance of the parametric estimate should be low also. It is not the case?


Minor concerns:

The color codes of Figure 1 are unclear. The color of curves in subfigures (b) (c) (d) corresponds to the color code of IDS.

The way in which \rho^2(s,a) is computed in algorithm 1 is not precisely described. In particular page 6, the equation \rho^2(s,a)=Var(Z_k(s,a)) raises some questions: Is \rho evaluated for a particular bootstrap k or is \rho is averaged over the K bootstraps ?
_____________________________________________________________________________________________________________________________________________

I read the answers of authors. I increased my rating.
","The review is positive about the paper's novelty, clarity, and contribution ('well-written', 'provides the clues', 'very interesting', 'main input'). However, it raises a major concern about the theoretical foundation of some tuning aspects, which impacts the overall positivity. Therefore, the sentiment is scored as moderately positive. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement.",60.0,80.0
"The authors extend the theoretical results of a paper previously presented in the last edition of ICLR (2018), where it was demonstrated that Recurrent Neural Network can be interpreted as a tensor network decomposition based on the Tensor-Train (TT, Oseledets et al, 2011).
While previous results covered the multiplicative nonlinearity only, the contribution of the current paper is the extension of the analysis of universality and depth efficiency (Cohen et al, 2016) to different nonlinearities, for example ReLU (Rectified Linear Unit), which is very important from the practical point of view.
The paper is well written and have a good structure. However, I found that some deep concepts are not well introduced, and maybe other more trivial results are discussed with unnecessary details. The following comments could help authors to improve the quality of presentation of their paper:
-	Section 3.1 (Score Functions and Feature Tensor) is a bit short and difficult to read. 
o	Maybe, a more motivating introduction could be included in order to justify the definition of score functions (eq. 2). 
o	It would be also nice to state that, according to eq. (3), the feature tensor is a rank-1 tensor. 
o	I would suggest moving the definition of outer product to the Appendix, since most readers know it very well.
o	It is said that eq. 2 possesses the universal approximation property (it can approximate any function with any prescribed precision given sufficiently large M). It is not clear which is the approximation function.
-	A Connection with Tensor-Ring (TR) format, if possible, could be helpful: It is known that TR format (Zhao et al, 2016, arXiv:1606.05535), which is obtained by connecting the first and last units in a TT model, helps to alleviate the requirement of large ranks in the first and last the core tensors of a TT model reaching to a decomposition with an evenly distributed rank bounds. I think, it would be interesting to make a connection of RNN to TR because the assumption of R_i < R for all i becomes more natural. I would like to see at least some comment from the authors about the applicability of TR in the context of analysis of RNN, if possible. Maybe, also the initial hidden state defined in page 5 can be avoided if TR is used instead of TT.
-	Fig 2 shows that Test accuracy of a shallow network (CP based) is lower and increases with the number of parameters approaching to the one for RNN (TT based). It would be necessary to show the results for an extended range in the number of parameters, for example, by plotting the results up to 10^6. It is expected that, at some point, the effect of overfitting start decreasing the test accuracy.
-       When scores functions are presented (eq. 2) it is written the term ""logits"" between brackets. Could you please clarify why this term is introduced here? Usually, logit of a probability p is defined as L(p)=p/(1-p). What is the usage of this term in this work? 
-      I think the theory is presented for a model with the two-classes only but used for multiple classes in the experimental sections. It should be necessary to make some comment about this in the paper.
-      Details about how the RNN based on TT is applied must be added. More specifically, the authors should provide answers to clarify the following questions: 
(i) Are patches overlapped or non-overlapped? 
(ii) What value of M is used? and is there any general rule for this choice? 
(iii) How the classification in the 10-classes is obtained? Are you using a softmax function in the last layer? Are you using one weight tensor W_c per class (c=1,2,...,10). Please provide these technical details. 
(iv) Please, specify which nonlinear activation sigma is used in the feature map f_\theta(x).
(v) How many feature maps are used? and, Are the matrix A and vector b learned from training dataset or only the TT-cores need to be learned? ","The review is positive overall. The reviewer praises the paper for extending previous work, being well-written, and having a good structure. While they list several suggestions for improvement, these are largely framed as constructive feedback to help improve the paper's clarity and address potential gaps. The language used is formal and polite, typical of academic peer reviews.",60.0,80.0
"This paper introduces a new dataset and method for chatbots. In contrast to previous work, this paper specifically probes how well a dialogue system can use external unstructured knowledge. 

Quality:
Overall, this is a very high-quality paper. The dataset is developed well, the experimental setup is well thought-through and the authors perform many ablation studies to test different model variants. The main criticism I have would be that the human evaluation is rather simple (rating 1-5), I would have expected more fine-grained categories, especially ones that relate to how much knowledge the system uses (I appreciate the ""Wiki F1"" metric, but that is an automatic metric). As it is, the human evaluation shows that most of their contributions are not appreciated by human annotators. Further, the paper ends a bit abruptly, I would have expected a more in-depth discussion of next steps.

Clarity:
The description of the work is clear in most places. I particularly like the abstract and introduction, which set up the rest of the paper nicely. In some places, perhaps due to space restrictions, method descriptions are a bit too short.

Originality:
The paper is fairly original, especially the aspect about specifically using external knowledge. The authors could have been more clear on how the work differs from other work on non-goal directed dialogue work though (last paragraph of related work section).

Significance:
The dataset is really well-developed, hence I believe many working in the dialogue systems community will re-use the developed benchmark and build on this paper.

More detailed comments:
- Missing reference for goal-oriented dialogue datasets: Wen et al. 2017, A Network-based End-to-End Trainable Task-oriented Dialogue System, https://arxiv.org/abs/1604.04562
- How does the proposed dataset differ from the Reddit and Wikipedia datasets discussed in the last paragraph of the related work section? This should be explained.
- Page 3, paragraph ""Conversational Flow"": what is the maximum number of turns, if the minimum is 5?
- Page 3, paragraph ""Knowledge Retrieval"": how were the top 7 articles and first 10 sentences choices made? This seems arbitrary. Also, why wasn't the whole text used?
- Page 3, paragraph ""Knowledge Selection and Response Generation"": how do you deal with co-reference problems if you only ever select one sentence at a time? The same goes for the ""Knowledge Attention"" model described in Section 4.
- Page 3, paragraph ""Knowledge Selection and Response Generation"": how often do annotators choose ""no sentence selected""? It would be interesting to see more such statistics about the dataset
- Section 4.2: did you run experiments for BPE encoding? Would be good to see as this is a bit of a non-standard choice.
- Section 4.2: it would be good to explain the Cer et al. 2018 method directly in the paper
- Section 4.2: is there a reference for knowledge dropout? Also, it would be good to show ablation results for this.
- Section 5.1: why did you choose to pre-train on the Reddit data? There should be some more in-depth description of the Reddit dataset to motivate this choice.
- Section 5.1: what is the setup you use for multi-task learning on SQuAD? Is it just a hard parameter sharing model, or?
- Section 5.3: as stated above, the human evaluation is a little bit underwhelming, both in terms of setup and results. I'd expect a more fine-grained way of assessing conversations by humans, and also an explanation of why the retrieval performer without knowledge was assessed as being on par with the retrieval transformer memnet.
- Section 5.3: I assume higher=better for the human scores? This should be made explicit.
- Section 5.3: Have others used the ""F1 overlap score""? If so, cite.
- Section 5.3: I don't understand the argument that the human evaluation shows that humans prefer more natural responses. How does it show that?
- Section 5.3: The Wiki F1 score is kind of interesting because it shows to what degree the model uses knowledge. But the side-by-side comparison with the human scores shows that humans don't necessarily prefer chatbot models that use a lot of knowledge. I'd expect this to be discussed, and suggestions for future work to be made accordingly.
- Section 6: The paper ends a bit abruptly. It's be nice to suggest future areas of improvement.","The reviewer explicitly states that this is a ""very high-quality paper"" and that the dataset is ""really well-developed."" They praise the clarity of the abstract and introduction, and the thoroughness of the experimental setup. While they offer constructive criticism and suggestions for improvement, the overall tone is positive and encouraging. The reviewer focuses on specific areas for improvement without resorting to harsh language. The suggestions are framed as opportunities for enhancing the paper rather than criticisms of the authors' work.",75.0,80.0
"- Summary
This paper proposes a residual non-local attention network for image restoration. Specifically, the proposed method has local and non-local attention blocks to extract features which capture long-range dependencies. The local and non-local blocks consist of trunk branch and (non-) local mask branch. The proposed method is evaluated on image denoising, demosaicing, compression artifacts reduction, and super-resolution.

- Pros
  - The proposed method shows better performance than existing image restoration methods.
  - The effect of each proposed technique such as the mask branch and the non-local block is appropriately evaluated.

- Cons
  - It would be better to provide the state-of-the-art method[1] in the super-resolution task. 
    [1] Y. Zhang et al., Image Super-Resolution Using Very Deep Residual Channel Attention Networks, ECCV, 2018.
  - The technical contribution of the proposed method is not high, because the proposed method seems to be just using existing methods.
  - The contribution of the non-local operation is not clear to me. For example, how does the global information (i.e., long-range dependencies between pixels) help to solve image denoising tasks such as image denoising?

Overall, the technical contribution of the proposed method is not so high, but the proposed method is valuable and promising if we focus on the performance.
","The reviewer acknowledges the good performance of the paper's method ('The proposed method shows better performance...') and finds the evaluation appropriate. However, they find the technical contribution low ('The technical contribution of the proposed method is not high...') and question the novelty and application of some aspects. The final statement summarizes that the paper is valuable from a performance standpoint, but not highly novel. This suggests a somewhat mixed but ultimately leaning towards the positive side.",40.0,60.0
"The proposed kernel recurrent learning (KeRL) provides an alternative way to train recurrent neural network with backpropagation through time (BPTT) where the propagation of gradients can be skipped over different layers. The authors directly assume the sensitivity function between two layers with a distance of tau in a form of Eq. (7). The algorithm of BPTT is then approximated due to this assumption. The model parameters are changed to learn the network dynamics. The optimization problem turns out to estimate beta and gamma of the kernel function. The learned parameters are intuitive. There are a set of timescales to describe the memory of each neuron and a set of sensitivity weights to describe how strongly the neurons interact on average. The purpose of this study is to save the memory cost and to reduce the time complexity for online learning with comparable performance. 

Pros:
1. KeRL only needs to compute a few tensor operations at each time step, so online KeRL learns faster than online BPTT for the case with a reasonably long truncation length.
2. Biologically plausible statements are addressed.
3. A prior is imposed for the temporal sensitivity kernel. The issue of gradient vanishing is mitigated.
4. Theoretical illustration for KeRL in Sections 3 and 4 is clear and interesting.

Cons:
1. The proposed method is an approximation to BPTT training. Suppose the system performance is constrained. Some guesses are made. The system performance can be further improved.
2. The experiment on time cost due to online learning is required so that the reduction of time complexity can be illustrated.
3. The format of tables 1 and 2 can be improved. Caption is required in Table 1. Overlarge size of Table 2 can be fixed.
4.  A number of assumptions in Sections 3 and 4 are assumed.  When addressing Section 3, some assumptions in Section 4 are used. The organization of Sections 3 and 4 can be improved.","The review acknowledges the merits of the paper, highlighting its advantages and interesting aspects. While it points out areas for improvement, it maintains a constructive and objective tone. The reviewer provides specific suggestions for improvement, indicating a desire to see the paper strengthened rather than rejected. Therefore, the sentiment leans positive, and the language used is polite and professional.",60.0,80.0
"The paper presents a very important problem of utilizing a model on different platforms with own numerical round-offs. As a result, a model run on a different hardware or software than the one on which it was trained could completely fail due to numerical rounding-off issues. This problem has been considered in various papers, however, the classification task was mainly discussed. In this paper, on the other hand, the authors present how the numerical rounding-off issue could be solved in Latent-Variable Models (LVM).

In order to cope with the numerical rouding-off issue, the authors propose to use integer networks. They consider either quantized ReLU (QReLU) or quantized Tanh (Qtanh). Further, in order to properly train the integer NN, they utilize a bunch of techniques proposed in the past, mainly (Balle, 2018) and (Balle et al., 2018). However, as pointed out in the paper, some methods prevent training instabilities (e.g., Eqs. 18 and 19). All together, the paper tackles very important problem and proposes very interesting solution by bringing different techniques proposed for quantized NNs together .

Pros:
+ The paper is well-written.
+ The considered problem is of great importance and it is rather neglected in the literature.
+ The experiments are properly carried out.
+ The obtained results are impressive.

Cons:
- A natural question is whether the problem could be prevented by post-factum quantization of a neural network. As pointed out in the Discussion section, such procedure failed. However, it would be beneficiary to see an empirical evidence for that.
- It would be also interesting to see how a training process of an integer NN looks like. Since the NN is quantized, instabilities during training might occur. Additionally, its training process may take longer (more epochs) than a training of a standard (float) NN. An exemplary plot presenting a comparison between an integer NN training process and a standard NN training process would be highly appreciated.
- (Minor remark). The paper is well-written, however, it would be helpful to set the final learning algorithm. This would drastically help in reproducibility of the paper.

--REVISION--
After reading the authors response and looking at the new version of the paper I decided to increase my score. The paper tackles very important problem and I strongly believe it should be presented during the conference.","The review starts with positive remarks, highlighting the importance and novelty of the paper's topic. The reviewer acknowledges the good quality of writing, experiments, and results. While the reviewer lists a few 'cons,' these are framed as constructive suggestions for improvement rather than harsh criticisms.  The final paragraph confirms a positive sentiment, advocating for the paper's acceptance. Therefore, the sentiment is positive, but not overly enthusiastic, as it includes suggestions for improvement. The language is polite and professional throughout, with constructive criticism and suggestions.",75.0,90.0
"The paper proposes a method to find adversarial examples in which the changes are localized to small regions of the image. A group-sparsity objective is introduced for this purpose and it is combined with an l_p objective that was used in prior work to define proximity to the original example. ADMM is applied to maximize the defined objective. It is shown that adversarial examples in which all changes are concentrated in just few regions can be found with the proposed method.

The paper is clearly written and results are convincing. But what I am not sure I understand is what is the purpose of this research. Among the 4 contributions listed in the end of the intro only the last one, Interpretability, seems to have a potential in terms on the impact. Yet am not quite sure how “obtained group-sparse adversarial patterns better shed light on the mechanisms of adversarial perturbations”. I think the mechanisms of adversarial perturbations remain as unclear as they were before this paper.

I am not ready to recommend acceptance of this paper, because I think the due effort to explain the motivation for research and its potential impacts has not been done in this case. 

UPD: the discussion and the edits with the authors convinced me that I may have been a bit too strict. I have changed my score from 5 to 6.
","The reviewer acknowledges the paper's clarity and convincing results, indicating a somewhat positive sentiment. However, they express uncertainty about the research's purpose and impact, pulling the sentiment towards neutral. The update to a score of 6 (out of 10, assuming a standard scale) further suggests a slightly positive leaning. The language used is professional and avoids harsh criticism, suggesting politeness. ",20.0,70.0
"The paper proposes a method to solve end-to-end learning tasks using a combination of deep networks and domain specific black-box functions. In many machine learning tasks there may be a sub-part of the task can be easily solved with a black-box function (e.g a hard coded logic).  The paper proposes to use this knowledge in order to design a deep net that mimics the black-box function. This deep net being differentiable can be utilized while training in order to perform back-propagation for the deep nets that are employed to solve the remaining parts of the task. 

The paper is well written and in my opinion the experiments are solid. They show significant gains over well-designed baselines. (It should be noted that I am not super familiar with prior work in this area and may not be aware of some related baselines that can be compared with.)

In Section 3.1.2 the authors discuss offline and online methods to train the mimicking deep network of a black-box function. The offline version suffers from wasting samples on unwanted regions while the online version will have a cold-start problem. However, I believe there can be better solution than the hybrid strategy. In fact there is a clear explore/exploit trade-off  here. Therefore, one may start with a prior over the input domain of the black-box function and then as the argument extractor learns well the posterior can be updated. Then we can Thompson sample the inputs from this posterior in order to train the mimicking network.  I think such a bandit inspired approach will be interesting to try out. ","The reviewer explicitly states the paper is well-written and the experiments are solid, showing significant gains. They do offer a suggestion, but it's presented as an area for potential improvement and further exploration, not a criticism of the existing work. This suggests a positive sentiment. The language used is constructive and professional throughout, indicating politeness.",75.0,80.0
"This paper presents an analysis of popularly-use RNN model for structure modeling abilities by designing Tensor Product Decomposition Networks to approximate the encoder. The results show that the representations exhibit interpretable compositional structure. To provide better understanding, the paper evaluates the performance on synthesized digit sequence data as well as several sentence-encoding tasks.

Pros:
1. The paper is well-written and easy to follow. The design of the TPDN and corresponding settings (including what an filler is and what roles are included) for experiments are understandable. It makes good point at the end of the paper (section 4) on how these analysis contribute to further design of RNN models, which seems useful.
2. The experiments are extensive to support their claims. Not only synthetic data but also several popularly-used data and models are being conducted and compared. An addition of analogy dataset further demonstrate the effect of TPDN on modeling structural regularities.

Cons:
1. More detailed and extensive discussion on the contribution of the paper should be included in the introduction part to help readers understand what's the point of investigating TPDN on RNN models.
2. Some details are missing to better understand the construction. For example, on page 4, Evaluation, it is unclear of how TPDN encoder is trained, specifically, which parameters are updated? What's the objective for training? It is also unclear of whether the models in Figure 3(c) use bidirectional or unidirectional or tree decoder? In Section 3, it could be better to roughly introduce each of the existing 4 models. How do TPDN trained for these 4 sentence encoding models need to be further illustrated. More reasons should be discussed for the results in Table 2 (why bag-of-words role seem to be ok, why skip-thought cannot be approximated well).
3. It could be better to provide the actual performance (accuracy) given by TPDN on the 4 existing tasks.
4. Further thoughts: have you considered applying these analysis on other models besides RNN?","The review is overall positive. It praises the paper's clarity, experimental setup, and interesting findings. While it lists several areas for improvement, these are mostly requests for clarification and further details, rather than fundamental flaws. The language is constructive and professional throughout.",60.0,80.0
"This paper describes a model for vision-and-language navigation. The proposed
model adds two components to the baseline model proposed by Fried et al. (2018):

- a panoramic visual attention (referred to in this paper as ""visual--textual
  co-grounding""), in which the full scene around the agent's current position is
  attended to prior to selecting a direction to follow

- an auxiliary ""progress monitoring"" loss which encourages the agent to to
  produce textual attentions from which the distance to the goal can be directly
  inferred

The two components combine to give state-of-the-art results on the Room2Room
dataset: small improvements over existing approaches on the ""-seen"" evaluation
set and larger improvements on the ""-unseen"" evaluation sets. These improvements
also stack with the data-augmentation approach of Fried et al.

I think this is a reasonable submission and should probably be accepted. However, I
have some concerns about presentation and a number of specific questions about
model implementation and evaluation.

PRESENTATION AND NAMING

First off: I implore the authors to find some descriptor other than ""self-aware""
for the proposed model. ""Self-aware"" is an imprecise description of the agent in
this paper---the agent is specifically ""aware"" of its visual surroundings and
its distance from the goal, neither of which is meaningfully an aspect of
""self"". Moreover, self-awareness means something quite different in adjacent
areas of cognitive science and philosophy; overloading the term in the specific
(and comparatively mundane) way used here creates confusion. See section 3.4 of
https://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something
like ""visual / temporal context-sensitivity"" to describe what's new here? A bit
clunky, but I think it makes the contributions of this work much clearer.

As suggested in the summary above, I also think ""visual--textual co-attention""
is also an unhelpfully vague description of this aspect of the contribution. The
textual attention mechanism used in this paper is the same as in all previous
work on the task. Representations of language don't even interact with the
visual attention mechanism except by way of the hidden state, and the salient
new feature of the visual attention is the fact that it considers the full
panoramic context before choosing a direction.

MODELING QUESTIONS

- p4: $y_t^{pm}$ is defined as the ""normalized distance from the current
  viewpoint to the goal"". Is this distance in units of length (as defined by the
  simulator) or units of time (i.e. the number of discrete ""steps"" needed to
  reach the goal)?

  The authors have already clarified on OpenReview that the progress monitor
  objective uses an MSE loss rather than a likelihood loss. Do I understand
  correctly that ground-truth distances are in [0, 1] but model predictions are
  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search
  candidates as $p_t^{pm} \times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?

- The input to the progress monitor is formed by concatenating the attention
  vector $\alpha_t$ to a vector of state features, and then multiplying by a
  fixed weight matrix. How is this possible? The size of $\alpha_t$ varies
  depending on the length of the instruction sequence. Are attentions padded out
  to the length of the longest instruction in the training set? If so, how can
  the model learn when it's reached the end of a short instruction sequence?
  What would happen if the agent encountered a sequence that was too long?

EVALUATION QUESTIONS

- The progress monitor is used both as an auxiliary training objective and as a
  beam search heuristic. Is it possible to disentangle these two contributions?
  (E.g. by ignoring the scores during beam search, or by doing augmented beam
  search in a model that was trained without the auxiliary objective.)

- Not critical, but it would be nice to know if the contributions here stack
  with the pragmatic inference procedure in Fried et al.

- While, as pointed out on OpenReview, it is not required to include SPL
  evaluations, I think it would be informative to do so---the preliminary
  results with no beam search look good!

MISCELLANEOUS

p1: ""without a map"" If you can do beam search, you effectively have a map.

p1: ""...smoothly"" What does ""smoothly"" mean in this context?

p2: ""the position of grounded instruction can follow past and future
    instructions"". Is the claim here that if instructions are of the form ""ACB""
    and the agent is supposed to do ""ABC"", that the proposed model will execute
    these instructions successfully and the baseline will not? This claim does
    not appear to be evaluated anywhere in the body of the paper.

p4: ""intelligently prunes"" ""Intelligently"" is unnecessary.

p4: ""for empirical reasons"" What does this mean?

p5: ""Intuitively, an instruction-following agent is required..."" The existence
    of non-attentive models that do reasonably well at these
    instruction-following tasks suggest that this is not actually a requirement.","The reviewer states ""I think this is a reasonable submission and should probably be accepted."", which indicates an overall positive sentiment. However, the review also lists several concerns and questions, suggesting that the reviewer doesn't perceive the paper as perfect. Therefore, a moderately positive score seems appropriate. 

The language used is quite formal and academic, but never disrespectful or rude. The reviewer does point out potentially misleading word choices, but does so in a constructive and helpful manner. Thus, the politeness score leans positive.",60.0,80.0
"The authors propose a policy transfer scheme which in the source domain simultaneously learns a family of policies parameterised by dynamics parameters and then employs an optimisation framework to select appropriate dynamics parameters based on samples from the target domain. The approach is evaluated on a number of simulated transfer tasks (either transferring from DART to MuJoCo or by introducing deliberate model inaccuracies).

This is interesting work in the context of system identification for policy transfer with an elaborate experimental evaluation. The policy learning part seems largely similar to that employed by Yu et al. 2017 (as acknowledged by the authors). This makes the principal contribution, in the eyes of this reviewer, the optimisation step conducted based on rollouts in the target domain. While the notion of optimising over the space of dynamics parameters is intuitive the question arises whether this optimisation step makes for a substantive contribution over the original work. This point is not really addressed in the experimental evaluation as benchmarking is performed against a robust and an adaptive policy but not explicitly against the (arguably) most closely related work in Yu et al. It could be argued, of course, that Yu et al. essentially use adaptive policy generation but they do explicitly learn dynamics parameters based on recent history of actions and observations. An explicit comparison therefore seems appropriate (or alternatively a discussion of why it is not required).

Another point which would, in my view, add significant value is explicit discussion of the baseline performances observed in the various experiments. For example, in the hopper experiment (Sec 5.2) the authors state that the baseline methods were not able to adapt to the new environment. Real value could be derived here if the authors could elaborate on why this is the case. The same applies in Sec 5.3-5.6. 

(I would add here, as an aside, that I thought the notion in Sec 5.6 of framing the learning of policies for handling deformable objects as a transfer task based on rigid objects to be a nice idea. And not one this reviewer has come across before - though this could merely be a reflection of limited familiarity with the literature).

The experimental evaluation seems thorough with the above caveat of a seemingly missing benchmark in Yu et al. I would also encourage the authors to add more detail in the experimental section in the main text specifically with regards to number of trials run to arrive at variances in the figures as well as what metric these shaded areas actually signify. 

A minor point: the J in equ 1 seems (to me at least) undefined. I suspect that it signifies the expected cumulative reward and was meant to be introduced in Sec 3 where the J may have been dropped from the latex?

If the above points were addressed I think this would make a valuable and interesting contribution to the ICLR community. As it stands I believe it is marginally below the acceptance threshold.

[ADDENDUM: given the author feedback and addition of the benchmark experiments requested I have updated my score.]


Pros:
———
- interesting work
- accessible
- effective
- thorough evaluation (though potentially missing a key benchmark)

Cons:
———
- potentially missing a key benchmark (and therefore seems somewhat incremental)
- only limited insight offered by the authors in the discussion of the experimental results
- some more details needed with regards to the experimental setup
","The reviewer finds the work interesting and well-executed overall, praising its thorough evaluation and accessible presentation. However, they express concerns about the incrementality of the contribution, suggesting a potentially crucial missing benchmark (Yu et al. 2017) that could significantly impact the assessment of the work's novelty. The reviewer also desires more in-depth discussion and analysis of the experimental results, particularly regarding baseline performances and the reasons behind their limitations. While acknowledging the positive aspects, the reviewer's emphasis on the missing benchmark and the need for more insightful analysis leads to a sentiment leaning slightly towards the negative side. The language used is polite and professional throughout, offering constructive criticism and suggestions for improvement.",-10.0,80.0
"The authors propose a benchmark for optimization algorithms specific to deep learning called DeepOBS. They provide code to evaluate an optimizer against a suite of standard tasks in deep learning, and provide well tuned baselines for a comparison. The authors discuss important considerations when comparing optimizers, including how to measure speed and tunability of an optimizer, what metric(s) to compare against, and how to deal with stochasticity.

A clear, standardized optimization benchmark suite would be very valuable for the field. As the others clearly state in the introduction, there have been many proposed optimization algorithms, but it is hard to compare many of these due to differences in how the optimizers were evaluated in the original papers. In general, people have different requirements for what the expect from an optimizer. However, this paper does a good job of discussing most of the factors that people should consider when choosing or comparing optimizers. Providing a set of well tuned baselines would save people a lot of time in making comparisons with a new optimizer, as well as providing a canonical set of tasks to evaluate against. I particularly appreciated the breadth and diversity of the included tasks.

I am a little worried that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons, but I think this benchmark would be a valuable resource for the community.

Some minor comments:
- In section 2.3, there is a recommendation for how to estimate per-iteration cost. I would mention in this section that this procedure is automated and part of the benchmark suite.
- I wanted to see how the baselines performed on all of the tasks in the suite (not just on the 8 tasks in the benchmark sets). Perhaps those figures could be included in an appendix.
- The authors might want to consider including an automated way of generating performance profiles (https://arxiv.org/abs/cs/0102001) across tasks as part of DeepOBS, as a way of getting a sense of how optimizers performed generally across all tasks.","The reviewer clearly states that the benchmark proposed would be ""very valuable for the field"" and that the paper ""does a good job"" discussing important considerations. They also express appreciation for the breadth and diversity of tasks. While they mention a concern about potential quibbles, their overall tone is positive and they believe the benchmark would be valuable. The suggestions are constructive and presented in a neutral, professional manner.",75.0,90.0
"The paper proposes a distributed optimization method based on signSGD. Majority vote is used when aggregating the updates from different workers.
 The method itself is naturally communication efficient. Convergence analysis is provided under certain assumptions on the gradient. It also theoretically shows that it is robust up to half of the workers behave independently adversarially. Experiments are carried out on parameter server environment and are shown to be effective in speeding up training. 

I find the paper to be solid and interesting. The idea of using signSGD for distributed optimization make it attractive as it is naturally communication efficient. The work provides theoretical convergence analysis under the small batch setting by further assuming the gradient is unimodal and symmetric, which is the main theoretical contribution. Another main theoretical contribution is showing it is Byzantine fault tolerant. The experiments are extensive, demonstrating running time speed-up comparison to normal SGD.  

It is interesting to see a test set gap in the experiments. It remains to be further experimented to see if the method itself inherently suffer from generalization problems or it is a result of imperfect parameter tuning. 

One thing that would be interesting to explore further is to see how asynchronous updates of signSGD affect the convergence both in theory and practice. For example, some workers might be lost during one iteration, how will this affect the overall convergence.
Also, it would be interesting to see the comparison of the proposed method with SGD + batch normalization, especially on their generalization performance. It might be interesting to explore what kind of regularization technique would be suitable for signed update kind of method.   

Overall, I think the paper proposes a novel distributed optimization algorithm that has both theoretical and experimental contribution. The presentation of the paper is clear and easy to follow. 

Suggestions: I feel the experiments part could still be improved as also mentioned in the paper to achieve competitive results. More experiments on different tasks and DNN architectures could be performed. 
","The reviewer explicitly states that they find the paper ""solid and interesting"",  highlighting its contributions. They provide constructive suggestions for improvement but do not express major concerns. This points towards a positive sentiment. The language used is polite and professional throughout, suggesting a high politeness score.",75.0,90.0
"In this paper the authors focus on the problem of weakly-supervised action localization. The authors state that a problem with weakly-supervised attention based methods is that they tend to focus on only the most salient regions and propose a solution to this which reduces the difference between the responses for the most salient regions and other regions. They do this by employing marginalized average aggregation to averaging a sample a subset of features in relation to their latent discriminative probability then calculating the expectation over all possible subsets to produce a final aggregation.

The problem is interesting, especially noting that current attention methods suffer from paying attention to the most salient regions therefore missing many action segments in action localization. The authors build upon an existing weakly-supervised action localization framework, having identified a weakness of it and propose a solution. The work also pays attention to the algorithm's speed which is practically useful. The experiments also compare to several other potential feature aggregators.

However, there are several weakness of the current version of the paper:

- In parts the paper feels overly complicated, particularly in the method (section 2). It would be good to see more intuitive explanations of the concepts introduce here. For instance, the author's state that c_i captures the contextual information from other video snippets, it would be good to see a figure with an example video and the behaviour of p_i and c_i as opposed to lamba_i. I found it difficult to map p_i, c_i to z and lambda used elsewhere.

- The experimental evidence does not show where the improvement comes from. The authors manage to acheieve a 4-5% improvement over STPN through their re-implemenation of the algorithm, however only have a ~2% improve with their marginalized average attention on THUMOS. I would like to know the cause in the increase over the original STPN results: is it a case of not being able to replicate the results of STPN or do the different parameter choices, such as use of leakly RELU, 20 snippets instead of 400 and only rejecting classes whose video-level probabilities are below 0.01 instead of 0.1, cause this big of an increase in results? There is also little evidence that the actual proposal (contextual information) is the reason for the reported improvement.

- There seems to be several gaps in the review of current literature. Firstly, the authors refer to Wei et al. 2017 and Zhang et al. 2018b as works which erase the most salient regions to be able to explore regions other than the most salient. The authors state that the problem with these methods is that they are not end-to-end trainable, however Li et al. 2018 'Tell Me Where to Look': Guided Attention Inference Network' proposes a method which erases regions which is trainable end-to-end. Secondly, the authors do not mention the recent work W-TALC which performs weakly-supervised action localization and outperforms STPN. It would be good to have a baseline against this method.

- The qualitative results in this paper are confusing and not convincing. It is true that the MAAN's activation sequence shows peaks which correspond to groundtruth and are not present in other methods. However, the MAAN activation sequence also shows several extra peaks not present in other methods and also not present in the groundtruth, therefore it looks like it is keener to predict the presence of the action causing more true positives, but also more false positives. It would be good to see some discussion of these failure cases and/or more qualitative results. The current figure could be easily compressed by only showing one instance of the ground-truth instead of one next to each method.

I like the idea of the paper however I am currently unconvinced by the results that this is the correct method to solve the problem.
","The reviewer acknowledges the importance of the problem and the proposed solution, using phrases like ""The problem is interesting"" and ""The work also pays attention to the algorithm's speed which is practically useful."" However, they also point out several weaknesses, including lack of clarity, unconvincing experimental evidence, and gaps in the literature review. The overall tone suggests that the reviewer sees potential in the paper but is not fully convinced, leading to a moderately positive sentiment. The language used is formal, professional, and avoids harsh criticism, indicating a polite approach.",40.0,80.0
"This paper introduces a new algorithm for differential game, where the goal is to find a optimize several objective functions simultaneously in a game of n players. The proposed algorithm is an interpolation between LOLA and LookAhead, and it perserves both the stability from LOLA and the ""convergence to fixed point"" property of LookAhead. The interpolation parameter is chosen in Section 3.2.

The paper looks novel, though some notations are not completely clear to me. For example, the defintions of the ""current parameters"" \hat{\theta}_1 and \hat{\theta}_2 in Section 3.1, and the stop-gradient operator. Also, how is the diag operator in Propostion 1 is defined? Normally it only represents the diagonal entries but here it might represent the diagonal blocks.


","The review starts with positive wording like ""novel"" and ""preserves both the stability..."", indicating a favorable view of the work. However, it also points out areas for improvement, suggesting the reviewer doesn't see the paper as perfect. Therefore, the sentiment is positive but not overly enthusiastic. The language used is constructive and typical of academic peer reviews, suggesting a neutral to polite tone.",60.0,70.0
"Paper summary:
The paper proposes to predict bouncing behavior from visual data. The model has two main components: (1) Physics Interface Module, which predicts the output trajectory from a given incoming trajectory and the physical properties of the contact surface. (2) Visual Interface Module, which predicts the surface properties from a single image and the impact location. A new dataset called Bounce Dataset is proposed for this task.

Paper strengths:
- The paper tackles an interesting and important problem.
- The data has been collected in various real scenes.
- The idea of training the physics part of the network with synthetic data and later fine-tuning it with real images is interesting.
- The experiments are thorough and well-thought-out.

Paper weaknesses:
- It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.

- It is not clear how the evaluation is performed. For instance, the length of the groundtruth and predicted trajectories might be different. How is the difference computed?

- The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.

- Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?

- The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.

- How is the bounce location encoded in VIM?

- I don't see any statistics about the objects being used for data collection. That should be added to the paper.

>>>>> Final score: The authors have addressed my concerns in the rebuttal. I believe this paper tackles an interesting problem, and the experiments are good enough since this is one of the first papers that tackle this problem. So I keep the initial score. 
","The reviewer acknowledges the paper's strengths and acknowledges the importance of the work ('this is one of the first papers that tackle this problem'). While the reviewer has several specific questions and suggestions for improvement, they are framed constructively and focus on clarifying the methodology and results. The final statement indicates overall positivity towards the paper after the authors addressed the concerns.  Therefore, the sentiment is moderately positive. The language used is polite and professional throughout, focusing on scientific discourse and suggestions.",60.0,80.0
"

The authors suggest a method to create combined low-dimensional representations for combinations of pairs of words which have a specific syntactic relationship (e.g. adjective - noun). Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors.

Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above. Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea. Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.

Their lack of willingness to ground their claims or decisions is even more apparent in two other cases. The authors claim that the Arora's RAND-WALK model does not capture any syntactic information. This is not true. The results presented by Arora et al. indeed show that RAND-WALK captures syntactic information, albeit to a lesser extent than other popular methods for word embedding (Table 1, Arora et al. 2015). Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment. The reason the authors provide for weighing the composition Tensor is the fact that in the unweighted version their model produced a worse performance than the additive composition. One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.

Arora's generative model for word embeddings, on which the current paper is largely based upon, not only make the mathematical relationship among different popular word embedding methods explicit, but also by making and verifying explicit assumptions with regard to properties of the word embeddings created by their model, they are able to explain why low-dimensional embeddings provide superior performance in tasks that implicate semantic relationships as linear algebraic relations. Present work, however interesting with regard to its potential implications, strays away from providing such theoretical insights and suffices with demonstrating limited improvements in empirical tasks.","The review is highly critical of the paper, pointing out significant flaws in the authors' arguments and methodology. The reviewer finds the authors' claims insufficiently supported and their choices unjustified. While the reviewer acknowledges the potential of the proposed method, they clearly express disappointment with the paper's lack of theoretical depth and reliance on limited empirical results. The tone, while critical, maintains a professional and academic decorum.",-50.0,50.0
"This work refines the NAS method for efficient neural architecture search. The paper brings new methods for gradient/reward updates and credit assignment. 

pros: 
1. An improvement on gradient calculation and reward back-propagation mechanism
2. Good experiment results and fair comparisons

cons:
1. Missing details on how to use the gradient information to generate child network structures. In eq.2, multiplying each one-hot random variable Zij to each edge (i, j) in the DAG can obtain a child graph whose intermediate nodes are xj. However, it is still unclear how to generate the child graph. More details on generating child network based on gradient information is expected. 
2. In SNAS, P(z) is assumed fully factorizable. Factors are parameterized with alpha and learnt along with operation parameters theta. The factorization of p(Z) is based on the observation that NAS is a task with fully delayed rewards in a deterministic environment. That is, the feedback signal is only ready after the whole episode is done and all state transitions distributions are delta functions. In eq. 3, the authors use the training/testing loss directly as reward, while the previous method uses a constant reward from validation accuracy. It is unclear why using the training/testing loss can improve the results? 
","The review starts by acknowledging the positive contributions of the paper, highlighting the improvements brought by the new methods and the strong experimental results. However, it also raises valid concerns and asks for clarifications, indicating that the paper might not be perfect in its current form. Therefore, the sentiment is mildly positive. The language used is constructive and professional throughout the review, suggesting a polite and respectful tone.",50.0,75.0
"[Summary]
- This work proposes a new complex latent space described by convolutional manifold, and this manifold can map the image in a more robust manner (when some part of the image are to be restored).

[Pros]
- The results show that the latent variable mapped to the image well represents the image, and it will be helpful for the image restoration problem.
- it seems novel to adapt the idea of DIP for defining complex latent space.

[Cons]
- The main concern is that there is no guarantee that the defined latent space is continuous. 
It means that it is difficult to judge whether the interpolated point (phi_in, s_in) between two points: (phi_1, s_1) and (\phi_2, s_2), will be matched to the image distribution. 
Equation 2 in the paper seems that it just fit the generator parameter theta to map the phi_i and x_i and memorize the mapping between the training images and the given latent convolutional variables. 
If the proposed algorithm just memorizes the training image and map them into given the latent convolution, the result cannot justify the proposal that the author proposes a new latent space.

[Summary]
- This work proposes an interesting idea of defining complex latent space, but It is doubtful that this work just memorized the mapping between the training images and the latent convolutional parameters.
- I want to see the (latent space) interpolation test for the proposed latent convolutional space. If the author provides a profound explanation of the problem, I would consider changing the rating.

--------------------------
See the additional comment for the changed rating
","The review acknowledges the novelty of the paper's idea and its potential usefulness for image restoration. However, it raises a significant concern about the core claim of the paper, suggesting that the proposed method might be simply memorizing training data rather than defining a truly continuous latent space. The reviewer's request for an interpolation test further emphasizes this doubt. While not outright negative, the skepticism expressed brings the sentiment down. The language used is polite and professional, focusing on the scientific content and avoiding personal remarks.",20.0,80.0
"This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration. The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration. The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments. And it can significantly outperform vanilla PPO for environments with sparse rewards.

The paper is clearly written. The introduced technique is interesting. I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration. I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.

The motivation of this paper is mostly about learning with sparse reward. I am curious whether the paper has other good side effects. For example, will the dropout cause the policy to be more robust? Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores. In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.

Overall, I like this paper. It is well written. The method seems technically sound and achieves good results. For this reason, I would recommend accepting this paper.","The reviewer explicitly recommends accepting the paper, stating they ""like this paper"" and finding it well-written, technically sound, and achieving good results. They also express interest in the technique and raise relevant questions, indicating engagement with the work. All of this points towards a positive sentiment. The language used is constructive, professional, and inquisitive without resorting to negativity, suggesting a polite tone.",85.0,90.0
"The paper presents and discusses a new phenomenon that infrequent words tend to learn degenerate embeddings. A cosine regularization term is proposed to address this issue.

Pros
1. The degenerate embedding problem is novel and interesting.
2. Some positive empirical results.

Cons and questions
1. The theory in Section 4 suggests that the degeneration problem originates from underfitting; i.e., there's not enough data to fit the embeddings of the infrequent words, when epsilon is small. However, the solution in Section 5 is based on a regularization term. This seems contradictory to me because adding regularization to an underfit model would not make it better. In other words, if there's not enough data to fit the word embeddings, one should feed more data. It seems that a cosine regularization term could only make the embeddings different from each other, but not better.
2. Since this is an underfitting problem (as described in Section 4), I'm wondering what would happen on larger datasets. The claims in the paper could be better substantiated if there are results on larger datasets like WT103 for LM and en-fr for MT. Intuitively, by increasing the amount of total data, the same word gets more data to fit, and thus epsilon gets large enough so that degeneration might not happen.
3. ""Discussion on whether the condition happens in real practice"" below Theorem 2 seems not correct to me. Even when layer normalization is employed and bias is not zero, the convex hull can still contain the origin as long as the length of the bias vector is less than 1. In fact, this condition seems fairly strong, and surely it will not hold ""almost for sure in practice"".
4. The cosine regularization term seems expensive, especially when the vocab size is large. Any results in terms of computational costs? Did you employ tricks to speed it up?
5. What would happen if we only apply the cosine term on infrequent words? An ablation study might make it clear why it improves performance.

UPDATE:
I think the rebuttal addresses some of my concerns. I am especially glad to see improvement on en-fr, too. Thus I raised my score from 5 to 7.","The reviewer acknowledges the novelty of the paper and provides positive feedback (""The degenerate embedding problem is novel and interesting.""). However, they also raise several valid concerns and questions, indicating that the paper needs further improvements. The reviewer's tone is critical but professional and polite, suggesting a slightly positive sentiment overall.",20.0,70.0
"The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.

Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.

In Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? 

The experiments could be strengthened by addressing the following:
* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.
* What is the performance of SQL if we use \rho as the action selector in \epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.
* Plotting beta over time
* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. 
* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it?
* How many seeds were run per game?
* How and why were the 19 games selected from the full set?

Comments:

The abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.

With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.

Could state that the stationary distribution is assumed to exist and be unique.

In Sec 3.1, why is the prior state independent?

In Sec 3.1, p(R = 1|\tau) is defined to be proportional to exp(\beta \sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \tau) is not defined?

Throughout, I suggest that the authors not use the phrases ""closed form"" and ""analytic"" for expressions that are in terms of intractable quantities. 

It should be noted that Sec 3.2 Optimal policy for a fixed prior \rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.

In Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?

I believe that the connection to MI can be simplified. Plugging in the optimal \rho into Eq 3, we can see that Eq 3 simplifies to \max_\pi E_q[ \sum_t \gamma^t r_t] - (1 - gamma)/\beta MI_p(s, a) where p(s, a) = d^\pi(s) * \pi(a | s) and d^\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.

In Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?

Sec 5 references the wrong Haarnoja reference in the first paragraph.

In Sec 5, alpha_beta = 3 * 10^5. Is that correct?

=====
11/26
At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.

====
12/7
The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).","The review is mostly positive about the paper's core idea (rated as ""interesting"") but expresses significant concerns about clarity, justification, and experimental details. The reviewer's willingness to raise their score based on revisions indicates openness and a hopeful outlook. The numerous specific questions and requests for clarification suggest a neutral-to-positive initial stance, as the reviewer is engaged and seeking improvement rather than outright rejection. The language is direct and professional throughout, without resorting to harsh or disrespectful phrasing.",40.0,80.0
"The authors introduce an algorithm that addresses the problem of online policy adaptation for model-based RL. The main novelty of the proposed approach is that it defines an effective algorithm that can easily and quickly adapt to the changing context/environments. It borrows the ideas from model-free RL (MAML) to define the gradient/recursive updates of their approach, and it incorporates it efficiently into their model-based RL framework. The paper is well written and the experimental results on synthetic and real world data show that the algorithm can quickly adapt its policy and achieve good results in the tasks, when compared to related approaches. 

While applying the gradient based adaptation to the model-free RL is trivial and has  previously been proposed, in this work the authors do so by also focusing on the ""local"" context (M steps within a K-long horizon, allowing the method to  recover quickly if learning from contaminated data, and/or its global policy cannot generalize well to the local contexts. Although this extension is trivial it seems that it has not been applied and measured in terms of the adaptation ""speed"" in previous works. Theoretically, I see more value in their second approach where they investigate the application of fast parameter updates within model-based RL, showing that it does improve over the MAML-RL and non-adaptive model-based RL approaches. This is expected but  to my knowledge has not been investigated to this extent before. 

What I find is lacking in this paper is insight into how sensitive the algorithm is in terms of the K/M ratio, and also how it affects the adaptation speed vs performance (tables 3-5 show an analysis but those are for different tasks); no theoretical analysis was performed to provide deeper understanding of it. The model does solve a practical problem (reducing the learning time and having more robust model), however, it would add more value to the current state of the art in RL if the authors proposed a method for optimal selection of the recovery points and also window ratio R/L depending on the target task. This would make a significant theoretical contribution and the method could be easily applicable to a variety of tasks. where the gains in the adaptation speed are important.","The reviewer provides a generally positive assessment of the paper, acknowledging the novelty and well-written nature of the work. They find value in the algorithm's ability to adapt quickly and its application within a model-based RL framework. While they point out some limitations, such as the lack of theoretical analysis on parameter sensitivity and optimal selection methods, their suggestions for improvement are constructive and aim to enhance the paper's contribution. The language used is professional and objective throughout.",65.0,80.0
"The idea is nice. It is well aligned with tools that are needed to understand neural networks. However, the experiments feel like they are missing motivation as to why this method is being used. The paper does not provide very significant evidence that this method is useful. The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.

More motivation for experimental section is needed. If the authors don't discuss a motivation then how will a reader know how to apply the tool? It seems there is no conclusion to take away from the experiments in section 5 (convolutions). 

The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method. In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.

The paper needs more discussion and experiments to explain how and why to use this approach. 

While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods. 

There are many typos and grammar errors

While I think the paper could be much more impactful if the experimental section was greatly reworked; I believe the first 5 pages of the paper are a very good contribution and it should be accepted.
","The reviewer acknowledges the paper's good idea and contribution in the first 5 pages. However, they express concerns about the experimental section lacking motivation and significant evidence for the method's usefulness. The reviewer uses phrases like ""the experiments feel like they are missing motivation"" and ""The paper does not provide very significant evidence"" which point towards a somewhat negative sentiment. However, the reviewer also mentions that the first 5 pages are a ""very good contribution"" and suggests acceptance after reworking, indicating an overall leaning towards acceptance. Therefore, the sentiment is slightly on the negative side but not overly so. The language used is polite and professional, providing constructive criticism without resorting to harsh or disrespectful language.",-20.0,70.0
"This paper presents a method for distilling multiple teacher networks into a student, by linearly combining feature representations from all networks at multiple intermediate layers, and gradually forcing the student to ""take over"" the learned combination.  Networks to be used as teachers are first pretrained on various initial tasks.  A student network is then trained on a target task (possibly different from any teacher task), by combining corresponding hidden layers from each teacher using learned linear remappings and weighted combinations.  Learning this combination allows the system to find appropriate teachers for the target task; eventually, a penalty on the combination weights forces all weight onto the student network, resulting in the distillation.

Applications to both reinforcement learning (atari game) and supervised image classification (cifar, svhn) are evaluated.  The reinforcement learning application is particularly fitting, since combining tasks together is less straightforward in this domain.

I wonder whether any experiments were performed where the layers correspondence between teacher models was less clear --- say, using teachers with different architectures.  Figure 1(a) (different teacher archs) as well as the text (""candidate set"" on p.4) indicate this is possible, but experiment details describe combinations of same-architecture teachers only.

In addition, I would have liked to see some further exploration of the KL term and use of ""theta_old"".  This seems potentially important, and also has ties to self-ensembling through teachers with exponential weight averaging.  Could an average network also be used here?  And how important is this term in linking student to teachers as the weights change?

Overall I find this a very interesting approach.  Rather than training a large joint model on multiple tasks simultaneously as a transfer initialization, this approach uses models already fully trained for different tasks.  This results in a potentially advantageous trade-off:  One no longer needs to carefully calibrate the different tasks and common task components in a joint model, but at the cost of requiring inference through multiple teachers when training the student.
","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive view of the paper. They find the approach interesting and acknowledge its advantages. While they raise questions and suggest further explorations, these are presented as opportunities for strengthening the work rather than criticisms. The language used is formal, respectful, and typical of academic peer reviews.",75.0,100.0
"Prons: 
This paper provides an optimistic mirror descent algorithm to solving minmax optimization problem. Its global convergence is guaranteed under the coherence property. The experimental results are promising.

Cons: 
1.	The coherence property is still a strong assumption. The sufficient conditions provided in Corollary 3.2 and 3.3 to guarantee coherence property are too specific to cover existing GAN models.         

2.	The current theoretical contribution seems incrementally. From the perspective of operator theory, the coherence property is highly related to the pseudo-monotone property. Extragradient method to solve the pseudo-monotone VIP has already existed in the literature [1]. The proposed OMD can be simply regarded a stochastic extension of [1] and simultaneously generalize the European distance in [1] to Bregman distance. 

3.	The integrating of Adam and OMD in the experiments is very interesting. To match the experiments, we highly recommend the authors to show the convergence of OMD + Adam with or without coherence condition, rather than requiring a diminishing learning rate.

[1] Noor, Muhammad Aslam, et al. ""Extragradient methods for solving nonconvex variational inequalities."" Journal of Computational and Applied Mathematics 235.9 (2011): 3104-3108.
","The review starts with positive remarks, highlighting the contribution and promising results. However, it raises significant concerns about the assumptions, novelty, and practical relevance of the theoretical results. The tone, while critical, remains professional and constructive throughout, suggesting specific improvements and further research directions.",20.0,60.0
"... I would have liked to see some more insights.

The authors present a method for distilling knowledge from individual models to train a multilingual model. The motivation stems from the fact that while most s-o-t-a multilingual models are compact (as compared to k individual models) they fall short of the performance of the individual models. The authors demonstrate that using knowledge distillation, the performance of the multilingual model can actually be better than the individual models. 

Please find below my comments and questions.

1) The authors have done a commendable job of validating their hypothesis on multiple datasets. Solid experimentation is definitely the main strength of this paper.

2) However, this strength also makes way for a weakness. The entire experimental section is just filled with tables and numbers. The same message is repeated across these multiple tables (multi+distill > single > multi). Beyond this message there are no other insights. For example, 

- How does the performance depend on the divergence between source and target language?
- Why is there more important on some languages and less on others ?
- Why are the improvements on the TED dataset so much higher as compared to the other 2 datasets.
- What happens when the target language is something other than English? All the experiments report results from X-->English, why not in the other direction? The model then is not really ""completely"" multilingual. It is multi-source-->single target. 
- Can you comment on the total training time ?
- What happens when you do not stop the distillation even when the accuracy of the student crosses that of the teachers ? What do you mean by accuracy here? Only later when you mention that \threshold = 1 BLEU it became clear that accuracy means BLEU in this context ?

3) Is it all worth it? One disappointing factor is that end of all this effort where you train K individual models and one monolithic model with distillation, the performance gain for most language pairs is really marginal (except on the TED dataset). I wonder if the same improvements could have been obtained by even more carefully fine tuning the baseline models itself.

4) On a positive note, I like the back-distillation idea and the experiments on top-K distillation

+++++++++++++++++++
I have updated my rating after reading author's responses

","The reviewer acknowledges the solid experimentation and validation but expresses disappointment due to lack of in-depth insights and marginal performance gains. The numerous questions and desire for further analysis point towards a more critical stance. However, the reviewer also points out positive aspects and acknowledges the authors' work, indicating the review isn't overly negative. The language remains professional and polite throughout, suggesting constructive criticism rather than harsh negativity.",-20.0,60.0
"
The authors proposed to exploit hyperbolic geometry in computing the attention mechanisms for neural networks. Specifically, they break the attention read operation into two parts: matching and aggregation. In matching step, they use the hyperbolic distance to quantify the macthing between a query and a key; in the aggregation step, they use the Einstein midpoint. Their experiments results based on synthetic and real-world data shows the new method outperforms the traditional method based on Euclidean distance. This paper is acceptable.


Question: In Figure 3(Center), the number of nodes 1000 and 1200 are pretty close. How about the results on 500 nodes and 2000 nodes? It seems the accuracy difference increases as the number of nodes increases. Is this true? 

","The review is mostly positive. The reviewer states the paper is ""acceptable"" and offers a suggestion rather than a critique. The language used is neutral and professional throughout.",60.0,50.0
"# Summary
This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems. Previous work has either used general anisotropic convolution or azimuthally isotropic convolution. The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly. The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity. This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps. The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.


# Strengths
The paper has several strong points. It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity. Much of the relevant related work is discussed, and this is done in a balanced way. Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution. The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator). The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.


# Weaknesses
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published. To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense. This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form. For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega). As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations). So the closure axiom of a group is violated.

This matters, because the notion of equivariance really only makes sense for a group. If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations. As we saw before, this is the whole rotation group. This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution. Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant. This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary. The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.

I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.

Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole. The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi. But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis. This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking. The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).

The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable. I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks. I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.


# Other comments

The experiments show that the method is quite effective. For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost. That they do not substantially outperform these and other methods is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods. An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.

It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2). Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers. It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.

Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1. I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned. Some more explanation / discussion would be good. 

It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant? 


Typos & minor issues

- Abstract: ""to extract non-trivial features"". The word non-trivial really doesn't add anything here. Similarly ""offers multi-level feature extraction capabilities"" is almost meaningless since all DL methods can be said to do so.
- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi). The order is reversed when inverting.
- ""Different notations of convolutions"" -> notions
- ""For spherical functions there is no consistent and well defined convolution operators."" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.
- ""rationally symmetric"" -> rotationally
- ""exact hierarchical spherical patterns"" -> extract
- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields. References would be in order. Similarly, hexagonal convolution has a history in DL and outside.
- Bottom of page 7, capitalize ""for"".
- ""principle curvatures"" -> principal.
- ""deferent augmentation modes"" -> different
- ""inspite"" -> in spite
- ""reprort"" -> report
- ""utlize"" -> utilize
- ""computer the convolution"" -> compute


# Conclusion

Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper. Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues. For now I will give an intermediate rating to the paper.


[1] Kondor, Trivedi, ""On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups""","The review is mostly positive, highlighting the paper's strengths such as clarity, efficient data structure, and effectiveness. However, it also points out a significant misunderstanding regarding the claimed SO(3)-equivariance of the proposed convolution. While recommending acceptance, the reviewer emphasizes the need to correct factual errors and provide a clearer discussion of the issues. The language is polite and professional, offering constructive criticism and suggestions for improvement.",50.0,80.0
"The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert’s and agent’s stationary state-action distributions. 

The proposed method outperforms existing imitation learning approach (GAIL & BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. 
The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose).

While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.).

Nonetheless, the paper overall presents a strong submission based on novelty & relevance of the proposed method and is recommended for publication. 

Minor issues:
- Related work: improve transitions between the section about trajectory tracking and BC.
- Ablation studies with less flexible probabilistic models would strengthen the experiment section further. 
- Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access.
- A release of the code base would further strengthen the contributions of the submission.

General recommendation:
- The authors are encouraged to further investigate off-policy corrections for improved convergence.
","The reviewer explicitly recommends the paper for publication, stating that it ""presents a strong submission"". They acknowledge the novelty and relevance of the work and praise the improvements in sample efficiency. While the reviewer suggests some areas for improvement, such as additional experiments and comparisons, these are presented constructively and do not detract significantly from the overall positive sentiment. The language used is polite and professional throughout.",75.0,80.0
"RELATIONAL FORWARD MODELS FOR MULTI-AGENT LEARNING

Summary: Model free learning is hard, especially in multi-agent systems. The authors consider a way of reducing variance which is to have an explicit model of actions that other agents will take. The model uses a graphical structure and the authors argue it is a) interpretable, b) predicts actions better and further forward than competing models, c) can increase learning speed.

Strong Points:
-	The main innovation here is that the model uses a graph conv net-like architecture which also allows for interpretable outputs of “what is going on” in a game.
-	The authors show that the RFM increases learning speed in several games
-	The authors show that the RFM does somewhat better at forward action prediction than a naïve LSTM+MLP setup and other competing models

Weak Point
-	The RFM is compared to other models in predicting forwards actions but is not compared to other models in Figure 5, so it is not clear that the graphical structure is actually required to speed up learning. I would like to see these experiments added before we can say that the RFM is adding to performance.
-	Related: The authors argue that an advantage of the RFM is that it is interpretable, but I thought a main argument of Rabinowitz et. al. was that simple forward models similar to the LSTM+MLP here were also interpretable? If the RFM does not improve learning above and beyond the LSTM+MLP then the argument comes down to more accurate action prediction (ok) and more interpretability (maybe) which is less compelling.

Clarifying Questions
-	How does the 4 player Stag Hunt work? Do all 4 agents have to step on the Stag together or just 2 of them? How are rewards distributed? Is there a negative payoff for Hunting the stag alone as in the Peysakhovich & Lerer paper?
-	Related: In the Stag Hunt there are multiple equilibria, either agents learn to get plants (which is safe but low payoff) or they learn to Hunt (which is risky but high payoff). Is the RFM leading to more convergence to the Hunting state or is it simple leading agents to learn the safe but low payoff strategies faster?
- The choice of metric in Figure 2 (# exactly correct prediction) is non-standard (not saying it is wrong). I think it would be good to also see a plot of a more standard metric such as loglikelihood of the model's for each of X possible steps ahead. It would help to clarify where the RFM is doing better (is it better at any horizon or is it just able to look further forward more accurately than the competitors?)


","The review is generally positive, acknowledging the strong points of the paper like the innovative use of graph convolutional networks and the demonstrated improvement in learning speed. However, it also raises valid concerns and requests further experiments to solidify the claims. The reviewer's tone is constructive and professional, suggesting improvements without being dismissive.",60.0,80.0
"The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). 
The proposed method defines a generative process that ties the prediction output and the adversarial input 
pattern via a set of shared neural net weights. These weights are then assinged a prior and 
the resultant posterior is approximated by variational inference.

Strength:
  * The proposed approach is incremental, but anyway novel.
  * The results are groundbreaking.
  * There are some technical flaws in the way the method has been presented, 
but the rest of the paper is very well-written.

Major Weaknesses:

  * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\sigma)).

  * Second, the constraint that x_adv should lie within the \gamma-ball of x has some implications on the validity of
the Jensen's inequality, which relates Equation 7 to proper posterior inference.

  * Blundell et al.'s algorithm should be renamed to ""Bayes-by-BACKprop"". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience.

And Some Minor Issues:

  * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles.

  * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python.

Overall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.","The reviewer acknowledges the paper's novelty and groundbreaking results, indicating a positive sentiment. However, they also point out technical flaws and suggest improvements, pulling the sentiment towards neutral. The language used is formal and professional, but the criticism, while constructive, is quite direct. ",60.0,50.0
"The results are intriguing. However, similar methods like BN-LSTM [3] and Variational RNNs [4] achieve arguably the same with very similar mechanisms. We do not think they can be considered as orthogonal. This should be addressed by the authors. Also, hard long-term experiments like sequentially predicting pixels (like through MDLSTM-based PixelRNN) or language modelling should be favoured over short sentence image captions. 

It is possible that we will improve our ratings once our concerns are addressed.

Paper Summary:

The authors claim that the gradient along the computational path that goes through the cell state (the linear temporal path or A gradient) of an LSTM carries information about long-term dependencies. Those gradients can be corrupted by the gradient of all other computational paths (i.e. the B gradient). They claim that this makes it hard to learn long-term dependencies and has, therefore, significant negative effects on the convergence speed, training stability, and generalisation performance. They propose a method called h-detach and run experiments on the delayed copy task, sequential MNIST, permuted sequential MNIST (pMNIST), and caption generation on the MS COCO dataset. All show either somewhat improved performance or much more stable learning curves. At every step, h-detach randomly drops all gradients that flow through the h of the standard LSTM, the B gradients, and only keeps the ones from the linear temporal path, the A gradients. Experiments also suggest that the A gradients carry more long-term information than B gradients and that LSTMs with h-detach do not need gradient clipping for successful training.

Positive:

The paper is written clearly. It is well structured and well motivated. H-detach is simple, effective, and somewhat novel (see below). Experiments indicate that its main benefit is training stability as well as minor performance improvements.

Negative:

We are not sure how significant these results are for the following reasons:

- MS COCO image caption generation is the only more challenging dataset, but it seems a bit misplaced as it has very short sentences, while the authors motivate their work through a focus on long-term dependencies. Why not apply h-detach to a language model such as [1] with official online implementations, e.g., [2]. A setting with PixelRNN [6] based on MD-LSTM [7] would also be a great testbed for h-detach.

- The purpose of h-detach is to scale down the B gradients. However, methods which apply e.g. BatchNorm to the hidden state learn a scale parameter which could be learned by the network explicitly. For the backward pass, this has the effect of scaling down the B gradient. Consider e.g. [3] which also achieves similar training stability on sequential MNIST and pMNIST with little overhead. 

- Another very related method is [4] which properly applies a random dropout mask over the recurrent inputs that is shared across timesteps of an RNN. We think that h-detach is essentially achieving the same in a similar way.

Problems with Introduction and Related Work Section:

- The vanishing gradient problem was first described by Hochreiter in 1991 [5] (not by Bengio in 1994). 

- Intro mentions GRU as if it was separate from LSTM. Clarify that GRU is essentially a variant of vanilla LSTM with forget gates [8]. Since one gate is missing, GRU is less powerful than the original LSTM [9]. 

[1] Zaremba et al. ""Recurrent neural network regularization."" arXiv:1409.2329 (2014).
[2] https://www.tensorflow.org/tutorials/sequences/recurrent
[3] Cooijmans et al. ""Recurrent batch normalization."" arXiv:1603.09025 (2016).
[4] Gal et al. ""A theoretically grounded application of dropout in recurrent neural networks."" NIPS 2016.
[5] Hochreiter, Sepp. ""Untersuchungen zu dynamischen neuronalen Netzen."" Diploma thesis, TUM (1991)
[6] Oord et al. ""Pixel recurrent neural networks."" arXiv preprint arXiv:1601.06759 (2016).
[7] Graves et al. ""Multi-Dimensional Recurrent Neural Networks"" arXiv preprint arXiv:0705.2011 (2011).
[8] Gers et al. “Learning to Forget: Continual Prediction with LSTM.“ Neural Computation, 12(10):2451-2471, 2000. 
[9] Weiss et al. On the Practical Computational Power of Finite Precision RNNs for Language Recognition. arXiv:1805.04908.


Comments after rebuttal:

The  paper has clearly improved. 

It leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. 

When dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:

Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 

Nevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points!

","The review starts with a rather critical tone, pointing out significant concerns about the novelty and significance of the work. The reviewers find the results ""intriguing"" but not entirely convincing. They suggest alternative approaches and highlight the need for stronger experiments. While the language is direct and critical, it maintains a professional and academic tone. The reviewers offer constructive feedback and suggestions for improvement. The final paragraph indicates a slightly more positive view after the rebuttal, acknowledging the paper's interest to the field. However, lingering doubts and further questions remain. Overall, the sentiment leans towards the skeptical side, but the politeness remains within the bounds of professional academic discourse.",-10.0,60.0
"This paper presents a class of neural networks that does not have bad local valleys. The “no bad local valleys” implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn’t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.

The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
* adding skip connections doesn’t harm the generalization.
* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.
* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.

However, from a theoretical point of view, I would say the contribution of this work doesn’t seem to be very significant, for the following reasons:
* In the first place, figuring out “why existing models work” would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.
* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally “equivalent” to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17’) it is easy to attain global minima.
* I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive.

Below, I’ll list specific comments/questions about the paper.
* Assumption 3.1.2 doesn’t make sense. Assumption 3.1.2 says “there exists N neurons satisfying…” and then the first bullet point says “for all j = 1, …, M”. Also, the statement “one of the following conditions” is unclear. Does it mean that we must have either “N satisfying the first bullet” or “N satisfying the second bullet”, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?
* The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.
* Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses.
* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it’s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.
* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn’t necessarily satisfy the assumptions?
* Can you show the “improvement” of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.

Minor points
* In the Assumption 3.1.3, the $N$ in $r \neq s \in N$ means $[N]$?
* In the introduction, there is a sentence “potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),” which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18’ and Yun et al. 18’).
* Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as “for example, in the fully connected network case, this means that all data points are distinct.”","The review starts with positive remarks, highlighting the impressive properties and interesting results. However, it transitions into a more critical analysis, questioning the significance of the theoretical contribution and raising concerns about practicality and novelty. The reviewer also lists several specific, technical questions and points out factual errors. While the language remains professional and polite throughout, the overall tone conveys a sense of skepticism and suggests areas needing substantial improvement. Therefore, the sentiment leans towards the negative side.",-20.0,70.0
"There has been a lot of work on limited precision training and inference for deep learning hardware, but in most of this work, the accumulators for the multiply-and-add (FMA) operations that occur for inner products are chosen conservatively or treated as having unlimited precision. The authors address this with  an analytical method to predict the number of mantissa bits needed for partial summations during the forward, delta and gradient computation ops for convolutional and fully connected layers. They propose an information theoretic approach to argue that by using fewer bits of mantissa in the accumulator than necessary, the variance of the resulting sum is less than what it would have been if sufficient bits of mantissa were used. This is surprising to me, as quantization is usually modeled as _adding_ noise, leading to an _increase_ in variance (Mc Kinstry et al. 2018), so this is a nice counterexample to that intuition. Unfortunately the result is presented in a way that implies the variance reduction is what causes the degradation in performance, while obviously (?) it's just a symptom of a deeper problem. E.g., adding noise or multiplying by a constant to get the variance to where it should be, will not help the network converge. The variance is just a proxy for lost information. The authors should make this more clear.

Loss of variance is regarded as a proxy to the error induced/loss of information due to reduced mantissa prevision. The authors present their metric called Variance Retention Ratio (VRR) as a function of the mantissa length of product terms, partial sum (accumulator) terms, and the length of the accumulation. Thereafter, the mantissa precision of the accumulator is predicted to maintain the error of accumulation within bounds by keeping the VRR as close to 1 as possible. The authors use their derived formula for VRR to predict the minimum mantissa precision needed for accumulators for three well known networks: AlexNet, ResNet 32 and ResNet 18. For tightness analysis they present convergence results while perturbing the mantissa bits to less than those predicted by their formula, and show that it leads to more than 0.5% loss in the final test error of the network.

Some questions that the manuscript leaves open in it's current form:

0. Does this analysis only apply to ReLu networks where all the accumulated terms are positive? Would a tanh nonlinearity, e.g. in an RNN, result in a different kind of swamping behavior? I don't expect the authors to add a full analysis for the RNN case if it's indeed different, but it would be nice to comment on it. 
1. Do the authors assume that the gradients and deltas will always be within the exponent range of representation? I do not find a mention of this in the paper. In other words, are techniques like loss scaling, etc. needed in addition? Other studies in literature analyzing IEEE fp16 seem to suggest so.
2. The authors do not provide details on how they actually performed the experiments when running convergence experiments. It is not straightforward to change the bit width of the accumulator mantissa in CPU or GPU kernel libraries such as CUDNN or Intel MKL. So how do they model this?
3. On page 7, the authors point out that they provide a theoretical justification of why the chunk size should neither be too small or too large - but I do not see such a justification in the paper. More detailed explanation is needed.

There are a few minor typos at a few places, e.g.
 
1. Page 4: “… , there is a an accumulation length….”
2. Page 6: “…floaintg-point format…""

Some figures, notably 2 and 5, use text that is unreadably small in the captions. I know this is becoming somewhat common practice in conference submissions with strict pages limits, but I implore the authors to consider shaving off space somewhere else. Some of us still read on paper, or don't have the best eyes!","The review is mostly positive. The reviewer finds the paper's subject relevant and the approach interesting, pointing out a ""surprising"" result. However, they also express concerns about the clarity and completeness of the paper, particularly regarding the explanation of variance reduction and the experimental setup. The reviewer also lists several specific questions and minor issues that need to be addressed. The language used is generally polite and professional, with a slightly informal tone in a couple of places (e.g., ""obviously (?)”, “Some of us still read on paper, or don't have the best eyes!”).",60.0,80.0
"This paper shows that deep convolutional networks (CNNs, without pooling) with a suitable prior over weights can be seen as shallow Gaussian processes (GPs) with a specific covariance function. It shows that this covariance function can be computed efficiently (when compared to previous attempts at resembling convolutional networks with GPs), with a cost that only depends linearly on the number of layers and the input dimensionality, i.e.~O(N^2 L D). 

To show the equivalence between deep CNNs and shallow GPs, the paper uses similar ideas to those proposed by Matthews et al (2018a) and Lee et al (2017), i.e. using the multivariate central limit theorem in very large networks, where in the case of this paper the limit is taken as the number of channels at each layer goes to infinity. Therefore, from a theoretical perspective, these ideas have been proposed before. However, the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own. 

However, the model setting for classification (where deep CNNs have been successful) and consequent evaluation on the MNIST dataset is less than convincing. One of the main motivations for Bayesian CNNs and GPs (and the paper argue for this in the intro) is to be able to provide good uncertainty estimates. However, the classification problem is framed in a regression setting, where neither probabilistic estimates are evaluated or even provided. Indeed, only the error rate is given on Table 1. To me, this is certainly not enough for a Bayesian/GP method and it is a critical deficiency of the paper in its current form. While I understand having a non-Gaussian likelihood will complicate things and conflate the kernel contribution with the approximations, I believe it is necessary to provide and evaluate such probabilistic estimates and compare them to other GP approaches (even using other less than satisfying methods such as calibration/scaling). Along a similar vein, it is unclear what objective function was used for hyper-parameter learning but, given that the authors actually “sample hyper-parameters”, I am guessing a proper probabilistic objective such as the marginal likelihood is out of the question.

Other (perhaps minor) deficiencies is that the method is not scalable to large datasets (I am even surprised the authors managed to run this on full MNIST) and that no theoretical analysis is done (e.g. as in Mattews et al, 2018a). 

Minor comments:

* In the intro, “Other methods such as Gaussian Processes”: GPs are not a method and I believe the authors really mean here Gaussian process regression. 
* The prior variance over filters in Eq (3) divides over the number of channels.  Why does a Gaussian prior with infinite precision make sense here?
* The authors should report the state of the art of using GPs for MNIST classification using non-convolutional kernels).
","The reviewer acknowledges the merits of the paper's novel and efficient approach to compute the convolutional kernel. However, they express significant concerns about the paper's experimental validation, particularly the lack of probabilistic estimates and comparison to other GP approaches. The reviewer also points out scalability issues and the absence of theoretical analysis as weaknesses. While the reviewer acknowledges the paper's contributions, the criticism regarding the experimental validation and other shortcomings leads to a sentiment leaning towards the negative side. The language used is formal and professional, without resorting to harsh or disrespectful language.",-20.0,80.0
"Paper’s contributions:
This paper considers the challenging problem of generalizing well to new RL tasks, based on having learned on a set of previous related RL tasks.  It considers tasks that differ only in their reward function (assume the dynamics are identical), and where the reward functions are constrained to be linear combinations over a set of given features.  The main approach, Universal Successor Features Approximators (USFAs) is a combination of two recent approaches:  Universal Value Function Approximators (UVFAs) and Generalized Policy Improvement (GPI).  The main claim is that while each of these methods leverages different types of regularity when generalizing to new tasks, USFAs are able to jointly leverage both types (and elegantly have both other methods as special cases).

Summary of evaluation:
Overall the paper tackles an important problem, and provides careful explanation and reasonably extensive results showing the ability of USFA to leverage structure.  I’m on the fence because I really wish the combination of generalization properties could be understood in a more intuitive way.  There are some more minor issues, such as lack of complexity analysis and a few notation details, that can be easily fixed.

Pros:
-	The problem of generalizing to new tasks in RL is an important open problem.
-	The paper is carefully written and provides clear explanation of most of the methods & results.

Cons:
-	The authors are diligent about trying to explain what type of regularities are exploited by each of UVFAs and GPI, and how this can be combined in USFAs.  However despite reading these parts carefully, I could not get a really good intuition, either in the methods or in the results, for the nature of the regularities exploited, and how it really differs.  Top of p.4 says that GPI generalizes well when the policy \pi(s) does well on task w’.  Can you give a specific MDP where Q is not smooth, but the policy does well?
-	There is no complexity analysis.  I would like to know the computational complexity of each of the key steps in Algorithm 1 (with comparison to simple UVFA and GPI).
-	It would be useful to see the empirical comparison with the approach of Ma et al. (2018), which also combines SFs and UFVAs. I understand there are differences in the details, but I would like to see confirmation of whether the claims about USFA’s superior ability to exploit structure is supported by results.

Minor comments:
-	The limitation to linear rewards is a reasonably strong assumption.  It would be good to support this, e.g. by references to domain that meet this assumption.
-	It seems the mathematical properties in Sec.3.1 could be further developed.
-	P.4: “Given a deterministic policy \pi, one can easily define a reward function r_\pi”.  I did not think this mapping was unique (see the literature on IRL, e.g. Ross et al.).  Can you provide a proof or reference to support this statement?
-	The definition of Q(s,a,w,z) is interesting. Can this be seen as a kernel between w and z?
-	\theta suddenly shows up in Algorithm 1. I presume these are the parameters of Q?  Should be defined.
-	The distribution used to sample policies seems to be a key step of this approach, yet not much guidance is given on how to do this in general.
","The reviewer provides both positive and negative points, but leans slightly towards the positive. They acknowledge the importance and clarity of the paper, but express a desire for better intuition and further development in certain areas. The language used is constructive and professional throughout.",20.0,70.0
"This paper has 3 principal contributions: it proposes a different way of measuring mutual information in a neural network, proposes a compression score to compare different models, then empirically analyses different activation functions and L2 weights.

This work seems like a welcome addition to the IB thread. To me the most interesting result is simply that activation functions aren't simply about gradient flow, and that they may each have properties that are more or less desirable depending on the domain they might be used on. The authors are careful in the wording of their conclusions, I think with reason; while these results are useful in that there seem to be consistently different behaviors coming from different hyperparameters, information planes show a relatively qualitative part of the picture.

Quantitatively, the proposed compression score is interesting, but as the authors say, simplistic. It seems to me that we care more about the converged models than the whole training trajectory; how does this score evolve with time?

I think an important part of discussion that lacks in this paper is a more in-depth take as to how these findings relate to the Zhang et al [1] memorization vs generalization paper and its follow ups. There seem to be many links to be drawn.

This work is overall a good contribution, but I'll have to agree with the authors' conclusion that more principled analysis methods are required to have a solid grasp of the training dynamics of DNNs. The writing of the paper is good, but the writing of the captions could be improved. (the hard page limit of ICLR is 10 pages and your paper has a lot of captions, so I think investing into a bit more text would be good)


Comments:
- It might be worth to re-explain what the information plane plots are in a figure caption, not just in the text (the text also doesn't really explain that each point is a moment in training, and each thread a different layer, this paper should be readable by someone who has never seen these plots before). 
- It's not clear what is going on in figure 5, I can guess but, again, this paper should be readable by anyone in the field. You mention different initializations, but which exactly? What makes you say that 5c has no compression but that 5a does compression first? It should be explained explicitly.
- I believe what you say about Figure 8, but the plots are so similar that it is hard to compare them visually. Maybe a different kind of superposition into a single plot would better illustrate the compression effect of L2?
- Typo in the x axis caption of figures 9.
- Figure 9a is not readable in greyscale (or by a colorblind person), consider using a different symbol for the softmax scatter (and adding this symbol to the legend).
- The first Schmidhuber citation of the paper seems a bit out of place. I think he himself would say that deep learning has been going on for much longer than since 2015. (in fact I think you could just remove the entire first paragraph, it is just unnecessary boilerplate)
- Why should there be a direct correlation between compression and generalization? For example, it is known that training DNNs with soft targets improves test accuracy in classification, or even forcing softness in both targets and representations [2] also improves test accuracy.
- I'm still personally not sold on binning as a strategy to evaluate MI. Did you perform experiments that show that the observed difference is consistent if more computation is done to approximate MI, and not just an artefact of max-entropy binning?

[1] Zhang et al (2016) https://arxiv.org/abs/1611.03530
[2] Verma et al (2018) https://arxiv.org/abs/1806.05236
","The reviewer provides a generally positive overview of the paper, highlighting its contributions and interesting results. While they offer constructive criticism and suggestions for improvement, the overall tone is encouraging and suggestive of the paper's potential value. The reviewer doesn't use harsh language and maintains a professional and respectful tone throughout.",60.0,80.0
"The main contribution of this paper in practice seems to be a way to initialize the Continuous Matrix Space Model so that training actually converges, followed by a slightly different contrastive loss function used to train these models. The paper explores the pure matrix model and a mixed matrix / vector model, showing that both together improve on simpler methods on many benchmark tasks.

My main concern is that the chained matrix multiplication involved in this method is not substantially simpler than an RNN or LSTM sentence encoding model, and there are no comparisons of training and inference cost between the models proposed in this paper and conceptually simpler RNNs and LSTMs. The FastSent paper, used here as a baseline, does compare against some deep models, but they choose far more complex baselines such as the NMT encoding, which is trained on a very different loss function. Indeed the models proposed here do not seem to outperform fasttext and fastsent despite having fairly similar computational costs.

I think this paper could use a little more justification for when it's appropriate to use the method proposed here versus more straightforward baselines.","The review starts by acknowledging the contribution of the paper, which is a positive point. However, it then raises a major concern about the computational cost of the proposed method, comparing it unfavorably to simpler methods like RNNs and LSTMs. The reviewer also points out the lack of justification for choosing this method over simpler baselines. The language used is professional and not aggressive, suggesting a neutral to slightly negative sentiment overall.",-10.0,50.0
"Understanding the stationary equilibrium helps to understand the practical performance of stochastic gradient descent. In this paper, the authors propose two fluctuation-dissipation relation to link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. An advantage over the existing study is that the results here hold for any stationary state and do not need the analogy with continuous-time differential equations. Empirical results are also reported to verify these fluctuation relation.

Comments:

(1) I do not quite understand the second identity of (16). In particular, it seems that the authors replace the first two $\theta(t+1)$ with $\theta(t)$, and do not use this replacement for the third $\theta(t+1)$ (this was addressed by (1)).

(2) It would be helpful to the readers if the authors can give the deduction process of (12). It is not easy for me to understand how it holds.

(3) It is not clear to me how the second fluctuation-dissipation relation helps to determine the properties of loss function landscape.

(4) In Section 2.3.1, can you give some explanation for the harmonic approximation. Also the notation $\theta^*$ seems not to be defined.","The review starts with a positive statement acknowledging the value of the paper's focus. However, it then lists four specific points where the reviewer found the paper lacking in clarity or completeness. While the comments are critical, they are framed as requests for clarification or further explanation, suggesting a desire to understand and potentially recommend the paper for publication. Therefore, the sentiment leans slightly positive, and the language is polite and constructive.",20.0,70.0
"This paper talks about music translation using a WaveNet-based autoencoder architecture.  The models are trained on diverse training sets and evaluated under multiple settings.  What reported in this paper seems to be interesting and the performance sounds good. However, I have following comments/concerns. 

1. The paper is not clearly written. Its exposition needs significant improvement.  There are numerous inconsistent definitions and vague descriptions that make the reading sort of difficult. 
    a)  It would be very helpful if the authors can put up a figure for the description of  the WaveNet  autoencoder instead of just using words in Section 3.1
    b) The paper itself should be self-contained instead of referring readers to other references for the details of model architectures.
    c) The math symbols are poorly defined.  What is the definition of C in Section 3.3?   It is defined or referred to as ""domain classification network"" and also ""domain confusion network"" but nowhere to find in Fig. 1.
   d) ""C is minimizes"" -> ""minimizes""
   e)  In Section 4,  it says that ""Each batch is first used to train the adversarial discriminator"".  Which adversarial discriminator? Where to find in Fig. 1 as it is the only description of the network architecture?  

2.  The authors mentioned a couple of observations that left unanswered.  
    a)   I am surprised to see that without data augmentation, the training does not even converge. 
    b)  The conversion from unseen domains is more successful than the learned domains.
    c)  The decoder starts to be creative when the size of the latent space is reduced. 
   I sense that these observations seem to point to some (serious) generalization issues of the proposed model.  I would like to hear explanations from the authors. 


After reading the rebuttal:
The authors have addressed my major concerns with regard to this paper.   I have lifted my score.  Thanks for the nice response.","The review starts with positive sentiment, acknowledging the interesting topic and good performance. However, it lists several significant concerns regarding clarity, consistency, and unanswered observations, which points towards a more critical stance. The language used is generally polite and professional, focusing on constructive feedback. The final paragraph indicates increased positivity after rebuttal, suggesting the initial concerns were addressed.",20.0,70.0
"The authors, motivated by work in topological graph analysis, introduce a new broadly applicable complexity measure they call  neural persistence--essentially a sum over norms of persistence diagrams (objects from the study of persistent homology).  The also provide experiments testing their parameter, primarily on MNIST with some work on CIFAR-10.

I'd like to preface my criticism with the following: this work is extremely compelling, and the results and experiments are sound.  I'm very interested to see where this goes.  Figure 2 is particularly compelling!

That said, I am extremely suspicious of proposals for measures of generalization which (1) do not make contact with the data distribution being studied, and (2) which are only tested on MNIST and CIFAR-10.  Additionally, (3) it is not clear what a ""good"" neural persistence is, a priori, and (4) I'm not entirely sure I agree with the author's assessment of their numerical data.

In more detail below:

1. At this point, there's a tremendous number of different suggested ways to measure ""generalization"" by applying different norms and bounds and measures from all of the far reaches of mathematics.  A new proposed measure **really needs** to demonstrate a clear competitive measure against other candidates.  The authors make a strong case that this measure is better than competitors from TGA, but I'm not yet convinced this measure is doing enough legwork.  For example, is it possible that a network has high neural persistence, but still has terrible test or validation error?  Why or why not?  Are there obvious counterexamples?  Are there reasons to think those obvious counterexamples aren't like trained neural networks?  These are all crucial questions to ask and answer if you want this sort of measure to be taken seriously.

2.  Most of your numerical experiments were on MNIST, and MNIST is weird.  It's getting to be a joke now in the community that your idea works on MNIST, but breaks once you try to push it to something harder.  Even Cifar-10 has its quirks, and observations that are true of some networks absolutely do not generalize to others.

3. While I'm convinced that neural persistence allows you to distinguish between networks trained in different ways, it isn't clear why I should expect a particular neural persistence to mean anything at all w.r.t. validation loss.  Are there situations in which the neural persistence has stopped changing, but validation loss is still changing appreciably?  Why or why not?

4. I'm concerned that the early stopping procedure used as a benchmark wasn't tuned as carefully as neural persistence was.  I also honestly cannot determine anything from Figure 4 except that your ""Fixed"" baseline is bad, and that persistence seems to do about the same as validation loss.  It even seems that Training loss is a better early stopping criteria (better than both validation and persistence!) from this plot, because it seems to perform just as well, and systematically stop earlier.  Am I reading this plot right (particularly for 1.0 fraction MNIST)?


This work currently seems like a strong candidate for the workshop track.  I would have difficulty raising my score above much more than a 6 without much more numerical data, and analysis of when the measure fails.

Edit: The authors have made a significant effort to address my concerns, and I'm updating my score to 7 from 5 in response.","The reviewer explicitly states ""this work is extremely compelling, and the results and experiments are sound.  I'm very interested to see where this goes.  Figure 2 is particularly compelling!"" This suggests a positive sentiment. However, they also have serious reservations and suggestions for improvement, which lowers the positivity. Therefore, the sentiment is moderately positive. The reviewer maintains a professional and respectful tone throughout, even when expressing criticism. They clearly articulate their concerns and provide constructive feedback. The language used is polite and objective, without resorting to personal attacks or harsh language.",60.0,80.0
"# Positive aspects of this submission

- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.

- The methodology in Sections 4 and 5 is very thorough and useful.

- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.

# Criticism

- In Section 3, even if the ""l1 + projection"" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).
Similarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.
Given that you also study ""o-greedy-hit"" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.","The review starts by highlighting positive aspects of the work, indicating a positive sentiment. However, the criticism, while constructive, points out potential weaknesses in a significant portion of the paper (Section 3). The language used is professional and polite, suggesting areas of improvement without resorting to harsh or negative phrasing.",40.0,80.0
"The paper tackles a very interesting problem about representations, especially of the connectionist kind -- how do we know if the learned representations capture the compositional structure present in the inputs, and tries to come up with a systematic framework to answer that question. The framework assumes the presence of an oracle that can give us the true compositional structure. Then the author try to answer some refreshing questions about the dynamics of learning and compositionality while citing some interesting background reading.

However, I’m a bit torn about the experiments. On the one hand, I like the pedagogical nature of the experiments. They are small and should be easy to reproduce. On the other hand, all of them seem to be fairly similar kinds of composition with very few attributes (mostly bigrams). So whether the intuitions hold for more complex compositional structures is hard to say.

Nevertheless, it’s a well written paper and is a helpful first step towards studying the problem of compositionality in vector representations.


Minor points
Pg 3 “_grammar_ for composing meanings *where* licensed by derivations” seems incorrect. 
Figure 5: seems quite noisy to make the linear relationship claim

EDIT: I still think the compositions under consideration are the simpler ones. Still with the new experiments the coverage seems nicer. Given the authors plan to release their source code, I expect there will be an opportunity for the rest of the community to build on these, to test TRE's efficacy on more complex compositions. I updated my scores to reflect the change.","The reviewer provides both positive and negative points. They find the paper interesting, well-written, and a helpful first step. However, they also express concerns about the simplicity of the experiments. The reviewer's tone is constructive and professional throughout.",50.0,75.0
"This paper proposed an interesting idea of learning representations of sets by permutation optimizations. Through learning a permutation of the elements of a set, the proposed algorithm can learn a permutation-invariant representation of that set. To deal with the underlying difficult combinatorial optimization problem, the authors proposed to relax the optimization constraints and instead optimize over the set of doubly-stochastic matrices with reparameterization using the Sinkhorn operator. The cost function of this optimization is related to a pairwise ordering cost, which compares the order for each pair of the elements.

The idea of using pairwise comparison information to learn permutations is interesting. The total cost function utilizes the comparison information and optimization over this cost function can lead to a permutation-invariant representation of the set. The idea of using the Sinkhorn operator to reparameterize the doubly-stochastic matrices makes the optimization objective differentiable. Also, the experiment results compared with some baseline algorithms showed the success of the proposed methods in many different tasks.

My major concern of the proposed method is on whether this method can be applied to large sets. Since the algorithm compares all pairs of elements in the set, we need O(N^2) comparisons for a set of size N and hence the proposed method might be slow if N is large. Is it possible to improve the efficiency for large sets?

Questions and Suggestions:

1. Since the authors wants to approximately solve the objective function in Equation (2), it is better if we can see a proof showing why this optimization problem is difficult.

2. For the experiment in Section 4.2, it seems that all methods (including the proposed methods and the baseline methods) are not performing well if the images are split to at least 4 * 4 equal-size tiles. I understand that currently the authors applied their method to the case of grid permutation by simply adding all cost functions of all rows and columns. Is it possible to extend the proposed method to the grid case in another way so that the results under this setting is better? 

3. It will be better if the authors can propose some more insights (probably with some theoretical analysis) when can the PO-U method performs better and when can the PO-LA method performs better.

4. The authors mentioned that, the proposed method can get good permutations even for only T=4 steps. What if we continue running the algorithm? Will the permutation converges stably?

5. The authors proposed to update the permutation matrix parameters in an alternative way (Equation (7)) and mentioned that this update works significantly better in the experiments. It will be great if the authors can have a theoretical analysis on why this is true since P and \tilde P can be quite different from each other for an arbitrary \tilde P matrix.


Minor comment:

I think there is a typo in Equation (5). The entry \tilde P_{pq} is related to not only the entry P_{pq}, but also the other entries of the matrix P. Hence, I think Equation (5) should be modified as a matrix multiplication.","The review is positive overall. The reviewer finds the idea interesting and acknowledges the success of the proposed method. While they raise a major concern about scalability, they present it constructively as an area for improvement rather than a flaw. The tone throughout is professional and the suggestions are framed as opportunities for the authors to strengthen their work.",60.0,80.0
"This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems. The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer. 

I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results. I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset. The paper is a reasonable dataset/analysis paper. Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).

I think that the dataset generation process is well-thought-out. There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion. The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks. I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly. Details on how to generate the dataset, however, can be moved into the appendix. I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?

One other thing I want to see is a test set with multiple different difficulty levels. The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)? Also, is there an option for ""unsolvable""? For example, the answer being a special ""this is impossible"" character for ""factorise x^2 - 5"" (if your training set does not use \sqrt, of course).","The reviewer explicitly states a positive view of the paper (""this paper makes a good contribution"") and is happy to see such work being done. They provide constructive criticism and suggestions for improvement, but this is typical of a peer review process and doesn't negate the positive sentiment. The language used is professional and polite throughout, without any harsh or disrespectful language.",65.0,80.0
"-- Paper Summary --

The primary contribution of this paper is the presentation of a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors featured in the literature. This is achieved by way of introducing a KL measure over stochastic processes which allows for priors to take the form of GP priors and other custom variations. Two approaches are given for training the model, one inspired by GANs, and a more practical sampling-based scheme. The performance of this training scheme is validated on a variety of synthetic and real examples, choosing Bayes by Backprop as the primary competitor. An experiment on contextual bandit exploration, and an illustrative Bayesian optimisation example  provided in the supplementary material showcase the effectiveness of this method in applications where well-calibrated uncertainty is particularly pertinent.

-- Critique --

This paper makes important strides towards giving more meaningful interpretations to priors in BNNs. To the best of my knowledge, the KL divergence between stochastic processes that gives rise to an alternate ELBO has not been featured elsewhere, making this a rather interesting contribution that is supplemented by suitable theorems both in the main text and supplementary material. The introductory commentary regarding issues faced with increasing the model capacity of BNNs is particularly interesting, and the associated motivating example showing how degeneracy is countered by fBNN is clear and effective.

The GAN-inspired optimisation scheme is also well-motivated. Although the authors understandably do not pursue that scheme due to the longer computation time incurred (rendering its use impractical), it would have been interesting to see whether the optimum found using this technique is superior to the sampling based scheme used throughout the remainder of the paper. The experimental evaluation is also very solid, striking an adequate balance between synthetic and real-world examples, while also showcasing fBNNs’ effectiveness in scenarios relying on good uncertainty quantification.

In spite of the paper’s indisputable selling points, I have several issues with some aspects of this submission. For clarity, I shall distinguish my concerns between points that I believe to be particularly important, and others which are less significant:

- Monte Carlo dropout (Gal & Ghahramani, 2016), and its extensions (such as concrete dropout), are widely-regarded as being one of the most effective approaches for interpreting BNNs. Consequently, I would have expected this method to feature as a competitor in your evaluation, yet this method does not even get a cursory mention in the text.

 - The commentary on GPs in the related work paints a dour picture of their scalability by mostly listing older papers. However, flexible models such as AutoGP (Krauth et al, 2017) have been shown to obtain very good results on large datasets without imposing restrictions on the choice of kernels.

 - The regression experiments all deal with a one-layer architecture, for which the proposed method is shown to consistently obtain better results. In order to properly assess the effectiveness of the method, I would also be interested in seeing how it compares against BBB for deeper architectures on this problem. Although the authors cite the results in Figure 1 as an indicator that BBB with more layers isn’t particularly effective, it would be nice to also see this illustrated in the cross-dataset comparison presented in Section 5.2.

 - Furthermore, given that all methods are run for a fixed number of iterations, it might be sensible  to additionally report training time along with the results in the table. This should reflect the pre-processing time required to optimise GP hyperparameters when a GP prior is used. Carrying out Cholesky decompositions for 1000x1000 matrices 10k times (as described in Section 5.2.2) does not sound insignificant.

- The observation regarding the potential instability of GP priors without introducing function noise should be moved to the main text; while those who have previously worked with GPs will be familiar with such issues, this paper is directed towards a wider audience and such clarifications would be helpful for those seeking to replicate the paper’s results. On a related note, I would be keen on learning more about other potential issues with the stability of the optimisation procedure, which does not seem to be discussed upfront in the paper but is key for encouraging the widespread use of such methods.

- The paper contains more than just a handful of silly typos and grammatical errors - too many to list here. This single-handedly detracts from the overall quality of the work, and I highly advise the authors to diligently read through the paper in order to identify all such issues.

 - The references are in an absolute shambles, having inconsistent citation styles, arXiv papers cited instead of conference proceedings, etc. While this is obviously straightforward to set right, I’m nonetheless disappointed that this exercise was not carried out prior to the paper’s submission.

 - The theory presented in Appendix A of the supplementary material appears to be somewhat ‘dumped’ there. Given that this content is crucial for establishing the correctness of the proposed method, linking them more clearly to the main text would improve its readability and give it a greater sense of purpose. I found it hard to follow in its current state.

** Minor **

 - In the introduction there should some mention of deep Gaussian processes which are implicitly a direct competitor to BNNs, and can now also be scaled to millions and billions of observations (Cutajar et al. 2017; Salimbeni et al. 2017). The former is particularly relevant to this work since the architecture can be assimilated to a BNN with special structure for emulating certain kernels.

 - Experiment 5.1.1 is interesting, and the results in Figure 2 are convincing. I would also be interested in seeing how fBNN performs when the prior is misspecified however, which may be induced by using a less appropriate GP kernel. This would complement the already provided insight on using tanh vs ReLU activations.

 - The performance improvement for the experiment on large regression datasets is quite subdued, so it might be interesting to see how both methods compare against each other when deeper BNN architectures are considered. 

- With regards to Appendix C.2, which order arccosine kernel is being used here? One can easily draw similarities between the first order arccosine kernel and NN layers with ReLUs, so perhaps it would be useful to specify which order is being used in the experiment.  

- Given that the data used for experiments in Appendix C.3 effectively has grid structure, I would be interested in seeing how KISS-GP performs on this task. There should be easily accessible implementations in GPyTorch for testing this out. Given how GPs tend to not work very well on image completion tasks due to smoothness in the kernel, this comparison may also be in fBNNs favour.

- Restating the basic architecture of the BNN being used for the contextual bandits experiment in the paper itself would be helpful in order to avoid having to separately check out Riquieme et al (2018) to find such details.

- I wonder if the authors have already thought about the extendability of their proposal to more complex BNN architectures such as Bayesian ConvNets?


-- Recommendation --

Whereas several ICLR submissions tend heavily towards validation by way of empirical evaluation, I find that the theoretic contributions presented in this paper are by themselves interesting and well-developed, which is very commendable. However, there are multiple telling signs of this being a rushed submission, and I am less inclined to argue ardently for such a paper’s acceptance. Although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, I think that the paper is in dire need of a thorough clean-up before being published.

Pros/Cons summary:

+   Interesting concepts that extend beyond empirical fixes.
+   Defining more interpretable priors is a very pertinent topic in the study of BNNs.
+   The presented ideas could potentially have notable impact.
+   Illustrative experiments and benchmark tests are convincing.
-   Not enough connection to MC dropout.
-   Choice of experiments and description of stochastic processes overly similar to other recent widely-publicised papers. It feels on trend, but consequently also somewhat reductive.
-   More than a few typos and grammatical errors.
-   Presentation is quite rough around the edges. The references are in a particularly dire state.","The reviewer acknowledges the paper's strengths, such as its novel theoretical contributions and solid experimental evaluation. They use terms like ""important strides,"" ""interesting contribution,"" and ""indisputable selling points."" However, they also express several concerns, including missing comparisons with relevant work, potential issues with the experimental setup, and a lack of polish in the presentation (typos, inconsistent references). They call it a ""rushed submission"" that needs a ""thorough clean-up."" Overall, the feedback is mixed but leans towards the positive side, as the reviewer sees the paper's potential but isn't fully convinced due to the mentioned flaws.",40.0,50.0
"The paper proposes a new discriminator loss for MMDGAN which encourages repulsion between points from the target distribution. The discriminator can then learn finer details of the target distribution unlike previous versions of MMDGAN. The paper also proposes an alternative to the RBF kernel to stabilize training and use spectral normalization to regularize the discriminator. The paper is clear and well written overall and the experiments show that the proposed method leads to improvements. The proposed idea is promising and a better theoretical understanding would make this work more significant. Indeed, it seems that MMD-rep can lead to instabilities during training while this is not the case for MMD-rep as shown in Appendix A. It would be good to better understand under which conditions MMD-rep leads to stable training. Figure 3 suggests that lambda should not be too big, but more theoretical evidence would be appreciated.
Regarding the experiments: 
- The proposed repulsive loss seems to improve over the classical attractive loss according to table 1, however, some ablation studies might be needed: how much improvement is attributed to the use of SN alone? The Hinge loss uses 1 output dimension for the critic and still leads to good results, while MMD variants use 16 output dimensions. Have you tried to compare the methods using the same dimension?
-The generalized spectral normalization proposed in this work seems to depend on the dimensionality of the input which can be problematic for high dimensional inputs. On the other hand, Myato’s algorithm only depends on the dimensions of the filter. Moreover, I would expect the two spectral norms to be mathematically related [1]. It is unclear what advantages the proposed algorithm for computing SN has.
- Regarding the choice of the kernel, it doesn’t seem that the choice defined in eq 6 and 7 defines a positive semi-definite kernel because of the truncation and the fact that it depends on whether the input comes from the true or the fake distribution. In that case, the mmd loss loses all its interpretation as a distance. Besides, the issue of saturation of the Gaussian kernel was already addressed in a more general case in [2]. Is there any reason to think the proposed kernel has any particular advantage?

Revision:

After reading the author's response, I think most of the points were well addressed and that the repulsive loss has interesting properties that should be further investigated. Also, the authors show experimentally the benefit of using PICO ver PIM which is also an interesting finding.
I'm less convinced by the bounded RBF kernel, which seems a little hacky although it works well in practice. I think the saturation issues with RBF kernel is mainly due to discontinuity under the weak topology of the optimized MMD [2] and can be fixed by controlling the Lipschitz constant of the critic.
Overall I feel that this paper has two interesting contributions (Repulsive loss + highlighting the difference between PICO and PIM) and I would recommend acceptance.






[1]: Sedghi, Hanie, Vineet Gupta, and Philip M. Long. “The Singular Values of Convolutional Layers.” CoRR 
[2]: M. Arbel, D. J. Sutherland, M. Binkowski, and A. Gretton. On gradient regularizers for MMD GANs.



","The review starts with positive statements, highlighting the clarity, novelty, and promising nature of the work. However, it then poses several questions and suggests areas for improvement, indicating a desire for more theoretical grounding and ablation studies. Despite the constructive criticism, the overall tone remains encouraging, particularly in the 'Revision' section where the reviewer acknowledges the authors' efforts and recommends acceptance. Therefore, the sentiment leans positive, but with room for improvement.",60.0,70.0
"This work addresses the problem of learning latent embeddings of high-dimensional time series data. The paper emphasises the need of interpretable representations accounting for the correlated nature of temporal data. To this scope, the study proposes to cluster the data in a latent space estimated through an auto-encoder. The clustering is obtained by leveraging on the idea of self-organising maps (SOM). Within this setting, the data is mapped into a 2D lattice where each coordinate point represents the center of an inner cluster. 
This construction motivates the formulation of the auto encoder through the definition of several cost terms promoting reconstruction, clustering, and consistency across latent mappings. 
This definition of the problem allows an heuristic for circumventing the non-differentiability of the discrete mapping. The enhance consistency over time, the model is further equipped with an additional cost term enforcing transition smoothness across data points and latent embeddings. 

The experiments are carried out with respect to synthetic 2D time-series, chaotic time-series from dynamical systems, and clinical data. In each case the proposed method shows promising results with respect to the proposed benchmark. 

The study presents some interesting methodological and technical ideas. On the other hand the manuscript presentation is quite convoluted, at the expense of a lacks of clarity in the details about the implementation of the methodology. Moreover, motivated by practical aspects, the model optimisation relies on computational strategies not completely supported from the theoretical point of view (such as the zeroing of the gradient in backpropagation, or the approximation of the clustering function to overcome non-differentiability). The impact of these modeling choices would deserve more investigation and discussion. 

Detailed comments:

- As also stated by the authors, the use of a 2D latent representation is completely arbitrary. It may be true that a 2D embedding provides a simple visualisation, however interpretability can be obtained also with much richer representations in a number of different ways (e.g. sparsity, parametric representations, …). Therefore the feeling is that the proposed structure may be quite ad-hoc, and one may wonder whether the algorithm would still generalise to more complex latent representations.
- Related to the previous comment, the number of latent points seems to be crucial to the performance of the method. However this aspect is not discussed in detail, while it would be beneficial to provide experiment about the sensitivity and accuracy with respect to the choice if this parameters.
- The method relies on several cost terms plugged together. While each of them takes care of specific consistency aspects of the model, their mutual relation and balance may be very critical. This is governed by a series of trade-off parameters whose effect is not discussed  nor explored throughout the study. I guess that the optimisation stability may be also quite sensitive to this trade-off, and it would be important to provide more details about this aspect. 
- Surprisingly, k-means seems to perform quite well in spite of its simplicity. Also, there is no mention about initialisation and choice of the parameter “k”. The authors may want to better discuss the performance of this algorithm, especially compared to its much lower modeling complexity with respect to the proposed method. 
- Still related to the comparison with respect to the state-of-art, interpretability in time series analysis can be achieved with much lesser assumptions and parameters by using standard approaches such as independent component analysis. I would expect this sort of comparison, especially in case of long-term data such as the one provided in the Lorenz system. 
- Clustering of short-term time series, such as the clinical ones, is a challenging task. The feeling is that a highly parametrised model, such as the proposed one,  may still not be superior with respect to classical methods, such as the mixture of linear regressions. This sort of comparison would be quite informative to appreciate the real value of the proposed methodology.","The review starts with a neutral summary of the work, not expressing overt positivity or negativity. However, it proceeds to point out several limitations and suggests areas of improvement. Phrases like ""on the other hand,"" ""quite convoluted,"" ""lacks of clarity,"" ""not completely supported from the theoretical point of view,"" and ""would deserve more investigation"" indicate a critical stance and a desire for significant revisions. While the reviewer acknowledges the ""interesting methodological and technical ideas,"" the overall tone leans towards the need for substantial refinement. The language, while direct and critical, maintains a professional and respectful tone. There are no personal attacks or disrespectful remarks. The reviewer focuses on the work's technical aspects and provides constructive criticism.",20.0,60.0
"This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.

This paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.

Remarks:
*) P1, line 2: it particular -> in particular.
*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.
*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.
*) P5, Figure 1: does a loop for the index $i$ missing?
*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \to R: \theta-> , where m is the number of nodes. Otherwise, it is not obvious to see the domain. 
*) P7, last paragraph: Tables 2 and 3 -> Figures 2 and 3.
*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.

","The reviewer states that the paper is ""well-written"" and that the proposed methods are ""potentially important."" They recommend publication after the authors address their comments. This suggests a positive sentiment. The reviewer also provides constructive criticism and specific recommendations in a professional and respectful manner, indicating politeness. Overall, the tone is encouraging and helpful rather than dismissive or harsh. Therefore, the sentiment is scored as moderately positive, and the politeness is scored as very polite.",60.0,80.0
"This paper gives the first nonvacuous generalization bounds for
meaningful Imagenet models.  These bounds are given in terms of the
bit length of compressions of learned models together with a method
for taking into account symmetries of the uncompressed parameters.

These bounds are nonvacuous only when the compressed models are small
--- on the order of 500 Kilobytes.  State of the art compressed models
of this size achieve Imagenet accuracies slightly better than Alexnet,
16% error for top 5, and this paper reports a nonvacuous
generalization guarantees of 89% error for top 5.  While there is
still a large gap between the actual generalization and the guarantee,
this would still be a significant accomplishment.

I have one major concern.  The generalization bound involves adding an
empirical loss and a regularization term computed from a KL
divergence.  I am convinced that the authors have correctly handles
the KL divergence term.  But the paper does not contain sufficient
detail to determine if the authors correctly handle the empirical loss
term.  It is NOT correct to use the training loss of the
(deterministic) compressed model.  The generalization bound requires
that the training loss be measured under the parameter noise of the
posterior distribution.  The paper needs to be clear that this has
been done. The comments in Appendix B on noise robustness are
disturbing in this regard.

If the training loss  has been calculated correctly in the bound,
the results are significant.

Assuming correctness, I would comment that the Catoni bound, while sqeaking
out all available tightness, is very opaque.  I might be good to
consider the more transparent bounds, claimed to be essentially the
same, given in McAllester's tutorial.  If the more transparent bounds
achieve equivalent numerical results, they would make the nature of
the bounds clearer.

Another comment involves a largely ignored detail in (Dzuigaite and
Roy 17). Their bounds become vacuous if they center their Gaussian
prior at zero.  Instead they center the prior on the initial value of
the parameters.  This yields a dramatic improvement in the bound.  In
the context of the present paper, this suggests a modification of the
prior distribution on the compressed model.  We represent the model by
first selecting the r code values.  I think a distribution could be
defined on the code book that would improve its log probability, but I
will ignore that.  Given the r code values we can define a
distribution over the possible compressed representations of a weight
w_i in terms of a prior on w_i defined in terms of its initial value.
This gives a probability distribution over the compressed
representation.  Using log probability of the compressed
representation should then be a significant improvement on the first
term in (8).  This shift in the prior on compressed models has no
effect on the second term of (8) so things should only get better.
","The review starts with positive sentiment, highlighting the significance of the paper's contribution. However, it raises a major concern about the correctness of the empirical loss calculation, which is crucial for the validity of the results. The reviewer acknowledges the potential significance if the concern is addressed. The remaining comments are suggestions for improvement and further exploration, indicating a generally positive outlook but with reservations about a key aspect.",60.0,70.0
"The authors splitted the features of multimodal representations to ""common"" (multimodal discriminative) and ""specific"" (modality-specific generative) factors. In this framework, their MFM can capture more detailed features. 

Pros:
(*) Learning the feature representations from two perspectives. 

(*) Even missing one modality, MFM can still achieve acceptable performance. 

(*) Using mutual information and gradient-based method to interpret their method. 

Cons:
(*) The work has some similarity to Hsu & Glass (2018), but the comparison between this work is only on CMU-MOSI.

(*) In Table. 3, it shows that language is the most informative feature for prediction. However, in Table. 2, it can be seen that if audio is missing, the result it the worse compared to the other two cases. It seems the interpretation is not convincing to me. Can you give us more explanation about this phenomenon? 

Comments:
(*) The details of SVHN-MNIST experiment are missing. Appendix B gave some information about models but specified the targeted datasets.

(*) The appendix is not clear, e.g. In Appendix B, it is said ""subsection 3.3"" but there is no section 3.3.  


","The review is generally positive, highlighting the strengths of the paper's approach and its contributions. The reviewer acknowledges the novelty of learning feature representations from two perspectives and the model's robustness to missing modalities. While the reviewer raises valid concerns and seeks clarification on certain aspects, such as the discrepancy between language's perceived importance and the performance drop when audio is missing, and the lack of clarity in the appendix, these are presented constructively with suggestions for improvement rather than outright criticism. The language used is formal and objective, typical of academic peer reviews.",60.0,70.0
"The authors combined several update steps together to achieve aggregated momentum. They showed that  it is more stable than the other momentum methods. Also, in Auto-encoder and image classification, AggMo outperforms than the other methods. 

Pros:
(+) Theoretical result is shown on the quadratic problem.

(+) Extensive numerical experiments are shown to illustrate the stability of AggMo.

Cons:
(+) The results are not convincing. For example, it said in the default setting (CM \beta=0.9), ResNet34 on CIFAR-10 has accuracy 90.22\%. However, it should be around 93\%.

(+)  This method is similar to multi-step gradient methods.



Comments:
(+) This is no “introduction” in the paper. 

(+) There should be “,” after mathematical equations. 
","The review acknowledges the positive aspects of the paper, such as the theoretical results and extensive experiments ('Pros' section). However, it also raises serious concerns about the results' validity and novelty ('Cons' section).  The comment about the introduction being missing further suggests weaknesses in the paper. Overall, the feedback indicates significant areas for improvement. Therefore, the sentiment leans towards the negative side. The language used is formal and provides constructive criticism without resorting to harsh or disrespectful language, indicating politeness.",-30.0,70.0
"# overview
This paper focuses on multi-agent reinforcement learning tasks that require communication between the agents, and further presupposes that the communication protocol is bandwidth constrained and contentious so that a scheduling mechanism is necessary.  To address this they introduce a new learned weighting scheme based scheduler and distributed actor, centralized critic based architecture which is evaluated on a couple of communication driven multi-agent tasks.

The two evaluation tasks had their bandwidth artificially constrained, and SchedNet time to convergence was shown to fall somewhere between having no communication and full communication, and somewhat better than a purely round-robin based scheduling scheme, which doesn't seem particularly informative.  From this it is difficult to assess the significance of the contributions.

# pros
* communication in multi-agent scenarios is an important aspect to consider, and this work shines a spotlight on scenarios in which bandwidth is constrained.
* general presentation fairly clear and easy to read

# cons
* Would have been more impactful to focus experiments on real-world scenarios in which bandwidth is constrained and naturally contentious

# other comments
* pg. 2 related work, suspect you meant to call out Foerster et al 2017b in second reference not Foerster et al 2017a twice.","The review is lukewarm in its assessment of the paper. While it acknowledges the importance of the topic and clarity of presentation, it raises concerns about the significance of the contributions and the artificial nature of the experiments. The reviewer suggests that focusing on real-world scenarios would have made the work more impactful. The tone of the review is critical but professional and polite.",20.0,50.0
"In this paper the authors distinguish between two families of training objectives for seq2seq models, namely, divergence minimization objectives and max-margin objectives. They primarily focus on the divergence minimization family, and show that the MRT and RAML objectives can be related to minimizing the KL divergence between the model's distribution over outputs and the ""exponentiated payoff distribution,"" with the two objectives differing in terms of the direction of the KL. In addition, the authors propose an objective using the Hellinger distance rather than the KL divergence, and they conduct experiments on machine translation and summarization comparing all the considered objectives.

The paper is written extremely clearly, and is a pleasure to read. While the discussion of the relationship between RAML and MRT (and MRT and REINFORCE) is interesting and illuminating, many of these insights appear to have been discussed in earlier papers, and the RAML paper itself notes that it differs from REINFORCE style training in terms of the KL direction.

On the other hand, the idea of minimizing Hellinger distance is I believe novel (though related to the alpha-divergence work cited by the authors in the related work section), and it's nice that training with this loss improves over the other losses. Since the authors' results, however, appear to be somewhat below the state of the art, I think the main question left open by the experimental section is whether training with the Hellinger loss would further improve state of the art models. Even if it would not, it would still be interesting to understand why, and so I think the paper could be strengthened either by outperforming state of the art results or, perhaps through an ablation analysis, showing what aspects of current state of the art models make minimizing the Hellinger loss unnecessary.

In summary,

Pros:
- well written and interesting
- a new loss with potential for improvement over other losses
- fairly thorough experiments

Cons:
- much of the analysis is not new
- unclear if the proposed loss will improve the state of the art, and if not why 

Update after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper.","The review starts with positive remarks, highlighting the clarity and interesting aspects of the paper. However, it also points out that some insights are not entirely novel. While acknowledging the proposed Hellinger distance as potentially novel and showing improvement, the reviewer expresses uncertainty about its impact on state-of-the-art models. The 'Pros' and 'Cons' sections further summarize this mixed sentiment, leaning slightly towards the positive due to the paper's clarity, the novelty of the Hellinger distance, and the thorough experiments. The post-revision update confirms a positive shift in the reviewer's opinion,  indicating overall positive sentiment but not overly enthusiastic.",60.0,90.0
"This paper proposes a Dynamic Parameter Generator (DPG) that given a test input modifies the parameters of a classification model. They also propose to regularize the training using a Data Generator (DG) to slow down catastrophic forgetting. DG is used to constrain the training that the internal representations of data generated by DG does not rapidly change. DG removes the need for storage of data or labels.

Positives:
- Both ideas of DPG and DG are novel in preventing catastrophic forgetting.
- DG is novel because it does not require storage of data and does not depend on labels.
- Experimental results are significantly better than the previous state-of-the-art.

Suggestions and clarification requests:
- Figures are very small and equations are cramped because of reduced spacing.
- There are some vague explanations in the intro that could be reduced. It would be nice to first introduce concrete math then give the intuitions. That saves some space.
- It would nice to compare to the recent Progress & compress [1]. Unfortunately, they have not provided results on benchmark MNIST tasks.
- This work is related to a recently proposed idea in architecture search [2] that learns to predict the weights of a network given its architecture.
- Can you clarify whether you have used DG at test time?
- Can you report results without using DG? It is not clear whether DPG is accountable for preventing the catastrophic forgetting or the sluggishness enforced by DG.
- Questions 1 and 2 need more formalization if the authors want to clearly prove a statement.
- As the answer to Question 1 suggests, have you explored enforcing a Lipschitz constraint?
- The answer to Question 2 is interesting. Could you rewrite it more formally? It seems like you can argue that DG’s objective encourages the employment of unused parameters which is important in tackling catastrophic forgetting.
- Can you elaborate on how much forgetting happens for DG?
- It seems that in figure 3.f and 3.c the MA method is unable to reach the best possible performance on the last task. Can you also report the table of accuracies on the last task?

[1] Schwarz, Jonathan, et al. ""Progress & Compress: A scalable framework for continual learning."" arXiv preprint arXiv:1805.06370 (2018).
[2] Brock, Andrew, et al. ""SMASH: one-shot model architecture search through hypernetworks."" arXiv preprint arXiv:1708.05344 (2017).","The review starts by highlighting the novelty of the paper's contributions and acknowledges the significant improvement in experimental results. This suggests a positive sentiment. The reviewer also provides constructive suggestions and requests for clarification, indicating a willingness to help improve the paper. The language used is polite and professional throughout, with phrases like ""It would be nice"" and ""Can you clarify."" The suggestions are framed as opportunities for improvement rather than criticisms.",60.0,80.0
"The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.

The proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \theta such that the number of datapoints x with r(x) > \theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \theta, and to port the original problem to a vanilla binary classification problem.

Concerns:
- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.
- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?
- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for ""unbiased PU learning (PU)"", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.
- Related work discussion is completely missing apart from one paragraph in the introduction.

Minor:
- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.
- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.
- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.
- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?
- Theorem 3: add some intuition and explain tradeoff on \epsilon
- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, ""we trained a classifier minimizing (4) and (7) with the model (10)"" is difficult to digest and follow.
- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}
- Too many acronyms and abbreviations.

","The review is critical of the paper, pointing out several flaws and demanding significant improvements. While the language is direct and to the point, it avoids harsh or disrespectful language, maintaining a professional tone. The reviewer points out missing comparisons with similar work, questions the practicality of assumptions, finds the experiments unconvincing, and suggests improvements in presentation and clarity. All of these are valid academic criticisms and are presented in a neutral, professional manner.",-20.0,60.0
"The paper presents a combination of evolutionary search methods (CEM) and deep reinforcement learning methods (TD3). The CEM algorithm is used to learn a Diagional Gaussian distribution over the parametes of the policy. The population is sampled from the distribution. Half of the population is updated by the TD3 gradient before evaluating the samples. For filling the replay buffer of TD3, all state action samples from all members of the population are used. The algorithm is compared against the plane variants of CEM and TD3 as well as against the evoluationary RL (ERL) algorithm. Results are promising with a negative result on the swimmer_v2 task.

The paper is well written and easy to understand. While the presented ideas are well motivated and it is certainly a good idea to combine deep RL and evoluationary search, novelty of the approach is limited as the setup is quite similar to the ERL algorithm (which is still on archive and not published, but still...). See below for more comments:
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants). 
- We are learning a value function for each of the first half of the population. However, the value function from the previous individual is used to initialize the learning of the current value function. Does this cause some issues, e.g., do we need to set the number of steps so high that the initialization does not matter so much any more? Or would it make more sense to reset the value function to some ""mean value function"" after every individual?
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper



","The reviewer provides a generally positive overview of the paper, highlighting its clarity and the good motivation behind the presented ideas. However, they also point out limitations in novelty and raise specific questions about the methodology. The tone remains constructive and professional throughout. Therefore, the sentiment leans slightly towards the positive side, while the politeness score reflects a neutral to positive tone.",60.0,70.0
"This paper analyzed the global convergence property of SGD in deep learning based on the star-convexity assumption. The claims seem correct and validated empirically with some observations in deep learning. The writing is good and easy to follow.

My understanding of the analysis is that all the claims seem to be valid when the solution is in a wide valley of the loss surface where the star-convexity holds, in general. This has been observed empirically in previous work, and the experiments on cifar10 in Fig. 2 support my hypothesis. My questions are:

1. How to guarantee the star-convexity will be valid in deep learning?
2. What network or data properties can lead to such assumption?

Also, this is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: 

Zhang et. al. CVPR'18. ""BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning"".
","The reviewer starts with positive statements about the paper, such as ""The claims seem correct and validated empirically"", ""The writing is good and easy to follow."" This indicates a positive sentiment. However, the reviewer also raises valid questions, suggesting areas of improvement. The questions are posed professionally and constructively, indicating politeness. Overall, the tone is balanced, leaning towards the positive side due to the initial praise. Therefore, the sentiment score is slightly positive, and the politeness score is positive.",60.0,80.0
"
The authors present a GAN based framework for Graphic Layouts. Instead of considering a graphic layout as a collection of pixels, they treat it as a collection of primitive objects like polygons. The objective is to create an alignment of these objects that mimics some real data distribution.

The novelty is a differentiable wireframe rendering layer allowing the discriminator to judge alignment. They compare this with a relation based discriminator based on the point net architecture by Qi et al. The experimentation is thorough and demonstrates the importance of their model architecture compared to baseline methods. 

Overall, this is a well written paper that proposes and solves a novel problem. My only complaint is that the most important use case of their GAN (Document Semantic Layout Generation) is tested on a synthetic dataset. It would have been nice to test it on a real life dataset.","The review starts with positive remarks, highlighting the novelty and thoroughness of the work. While it points out a shortcoming (lack of testing on real-life datasets), it frames it as a suggestion rather than a harsh criticism and reiterates that the paper is well-written and addresses a novel problem. Overall, the tone is constructive and encouraging.",75.0,80.0
"The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets.

pros)
(+) The authors provide a theoretical ground.
(+) The theoretical analysis of the convergence of the proposed algorithm is well provided.
(+) The computational overhead reduced by the proposed method compared with BN and GN looks good.

cons)
(-) There is no comparison with other weight reparameterization methods such as Weight Normalization, Normalization propagation, Instance Normalization, or Layer Normalization. 
(-) The evidence why functionally equivalence is connected to the performance or generalization ability is not clarified.  
(-) The experimental results cannot consistently show the effectiveness of the proposed method in test accuracy. In Table 4, the proposed method outperforms BN, but In Table 2 and 3, BN is mostly better than the proposed method.
(-)  The batch size shown in Table 2 and 3 may be intended to show the batch-independent property of the proposed method, but BN is also doing well in those tables. Therefore, Table 2 and 3 are not adequate to show the batch-independent property.
(-) The proposed method should evaluate with deeper networks (e.g., ResNet-50, ResNet101, or DenseNet-169) to support the superiority over BN and GN.  
(-) Adjusting c does not seem to be promising. In Table 2 and 3, ENorm-1 is better than ENorm-1.2, and also in Table 4, only the result of ENorm-1 is provided. The authors should do a parameter study with c to make all the experiments more convincing.

Comments) 
- The experimental settings are not consistent. The authors should provide the reason why they set those settings or should include some studies about the parameters (for example about the paramter c). 
- Section 3.7 is not clear to me.  How's the performance going on when adjusting c < 1?
- It is better for the authors to provide the Sinkhorn-Knopp algorithm (SK algorithm), which gave them an inspiration for this work, for better readability. 
- Why eq.(4) is necessary? For iterative optimization? If so, the authors should incorporate a detailed explanation about this in the corresponding section.
- The authors should provide a detailed description of the parameter c. It is not clear why c is necessary, and please make sure the overall derivation does not need to be modified due to the emergence of c.
- It seems that the authors could compact the paper by highlighting key ideas. 
- Typo: Annex A (on p.5).

The paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method.","The review acknowledges the strengths of the paper, such as its theoretical grounding and analysis. However, it raises significant concerns about the experimental validation and consistency, ultimately questioning the effectiveness of the proposed method. The tone is critical but professional and provides constructive feedback for improvement. Therefore, the sentiment leans towards the negative side due to the unconvincing experimental results. The politeness remains neutral as the reviewer maintains a professional and respectful tone throughout.",-20.0,0.0
"This is an interesting paper that studies the latent variable modeling from an information theoretic perspective. Specifically, the authors argue that the rate-distortion theory for lossy compression provides a natural toolkit for studying latent variable models, and they propose a lower bound (also a gap function) that could be used to assess the goodness of data fitting given a pair of prior distribution over latent factor and a likelihood function. Overall the paper is very well-written, clear to follow, and the authors did a great job in not overclaiming their results. 

Several questions follow: 
1.  In Eq. (3), why the R.H.S. is an upper bound of the L.H.S.? Under the assumption of (1) should this be equal? 
2.  In section 2, ""must use at use"" -> ""must use at least"". 
3.  Since the mutual information is convex in the conditional distribution Q(Z|X), when considering the Lagrangian, since \alpha is constrained to be positive, should the sign before \alpha be positive instead of negative? 
4.  In section 3.3, ""An very common"" -> ""A very common"". 

To me the most interesting result in this paper is in Thm. 1, Eq. (9), where the authors show that the optimization over the prior in latent variable modeling is exactly equivalent to the optimization of the channel in rate-distortion theory. Following this line the authors propose a gap function that could be used to assess the goodness of a model. One drawback of the current framework is that it only links the optimization of the prior, rather than the likelihood function, to rate-distortion theory, while in practice it is usually the other way around. Although the authors argue in section 3.3 that similar conclusion could be achieved for a family of likelihood functions, the analysis is only possible under the very restrictive (in my personal view) assumption that relies on the existence of a smooth and invertible mapping. This assumption usually does not hold in practice, e.g., the ReLU network, and as a result the analysis here is only of theoretical interest. 

The experimental validation basically shows the usefulness of the proposed gap function in assessing the goodness of model fitting in latent variable models. It would be great if there are more direct use of the proposed lower bound, but I appreciate the novelty in this paper on bridging the two subfields. 
","The review starts with positive statements, highlighting the interesting aspects and clarity of the paper. The reviewer acknowledges the authors' efforts and contributions. While the reviewer raises valid questions and points out limitations, they are presented constructively and with a focus on improving the paper. The reviewer also recognizes the novelty of the work and its potential value. Overall, the tone is balanced, professional, and suggestive of further development rather than dismissive.",60.0,80.0
"The paper presents a novel model for neural speed reading. In this new model, the authors combined several existing ideas in a nice way, namely, the new reader has the ability to skip a word or to jump a sequence of words at once. The reward of the reader is mixed of the final prediction correctness and the amount of text been skipped. The problem is formulated as a reinforcement learning problem. The results compared with the existing techniques on several benchmark datasets show consistently good improvements.

In my view, one important (also a little surprising) finding of the paper is that the reader can make jump choices successfully with the help of punctuations. And, blindly jumping a sequence of words without even lightly read them can still make very good predictions.

The basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way. The results show good improvements majorly in FLOPS.

The way of defining state, rewards and value function are not very clear to me. Two value estimates are defined separately for the skip agent and the jump agent. Why not define a common value function for a shared state? Two values will double count the rewards from reading. Also, the state of the jump agent may not capture all available information. For example, how many words until the end of the sentence if you make a jump. Will this make the problem not a MDP? 

Overall, this is a good paper.

I read the authors' response. The paper should in its final version add the precise explanation of how the two states interact and how a joint state definition differs from the current one.","The reviewer provides a generally positive assessment, acknowledging the novelty and effectiveness of the approach. They highlight the interesting finding regarding punctuation-aided jumps. While they raise valid concerns about the clarity of state, reward, and value function definitions, they ultimately deem it a good paper and suggest revisions for improvement.",70.0,80.0
"Summary
The paper focuses on pruning neural networks. They propose to identify the nodes to be pruned even before training the whole network (conventionally, it is done as a separate step after the nn was trained and involves a number of iterations of retraining pruned nn). This initial step that identifies the connections to be pruned works off a mini-batch of data.

Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed. Reformulating the optimization problem and relaxing the constraints on the binary weights, they approximate the sensitivity of the loss with respect to these indicator variables via the gradient. Then the normalized magnitude of these gradients is used to chose the connections to keep (keeping top k connections)

Clarity:
Well written, easy to follow

Detailed comments
Overall, very interesting. Seemingly very simple idea that seem to work well. 
Table 2 does look impressive and it seems that it also reduces the overfiting, and the experiment with random labels on mnist seem to demonstrate that the method indeed preserves only connections relevant to the real labels, simplifying the architecture to a point when it cant just memorize the data

Several questions/critiques:
- When you relax the binary constraints, it becomes an approximation to an optimization problem, any indication of how far you are off solving it this way? 
- For the initialization method of the weights, you seem to state that VS-H is the one to use. I wonder if it actually task dependent and architecture dependent. If yes, then the propose method still has a hyperparameter - how to initialize the weights initially
- How does it compare with just randomly dropping the connections or dropping them based on the magnitude of the initial weights.  It seems that the meat comes from the fact that you are able to use the label and good initial values, i wonder if just doing a couple of iterations of forward-backprop and then dropping the weights based on their magnitude can give you comparable results 
- How does it compare to a distillation - it does not involve many cycles of retraining and can speed up inference time too
-Can it replace the architecture search - initialize a large architecture, use the method to prune the connections and here you go. Did you try that instead of using already pre-tuned architectures like AlexNet.

","The review starts with positive statements, highlighting the interesting and simple idea that seems to work well. The reviewer finds the results impressive. However, they also raise several questions and critiques, indicating a desire for further investigation and comparison. Overall, the tone is constructive and encouraging, suggesting potential but requiring more validation. Therefore, the sentiment is moderately positive.",60.0,80.0
"In this papers, the authors introduce a new technique to output uncertainty estimates from any family of neural nets. The key insight in this paper is that when considering existing SGD methods the following behavior occurs: if we think of ""easy"" and ""hard"" to classify datapoints, a NN trained with SGD will output good uncertainty estimates early on in training, but once the network focusses on tuning the parameters for the hard cases, the uncertainty estimates for the easy datapoints deteriorates. The algorithms proposed by the authors takes an existing uncertainty method (or confidence score function) and uses intermediate snapshots of SGD training to improve the final uncertainty estimates. Note that the focus in this work is on ranking uncertainties (and the authors suggest to leave calibrating uncertainties to existing methods).

The paper generally is well written (e.g. section 5) although I found section 3 to be a bit hard to follow. I'm not very familiar with the area itself but I was surprised to see in Section 7 that the results are not compared to full Bayesian methods (possibly on a dataset that lends itself well to that).

Notes:
- Section 3, ""A selective classifier ..."" -> I think this section could use some additional untuition to make the explanation more understandable.
- Section 3, ""defined to be the selective risk as a function of coverage."" -> do you mean as a sequence of functions g?
- ","The reviewer provides a generally positive overview of the paper, highlighting its key insights and praising the writing in certain sections (e.g., ""The paper generally is well written""). While they express some concerns about clarity in Section 3 and the lack of comparison to Bayesian methods, these are presented constructively as areas for improvement rather than harsh criticisms. The use of phrases like ""I found"" and ""I was surprised"" further softens the critique, making it clear that these are personal observations rather than objective flaws.",60.0,80.0
"This paper analyzes that the Integral Probability Metric (IPM) can be a good approximation of Wasserstein distance under some mild assumptions. They first showed two theorems based on simple cases (Gaussian Distribution and Exponential Families). Then, they proved that, for an invertible generator, a special designed neural network can approximate Wasserstein distance with IPM. The main contribution is that, for a stable generator (i.e., invertible generator), a discriminator can reversely “re-visit” inner status of the generator, then use this information to make a decision. 

In the appendix, several numerical examples are presented to support their theoretical bound. 

Q: Assumption 1, \sigma(t) is twice differentiable. However, Leaky ReLU is not twice differentiable at t=0. Do I misunderstand some part?

Q: The invertible generator assumption is not held in practice. Is that possible to extend the theorem to this case, even with a shallow network (e.g. 2 layers)?

Q: The numerical examples are all based on synthetic data. Did you have any results based on the real dataset?
","The review provides a summary of the paper's contributions and then poses specific questions seeking clarification and extension of the work. This suggests the reviewer likely finds the paper interesting and potentially valuable (positive sentiment). The questions are framed politely and professionally, without any demeaning or disrespectful language.",60.0,80.0
"This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models.

Pros:
- This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area

- The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments

Cons:
- This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification.  Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies.  Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).

- The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example ""risk rewrite"" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, ""risk rewrite"" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...).  Also intuition could be briefly given about the theorem proof strategies.

- The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere.

- Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural

Overall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.

Other minor points:
- The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal (""we argue that..."").  This is an important point...
- Theorem 1 proof seems fine, but some intuition in the main body would be nice.
- What does ""classification calibrated"" mean?
- Saying that three U sets are needed, where this includes the test set, seems a bit non-standard?  Also I'm confused- isn't a labeled test set used?  So what is this third U set for?
- The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct…?
- Stating both Lemma 5 and Thm 6 seems unnecessary
- In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing?  In particular, small PN?  Also, a table of the final test set accuracies would have been very helpful.
- More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging?  It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta
","The reviewer acknowledges the paper's contributions (novelty in a specific area, thoroughness, empirical validation) but also points out several limitations. These limitations, particularly the lack of novelty in the broader field and the need for improved clarity, suggest a somewhat subdued positive sentiment. The language used is professional and suggestive, typical of academic peer reviews, indicating a polite tone.",50.0,70.0
"This paper proposes an auxiliary variable MCMC scheme involving variational inference for efficient MCMC. Given a target distribution p(x), the authors introduce an auxiliary variable a, and learn conditional distributions p(a|x) and q(a|x) by minimizing the KL divergence between p(x)p(a|x) and q(a)q(x|a), with q(a) something simple (the authors use Gaussian). A MH proposal step involves simulating x givea the current MCMC sample x (from p(a|x), taking a step in A-space, and then returning back to the X space (using q(x|a)).  The authors show how to calculate the acceptance probability. 

I think the idea is nice and useful (I'm surprised people haven't thought of this before), though I think the paper presents this in a less clear way (as an extension of ideas from Agakov and Barber's ""Auxiliary variational method""). While this is correct and perhaps more general, in my mind it slightly obscures the main idea, as well as the strong ties with variational autoencoders: express a complex distribution as a (learnt) transformation of a simple distribution (this is the actual approach taken in the experiments). 

The motivation of the approach is that the nonlinear encoding network can transform the complex p(x) into a simpler q(a). 
For this reason, I think an important baseline is the independent MH sampler from equation 8 (I think this essentially uses a trained VAE generative model as a proposal distribution). The authors talk about how producing independent proposals can be sub-optimal, yet it seems to me that if the encoder and decoder neural networks are powerful enough, this should do a good job. I think excluding this baseline hurts the paper a bit.

The proof of correctness while correct is a bit unclear, can perhaps be simplified if you view the MCMC algorithm as operating on an augmented space (x,a,x') with stationary distribution p(x)q(a|x)q(x'|a) (writing writing q for \tilde(q)). This clearly has the right distribution over x. Each MCMC iteration starts with x and proceeds as follow:
  1) Given x, sample a and x' from q(a|x) and q(x'|a)
  2) Make a deterministic proposal on the augmented space to swap (x,x'). The acceptance probability is now equation 2.
  3) Discard a,x'.

In figure 4, the authors use HMC as an ""improved MCMC algorithm"", yet this is not an algorithm that deals with multimodality well. More useful would be to include some tempering algorithm like serial or parallel tempering.

While I like the idea, I unfortunately don't think the experiments are very convincing (and the authors barely discuss their results). Other than mixture of Gaussians, HMC (which involves no training) appears to be superior. With some tempering, I expect it to outperform the proposed method for the MoG case

Table 2 left: since HMC involves no training, does this mean that, taking training time into account, HMC is 5-6 orders of magnitude more efficient. L?ke I mentioned earlier, these results need more discussion. 

It would also help to provide absolute training and run times, so the reader can better understand whether the proposed method of ANICE is better.

Figure 3: why don't the authors also plot the histogram of values in the auxiliary space, p(a). It would be interesting to see how Gaussian this is (this is what variational inference is trying to achieve). Also, does Figure 3(a) mean that conditioned on x, p(a|x) is basically a delta function? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space? There is some work in this direction in the statistics literature, e.g. 
""Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm""

The authors some refers to the distribution of a|x as q(a|x) sometimes (in section 2.1) and sometimes as p(a|x) which is a bit confusing.

Figure 2: the labels are wrong.","The reviewer acknowledges the proposed idea as ""nice and useful"" and even expresses surprise that it hasn't been explored before. This suggests a positive sentiment. However, the review also includes several criticisms regarding clarity, missing baselines, unconvincing experiments, and the need for more discussion. These criticisms bring the sentiment down from very positive to moderately positive. The language used is professional and typical for academic peer reviews. While the reviewer points out flaws and areas for improvement, they do so constructively and without resorting to harsh or disrespectful language.",60.0,70.0
"The paper studies the problem of generating synthetic datasets (while ensuring differential privacy) via training a GAN. One natural approach is the teacher-student framework considered in the PATE framework.  In the original PATE framework, while the teachers are ensured to preserve differential privacy, the student model (typically a GAN) requires the presence of publicly data samples. The main contribution of this paper is to get around the requirement of public data via using uniformly random samples in [0,1]^d.

Differentially private synthetic data generation is clearly an important and a long-standing open problem. Recently, there has been some work on exploiting differentially private variants of GANs to generate synthetic data. However, the scale of these results is far from satisfactory. The current paper claims to bypass this issue by using the PATE-GAN approach.

I am not an expert on deep learning. The idea of bypassing the use of public data by taking uniformly random samples seems interesting. In my view, these random vectors are used in the GAN as some sort of a basis. It is interesting to see if this result extends to high-dimensional settings (i.e., where d  is very large).","The reviewer provides a relatively neutral sentiment. They find the topic and approach interesting, highlighting the importance of the problem and novelty of the proposed solution. However, they also state not being an expert in deep learning and don't provide a definitive judgment on the effectiveness of the approach. The language used is polite and professional throughout.",20.0,80.0
"This paper considers the compression of the model parameters in deep neural networks. The authors propose minimal random code learning (MIRACLE), which uses a random sample of weights and the variational framework interpreted by the bits-back argument. The authors introduce two theorems characterizing the properties of MIRACLE, and demonstrate its compression performance through the experiments.

The proposed approach is interesting and the performance on the benchmarks is good enough to demonstrate its effectiveness. However, since the two main theorems are based on the existing results by Harsha et al. (2010) and Chatterjee & Diaconis (2018), the main technical contribution of this paper is the sampling scheme in Algorithm 1.

Although the authors compare the performance trade-offs of MIRACLE with that of the baseline methods quoted from source materials, isn't it possible or desirable to include other competitors or other results for the baseline methods? Are there any other methods, in particular, achieving low error rate (with high compression size)? Little is discussed on why the baseline results are only a few. 

minor comment: 
- Eq.(4) lacks p(D) in front of dD.    

Pros:
- Interesting approach based-on the bits back argument
- Good performance trade off demonstrated through experiments
Cons:
- Only a few baseline results, in particular, at high compression size
","The review starts with positive remarks, highlighting the interesting approach and good performance. However, it then raises concerns about the technical contribution being potentially limited and the lack of comprehensive comparison with other methods. The tone is mostly neutral and professional, but the use of phrases like ""isn't it possible or desirable"" and pointing out a ""minor comment"" suggests a slightly less positive stance compared to the opening. Overall, the feedback is constructive and suggestive of improvements, indicating a review that leans towards the positive side.",50.0,70.0
"The paper design a low variance gradient for distributions associated with continuous or discrete random variables. The gradient is designed in the way to approximate the  property of reparameterization gradient.  The paper is comprehensive and includes mathematical details. 

I have following comments/questions

1. What is the \kappa in “variable-nabla” stands for? What is the gradient w.r.t. \kappa?

2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method. How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally? 

3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive. What is the computation complexity of GO? How to explain the fast speed shown in the experiments?

4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations. Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples? 

5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO. Does the hyper parameter setting favor the GO gradient in the reported experiments? Error bar may also be needed for comparison. What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?

6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?

","The review starts with positive statements, highlighting the paper's comprehensiveness and mathematical rigor. However, it then poses a series of in-depth questions that suggest potential areas of improvement or further clarification. While the questions are critical, they are framed constructively and aim to elicit more information or justification. The tone remains professional and objective throughout.",50.0,75.0
"Summary: This paper observes that a major flaw in common image-classification networks is their lack of robustness to common corruptions and perturbations. The authors develop and publish two variants of the ImageNet validation dataset, one for corruptions and one for perturbations. They then propose metrics for evaluating several common networks on their new datasets and find that robustness has not improved much from AlexNet to ResNet. They do, however, find several ways to improve performance including using larger networks, using ResNeXt, and using adversarial logit pairing.

Quality: The datasets and metrics are very thoroughly treated, and are the key contribution of the paper. Some questions: What happens if you combine ResNeXt with ALP or histogram equalization? Or any other combinations? Is ALP equally beneficial across all networks? Are there other useful adversarial defenses?

Clarity: The novel validation sets and reasoning for them are well-explained, as are the evaluation metrics. Some explanation of adversarial logit pairing would be welcome, and some intuition (or speculation) as to why it is so effective at improving robustness.

Originality: Although adversarial robustness is a relatively popular subject, I am not aware of any other work presenting datasets of corrupted/perturbed images.

Significance: The paper highlights a significant weakness in many image-classification networks, provides a benchmark, and identifies ways to improve robustness. It would be improved by more thorough testing, but that is less important than the dataset, metrics and basic benchmarking provided.

Question: Why do authors do not recommend training on the new datasets? ","The review is positive about the paper's contribution (datasets and metrics), highlighting its significance and originality. While it suggests areas for improvement, it frames them as additional explorations rather than criticisms. The tone is constructive and inquisitive, not demanding or negative.",75.0,80.0
"The authors present a deep reinforcement learning approach that uses a “self-attention”/“transformer”-style model to incorporate a strong relational inductive bias. Experiments are performed on a synthetic “BoxWorld” environment, which is specifically designed (in a compelling way) to emphasize the need for relational reasoning. The experiments on the BoxWorld environment clearly demonstrate the improvement gained by incorporating a relational inductive bias, including compelling results on generalization. Further experimental results are provided on the StarCraft minigames domain. While the results on StarCraft are more equivocal regarding the importance of the relational module—the authors do set a new state of the art and the results are suggestive of the potential utility of relational inductive biases in more general RL settings.

Overall, this is a well-written and compelling paper. The model is well-described, the BoxWorld results are compelling, and the performance on the StarCraft domain is also quite strong. The paper clearly demonstrates the utility of relational inductive biases in reinforcement learning.

In terms of areas for potential improvement:

1) With regards to framing, a naive reader would probably get the impression that this is the first-ever work to consider a relational inductive bias in deep RL, which is not the case, as the NerveNet paper (Wang et al., 2018) also considers using a graph neural network for deep RL. There are clear differences between this work and NerveNet—most prominently, NerveNet only uses a relational inductive bias for the policy network by assuming that a graph-structured representation is known a priori for the agent. Nonetheless, NerveNet does also incorporate a relational inductive bias for deep RL and shows how this can lead to better generalization. Thus, this paper would be improved by properly positioning itself w.r.t. NerveNet and highlighting how it is different. 

2) As with other work using non-local neural networks (or fully-connected GNNs), there is the potential issue of scalability due to the need to consider all input pairs. A discussion of this issue would be very useful, as it is not clear how this approach could scale to domains with very large input spaces. 

3) Some details on the StarCraft experiments could be made more rigorous and quantitative. In particular, the following instances could benefit from more experimental details and/or clarifications: 

Figure 6: The performance of the control model and relational model seem very close. Any quantitative insight on this performance gap would improve the paper. For instance, is the gap between these two models significantly larger than the average gap between runs over two different random seeds? It would greatly strengthen the paper to clarify that quantitive aspect. 

Page 8: ”We observed that—at least for medium sized networks—some interesting generalization capabilities emerge, with the best seeds of the relational agent achieving better generalization scores in the test scenario” — While there is additional info in the appendix, without quantitative framing this statement is hard to appreciate. I would suggest more quantitive detail and rigorous statistical tests, e.g.,  something like “When examining the best 10 out of ??? seeds, the relational model achieved an average performance increase of ???% compared to the control model (p=???, Wilcoxon signed-rank test). However, when examining all seeds ???? was the case.” 

Page 8: “while the former adopted a ""land sweep strategy"", controlling many units as a group to cover the space, the latter managed to independently control several units simultaneously, suggesting a finer grained understanding of the game dynamics.” This is a great insight, and the paper would be greatly strengthened by some quantitive evidence to back it up (if possible). For instance, you could compute the average percentage of agents that are doing the same action at any point in time or within some distance from each other, etc. Adding these kinds of quantitative statistics to back up these qualitative insights would both strengthen the argument, while also making it more explicit how you are coming to these qualitative judgements. 

Figure 8 caption: “Colored bars indicate mean score of the ten best seeds” — how bad is the drop to the n-10 non-best seeds? And how many seeds where used in total?

Page 13: “following Table 4 hyperparameter settings and 3 seeds” — if three seeds are used in these experiments, how are 10+?? seeds used for the generalization experiments? The main text implies that the same models for the “Collect Mineral Shards” were re-used, but it appears that many more models with different seeds were trained specifically for the generalization experiment. This should be clarified. Alternatively, it is possible that “seeds” refers to both random seeds and hyperparameter combinations, and it would improve the paper to clarify this. It is possible that I missed something here, but I think it highlights the need for further clarification. ","The review starts with positive statements, highlighting the strengths of the paper such as ""well-written and compelling"", ""compelling results"", and ""clearly demonstrates the utility"". This suggests a positive sentiment. However, the review also provides a list of ""areas for potential improvement"", indicating that the paper is not perfect. Therefore, the sentiment is likely positive but not overly enthusiastic. The language used throughout the review is constructive and respectful, employing a professional and helpful tone. There's no use of harsh language or personal attacks, suggesting a high level of politeness.",60.0,80.0
"The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. Both exploit a graph structure over the features by considering only subsets of neighborhoods of features (rather than all subsets). The authors give some approximation guarantees under certain Markovian assumptions on the graph. The paper concludes with experiments on text and images.

The paper is generally well written, albeit somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation). The problem is important, and exploiting graphical structure is only natural. The authors might benefit from relating to other fields where similar problems are solved (e.g., inference in graphical models). The approximation guarantees are nice, but the assumptions may be too strict. The experimental evaluation seems valid but could be easily strengthened (see comments).

Comments:

1. The coefficients in Eq. (6) could be better explained.

2. The theorems seem sound, but the Markovian assumption is rather strict, as it requires that a feature i has an S that ""separates"" over *all* x (in expectation). This goes against the original motivation that different examples are likely to have different explanations. When would this hold in practice?

3. While considering chains for text is valid, the authors should consider exploring other graph structures (e.g., parsing trees).

4. For Eqs. (8) and (9), I could not find the definition of Y. Is this also a random variable representing examples?

5. The authors postulate that sampling-based methods are susceptible to high variance. Showing this empirically would have strengthened their claim.

6. Can the authors empirically quantify Eqs. (8) and (9)? This might shed light as to how realistic the assumptions are.

7. In the experiments, it would have been nice to see how performance and runtime vary with increased neighborhood sizes. This would have quantified the importance of neighborhood size and robustness to hyper-parameters.

8. For the image experiments, since C-Shapley considers connected subsets, it is perhaps not surprising that Fig. 4 shows clusters for this method (and not others). Why did the authors not use superpixels as features? This would have also let them compare to LIME and L-Shapley.

","The review is overall positive. The reviewer finds the paper well-written, the problem important, and the approach natural. While the reviewer provides constructive criticism and suggestions for improvement, they are specific and actionable, indicating interest in the paper's success. The language used is formal and polite, typical of academic peer reviews.",60.0,80.0
"This paper proposed Whitening and Coloring (WC) transform to replace batch normalization (BN) in generators for GAN. WC generalize BN by normalizing features with decorrelating (whitening) matrix, and then denormalizing (coloring) features by learnable weights. The main advantage of WC is that it exploits the full correlation matrix of features, while BN only considers the diagonal. WC is differentiable and is only 1.32x slower than BN. The authors also apply conditional WC, which learn the parameters of coloring conditioned on labels, to conditional image generation.  Experimental results show WC achieves better inception score and FI distance comparing to BN on CIFAR-10, CIFAR-100, STL-10 and Tiny Imagenet. Furthermore, the conditional image generation results by WC are better than all previous methods.

I have some detailed comments below.

+ The paper is well written, and I generally enjoyed reading the paper.
+ The experimental results look sufficient, and I appreciate the ablation study sections. 
+ The score on supervised CIFAR-10 is better than previous methods. 

- The main text is longer than expectation. I would suggest shorten section 3.1 Cholesky decomposition, section 4 conditional color transformation and the text in section 5 experiments.
- The proposed WC transform is general. It is a bit unclear why it is particularly effective for generator in GAN. Exploiting the full correlation matrix sounds reasonable, but it may also introduce unstability. It would help if the authors have an intuitive way to show that whitening is better than normalization.
- It is unclear why conditional WC can be used for generation conditioned on class labels. In Dumoulin 2016, conditional instance normalization is used for generating images conditioned on styles. As image styles are described by Gram matrix (correlation) of features, changing first order and second order statistics of features is reasonable for image generation conditioned on styles. I cannot understand why conditional WC can be used for generation conditioned on class labels. I would like the authors to carefully explain the motivation, and also provide visual results like using the same random noise as input, but only changing the class conditions. 
- It is unclear to me why the proposed whitening based on Cholesky decomposition is better than ZCA-based in Huang 2018. Specifically, could the authors explain why WC is better than W_{aca}C in Table 3? 
- The authors claim progressive GAN used a larger generator to achieve a better performance than WC. The WC layer is generally larger than BN layer and has more learnable parameters. Could the authors compare  the number of parameter of generator in BN-ResNet, WC-ResNet, and progressive GAN?
- In Table 3, std-C is better than WC-diag, which indicates coloring is more important. In Table 6, cWC-diag is better than c-std-C, which indicates whitening is more important. Why? 
- What is the batch size used for training? For conditional WC, do the samples in each minibatch have same label?
- Having ImageNet results will be a big support for the paper.


===========  comments after reading rebuttal ===========

I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. 

I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns.

- I strongly encourage the authors to shorten the paper to the recommended 8-page. 

- The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC.

- The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting.

- Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. 

====== second round after rebuttal =============
I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. 

- GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. 

- I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. 
","The review starts with positive sentiment, highlighting the paper's merits such as good writing, sufficient experiments, and strong results. However, it then delves into a considerable list of concerns and questions, indicating that the reviewer, while appreciative of the work, sees significant areas for improvement. The continuous back and forth with the authors and the persistent concerns even after rebuttals suggest a sentiment leaning towards the cautious side of positive. The language is constructive and inquisitive, lacking any demeaning or disrespectful tone. The reviewer maintains a professional and polite demeanor throughout.",60.0,80.0
"After reading the authors' response, I'm revising my score upwards from 5 to 6.

The authors propose a defense against adversarial examples, that is inspired by ""non local means filtering"". The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be ""filtered away"" by using features from other images. While this assumption seems plausible,  no analysis has been done to verify it in a systematic way. Some examples of verifying this are:

1. How does varying the number of nearest neighbors change the network behavior?
2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?
3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? 
4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?

Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.","The reviewer increased the score from 5 to 6, indicating a more positive sentiment after the authors' response. However, they still pose several questions and requests for further analysis, suggesting the review is not entirely positive. The language used is constructive and inquisitive, not dismissive or aggressive, indicating politeness. Therefore, the sentiment is slightly positive, and the politeness is neutral to slightly positive.",20.0,60.0
"This paper proposes a dynamical neural network for sparse coding where all the interactions terms are learned.  In previous approaches (Rozell et al.) some weights were tied to the others.  Here the network consists of feedforward, lateral, and feedback weights, all of which have their own learning rule.  The authors show that the learned weights converge to the desired solution for solving the sparse coding objective.  This seems like a nice piece of work, an original approach that solves a problem that was never really fully resolved in previous work, and it brings things one step closer to both neurobiological plausibility and hardware implementation.

Other comments:

What exactly is being shown in Figure 2 is still not clear to me.

 It would be nice to see some other evaluations, for example sparsity vs. MSE tradeoff (this is reflected in the objective function in part but it would be nice to see the tradeoff).  

There is recent work from Mitya Chklovskii's group on ""similarity matching"" that also addresses the problem of developing a fully local learning rule.  The authors should incorporate a discussion of this in their final paper.
","The review starts with positive statements, highlighting the originality and effectiveness of the work. Words like ""nice"", ""original"", and phrases like ""brings things one step closer"" clearly indicate a positive sentiment. While the reviewer suggests additional analyses and comparisons, these are constructive and common in peer reviews. The language remains polite and professional throughout, without resorting to negativity or harsh criticism.",75.0,80.0
"Summary
=======
This paper introduces a method for learning neural networks with quantized weights and activations. The main idea is to stochastically – rather than deterministically – quantize values, and to replace the resulting categorical distribution over quantized values with a continuous relaxation (the ""concrete distribution"" or ""Gumbel-Softax distribution""; Maddison et al., 2016; Jang et al., 2016). Good empirical performance is demonstrated for LeNet-5 applied to MNIST, VGG applied to CIFAR-10, and MobileNet and ResNet-18 applied to ImageNet.

Review
======
Relevance:
Training non-differentiable neural networks is a challenging and important problem for several applications and a frequent topic at ICLR.

Novelty:
Conceptually, the proposed approach seems like a straight-forward application/extension of existing methods, but I'm unaware of any paper which uses the concrete distribution for the express purpose of improved efficiency as in this paper. There is a thorough discussion of related work, although I was missing Williams (1992), who used stochastic rounding before Gupta et al. (2015), and Soudry et al. (2014), who introduced a Bayesian approach to deal with discrete weights and activations.

Results:
The empirical work is thorough, achieving state-of-the-art results in several classification benchmarks. It would be interesting to see how well these methods perform in other tasks (e.g., compression or even regression), even though the literature on quantization seems to focus on classification.

Clarity:
The paper is well written and clear.","The review is positive overall. It highlights the relevance and clarity of the paper, acknowledges the novelty in the application of the concrete distribution, and praises the thorough empirical work and state-of-the-art results. While it points out some missing references and suggests further exploration in other tasks, these are presented as constructive suggestions rather than criticisms.",75.0,100.0
"Summary

This paper derives a new policy gradient method for when continuous actions are transformed by a
normalization step, a process called angular policy gradients (APG). A generalization based on
a certain class of transformations is presented. The method is an instance of a 
Rao-Blackwellization process and hence reduces variance.


Detailed comments

I enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications. I am not convinced that the measure theoretic perspective is always
necessary to convey the insights, although I appreciate the desire for technical correctness. Still,
appealing to measure theory does reduces readership, and I encourage the authors to keep this in
mind as they revise the text.

Generally speaking it seems like a lot of technicalities for a relatively simple result:
marginalizing a distribution onto a lower-dimensional surface.

The paper positions itself generally as dealing with arbitrary transformations T, but really is 
about angular transformations (e.g. Definition 3.1). The generalization is relatively 
straightforward and was not too surprising given the APG theory. The paper would gain in clarity
if its scope was narrowed. 

It's hard for me to judge of the experimental results of section 5.3, given that there are no other 
benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.

Def 4.4: ""a notion of Fisher information"" -- maybe ""variant"" is better than ""notion"", which implies there are different kinds of Fisher information 
Def 3.1 mu is overloaded: parameter or measure?
4.4, law of total variation -- define 


Overall

This was a fun, albeit incremental paper. The method is unlikely to set new SOTA, but I appreciated
the appeal to measure theory to formalize some of the concepts.


Questions

What does E_{pi|s} refer to in Eqn 4.1?
Can you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)
Experiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian
on the angle?


Suggestions

Paragraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.
I would include a short 'measure theory' appendix or equivalent reference for the lay reader.

I wonder if the paper's main aim is not actually to bring measure theory to the study of policy
gradients, which would be a laudable goal in and of itself. ICLR may not in this case be the right
venue (nor are the current results substantial enough to justify this) but I do encourage authors to
consider this avenue, e.g. in a journal paper.

= Revised after rebuttal =

I thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to
1) Provide stronger empirical results (these are not too convincing).
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.
","The reviewer explicitly states enjoying the paper's concept and finding value in the work done. While they have constructive criticism and suggestions for improvement, their overall tone remains positive and encouraging. They see the paper's potential, even suggesting a more suitable venue for a more developed version. Therefore, the sentiment leans positive. The language used is polite and professional throughout, with constructive criticism offered respectfully.",60.0,80.0
"This paper presents a new code-to-sequence model called code2seq that leverages the syntactic structure of programming languages to encode source code snippets, which is then decoded to natural language using a sequence decoder. The key idea of the approach is to represent a program using a set of randomly sample k paths in its abstract syntax tree. For each path, the path is encoded using a recurrent network and concatenated with the embeddings of the two leaf terminal values of the path. The path encodings are then averaged to obtain the program embedding, which is then used to initialize a sequence decoder that also attends over the path embeddings. The code2vec model is evaluated over two tasks: 1) Code summarization: predicting a method’s name from its body, and 2) Code captioning: generating a natural language sentence from method’s body depicting its functionality. The code2seq model significantly outperforms the other baseline methods, and the ablation study shows the importance of various design choices.

This paper presents an elegant way to represent programs using a set of paths in the AST, which are then weighted using an attention mechanism to attend over relevant path components. The code2seq model is extensively evaluated over two domains of code summarization and code captioning, and results show significant improvements.

The novelty of the code2seq model is somewhat limited compared to the model presented in code2vec (Alon et al. 2018a) paper. In code2vec, a program is encoded as a set of paths, where each path comes from a fixed vocabulary. The code2seq model instead uses an LSTM to encode individual paths, which allows it to generalize to new paths. This is a more natural choice for embedding paths, but it doesn’t appear to be a big conceptual advance in the model architecture. The use of subtoken embeddings for encoding/decoding identifier names is different in code2seq, but it has been proposed earlier in other code embedding models.

For the code summarization evaluation, would it be possible to evaluate the code2seq model on the dataset used by the code2vec paper? On that dataset, the code2vec approach gets a precision score of 63.1, recall of 54.4, and F1 score of 58.4, [Table 3 on page 18] which are comparable to overall scores of the code2seq model.

One of the key findings of the paper is that syntactic structure of programs is important to encode. Similar observations have been made in other program embedding papers that use for example Tree-RNN [1] or graph neural networks (GNN) [Allamanis et al. 2018]. It would be quite valuable to compare the current results with the Tree-RNN or GNN models (without performing additional dataflow and control-flow post processing) to see how well the paths-based embeddings work in comparison to these models.

The value of k=200 seems a bit large for the examples presented in the paper. What happens when smaller values of k are used (e.g. k=10, 20?) What are the average number of paths in the java programs in the dataset?

1. Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas. Learning Program Embeddings to Propagate Feedback on Student Code
ICML 2015
","The review starts with a positive summary of the paper's contributions, highlighting its elegance and significance. However, it then raises concerns about the novelty of the approach, comparing it to the code2vec model and suggesting that the advancements might not be substantial. The reviewer also provides constructive criticism by suggesting additional evaluations and comparisons with other models. While the reviewer challenges some aspects of the paper, the tone remains respectful and focused on improving the work. Overall, the sentiment leans slightly towards the positive side due to the initial praise and constructive feedback.",60.0,70.0
"The paper introduces an iterative method to generate deformed images for adversarial attack. The core idea is to perturb the correctly classified image by iteratively applying small deformations, which are estimated based on a first-order approximation step, until the image is misclassified. Experimental results on several benchmark datasets (MNIST, ImageNet) and commonly used deep nets (CNN, ResNet, Inception) are reported to show the power of adversarial deformations. 

The idea of gradually adding deformations based on gradient information is somewhat interesting, and novel as far as the reviewer knows about. The method is clearly presented and the results are mostly easy to access.  However, the intuition behind the proposal does not make strong sense to the reviewer: since the main focus of this work is on model attack, why not directly (iteratively or not) adding random image deformations to fool the system? Particularly, the first-order approximation strategy (as shown in Eq.4 and Eq.5) is quite confusing. On one side (see Eq.4), the deformation \tau should be small enough in scale to make an accurate approximation. On the other side (see Eq. 5), \tau is required to be sufficiently large in order to generate misclassification. Such seemingly conflicting rules for estimating the deformation makes the proposed method less rigorous in math. 
As another downside, the related adversarial training procedure is not fully addressed. The authors briefly discussed this point in the experiment section and provided a few numerical results in Table 2. These results, as acknowledged by the authors, do not well support the effectiveness of deformation adversarial attack and defense. In the meanwhile, the mentioned adversarial training framework follows straightforwardly from PGD (Madry et al. 2018), and thus the novelty of this contribution is also weak. More importantly, it is not clear at all, both in theory and algorithm, whether the advocated gradual deformation attack and defense can be unified inside a joint min-max/max-min learning formulation, as what PGD is rooted from.

Pros: 

- The way of constructing deformation adversarial is interesting and novel
- The paper is mostly clearly organized and presented.

Cons:

- The motivation of approach is questionable. 
- The related adversarial training problem remains largely unaddressed.
- Numerical study shows some promise in adversarial attack, but is not supportive to the related defense capability. ","The review acknowledges the novelty of the idea and clarity of the paper but raises significant concerns about the method's motivation and mathematical rigor. The reviewer also finds the adversarial training aspect underdeveloped and not well-supported by experiments. The tone, while critical, remains professional and avoids personal attacks. Therefore, the sentiment leans towards negative due to the unresolved issues, and the politeness stays within a respectful academic discourse.",-30.0,60.0
"
The authors provide a clean and easily understood sufficient
condition for spurious local minima to exist in networks with
a hidden layer using ReLUs or leaky ReLUs.  This condition,
that there is not linear transformation with zero loss,
is satisfied for almost all inputs with more examples than
input variables.

The construction is elegant.  The mathematical writing in the paper,
especially describing the proof of Theorem 1, is very nice -- they
expose the main ideas effectively.

I do not know of another paper using a similar proof, but I have not
studied the proofs of the most closely related papers prior to doing
this review, so I have limited ability to vouch for this paper's
technical novelty.

The authors also show that networks using many other popular
activation functions have spurious local minima for a very
simple dataset.  All of these analysis are unified using a
simple, if technical, set of conditions on activation function.

Finally, the authors prove a somewhat technical theorem about
optima in deep linear networks, which generalizes some
earlier treatments of this topic, providing an checkable
condition for global minimality.

There is extensive discussion of related work.  I am not aware of
related work not covered by the authors.

In some cases, when the authors discuss previous work, they write as
if restriction to the realizable case is an assumption, when it seems
to me to be more of a constraint.  In other words, it seems harder to
prove the existence of spurious minima in the realizable case.
They seem to acknowledge this after their statement of their Theorem 2,
which also uses a realizable dataset.

Also, a few papers, including the Venturi, et al paper cited by
the authors, have analyzed whether spurious local minima exist
in subsets of the parameter space, including those likely to
be reached during training with different sorts of initializations.
In light of this work, the authors might want to tone down claims
about how their work shows that results about linear networks do
not generalize to the non-linear case.  In particular, to make
their construction work in the case of wide networks, they
need an overwhelming majority of the hidden units to be ""dead"",
which seems as it is unlikely to arise from training with
commonly used initializations.

Overall, I think that this paper makes an interesting and
non-obvious contribution on a hot topic.","The review is largely positive, highlighting the paper's clarity, elegance, and contribution to a relevant topic. While it acknowledges potential limitations and suggests areas for improvement, it doesn't diminish the overall positive view of the work. The language used is constructive and professional throughout.",75.0,80.0
"The method proposes to use physiological signals to improve performance of reinforcement learning algorithms. By measuring heart pulse amplitude the authors build an intrinsic reward function that is less sparse that the extrinsic one. It helps to be risk averse and allows getting better performances than the vanilla RL algorithm on a car-driving task. 

I found the paper well written and the idea is quite nice. I like the idea that risk aversion is processed as a data-driven problem and not as an optimisation problem or using heuristics. I think this general idea could be pushed further in other cases (like encourage fun, surprise, happiness etc. ). 

There are some issues with this paper yet. First, modifying the reward function also modifies the optimal policy. In the specific case of car driving, it may not be bad to modify the policy so that it makes passenger less stressed but in general, it is not good. This is why most of works based on intrinsic motivation also schedule the lambda parameter to decrease with time. This is not something explored in this paper. Also, this work is well suited to the car-driving scenario because stress is closely related to risk and accident. But it may not work with other applications. I would thus suggest that the title of the paper reflects the specific case of risk aversion. ","The reviewer explicitly states positive aspects like ""well written"", ""idea is quite nice"", and ""I like the idea"". They also see further potential in the presented approach. While they point out issues and suggestions for improvement, these are constructive and typical for the peer-review process. The language remains objective and professional throughout.",60.0,80.0
"PROS:
- The text is very well written, with a good balance between mathematical details and intuitions.
- I really like the high-level description of the algorithms and proof techniques

CONS:
to be completely honest, I am not sure I have learnt anything new from the paper. 
1) the proof techniques are very standard
2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:
a. large deviation principles
b. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.)

and
c. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.

I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al. 

REMARKS:
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.

","The reviewer starts with positive remarks, highlighting the writing quality and clarity of the paper. However, the overall tone becomes quite negative as the reviewer expresses doubts about the paper's novelty and originality. They point out similarities to existing work by Dupuis and others, suggesting the contributions might be incremental. While the reviewer encourages the authors to clarify the novel aspects, the extensive critique of the paper's originality leans the sentiment towards the negative side. The language used, while direct, maintains a professional and academic tone. There are no personal attacks or disrespectful phrasing.",-30.0,60.0
"In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. 

They show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. 


The paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ","The review is positive about the paper. It highlights the positive aspects of the paper, such as the derivation of exact formulas, the use of FFT transformations for faster computation, and the clear writing and easy-to-follow proofs. The reviewer also doesn't point out any major flaws in the paper. The slightly critical point about the potential expensiveness of the alternating projection scheme is presented as a suggestion for improvement rather than a flaw. The language used throughout the review is neutral and professional.",75.0,100.0
"Summary: 
The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper. It also provides an analysis on the mode collapse and lack of stability of classical GANs. The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties. 

Positive points:
The paper is interesting to read and well illustrated. 
An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.

Points to improve: 

If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting. Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem. WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few. 
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction. 
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN. 
The imagenet experiment lacks details.   ","The review acknowledges the paper's merits (well-written, interesting, good illustration of progress) which points towards a positive sentiment. However, it also raises several points for improvement, suggesting the paper is not perfect. The critique is constructive and specific, without resorting to harsh language, indicating politeness. Therefore, the sentiment leans slightly positive, and the politeness is notably positive.",60.0,80.0
"Summary: This paper mixes automated theorem proving with machine learning models. The final goal, of course, is to be able to train a model that works in conjunction with an automated theorem proving system to efficiently prove theorems, and, ideally, in a way that resembles the way humans prove theorems. This is a distant goal, and the authors instead focus on several tractable tasks that are required for future progress in this direction. They start by integrating the Coq theorem proving environment with ML frameworks, allowing for the creation of models that perform various tasks related to theorem proving. In particular, they focus on two tasks. One is to estimate how many steps are left to complete the proof given a current proof state. The other is to determine what is a good choice of next step. Finally, they also consider issues surrounding representations of the various data structures involved in proofs (i.e., the proof tree, variables, etc.). They test various models on a synthetic nearly trivial logical expression proof, along with a more complicated (and meaningful real world) group theory result.

Strengths: This is a very important area. Automated theorem proving has a potentially very significant impact, and being able to take advantage of some of the recent successes in ML would be excellent. The main environment proposed here, integrating PyTorch with Coq could potentially be a very useful platform for future research in this area. The paper exposes many interesting questions, and I generally think we need more exploratory papers that open up an area (as opposed to seeking to finalize existing areas) 

Weaknesses: The paper is pretty tough to understand without a lot of background in all of the existing theorem proving work (which might be fine for a conference in this area, but for this venue it would be nice to be more self-contained). The organization could also use some work, since it's often tough to figure out what the authors actually did. The experimental results seem very preliminary---although it's hard to say, as there is no easy way to compare the results to anything else out there. In general a lot of details seem missing.

Verdict: The authors admit this is a preliminary work, and I agree with that. The paper certainly introduces many more questions than it answers. However, I think that in this case it's a good thing, and this type of paper has the potential to inspire a lot of new and exciting research, so I voted for acceptance.

Comments and questions:

- As mentioned, a lot of the terminology is introduced very quickly and could stand to be more self-contained, i.e., ""tactics"" could be defined as being simple transformations that are applied to a current proof state to obtain another proof state, and each language has a library of tactics available.

- Probably the major contribution of the work is the integration of the CoQ and Pytorch, so a bit more content describing how the Python data structures that wrap around Coq structures would be interesting here.

- I didn't really understand one of the major contributions: the embedding function for the M_i conditioned on the environment. How does the sampled Gaussian vector work here? In general this section is pretty confusing, it would be great to include a schematic to show how the different levels of embeddings for different structures work here.

- How does the real-world dataset work? Does the dataset contain one automated proof of the entire theorem, or several different proofs (ultimately produced by different user choices)? Are you measuring accuracy on the proofs of individual lemmas?","The reviewer explicitly states ""this type of paper has the potential to inspire a lot of new and exciting research, so I voted for acceptance."" This indicates a positive sentiment. The reviewer acknowledges limitations but frames them constructively as opportunities for further exploration. While the reviewer lists weaknesses, they are presented in a neutral, matter-of-fact manner, suggesting suggestions for improvement rather than harsh criticism.",60.0,70.0
"This paper investigates batch normalization from three points of view. i) Loss decomposition, ii) learning rate selection, iii) generalization. If carefully read, I believe authors have interesting results and insightful messages. However, as a whole, I found the paper difficult to follow. Too much content is packed into too little space and they are not necessarily coherent with each other. Many of the technical terms are not motivated and even not defined. Overall, cleaning up the exposition would help a lot for readability. 

I have a few other technical comments.
1) Theorem 1 is not acceptable for publication. It is not a rigorous statement. This should be fixed.
2) Effective and maximum learning rate is not clear from the main body of the paper. I can intuitively guess what they are but they lack motivation and definition (as far as I see).
3) In Section 3 I believe random data is being assumed (there is expectation over x in some notation). This should be stated upfront. Authors should broadly comment on the applicability of the learning rates calculated as N->\infty in the finite N,P regime?","The reviewer acknowledges the potential of the paper (""interesting results"", ""insightful messages"") but expresses concerns about the clarity and presentation. They find the paper difficult to follow and point out technical issues like a lack of definitions and a non-rigorous theorem. The language used, while direct, is professional and within the bounds of academic critique.",20.0,50.0
"Cons

1.	It’s unclear why LABC produces lower scores than ‘normal’ training on ‘normal’ testing.
2.	The text says nothing I can find to explain why in Fig 5 the ‘entity’ vectors have all 0s except in one dimension, which seems to make the problem considerably easier.
3.	In a sense, there is no cross-domain adaptation required in the symbolic task: min is min, whether it operates on dimension k of the source vectors or dimension j of the target vectors. On the other hand, dimensions are processed independently in the model, as far as I can tell, so there’s no free transfer of learning min on dimension k to knowing min on dimension j. It would be good to comment on this issue.
4.	There seem to be obvious analogies (so to speak) to GANs, and it is very curious that this is not mentioned anywhere that I can see. This is particularly glaring in Sec. 5.3.
5.	The quantitative results are scattered throughout the prose; it would be challenging, but worthwhile, to gather them into an actual table.

Pros

6.	The basic idea (“We should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data”, p. 14) is very intuitive, common-sensical, bordering on obvious. But it is not at all obvious that the idea has as much power as is demonstrated in the experiments. The transfer to novel domain combinations, novel domains, and novel values of dimensions is impressive and surprising.
7.	The result that the proposed training, designed to promote generalization on analogy tasks, also seems to promote improved sensory processing is interesting. Whether it really instantiates the parallel connection argued for by the High-Level Perception view from psychology/philosophy is debatable, but that is itself an interesting connection that the authors should be praised for identifying.
8.	In general, the connection to the cognitive literature is creative and tantalizing and provides good scientific grounding for the work.
9.	The linking to the flexibility of word meanings in the final paragraph pushes the limit of the plausibility of connection to broader cognitive issues, but I’m inclined to indulge the authors for at least bringing up this important and relevant issue.  
","The review presents a balanced perspective, outlining both positive (pros) and negative (cons) aspects of the work. However, the positives outweigh the negatives. The reviewer acknowledges the significance of the findings (pros 6, 7), praises the connection to cognitive literature (pro 8), and finds the overall idea impressive (pro 6). While the reviewer raises valid concerns (cons 1-5), the language remains constructive and suggestive, rather than dismissive or aggressive. The reviewer aims to improve the paper through specific recommendations, indicating a positive attitude towards the work's potential.",60.0,80.0
"This paper is one of a sequence of works trying to learn heuristics for solving combinatorial optimisation problems. Compared to its predecessors, its contributions are three-fold. First, it introduces a tweak on the REINFORCE learning algorithm, outperforming more complicated methods. Second, it introduces a new model for combinatorial tasks which delivers interesting results on several tasks which are varied though related. Finally, it evaluates this model on many tasks.

****Quality and clarity****
This is a very high-quality paper. 
The writing is clear and sharp, and the reading experience is quite enjoyable (the witty first paragraph sets the tone for what is to follow), even if the text is at times a bit verbose. 
Another point to commend is the honesty of the paper (see e.g. the comment on the performance of the model on TSP vs specialised solvers such as Concord).
The related work section is complete and well documented.
Finally, the experimental results are clearly presented and well-illustrated.

****Originality and significance****
On the theoretical side, the contributions of this paper are interesting but not ground-breaking. The REINFORCE tweak is close to other algorithms that have been tried in the last few years (such as indeed the one presented in Rennie et al, 2016). The model architecture, while successful, is not a large departure from the Transformer presented in Vaswani et al, 2017.

More significant is the complete set of experiments on a varied subset of combinatorial tasks, which showcases one of the promises of using machine learning for combinatorial optimisation: reusability of a single model for many tasks.

****Conclusion****
Overall, this is a nice, very well-written paper. Its contributions, though not ground-breaking, are significant to the field, and constitute another step in the right direction.

Pros
- high-quality writing
- very clear
- complete experiments on a variety of tasks, some of which do not have optimal solvers
- honest assessment of the model

Cons
- the theoretical contributions are not ground-breaking (either the the tweak on REINFORCE or the model architecture)
- the model is still far from obtaining meaningful results on TSP (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...)

Details
- Dai et al has been published at NIPS and is no longer an arxiv preprint
- the comparison to AlphaGo should either be expanded upon or scratched. Although it could be quite interesting, as it is it's not very well motivated.","The reviewer explicitly states that this is a ""very high-quality paper"" and ""a nice, very well-written paper"" whose contributions are ""significant to the field"". They commend the authors on the clarity, honesty, and experimental setup. While they note the theoretical contributions aren't groundbreaking, their overall tone is very positive. The language used throughout is constructive, professional, and polite.",85.0,90.0
"The paper is interesting and I like it. I draws parallels from biological learning and the well known critical learning phases in biological systems to artificial neural network learning. 
A series of empirical simulation experiments that all aim to disturb the learning process of the DNN and to artificially create criticality are presented. They are providing food for thought, in order to introduce some quantitative results, the authors use well known Fisher Information to measure the changes. So far so good and interesting.
I was disappointed to see Tishby's result (2017) only remotely discussed, an earlier work than the one by Tishby is by Montavon et al 2011 in JMLR. Also in this work properties of successive compression and dimensionality reduction are discussed, perhaps the starting point of quantitative analysis of various DNNs. 

To this point the paper presents no theoretical contribution, rather empirical findings only, that may or may not be ubiquitous in DNN learning systems. The latter point may be worthwhile to discuss and analyse. 
Overall, the paper is interesting with its nice empirical studies but stays somewhat superficial. To learn more a simpler toy model may be worthwhile to study. 

","The reviewer explicitly states positive aspects like ""interesting"" and ""food for thought."" However, they also express disappointment about the limited discussion of related work and call the contribution ""somewhat superficial."" Overall, the sentiment leans towards the positive due to the interest shown, but the criticism pulls it away from a very positive score. The language used is professional and polite throughout, without resorting to harsh or derogatory terms. The critique is presented constructively.",60.0,80.0
"Summary:
This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.

It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.

- Quality: Several interesting differences to prior work. Well-done experiments
- Clarity: Clean derivation, easy to understand. Some details could be spelled out better
- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to ""neural processes"" work, but this happened roughly at the same time
- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive

Interesting about this work:
- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether
   an inference network of this simple structure (no correlations, sum combination
   of datapoints, same network for each class) can deliver a good approximation to
   the true posterior.
- Different to previous work, task-specific inference is done only on the weights of
   single-layer head models (logistic regression models, with shared features).
   Highly encouraging that this is sufficient for state-of-the-art few-shot classification
   performance. The authors could be more clear about this point.
- Simple and efficient amortized inference model, which along with the neural
   network features, is learned on all data jointly
- Optimization criterion is based on predictive distributions on train/test splits, not
   on the log marginal likelihood. Has some odd consequences (question below),
   but clearly works better for few-shot classification

Experiments:
- 5.1: Convincing results, in particular given the simplicity of the model setup and
   the inference network. But some important points are not explained:
   - Which of the competitors (if any) use the same restricted model setup (inference
      only on the top-layer weights)? Clearly, MAML does not, right? Please state this
      explicitly.
   - For Versa, you use k_c training and 15 test points per task update during
      training. Do competitors without train/test split also get k_c + 15 points, or
      only k_c points? The former would be fair, the latter not so much.
- 5.2: This seems a challenging problem, and both your numbers and reconstructions
   look better than the competitor. I cannot say more, based on the very brief
   explanations provided here.
   The main paper does not really state what the model or the likelihood is. From
   F.4 in the Appendix, this model does not have the form of your classification
   models, but psi is input at the bottom of the network. Also, the final layer has
   sigmoid activation. What likelihood do you use?
   One observation: If you used the same ""inference on final layer weights"" setup
   here, and Gaussian likelihood, you could compute the posterior over psi in closed
   form, no amortization needed. Would this setup apply to your problem?

Further questions:
- Confused about the input to the inference network. Real Bayesian inference would
   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in
   then?
   Please do improve the description of the inference network, this is a major
   novelty of this paper, and even the appendix is only understandable by reading
   other work as well. Be clear how it depends on theta (I think nothing is lost by
   feeding in the h_theta(x)).
- The learning criterion based on predictive distributions on train/test splits seem
   to work better than ELBO-like criteria, for few-shot classification.
   But there are some worrying aspects. The marginal likelihood has an Occam's
   razor argument to prevent overfitting. Why would your criterion prevent overfitting?
   And it is quite worrying that the prior p(psi | theta) drops out of the method
   entirely. Can you comment more on that?

Small:
- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more
   general notation early on, if you do not do it later on. This is confusing
","The review is positive overall. The reviewer highlights the strengths of the work, such as its novelty, clarity, and strong experimental results. While the reviewer poses several questions and suggestions for improvement, these are constructive and aim to further enhance the paper. The language used is formal and polite, typical of academic peer reviews.",75.0,80.0
"This paper introduces a novel architecture for sequence modeling, called the trellis network. The trellis network is in a sense a combination of RNNs and CNNs. The authors give a constructive proof that the trellis network is a special case of a truncated RNN. It also resembles CNNs since the neurons at higher levels have bigger receptive fields. As a result, techniques from RNN and CNN literature can be conveniently brought in and adapted to trellis network. The proposed method is evaluated on benchmark tasks and shows performance gain over existing methods.

The paper is well-written and easy to follow. The experimental study is extensive. The reviewer believes that this paper will potentially inspire future research along this direction. However, the novelty of the proposed method compared to the TCN seems limited: only weight sharing and input injection. It would be great to include the performance of the TCN on the PTB dataset, on both word and character levels in Table 1 and 2.

According to Theorem 1, to model an M-truncated L-layer RNN, a trellis network needs M + L − 1 layers. When M is large, it seems that a trellis network needs to be deep. Although this does not increase to model size due to weight sharing, does it significantly increase computation time, both during training and inference?

The review might have missed it, but what is the rationale behind the dotted link in Figure 1a, or the dependence of the activation function $f$ on $z_t^{(i)}$? It seems that it is neither motivated by RNNs nor CNNs. From RNN's point of view, as shown in the proof of Theorem 1, $f$ only depends on its first argument. From CNN's point of view, the model still gets the same reception field without using $z_t^{(i)}$.

Minor comments:
The authors might want to give the full name of TCN (temporal convolutional networks) and a short introduction in Section 2 or at the beginning of Section 4.","The reviewer finds the paper well-written, easy to follow, and with an extensive experimental study. They believe it will inspire future research. However, they also point out a limitation in novelty compared to TCN and ask for additional results and clarifications. The tone is constructive and professional throughout.",60.0,80.0
"The paper proposes a code completion task that given the rest of a program, predicts the content of an expression. This task has similarity to code completion tasks in the code editor of an IDE. The paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.

The work builds on prior research by Allamanis et al. 2018b that performs such completions of single variables by picking from the variables in the scopes. The difference here is that portions of parse trees are predicted as opposed to a single variables, where the algorithm from the prior research is used to predict single variables.

Writing-wise the paper is hard to read on the technical part with many unclear details and this portion needs a good amount of extra explanations. The Epsilon set includes triples which are not described and need understanding equation (2). The first element of this triple is an edge label <edge>($a$, $v$) where $a$ is an AST and $v$ is a node. Thus, edges of the graph end up between entire ASTs and nodes? While I can see how could this make sense, there is certainly lack of explanation going on here. Overall, this part is hard to parse and time-consuming to understand except at high level. Furthermore, the text has many functions without signatures and they seem to be used before they are defined (e.g. getRepresentation).

Technically, the approach also seems very similar to N3NN by Parisotto et al, ICLR 2017. There should be more elaboration on what is new here. Otherwise, the novelty of the paper really is just combining this work with Allamanis et al. 2018b.

In terms of evaluation, the task seems to be on a different set of expressions than the one explained in the exposition. How many expressions where there in the evaluation programs and how many were chosen to evaluate on and based on what criteria. It seems from the exposition that expressions with field accessed and function calls are not possible to be generated, but then some completions show method calls. How much of the full task is actually solved? In particular, several of the cited prior works solve specific problems like constants that are ignored here.

The evaluation is mostly an ablation studies of the proposed approach by removing edges from the final idea. 
Besides this, the paper also introduces a new dataset for showcasing the technique and does not report sizes and running times, essentially not answering basic questions like what is the trade-off between the different techniques. Comparison to actual prior works on similar tasks is also lacking (some TODO is left in the paper), but there is the claim that existing neural techniques such as seq2seq perform ""substantially worse"". I guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.
","The reviewer acknowledges the paper's interesting proposal but expresses concerns about clarity, novelty, and evaluation. They find the technical explanation difficult to follow and point out similarities to existing work. The reviewer also questions the evaluation methodology and lack of comparison with prior work, suggesting potential issues with completeness and rigor. The tone, while direct, maintains a professional and constructive approach.",-20.0,60.0
"* Summary
This paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering.


* Strengths
- I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering).
- Encouraging results on specialized hardware implementation.


* Weaknesses
- Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution.
- All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough.
- Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims ""binaryconnect method fails"" and ""our method [...] outperforms all the existing quantization methods"" seem unfounded (Section 5.5).
- In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified.
- In the introduction on page 2 is argued that the proposed method ""eliminates the need for multiplications"" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? 
- The notation is quite confusing. For starters, in Section 2 you mention ""a fixed scaling factor A"" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace ""="" with ""\approx"". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation.


* Minor Comments
- Abstract: What is ASIC? It is not referenced in Section 6.
- Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. 
- Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time.
- Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point.
- The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results.
- Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is?
- Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss?","The review acknowledges the strengths of the paper, such as the variety of tasks and encouraging hardware results. However, it raises several significant concerns, including the incremental nature of the contribution, underperformance compared to baselines, and unconvincing results on question answering. The reviewer also points out confusing notation and unclear explanations. While the reviewer provides constructive criticism and suggestions for improvement, the overall tone suggests that the paper needs substantial revisions before it can be considered for publication. The language used is formal and professional, without resorting to personal attacks or harsh language.",-20.0,80.0
"Summary

This paper decomposes the image restoration task in two part: the restoration part handled by a restoration RNN, and the number of steps to apply the RNN is determined using a policy unit. 
State of the art results are achieved on blind grey level Gaussian noise denoising on the BSD68 dataset.

The approach is novel to my knowledge, the paper is well written, the results are good and well illustrated.

Questions:
-It would be nice to present results on color images, and on datasets that contains natural noises.  
-Lowering the learning rate on plateaus during training is done by hand or is there an automatic criterion to define the plateaus?

Minor:
page 1: extra "")"" after ref to Bredies et al 2010
could cite Chen, Zu, Koltun ICCV17 in deep models for restoration
Several ""L"" have been replaced by ""_' e.g. under review at IC_R, R_-based, etc in the whole paper
p.4: rain-> train
greatly influence -> greatly influences
p5: typo performace
make a uniform bib: whole first name or abbr. , no URL, etc.
p6: the weight -> the set of weights 
add the specification that the noise is Gaussian
the sentence ""the training set and testing set of ..."" is used twice, remove one.
p7 Table 1: the perf of DnCNN-B is 29.16 and not 29.15 for sigma 25, right?","The review starts with positive statements, highlighting novelty, good writing, and strong results. While it does raise questions and suggest minor revisions, these are presented constructively and aim to improve the paper. The tone throughout is professional and courteous.",75.0,85.0
"This paper trains a neural network to solve the satisfiability problems. Based on the message passing neural network, it presents NeuroSAT and trains it as a classifier to predict satisfiability under a single bit of supervision. After training, NeuroSAT can solve problems that are larger and more difficult than it ever saw during training. Furthermore, the authors present a way to decode the solutions from the network's activations. Besides, for unsatisfiable problems, the paper also presents NeuroUNSAT, which learns to detect the contradictions in the form of UNSAT cores.

Relevance: this paper is likely to be of interest to a large proportion of the community for several reasons. Firstly, satisfiability problems arise from a variety of domains. This paper starts with a new angle to solve the SAT problem. Secondly, it uses neural networks in the SAT problem and establishes that neural networks can learn to perform a discrete search. Thirdly, the system used in this paper may also help improve existing SAT solvers.

Significance: I think the results are significant. For the decoding satisfying assignments section, the two-dimensional PCA embeddings are very clear. And the NeuroSAT's success rate for more significant problems and different problems has shown it's generalization ability. Finally, the sequences of literal votes in NeuroUNSAT have proved its ability to detect unsatisfied cores.

Novelty: NeuroSAT’s approach is novel. Based on message passing neural networks, it trains a neural network to learn to solve the SAT problem. 

Soundness: This paper is technically sound. 

Evaluation: The experimental section is comprehensive. There are a variety of graphs showing the performance and ability of your architecture. However, the theoretical analysis isn't very sufficient. For instance, why does the change of the dataset from the original SR(n) to SRC(n,u) lead to the change of the behavior of the network from searching for a satisfying assignment indefinitely to detecting the unsatisfiable cores?

Clarity: As a whole, the paper is clear. The definition of the problem, the model structure, the data generation, the training procedure, and the evaluation are all well organized. However, there is still a few points requiring more explanation. For instance, in figure 3, I am not sure whether darker value means larger value or smaller value because the authors only mentioned that white represents zero, blue negative and red positive. Also, in figure 7, I am not sure whether those black grids represent higher positive values or lower negative values.

A few questions:

What's the initialization of the two vectors the authors use for tiling operation? Does the initialization differ for different types of SAT problems?

How do the authors decide the number of iterations necessary for solving a particular SAT problem?

","The review is overwhelmingly positive. The reviewer highlights the relevance and significance of the work, praises its novelty and soundness, and finds the experimental section comprehensive. While they point out areas for improvement in the theoretical analysis and clarity of figures, these are presented constructively as suggestions rather than harsh criticisms. The use of phrases like ""likely to be of interest"", ""very clear"", ""has shown its generalization ability"", and ""well organized"" all indicate a positive sentiment. The reviewer maintains a polite and respectful tone throughout, using professional language and framing suggestions as questions or requests for clarification.",85.0,90.0
"The paper addresses the latent space distribution mismatch in VAEs and GANs. The authors try to solve the issue by optimal transport theory and the proposed method on the latent space yields better quality in the generated samples.

To me, the motivation is not very strong. In DCGAN, amazingly, latent space linear operations can carry over to the generated images. But it’s not something people are usually concerned with in GANs.  I understand that latent space operations can provide insights into how the trained generator works. But how can it improve the actual GAN training? Choosing Gaussian or uniform distribution for the latent variable is mainly for ease of computation and I am not sure if the motivation to match the distributions is very strong in GAN applications. Perhaps it more important in the context of VAEs.

At the first glance, the proposed form of transformation is not surprising. Though optimal transport is a very powerful theoretical tool, it serves more like an explanation or validation, rather than the motivation. I felt the theory part could be simper. 

In the quantitative comparisons with other methods, all simulations seem to be in the context of GAN. The difference in 2-point cases (table 2) is not significant and the author only compares with linear interpolation but not SLERP. I would like to see more quantitative comparisons with other methods and also some empirical studies in the context of VAEs. ","The reviewer raises several concerns about the paper, questioning the significance of the problem addressed, the novelty of the proposed solution, and the strength of the experimental results. While they acknowledge the paper's contributions, their criticisms suggest a lukewarm reception overall. The language used is professional and avoids harsh or disrespectful tones, indicating a neutral to slightly negative but polite stance.",-20.0,60.0
"This paper proposes a variant of GEM called A-GEM that substantially improves the computational characteristics of GEM while achieving quite similar performance. To me the most interesting insight of this work is the proof that an inner product between gradients can suffice instead of needing to solve the quadratic program in GEM – which I have found to be a major limitation of the original algorithm.  The additional experiments using task descriptors to enable zero shot learning are also interesting.  Moreover, the discussion of the new evaluation protocol and metrics make sense with further clarification from the authors. Overall, I agree with the other reviewers that this paper makes a clear and practical contribution worthy of acceptance. 
","The review is overwhelmingly positive. The reviewer finds the work interesting, insightful, and a clear and practical contribution. They use phrases like ""most interesting insight"", ""interesting"", ""make sense"", and ""clear and practical contribution worthy of acceptance."" The language is constructive and polite throughout.",90.0,100.0
"This paper proposes a novel method to perform meta-learning for stochastic gradient MCMC. They utilize a general family of SDEs that guarantees preservation of the target density with somewhat loose constraint on the drift and diffusion functions (from Ma et al. (2015)). Then, they propose learning these functions on a set of training tasks and evaluating on unseen, different tasks, in a meta-learning fashion.

This paper is well written and easy to follow. They do a very good job presenting the motivation for their work as well as seminal work in SG-MCMC. The idea is fairly natural, especially in light of recent success of meta-learning and learning optimizers. They do a thorough survey of related work and also do a good job presenting their method in context of very modern work on MCMC and SG-MCMC.

I am not completely convinced by the meta-training objective; both losses seem natural but quite intractable to compute in practice. The use of Stein indicates that the kernel must probably be *very* carefully crafted and given that the whole method relies on this objective, it seems like this could be a breaking point. I am also curious to know how you diagnostic/evaluate the choice of these kernels.

In terms of evaluation, the experimental results are not the most convincing given that across the board, they are (except in one case) in 4 case within 0.2% of SGHMC and in the two others, within 0.5% and 0.8% respectively. This seems a bit weak, especially considering the compute invested both at training time and for each SG-MCMC step (i.e. getting the outputs from the neural networks vs simply doing HMC). Is there really a case for using the method over SG-HMC? I would have also very much liked to see a run-time evaluation.","The review starts with positive statements, highlighting the paper's clarity, good presentation, and thoroughness. However, it raises significant concerns about the core methodology, particularly the meta-training objective and the choice of kernels. The reviewer finds the experimental results somewhat unconvincing due to the marginal improvement over the baseline. While the reviewer doesn't use harsh language, the criticism is quite pointed. Therefore, the sentiment leans towards the negative side, but not extremely so.",-30.0,60.0
"This paper introduces a domain adaptation approach based on the idea of Cyclic GAN. Two different algorithms are proposed. The first one incorporates a semantic consistency loss based on domain-specific classifiers acting on full cycles of the of the generators. The second one also makes use of domain-specific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other.

Strengths:
- The different terms in the proposed loss functions are well justified.
- The results on low-resources supervised domain adaptation indicate that the method works better than the that of Motiian et al. 2017.

Weaknesses:
- Novelty is limited: The two algorithms are essentially small modification of the semantic consistency term used in Hoffman et al. 2018. They involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other. While the modifications are justified, I find this a bit weak for ICLR.

- It is not clear to me why it is worth presenting the relaxed cycle-consistency object, since it always yields worse results than the augmented one. In fact, at first, I though both objectives would be combined in a single loss, and was thus surprised not to see Eq. 5 appear in Algorithm 1. It only became clear when reading the experiments that the authors were treating the two objectives as two different algorithms. Note that, in addition to not performing as well as the augmented version, it is also unclear how the relaxed one could work in the unsupervised scenario.

- Experiments:
* In 4.1, the authors mention that 10 samples per class are available in the target domain. Are they labeled or unlabeled? If labeled, are additional unlabeled samples also used?
* In Table 1, and in Table 3, is there a method that corresponds to CyCADA? I feel that this comparison would be useful considering the similarity. That said, I also understand that CyCADA uses both a reconstruction term (as in Eq. 4) and a semantic consistency one, whereas here only a semantic reconstruction term is used. I therefore suggest the authors to also compare with a baseline that replaces their objective with the semantic consistency one of CyCADA, i.e., CyCADA without reconstruction term.
* In 4.2, it is again not entirely clear if the authors use only the few labeled samples, or if this is complemented with additional unlabeled samples. In any event, does this reproduce the setting used by Motiian et al. 2017?
* As the argument is that the proposed loss is better than the reconstruction one and that of Hoffman et al. 2018 for low-resource supervised adaptation, it would be worth demonstrating this empirically in Table 2.

Summary:
The proposed objective functions are well motivated, but I feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at ICLR.

After Response:
After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR.
","The reviewer acknowledges the strengths of the paper, such as well-justified loss functions and good results compared to a previous work. However, they find the novelty limited, viewing the proposed algorithms as ""small modifications"" of existing work. The reviewer also raises several concerns about the clarity and completeness of the experiments. While they appreciate the authors' response and additional results, they ultimately still believe the contribution is not strong enough for ICLR, indicating a slightly negative sentiment overall.",-20.0,50.0
"This work is concerned with the problem of batch contextual bandits, in which a target contextual bandit policy is optimized on the data generated by a different logging policy. The main problem is to come up with a low-variance low-bias estimator for the value of the target policy. Many of the known techniques are based on an unbiased estimator known as inverse propensity scoring (IPS), which uses the distribution over actions of the logging policy, conditioned on the observed contexts. However, IPS suffers from large variance. The paper's idea is to do a maximum likelihood fit of a simple surrogate policy to the logged data, and then use the conditional distribution over actions of the surrogate policy to compute inverse propensity scores.
The theoretical results show that the bias of this estimator vanishes asymptotically, whereas the variance is smaller than IPS. Experiments using known/unknown logging policies on artificial/real-world bandit data show that the IPS scores computed with the proposed technique are empirically better than those computed directly using the logging policy. Moreover, the advantage increases when the distribution extracted from the surrogate policy is used to compute more sophisticated estimators than IPS.

The off-policy evaluation in contextual bandits is an important problem, and this paper appears to make some progress. However, the theoretical analysis is a bit disappointing, as it does not shed much light on the reasons why using a surrogate policy should help. Some additional discussion would add value to the paper.

The result about the decrease in variance depends on assumptions that are not clearly justified, and is expressed in terms of abstract quantities that hard to connect to concrete scenarios. In the end, one does not get many new insights from the theory.

In Assumptions 3.3-3-4, what is the variable w.r.t the asymptotic notations are understood? By that I mean, the variable n such that f(n) = O(g(n)).

The experiments are competent and quite elaborated. However, the statistical significance of the improvements in Table 1 is unclear.

The evaluation criterion for the Criteo experiment is unclear. As a consequence it is hard to appreciate the significance of the improvements in this case.","The review acknowledges the paper's contribution to off-policy evaluation in contextual bandits, which is considered an important problem. However, it expresses reservations about the theoretical analysis, finding it ""disappointing"" and lacking clarity. The reviewer also points out concerns about the assumptions made and the lack of statistical significance in some experiments. While the review acknowledges the competence of the experiments, the overall tone suggests areas for improvement rather than outright praise. Therefore, the sentiment leans slightly towards the negative side. The language used is professional and polite, directly addressing the authors' work without resorting to personal attacks or disrespectful language.",-20.0,70.0
"Summary:

This paper addresses the computational aspects of Viterbi-based encoding for neural networks. 

In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns.  (The value of each nonzero is stored separately.)  However, it is clear that not all messages are possible, only those in the ""range space"" of my codes. (This part is previous work Lee 2018.) 

The ""Double Viterbi"" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. 

Pros:
 - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. 
 - The idea is theoretically sound and interesting.

Cons: 
 - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.
 - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. 
 - Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)

","The reviewer acknowledges the importance of the work and finds the idea theoretically sound, which points towards a positive sentiment. However, they also express concerns about the clarity of the paper and the lack of evaluation for the novel contribution, which brings the sentiment down. Overall, the tone is critical but professional and polite, suggesting room for improvement but not outright rejection. ",50.0,60.0
"The submission analyzes parameter averaging in GAN training, positing that using the exponential moving average (EMA) leads to more well-behaved solutions than using moving averages (MA) or no averaging (None). 

While reading the submission, the intuitively given explanations for using EMA (cycling, mainly) seem reasonable. However, I do not think there is sufficient understanding of the (non-)convergence behavior in real-world GAN settings, and this submission does not contribute much to it.
The theoretical underpinnings in Section 3.1 are quite thin, and focus on describing one particular example of a bilinear saddle problem, which is quite far from a typical GAN, as used e.g. in computer vision problems. Although interesting to read, I would not draw any wider-reaching conclusions from this carefully constructed example.

Instead, the submission serves mainly as an experimental study on why EMA works better in some of the tested cases than MA/None. Main quantitative measures are the often-used IS and FID. It is clear from both the provided quantitative values as well as the provided qualitative images that either averaging method is likely better then no averaging.

Unfortunately, IS and FID contradict each other somewhat for EMA vs. MA in Table 2, which is attributed to IS being [more] flawed [than FID]. Neither measure is flawless, however, which diminshes the usefulness of the numeric results somewhat. Well designed human studies may be complicated to set up and costly to conduct, but these could demonstrate additional confirmation of the usefulness of the proposed method.

EMA introduces an additional hyperparameter, beta, which is only discussed very briefly, and only in the context of qualitative results. I missed a more thorough discussion of the impact of beta.

Overall, the submission makes an interesting proposition (usage of EMA during GAN training), but falls short in convincing me that this is a useful thing to do in broader contexts. Overall originality is minor; projected significance is minor to medium.

EDIT: After the rebuttal, resulting in several changes and additions to the paper, I am changing my rating from 5 -> 6.","The reviewer acknowledges the interesting proposition and some merits of the submission (e.g., ""intuitively given explanations for using EMA seem reasonable"", ""clear that either averaging method is likely better then no averaging""). However, they also express significant concerns regarding the theoretical depth, the contradicting results of IS and FID, and the lack of discussion on the impact of the hyperparameter beta. They find the overall originality and significance to be minor, only slightly improving their assessment after the rebuttal. All of this points to a lukewarm, rather critical review, reflected in a slightly negative sentiment score. The language used is formal, academic, and devoid of any insults or personal attacks, indicating a polite tone.",-25.0,75.0
"This paper presents a mixed integer programming technique for verification of piecewise linear neural networks. This work uses progressive bounds tightening approach to determine bounds for inputs to units. The authors also show that this technique speeds up the bound determination by orders of magnitude as compared to other complete and incomplete verifiers. They also compare the advercerial accuracies on MNIST and CIFAR and improve on the lower bounds as compared to PGD and upper bounds as compared to SOA. The paper is well written and presents a valuable technique for evaluating robustness of classifiers to adversarial attacks. 
","The review is overwhelmingly positive. The reviewer finds the paper well-written, the technique valuable, and the results impressive. There are no negative remarks, only praise for the work.",90.0,100.0
"Paper summary: The paper presents a robust Analysis by Synthesis classification model that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. The architecture involves training VAEs for each class to learn p(x|y) and performing exact inference during evaluation. The authors show that ABS and binary ABS outperform other models in terms of robustness for L2, Linf and L0 attacks respectively. 

The paper in general is well written and clear, and the approach of using generative methods such as VAE for better robustness is good. 

Pros: 
Using VAEs for modeling class conditional distributions for data is an exhaustive approach. The authors show in Fig 4 that ABS generates adversarials that are semantically meaningful for humans, which is not achieved by Madry et al and other models. 

Cons: 
1) The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. 
2) Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs. 
3) It would be nice to see this model behaves for skewed datasets. 

","The reviewer acknowledges the merits of the paper, using phrases like ""well written and clear"", ""good"", and ""nice idea"". They also point out the limitations but in a constructive manner. Overall, the feedback is positive, but the concerns raised suggest there's room for improvement, hence not a very high positive score.",60.0,80.0
"This paper seeks to answer the question of whether models which process sequences, but are not strictly classical RNNs, are Turing complete.

The authors present proofs that both the Transformer and Neural GPU are turing complete, under certain conditions. I do not consider myself qualified to properly verify the proof but it seems to be presented clearly. The authors note that the conditions involved are not how these models are used in the real world. Given the complex construction required for this more theoretically based proof, it seems reasonable that this should be published now, rather than waiting until the further work discussed in the final section is completed.

I have a number of questions where if a brief answer is possible, this would enhance the manuscript. The main question is, of the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice? For example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?

The rational numbers assumption is interesting - again I wonder how this would affect the model in reality, obviously all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.

Does the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?

Overall this paper is novel and interesting, I have to give a slightly low confidence score because I'm unfamiliar with a lot of the background here (eg the Siegelamnn & Sontag work). The paper does seem concise and well written.

typos and minor points:

Circular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?

paragraph above equation 5, 'vectores' -> 'vectors'","The reviewer provides constructive criticism and expresses curiosity about the practical implications of the research. They acknowledge their own limitations in verifying the proof but find the paper clearly presented and the publication justified. The reviewer asks clarifying questions and points out minor issues, indicating a positive attitude towards the paper's potential.",60.0,90.0
"Summary: the authors propose a new algorithm, APL, for a few-shot and a life-long learning based on an external memory module. APL uses a surprise-based signal to determine which data points to store in memory and an attention mechanism to the most relevant points for prediction. The authors evaluate APL on a few-shot classification task on Omniglot dataset and on a number analogy task.

Quality: the authors consider interesting approach to life-long learning and I really liked the idea of a surprise-based signal to choose the data to store. However, I am not convinced by the learning setting that authors study. While a digit-symbol task from the introduction is interesting to study the properties of APL, I fail to see any real world analogy where it is useful. The same happens in a few-shot omniglot classification. The authors decided to shuffle the labels within episodes that, I guess, is supposed to represent different tasks in a typical life-long learning scenario. Again, it maybe interesting to study the behaviour of the algorithm, but I don't see any practical relevance here. It would make more sense to study the algorithm in a life-long learning setting, for example, considered in [1] and [2].

Clarity: the paper is well-written in general. I failed to decode the meaning behind the paragraph under Figure 3 on page 4 and would advise the authors to re-write it. The same goes to the first paragraph on page 3.

Originality: the paper builds on the prior work of Kaiser et al., 2017 and Santoro et al., 2016, but the proposed modifications are novel to my best knowledge.

Significance: below average: the paper combines interesting ideas that potentially can be used in different learning contexts and with other algorithms, however, the evaluation does not show the benefit in an obvious way.

Other comments: 
* throughout the whole paper it is not clear if the embeddings are learned or not. I suppose they are, but what then happens to the ones in memory? If they are not, like in ImageNet example, where do they come from?
* the hyperparameter \sigma: the authors claim ""the value of \sigma seems to not matter too much"". Matter for what? It's great if the performance is stable for a wide range of \sigma, but it seems like it should have a great influence over the memory footprint of APL. I feel this is an important point that needs more attention.
* it would be interesting to see how APL performs with a simple majority vote instead the decoder layer. This would count for an ablation study and could emphasize the role of the decoder.
* Figure 4, b) plots are completely unreadable on black-and-white print, the authors might like to address that
* In conclusion, the first claim about state-of-the-art accuracy with smaller memory footprint: I don't think that the results of the paper justify this claim.

[1] Yoon et al, Lifelong Learning with Dynamically Expandable Networks, ICLR 2017
[2] Rebuffi et al,  iCaRL: Incremental Classifier and Representation Learning, CVPR 2017

********************
After authors response:

Thanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper.","The reviewer finds the core idea interesting and acknowledges the novelty of the proposed modifications. However, they express concerns about the chosen learning setting and its practical relevance. While the language is critical, it's within the bounds of academic discourse and offers concrete suggestions for improvement. The reviewer's willingness to reconsider their evaluation after the authors' response further indicates a balanced and constructive approach.",40.0,60.0
"This paper empirically explores heuristics commonly used in deep learning: learning rate restarts, warmup and distillation. The authors utilize two recently proposed tools for neural network analysis: mode connectivity (MC) finding a low loss pathway between two given points in the space of DNN parameters and CCA measuring the correlation of  DNN layer activations. Conducting a set of experiments and analyzing the results the authors refine the intuition behind the considered heuristics and dynamics of corresponding training procedures. 

Strengths:

+ The authors conduct experiments ensuring robustness of MC framework.
+ In the chosen settings the experimental methodology of the paper sounds reasonable. I find the idea of DNN analysis from both perspectives of weight space and activations important.
+ Paper is well-written and organized clearly. All the used methods and experiments are adequately described.
+ The authors draw connections between obtained results and hypotheses introduced in prior work.

Weaknesses:

- There is a possible flaw in the choice of experimental settings. Authors mention Batch Normalization (BN) among heuristics widely used in deep learning. It is known that properties of both loss surface and activations are different between DNN architectures which include BN layers and those which do not. To emphasize generality of obtained results, it would be beneficial to conduct experiments for both types of DNN architectures as at the moment the majority of the results are presented for VGG architecture which typically does not include BN. Impact of other architecture modifications (e.g. skip connections) might be considered as well.

- I find the significance of the results unclear. Although the particular insights of the learning procedures are revealed there is not enough attention paid to their value for possible improvements of the procedures and their applications. There is only one idea proposed by the authors based on the experimental results – fixing the deeper layers during the warmup phase, but the practical implications of this idea are not discussed.

Other comments:

* The scale used in Figure 3 and similar figures in the appendix is not easily comprehensible. I recommend to comment further on the scale or possibly adjust it.
","The review acknowledges the strengths of the paper, such as its robust experiments, clear writing, and connection to prior work. While it points out weaknesses like potential flaws in experimental settings and unclear significance of results, it maintains a constructive and helpful tone. The reviewer offers specific suggestions for improvement, indicating a desire to see the paper succeed. Overall, the feedback is balanced and leans positive.",60.0,80.0
"This paper focuses on multi-choice QA and proposes a coarse-to-fine scoring framework. Where the coarse-grained answer scoring model computes the scores with the attention over the whole passages, and the fine-grained one only uses local contexts for each answer option (candidate).

The proposed approach was evaluated on the only dataset of WikiHop, and achieved large improvement over the other methods on the leaderboard. However, I found the paper lack of motivation about the designs of the coarse and fine scoring models. For example, why using self-attention after GRU and co-attention in the two answer scoring models?

Another concern I have is about the novelty. Besides the complicated model designs, the coarse and fine scoring models are both following some common ideas in previous work. And each model could achieve on-par results compared to previous baselines. This makes me feel that the whole approach looks more like model combination of two not-so-novel (and not very well-motivated) models.

Thirdly, the only evaluation on WikiHop brings more problems to the above two points. Since the motivation of the architecture design is not very clear, I am not sure whether the architectures could generalize to other benchmarks. Similar concern for the model combination approach.

Moreover, the proposed approach is a general architecture for multiple-choice datasets requiring multiple evidence. To verify its generalizability, I suggest the authors add further experiments on one dataset from the following ones: either multi-choice QA datasets like ARC and RACE/RACE-open, or other open-domain QA datasets like TriviaQA, by treating the re-ranking of answer predictions as multi-choice QA problems (like the approach in Evidence Aggregation for Open-Domain QA from ICLR2018).

A minor question: why the CFC w/o encoder could still work so well? At least the fine-grained scoring model should heavily rely on encoders. Otherwise, according to Eq (17), the fine-grained model cannot use any contextual information.","The reviewer acknowledges the paper's contribution (improved accuracy) but expresses several concerns about the design choices, novelty, and limited evaluation. They find the motivation and generalizability of the approach unconvincing. The language is critical but professional and not disrespectful. Therefore, the sentiment leans towards the negative side, although it acknowledges some positive aspects. The politeness remains largely neutral.",-30.0,20.0
"The paper studies the problem of representation learning in the context of hierarchical reinforcement learning by building on the framework of  HIRO (Nachum et al. (2018)). The papers propose a way to handle sub-optimality in the context of learning representations which basically refers to the overall sub-optimality of the entire hierarchical polity with respect to the task reward. And hence, the only practical different from the HIRO paper is that the proposed method considers representation learning for the goals, while HIRO was directly using the state space.


I enjoyed reading the paper. The paper is very *well* written. 

Experimental results:  The authors perform the series of experiments on various high dimensional mujoco env, and  show that the representations learned using the proposed method outperforms other methods (like VAEs, E2C etc), and can recover the controllable aspect of the agent i.e the x, y co-ordinate. This is pretty impressive result.

Some questions:

[1] Even though the results are very interesting, I'm curious as to how hard authors try to fit the VAE baseline. Did authors try using beta VAEs (or variants like InfoVAE) ?  Since the focus of the entire paper is about representation learning (as well as the focus of the conference), it is essential to make sure that baselines are strong. I would have suspected that it would have been possible to learn x,y co-ordinate in some cases while using improved version of VAE like beta VAE etc.

[2] One of the major intuitions behind sub-optimality is to learn representations that can generalize, as well as can be used for continual learning (or some variant of it!). This aspect is totally missing from the current paper. It would be interesting to show that the representations learned using the proposed method can transfer well to other scenarios or can generalize in the presence of new goals, or can be sample efficient in case of continual learning. 

I think, including these results would make the paper very strong.  (and I would be happy to increase my score!).

","The reviewer clearly states that they ""enjoyed reading the paper"" and found the results ""pretty impressive."" They provide constructive criticism and suggestions for improvement but do not express any major concerns. The suggestions for strengthening the paper, such as including results related to generalization and continual learning, indicate a positive outlook on the paper's potential. Overall, the tone is encouraging and suggests that the reviewer believes the paper has merit. ",75.0,100.0
"This paper presents two new ideas on leveraging program semantics to improve the current neural program synthesis approaches. The first idea uses execution based semantic information of a partial program to guide the future decoding of the remaining program. The second idea proposes using an ensembling approach to train multiple synthesizers and then select a program based on a majority vote or shortest length criterion. The ideas are evaluated in the context of the Karel synthesis domain, and the evaluation shows a significant improvement of over 13% (from 77% to 90%).

The idea of using program execution information to guide the program decoding process is quite natural and useful. There has been some recent work on using dynamic program execution in improving neural program repair approaches, but using such information for synthesis is highly non-trivial because of unknown programs and when the DSL has complex control-flow constructs such as if conditionals and while loops. This paper presents an elegant approach to handle conditionals and loops by building up custom decoding algorithms for first partially synthesizing the conditionals and then synthesizing appropriate statement bodies.

The idea of using ensembles looks relatively straightforward, but it hasn’t been used much in synthesis approaches. The evaluation shows some interesting characteristics of using different selection criterion such as shortest program or majority choice can have some impact on the final synthesized program.

The evaluation results are quite impressive on the challenging Karel domain. It’s great to see that execution and ensembling ideas lead to practical gains.

There were a few points that weren’t clear in the paper:

1. Are the synthesis models still trained on original input-output examples like Bunel et al. 2018? Or are the models now trained on new dataset comprising of (partial-inputs-->final-output) pairs obtained from the partial execution algorithm?

2. In algorithm 2, the algorithm generates bodies for if and else branches until generating the else and fi tokens respectively. It seems the two bodies are being generated independently of each other using the standard synthesizer \Tau. Is there some additional context information provided to the two synthesis calls in lines 8 and 9 so that they know to produce else and fi tokens?

3. Is there any change to the beam search? One can imagine a more sophisticated beam search with semantic information can help as well (e.g. all partial programs that lead to the same intermediate state can be grouped into 1).
","The reviewer explicitly states the ideas are ""quite natural and useful"", ""elegant approach"", and that the results are ""quite impressive"" and lead to ""practical gains."" The reviewer also expresses interest in the novelty of the work, highlighting that certain aspects ""haven't been used much in synthesis approaches."" All of this points to a positive sentiment. The questions asked are constructive and aim to clarify specific aspects of the paper, indicating a polite and professional tone.",75.0,80.0
"This paper proposes N-ball embedding for taxonomic data. An N-ball is a pair of a centroid vector and the radius from the center, which represents a word.

Major comments:

- The weakness of this paper is lack of experimental comparisons with other prominent studies. The Poincare embedding and the Lorentz model are recently proposed and show a good predictive performance in hypernymy embedding.
- WordNet concepts are actually structed in DAG. Recent studies on structure embedding can hadle DAG data. It is not clear how to extend N-ball embedding for handling DAT structures. 

- Related work is not sufficiently described.

- It is not clear why N-ball embedding is suitable for hierarchical structures.
","The review is critical of the paper, pointing out significant weaknesses and flaws. It does not find strong positive aspects in the work. The language used, while direct, is professional and typical of academic peer reviews. It avoids harsh or disrespectful phrasing.",-60.0,50.0
"Summary:
Proposes Counterfactual Guided Policy Search (CF-GPS), which uses counterfactual inference from sampled trajectories to improve an approximate simulator that is used for policy evaluation. Counterfactual inference is formalized with structural causal models of the POMDP. The method is evaluated in partially-observed Sokoban problems. The dynamics model is assumed known, and a learned model maps observation histories to a conditional distribution on the starting state. CF-GPS outperforms model-based policy search and a ""GPS-like"" algorithm in these domains. GPS in MDPs is shown to be a particular case of CF-GPS, and a connection is also suggested between stochastic value gradient and CF-GPS.

Review:
The work is an interesting approach to a relevant problem. Related literature is covered well, and the paper is well-written in an approachable, conversational style. 

The approach is technically sound and generally presented clearly, with a few missing details. It is mainly a combination of existing tools, but the combination seems to be novel. 

The experiments show that the method is effective for these Sokoban problems. A weakness is that the setting is very ""clean"" in several ways. The dynamics and rewards are assumed known and the problem itself is deterministic, so the only thing being inferred in hindsight is the initial state. This could be done without all of the machinery of CF-GPS. I realize that the CF-GPS approach is domain-agnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties. The issue of inaccurate dynamics models seems especially relevant, and is not addressed by the Sokoban experiment. It's also notable that the agent cannot affect any of the random outcomes in this problem, which I would think would make counterfactual reasoning more difficult.

Comments / Questions:
* Please expand on what ""auto-regressive uniformization"" is and how it ensures that every POMDP can be expressed as an SCM
* What is the prior p(U) for the experiments? 
* ""lotion-scale"" -> ""location-scale""

Pros:
* An interesting and well-motivated approach to an important problem
* Interesting connections to GPS in MDPs

Cons:
* Experimental domain does not ""exercise"" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known
* Work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper-)parameter settings ","The review is overall positive. The reviewer finds the approach interesting and well-motivated, the paper well-written, and the technical work sound. However, the reviewer also points out limitations in the experimental evaluation, mainly that the chosen setting is too simplistic and doesn't fully demonstrate the capabilities of the proposed method. The language used is polite and constructive throughout the review.",60.0,80.0
"Summary:
The authors propose a network for VQA incorporating hand-crafted modules and their hierarchy, each of which is a network for a high-level vision task. Some modules may share the same sub-modules at a different level in the module hierarchy. Each module is individually (not end-to-end) trained with a dataset containing a dedicated annotation for their high-level tasks. The proposed model shows comparable scores to the existing models.

Presentation and clarity:
The paper is well written and easy to follow and contains reasonable experiments for understanding the proposed method.

Originality and significance:
I mainly do not agree that this work generalizes NMN. Instead, I believe that this work is a special case of NMN where the modules and their hierarchy are manually defined based on the authors' intuition. Meanwhile, the proposed network architecture is static, and thus the main idea of having multiple modules in a network is not novel as other approaches using static network architectures such as [A] also facilitate multiple modules for different sub-procedures (e.g., RNN for questions and CNN for image) and sometimes share modules in multiple stages too. The main difference between this and previous works is that the modules in this work deal with high-level tasks chosen by the authors. I am not convinced that designing the modules with high-level tasks is a better choice over designing modules that are less task-specific. Rather, I see more drawbacks as the proposed method requires multiple datasets with diverse task-specific annotation. Also, the modules and their connectivity are less scalable and extendable as they are not learned.

Considering all the model and dataset complexities, the improvements over black-box models are mostly marginal. The main benefits we get from all these complexities are the interpretability. However, for many modules, the interpretability comes from indirect signals that are often not clear how to interpret for the question answering. On the other hand, the manually designed sub-tasks may cause error propagation in the network as these modules are not directly optimized for the final objective.

Some questions and comments:
I do not understand why it is necessary to have the image captioning module as it does not directly relate to the question answering. Moreover, the caption itself is generated without conditioning on the question.

[A] Yang, Zichao, et al. ""Stacked attention networks for image question answering."" CVPR 2016.


== After discussion phase
Based on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating.","The reviewer raises significant concerns about the paper's originality, arguing that it's a specialized case of existing work rather than a generalization. The reviewer finds the model complex and not convincingly justified by its marginal improvements, questioning the necessity of certain modules. While acknowledging the paper's clarity, the overall tone is critical, particularly regarding the significance of the contributions. However, the final line indicating a change of heart after further discussion points to the initial negativity being tempered by the author's response. Therefore, the sentiment is somewhat negative but not overly harsh.",-50.0,50.0
"Following recent work on Hindsight Experience Replay (Andrychowicz et al. 2017), the authors extend the idea to policy gradient methods. They formally describe the goal-conditioned policy gradient setup and derive the extensions of the classical policy gradient estimators. Their key insight to deriving a computationally efficient estimator is that for many situations, only a small number of goals will be ""active"" in a single trajectory. Then, they conduct extensive experiments on a range of problems and show that their approach leads to improvements in sample efficiency for goal-conditioned tasks.

Although the technical novelty of the paper is not high (many of the estimators follow straightforwardly from previous results, however, the goal subsampling idea is a nice contribution), the paper is well written, the topic is of great interest, and the experiments are extensive and insightful. I expect that this will serve as a nice reference paper in the future, and launching point for future work. 

The only major issue I have is that there is no comparison to HER. I think it would greatly strengthen the paper to have a comparison with HER. I don't think it diminishes their contributions if HER outperforms HPG, so I hope the authors can add that.

Comments:

In Sec 6.1, it seems surprising that GCPG+B underperforms GCPG. I understand that HPG+B may underperform HPG, but usually for PG methods a baseline helps. Do you understand what's going on here?

In Sec 6.2, it would be helpful to plot the average return of the optimal policy for comparison (otherwise, it's hard to know if the performance is good or bad). Also, do you have any explanations for why HPG does poorly on the four rooms?

====

Raising my score after the authors responded to my questions and added the HER results.","The reviewer explicitly states the paper is well-written, the topic relevant, and the experiments insightful. They believe it will serve as a good reference point for future work. Although they have some questions and requests, their overall tone is positive and they ultimately raise their score after the authors address their concerns. There is no use of harsh language and the reviewer maintains a respectful and professional tone.",75.0,100.0
"last time i had two comments:
1. the real data motifs did not look like what i'd expect motifs to look like. now that the authors have thresholded the real data motifs, they do look as i'd expect.
2. i'm not a fan of VAE, and believe that simpler optimization algorithms might be profitable.  i acknowledge that SCC requires additional steps; i am not comparing to SCC. rather, i'm saying given your generative model, there are many strategies one could employ to estimate the motifs.  i realize that VAE is all the rage, and is probably fine.  in my own experiments, simpler methods often work as well or better for these types of problems.  i therefore believe this would be an interesting avenue to explore in future work.","The reviewer provides a positive comment initially, indicating satisfaction with the authors' revisions ('now that the authors have thresholded the real data motifs, they do look as I'd expect'). However, they also express skepticism towards the authors' choice of VAE, suggesting simpler methods might be equally effective. While this suggestion is not a negative criticism, it does introduce a note of doubt. Overall, the tone is neutral to mildly positive, leaning towards politeness with phrases like 'I acknowledge,' 'I realize,' and 'I believe this would be an interesting avenue to explore.'",20.0,70.0
"Summary: the paper proposes a method for Deep Neural Networks (DNN) that identifies automatically relevant features of the set of the classes, enriching the predictions made with the visual features that contributed to that class, supporting, thus, interpretation (understanding what the model has learned) and explanation (justification of the predictions/classifications made by the model). This scheme does not rely on additional annotations, like earlier techniques do.

The contributions of this paper are relevant to, I would say, a large segment of the AI community, since interpretability and explainability of AI (XAI) is the focus of many current works in the area, and there are still many unresolved issues. I consider this paper suitable for ICLR 2019, in particular, it fits the call for papers topic “visualization or interpretation of learned representations”.

The authors also present a new dataset (am8Flower) that can be used by the community for future evaluations of explanation methods for DNN. From my point of view, this is a significant contribution, since there is a lack of datasets that can be used for evaluation.

The authors motivate properly the need for this research/study, addressing the main weakness of the two more common strategies for interpreting DNN, (1) manually inspecting visualizations of every single filter or (2) comparing the internal activations produced by a given model w.r.t. a dataset with pixel-wise annotations of possibly relevant concepts.

I would encourage the authors to write the limitations and weakness of their proposal w.r.t. similar approaches they reviewed. I am aware that the space is limited, but in p.8, section 4.3, when Table 1 is introduced and the authors confirm that their proposal has higher IoU than other methods, the authors could explain, in brief, what are the weaknesses of their method w.r.t. the other approaches analyzed.

Another clarification concerns the initialization of input parameters, such as sparsity; e.g., p.6 sparsity is initialized with 10 for all datasets, why? How has this value been selected and how sensitive is the performance regarding variations of this value?

Once again, I know that the space is limited, but I would like to be able to see some of the figures better (since this is an essential part of the paper). The additional material complements very well the paper and shows larger figures, but I think that the paper itself should be self-sufficient, and figures like Fig. 5 should be enlarged so it is easier to see some details.

Just a concern or something that I quite did not understand about one of the arguments the authors use to justify the evaluation carried out: the authors claim that they want to avoid the subjectivity introduced by humans (citing Gonzalez-Garcia et al. 2017), and prefer to avoid user studies, presenting a more objective approach in their evaluation. Ok, but then, the analysis presented in, for example, page 7, is based mainly in their interpretation of the results, a qualitative analysis of the images (we can see fur patterns, this and that, etc.). So aren’t they interpreting the results obtained as users? So after all, aren’t the visual explanations and feedback intended for users? Why should we claim that we want to avoid the subjectivity introduced by humans in the evaluation when the method proposed here is actually going to be used by users –with their inherent subjectivity? I do not mean that the evaluation carried out is not interesting per se, but it could be motivated differently, or it could be complemented later on with future user studies (that would make an interesting addition to the paper). Moreover, I also wonder whom the authors see as intended users for the proposed scheme.

Small comments:
P.1 “useful insights on the internal representations”  insights into the internal representations.
P. 2: space needed in “back-propagation methods.Third,”
P. 3: Remove “s” in verb (plural authors): “Similarly, Bach et al. (2015) decomposes the classification”  decompose or decomposed
P.3: n needed “Chattopadhyay et al. (2018) exteded”  extended
P.3: “This saliency-based protocol assume that”  protocol assumes
P.3: “highlighted by the the explanations”  remove one “the”
P. 5: “space. As as result we get”  remove one “as”
P. 5: “and compensate this change”  compensate for this change
P. 6: “In this experiment we verify”  In this experiment, we verify
P. 6: “To this end, given a set of identified features we”  To this end, given a set of identified features, we
P. 6: “Note that the OnlyConv method, makes the assumption”  remove “,” after method
P. 7: “In order to get a qualitative insight of the type of”  insight into the
P. 7: I would write siamese and persian cat with capital “S” and “P” (Siamese, Persian)
P. 7: others/ upper “Some focus on legs, covered and uncovered, while other focus on the upped body part.”  while others focus on the upper body part
P. 7: “These visualizations answers the question”  answer
P. 7:  “In this section we assess”  In this section, we
P. 7: Plural “We show these visualization for different”  these visualizations
P. 7: In “Here our method reaches a mean difference on prediction confidence”  difference in prediction …
P. 7: “This suggest that our method is able”  This suggests that
P. 8: state-of-the-art
P. 8: “has higher mean IoU”  has a higher mean IoU
Whole document: when using “i.e.” add “,” after: i.e.,

References: Some of the references in the list have very little information to be able to find it/proper academic citation, e.g. , Yosinski et al. 2015; Vedaldi and Lenc, 2015:

Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. 2015.

A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for matlab. In MM, 2015.

Ref Doersch et al.: What makes paris look like paris?  Paris
","The reviewer clearly states that they consider the paper a good fit for the conference and praises multiple aspects of it, such as the contribution to XAI and the new dataset. They provide constructive criticism and suggestions for improvement but maintain a positive and encouraging tone. The issues raised are typical for peer reviews and do not express strong negativity.",75.0,90.0
"EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference.

Summary:
This paper provides a method to jointly learn from crowdsourced worker labels and the actual data. The key claimed difference is that previous works on crowdsourced worker labels ignored the data. At a higher level, the algorithm comprises maximizing the mutual information gain between the worker labels and the output of a neural network (or more generally any ML model) on the data. 

Evaluation:
I like the idea behind the algorithm. However there are several issues on which I ask the authors to provide some clarity. I will provide a formal ""evaluation"" after that. (For the moment, please ignore the ""rating"". I will provide one after the rebuttal.) 

(1) As the authors clarified, one key aspect of the ""information intersection"" assumption is that the crowdsourced labels are statistically independent from the data when conditioned on the ground truth. How strongly does this coincide with reality? Since the work is primary empirical, is there any evidence on this front?

(2) In the abstract, introduction etc., what does it mean to say that the algorithm is an ""early algorithm""?
-- Thanks for the clarification. I would suggest using the term ""first algorithm"" in such cases. However, is this the first algorithm towards this goal? See point (3).

(3) The submitted paper misses an extremely relevant piece of literature: ""Learning From Noisy Singly-labeled Data"" (arXiv:1712.04577). This paper also aims to solve the label + features problem together. How do the results of this paper compare to that of this submission?

(4) ""Model and assumptions"" Is the i.i.d. assumption across the values of ""i""? Then does that not violate the earlier claim of accommodating correlated mistakes?

(5) Recent papers on crowdsourcing (such as Achieving budget-optimality with adaptive schemes in crowdsourcing arXiv:1602.03481 and  A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness arXiv:1606.09632) go beyond restricting workers to have a common confusion matrix for all questions. In this respect, these are better aligned with the realistic scenario where the error in labeling may depend on the closeness to the decision boundary. How do these settings and algorithms relate to the submission?

(6) Page 5: ""Later we will show....""   Later where? Please provide a reference.

(7) Theorem 3.4, The assumption of existence of experts such that Y^S is a sufficient statistic for Y: For instance, suppose there are 10 experts who all have a 0.999 probability of correctness (assume symmetric confusion matrices) and there are 5 non-experts who have a 0.001 probability of correctness and even if we suppose all are mutually independent given the true label, then does this satisfy this sufficient statistic assumption? This appears to be a very strong assumption, but perhaps the authors have better intuition?

(8) The experiments comprise only some simulations. The main point of experiments (particularly in the absence of any theoretical results) towards bolstering the paper is to ensure that the assumptions are at least somewhat reasonable. I believe there are several datasets collected from Amazon Mechanical Turk available online? Otherwise, would it be possible to run realistic experiments on some crowdsourcing platforms?
","The reviewer provides constructive criticism and requests clarifications, indicating a neutral-to-positive stance. While they haven't given a final evaluation, the initial ""I like the idea"" and the detailed questions suggest they are engaged and see potential in the work. The language is formal and polite throughout, using phrases like ""I would suggest"" and ""perhaps the authors have better intuition."" There's no harsh language or dismissive tones.",50.0,80.0
"1) Summary
This paper proposes a hierarchical reinforcement learning (HRL) method for visual motor control of humanoid agents. The method is decomposed into a high-level controller that takes in visual input and proprioceptive information, and a low-level controller (they compare may ways of doing this) that takes care of the agent’s motor control. In experiments, the proposed method is tested on a variety of RL tasks where the many low-level controllers presented in the paper are compared against each other.

2) Pros:
+ Novel high-level controller that takes in front-view visual information
+ Novel multi-policy low level controller
+ Interesting experimental section

3) Cons:
Numerical comparison to previous methods:
- The only issue I found with this paper is that there is no comparison with other methods. Even if the other methods do not take in front-view visual input, it would be nice to compare with them. Maybe visual inputs results in better high-level controller? Or even show that performance is similar would be an interesting result.

4) Comments:
Jerky transitions in switching controller:
- Due to the fact that one policy takes over after each other based on the high-level controller choice, there is a jerk artifact that shows when the policies are being changed/executed. Did you guys try to add a connection in feature space between policies rather than only passing the state of the agent? This may be able to help with that artifact that sampling noise adds to the actions. Can the authors comment on this?

Steerable controller limited rotation:
- From observing the steerable controller policy in action, it seems the policy learned a steering that is somewhat independent of what the limbs are doing. Maybe adding a mechanism where the leg motion intensity depends more on the direction of movement could be a way to fix the issue where this policy moves to fast for the turning it tries to do. Maybe an energy based objective to minimize the torques or something in that line.

4) Conclusion:
To the best of my knowledge, this paper proposes a novel interesting method for modeling humanoid motor skills with front-view visual input. However, as mentioned above, the paper lacks of numerical comparisons with other methods, and only compares against its own variations which is more of an ablation study. I am willing to increase my review score if the authors successfully address the concerns mentioned above","The reviewer acknowledges the novelty of the work and finds the experimental section interesting. However, they point out a significant flaw - the lack of comparison with existing methods. While the reviewer provides constructive criticism and suggestions for improvement, the absence of external comparisons significantly impacts their perception of the paper.",50.0,80.0
"PAPER SUMMARY:

This paper proposes a new POVI method for posterior inference in BNN. Unlike existing POVI techniques that optimize particles in the weight space which often yields sub-optimal results on BNN due to its over-parameterized nature, the new POVI method aims to maintain and update particles  directly on the space of regression functions to overcome this sub-optimal issue.

NOVELTY & SIGNIFICANCE:

In general, I am inclined to think that this paper has made an important contribution with very promising results but I still have doubts in the proposed solution technique (as detailed below) and am not able to converge to a final rating at this point.

TECHNICAL SOUNDNESS:

The authors claim that the new POVI technique operates directly on the function-space posterior to sidestep the over-parameterized issue of BNN but ultimately each function particle is still identified by a weight particle (as detailed in Eq. (2)). In terms of high-level ideas, I am not sure I understand the implied fundamental differences between this work and SVGD and how significant is it.

On the technical level, the key difference between the proposed work and SVGD seems to be the particle update equation in (2): The gradient flow is multiplied with the derivative of the BNN evaluated at the corresponding weight particle (in SVGD, the gradient flow was used alone). The authors then mentioned that this update rule results from minimizing the difference between f(X, theta) and f(X, theta) + \epsilon * v(f(., theta))(X). I do not follow this step -- please elaborate.

The theoretical justification that follows Eq. (3) is somewhat incoherent: What is \Epsilon(q(f(x)))? This has not been defined before or anywhere in the main text. Furthermore, the paragraph that follows the theoretical justification implies the computation of the gradient flow in (3) involves the likelihood term -- why is that?

In Algorithm 1, why do we sample from both the training set and some measure \mu? I am sure there must be a reason for this but I could not find it anywhere except for a short statement that ""for convenience, we choose \mu in such a way that samples from \mu always consists a mini-batch from X"". Please elaborate.

Will the proposed POVI converge?

CLARITY:

I think this paper has clarity issue with the technical exposition. The explanation tends to be very limited and even appear coherent at important points. For example, see
my 3rd point above. 
","The reviewer expresses a positive sentiment initially, acknowledging the paper's potential significance and promising results. However, they also raise several concerns regarding the technical soundness and clarity of the paper, particularly about the novelty of the proposed method, the theoretical justification, and the clarity of explanations. The numerous questions and requests for elaboration indicate a lack of clarity. The reviewer's tone remains professional and polite throughout, focusing on constructive criticism and seeking clarification rather than resorting to harsh language.",40.0,70.0
"The main contribution of the paper are methods for propagating approximate uncertainty in neural networks through max and argmax layers. The proposed methods are explained well. The paper is clearly written. The methods are validated in small scale experiments and seem to work well.

The proposed approach is not much more accurate than Monte Carlo dropout, but is more computationally efficient. The standard way of efficiently predicting at test time with a dropout-trained network is to simply scale the weights. Could the authors try calibration on networks of this type and compare against the proposed method with calibration? (i.e. scale the predicted logits of the standard test-time network to be on the same scale as the logits under your approach)","The review starts with positive statements - ""well explained"", ""clearly written"", ""seem to work well"". However, the reviewer then points out that the proposed method is ""not much more accurate"" than a simpler approach. While this is not a negative statement per se, it does highlight a limitation. The suggestion is constructive and neutral in tone. Overall, the sentiment is mildly positive due to the initial praise.",60.0,80.0
"Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this.

Summary:
The paper proposes an HRL system in which the mutual information of the latent (option) variable and the state-action pairs is approximately maximized. To approximate the mutual information term, samples are reweighted based on their estimated advantage. TD3 is used to optimize the modules of the system. The system is evaluated on continuous control task from OpenAI gym and rllab.

For the most part, the paper is well-written and it provides a good overview of related work and relevant terminology. The experiments seem sound even though the results are not that impressive. The extra analysis of the option space and temporal distribution is interesting. 

Some parts of the theoretical justification for the method are not entirely clear to me and would benefit from some clarification. Most importantly, it is not clear to me why the policy in Equation 7 is considered to be optimal. Given some value or advantage function, the optimal policy would be the one that picks the action that maximizes it. The authors refer to earlier work in which similar equations are used, but in those papers this is typically in the context of some entropy maximizing penalty or KL constraint. A temperature parameter would also influence the exploration-exploitation trade-off in this ‘optimal’ policy. I understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it. However, this is not the policy that is used to generate behavior. In short, the paper is clear enough about how the method is constructed but it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from.

HRL is an interesting area of research with the potential to learn complicated behaviors. However, it is currently not clear how to evaluate the importance/usefulness of hierarchical RL systems directly and the tasks in the paper are still solvable by standard systems. That said, the occasional increase in sample efficiency over plain TD3 looks promising. It is somewhat disappointing that the number of beneficial option is generally so low. To get more insight in the methods it would have been nice to see a more systematic ablation of related methods with different mutual information pairings (action or state only) and without the advantage weighting. Could it be that the number of options has to remain limited because there is no parameter sharing between them? It would be interesting to see results on more challenging control problems where the hypothesized multi-modal advantage structure is more likely to be present.

All in all I think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification. In addition, experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.

Minor issues/typos:
- Contributions 2 and 3 have a lot of overlap.
- The ‘o’ in Equation 2 should not be bold font. 
- Appendix A. Shouldn’t there be summations over ‘o’ in the entropy definitions?


","The review starts with a positive note, acknowledging the authors' effort and improvements made. The reviewer finds the idea interesting and acknowledges its merit. However, they also express reservations about the theoretical clarity and limited scope of experiments. The tone is critical but constructive, suggesting specific areas for improvement. Overall, the sentiment leans towards the positive side due to the initial appreciation and constructive criticism, but not overly positive due to the reservations expressed.",60.0,70.0
"In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks. In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features. The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated. The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks.

Overall, the paper is well written with clarity. Experimental setup is easy to understand. The authors provided code, which could help other researchers reproduce their result.

Weaknesses: 

1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards. The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find. But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.

2) it is surprising that random features perform so well in the experiments. The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.","The reviewer highlights several positive aspects like clarity, experimental setup, and code availability. While they point out weaknesses, these are framed as suggestions for improvement rather than harsh criticisms. The tone throughout is constructive and objective.",60.0,80.0
"The paper proposes to use a differentiable drawing environment to synthesize images and provides information about some initial experiments. 

Not yet great about this paper: 
 - the paper feels premature: There is a nice idea, but restricting the drawing environment to be 
 - Some of the choices in the paper are a bit surprising, e.g. the lines in the drawing method are restricted to be at most 16 points long. If you look at real drawing data (e.g. the quickdraw dataset: https://quickdraw.withgoogle.com/data) you will find that users draw much longer lines typically. 
EDIT: the new version of the paper is much better but still feels like a bit incomplete. I personally would prefer a more complete evaluation and discussion of the proposed method. 
 - the entire evaluation of this paper is purely qualitative (and that is not quite very convincing either). I feel it would be important for this paper to add some quantitative measure of quality. E.g. train an MNIST recognizer synthesized data and compare that to a recognizer trained on the original MNIST data. 
 - a proper discussion of how the proposed environment is different from the environment proposed by Ganin et al (Deepmind's SPIRAL) 

Minor comments: 
 - abstract: why is it like ""dreaming"" -> I do agree with the rest of that statement, but I don't see the connection to dreaming
 - abstract: ""upper agent"" -> is entirely unclear here. 
 - abstract: the footnote at the end of the abstract is at a strange location
 - introduction: and could thus -> and can thus 
 - introduction: second paragraph - it would be good to add some citations to this paragraph. 
 - resulted image-> resulting image
 - the sentence: ""We can generate....data is cheap"" - is quite unclear to me at this time. Most of it becomes clearer later in the paper - but I feel it would be good to put this into proper context here (or not mention it)
 - we obtained -> we obtain
 - called a generator -> call a generator 
 - the entire last paragraph on the first page is completely unclear to me when reading it here. 
 - equations 1, 2: it's unclear whether coordinates are absolute or relative coordinates. 
- fig 1: it's very confusing that the generator, that is described first is represented at the right. 
 - sec 3.2 - first line: wrong figure reference - you refer to fig 2 - but probably mean fig 1
 - page 3 bottom: by appending the encoded color and radius data we have a feature with shape 64x64xn -> I don't quite see how this is true. The image was 64x64 -> and I don't quite understand why you have a color/radius for each pixel. 
 - sec 3.3 - it seem sthat there is a partial sentence missing 
 - sec 3.4 - is it relevant to the rest of the paper that the web application exists (and how it was implemented). 
 - fig 2 / fig 3: these figures are very hard to read. Maybe inverting the images would help. Also fig 3 has very little value.  ","The review is critical of the paper, pointing out a lack of quantitative evaluation and comparing it unfavorably to existing work.  While the language is direct and points out flaws, it does not resort to personal attacks and offers concrete suggestions for improvement. The reviewer also acknowledges improvements in a revised version, indicating an openness to the authors' ideas.",-20.0,60.0
"This paper presents a dialogue response generation model based on the framework of adversarial autoencoder. Specifically, the proposed model uses an autoencoder to encode and decode a response in a dialogue, conditioning on the context of the dialogue. The RNN encoded context is used as the prior of the latent variable in the autoencoder, and the whole dialogue (context + response) is used to infer the posterior of the latent variable. The inference is done by the adversarial training to match the prior and the posterior of the latent variable. Besides constructing the prior with a single Gaussian, the variant of the proposed model is also proposed where the prior is constructed with a Gaussian mixture model.

My comments are as follows:

1. The paper is well-written and easy to follow.

2. The experiments seem quite strong and the compared models are properly selected. I'm not an expert in the specific area of the dialogue generation. But to me, the results seem convincing to me. 

3. The usage of the Wasserstein distance in the proposed model does not make sense to me. Both the adversarial training in AAE and minimising the Wasserstein distance are able to match the prior and posterior of the latent variable. If the former is used in the proposed model, then how is the Wasserstein distance used at the same time? I also checked Algorithm 1 and did not find how the Wasserstein distance comes in. This is the first question that needs the authors to clarify.

4. To me, the significance of this paper mainly goes to combining several existing frameworks and tricks into the specific area of dialogue generation. Although the empirical results show the proposed model outperforms several existing models, my concern is still on the originality of the paper. Specifically, one of the main contributions goes to using the Gaussian mixture to construct the prior, but this is not a whole new idea in VAE or GAN, nor using the Gumbel trick. 

5. It is good to see that the authors showed some comparisons between DialogWAE and DialogWAE-GMP, letting us see GMP does help the performance. But a minor concern is that it seems hard to identify which part makes DialogWAE get superior performance than others. Are all the models running with the same experiment settings including the implementation of the RNNs?","The reviewer provides both positive and negative feedback. They praise the paper's clarity, experimental strength, and the choice of compared models. However, they also raise concerns about the originality and clarity regarding the use of Wasserstein distance. The overall tone is neutral, leaning slightly towards positive due to the initial praise.",20.0,80.0
"The paper introduces RCPO, a model-free deep RL algorithm for learning optimal policies that satisfy some per-state constraint on expectation. The derivation of the algorithm is quite straightforward, starts from the definition of constrained optimization problem, and proceed by forming and optimizing the Lagrangian. Additionally, a value function for the constraint is learned. The algorithm is only compared to a baseline optimizing the Lagrangian directly using Monte-Carlo sampling.

The paper has two major problems. First, while the derivation of the method makes intuitively sense, it is supported by vaguely stated theorems, which mixes rigorous guarantees with practical approximations. For example, Equation 4 assumes strong duality. How would the result change if weak duality was used instead? The main result in Theorem 1 makes the assumption that dual variable is constant with respect the policy, which might be true in practice, but it is not obvious how the approximation affects the theory. Further, instead of simply referring to prior convergence results, I would strongly suggest including the exact prior theorems and assumptions in the appendix.

The second problem is the empirical validation, which is incomplete and misleading. Constrained policy optimization is not a new topic (e.g. work by Achiam et al.), so it is important to compare to the prior works. It is stated in the paper that the prior methods cannot be used to handle mean value constraints. However, it would be important to include experiments that can be solved with prior methods too, for example the experiments in Achiam at al. for proper comparison. The results in Table 2 are confusing: what makes the bolded results better than the others? If the criterion is highest return and torque < 25%, then \lambda=0.1 should be chosen for Hopper-v2. Also, The results seem to have high variance, and judging based on Table 2 and Figure 3, it is not obvious how well RCPO actually works.

To summarize, while the topic is undoubtedly important, the paper would need be improved in terms of better differentiating the theory from practice, and by including a rigorous comparison to prior work.

Minor points:
- What is exactly the difference between discounted sum constraint and mean value constraint?
- Could consider use colors in Table 1.
- Section 4.1.: What does “... enables training using a finite number of samples” exactly mean in this case?
- Table 2: The grid for \lambda is too sparse. 
- Proof of Theorem 1: What does it mean \theta to be stable?
- Proof of Theorem 2: “Theorem C” -> “Theorem 1”
","The review is critical of the paper, pointing out significant weaknesses in both the theoretical grounding and the empirical evaluation. While acknowledging the importance of the topic, the reviewer suggests major revisions are needed. The language used is direct and professional, typical of academic peer reviews. There's no personal criticism or disrespectful tone. Therefore, the sentiment is rather negative, and the politeness is neutral.",-40.0,0.0
"Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space.


The paper proposes a latent tree superstructure for the latent space of VAE’s. The idea itself is novel and interesting, and could have major impact in learning structured manifolds.

The overall presentation of the method is direct but slightly confusing. It seems that the zb grouping corresponds to different dimensions of the full z_i-vector of a single data point x_i. This should be made more explicit. 

The method itself has three levels of groupings: the zb’s, the conditioned variables Yb, and the connections between the Y’s. The method is also called a  Bayesian Network, but the paper seems to avoid defining it as a BN. I wonder if the method could be presented in a simpler form, if all the structure is necessary, and if the method could be defined directly as a BN. For instance, why do the Y’s have to have a hierarchical tree structure, wouldn’t a “flat” grouping into zb's be sufficient? 

In eq 2 the p(z) is defined as a mixture of Y-conditioned Gaussians, while in eq 4 its defined in the conventional encoder form N(z ; mu_x, sigma_x). These forms don’t seem to be compatible with each other. The term H seems to be entropy, but its not explained. It can’t be computed if we use the eq 2 definition of p(z). The interplay between these two structures is unclear. Furthermore, in fig 1 the tree is showed as a network (no arrows), while in fig 2 its a tree. I can’t find the definition for the dependencies P(Y | Y’), are these simply conditional density tables, or are they implicit? I also can’t see how are the \Sigma_{yb} defined. Are they of full rank? What is their dimension?

The inference sections are well motivated and efficient techniques are used. 

The synthetic experiment has 4 dimensional “z”, but the “W” matrix is 10x2, these do not match. What is the connection between Y_1 and Y_2 (in fig4 there is a dependency between)? Why is the dependency undirected if the model is a tree? The fig4b does not show ground truth to assess how well the model fits. The experiment should also include comparisons to the mentioned earlier works, and show how they perform. Why is there an arrow from the green scatter to the z3/z4? The main problem of the synthetic example is that it does not demonstrate why the tree structure learning is useful. The experiment should highlight a case where there is a natural latent tree structure corresponding to some realistic phenomena in real datasets.

The section 4.3. shows that the proposed method does find better representations of the MNIST than VAE, but does not mention that there are numerous extended VAE methods (and others) that would perform better than the LTVAE here. Those should be at least acknowledged, and preferably compared to.

The main results of the paper are very good with great performance in clustering, and the facets and clusters look great. The system has clearly learnt meaningful latent structures.

There are no learning curves or running time analyses. One would expect the proposed method to be slow with multiple levels of inference (tree structure, tree parameters, AE networks), and this should be discussed. How large datasets can it handle?

Overall the paper proposes a BN-style structure on VAE latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.","The review is positive overall, praising the novelty and potential impact of the proposed method and acknowledging the strong results and meaningful latent structures learned. However, it also raises several concerns and suggestions for improvement, indicating a need for clarification, simplification, and further experimentation. The reviewer's tone is direct and critical but remains professional and constructive throughout.",60.0,60.0
"In this submission, the authors present a variational smoothing interpretation of the data noising approach presented in (Xie et al., 2017). Although the theoretical coverage of the problem gives interesting insights. However, a comparison to related work w.r.t. alternative regularization approaches is missing. Similarly, the perplexity values reported in the experimental results on Penn Treebank are far away from state-of-the-art results published by many competitors on this task, e.g. see the current state-of-the-art results on Penn Treebank by (Yang et al., 2017, https://arxiv.org/pdf/1703.02573.pdf and references therein). It is bad practice to ignore existing work completely like this. The interesting question here would be, inhowfar the presented smoothing/regularization methods are complementary to existing approaches, and if the presented methods do provide improvements on top of these.

Finally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
","The review starts with a mildly positive statement acknowledging the interesting insights of the paper. However, it quickly transitions into a series of negative points, criticizing the lack of comparison with related work, the subpar results, and the limited scope of the evaluation. The phrase ""It is bad practice to ignore existing work completely like this"" stands out as particularly harsh. The concluding remarks further emphasize the shortcomings and lack of practical relevance. Overall, the tone suggests skepticism and a lack of enthusiasm towards the paper.",-30.0,40.0
"This paper presents an improvement on the local/derivative-free learning algorithm equilibrium propagation. Specifically, it trains a feedforward network to initialize the iterative optimization process in equilibrium prop, leading to greater stability and computational efficiency, and providing a network that can later be used for fast feedforward predictions on test data. Non-local gradient terms are dropped when training the feedforward network, so that the entire system still doesn't require backprop. There is a neat theoretical result showing that, in the neighborhood of the optimum, the dropped non-local gradient terms will be correlated with the retained gradient terms.

My biggest concern with this paper is the lack of significant literature review, and that it is not placed in the context of previous work. There are only 12 references, 5 of which come from a single lab, and almost all of which are to extremely recent papers. Before acceptance, I would ask the authors to perform a literature search, update their paper to include citations to and discussion of previous work, and better motivate the novelty of their paper relative to previous work. Luckily, this is a concern that is addressable during the rebuttal process! If the authors perform a literature search, and update their paper appropriately, I will raise my score as high as 7.

Here are a few related topic areas which are currently not discussed in the paper. *I am including these as a starting point only! It is your job to do a careful literature search. I am completely sure there are obvious connections I'm missing, but these should provide some entry points into the citation web.*
- The ""method of auxiliary coordinates"" introduces soft (often quadratic) couplings between post- and pre- activations in adjacent layers which, like your distributed quadratic penalty, eliminate backprop across the couplings. I believe researchers have also done similar things with augmented Lagrangian methods. A similar layer-local quadratic penalty also appears in ladder networks.
- Positive/negative phase (clamped / unclamped phase) training is ubiquitous in energy based models. Note though that it isn't used in classical Hopfield networks. You might want to include references to other work in energy based models for both this and other reasons. e.g., there may be some similarities between this approach and continuous-valued Boltzmann machines?
- In addition to feedback alignment, there are other approaches to training deep neural networks without standard backprop. examples include: synthetic gradients, meta-learned local update rules, direct feedback alignment, deep Boltzmann machines, ...
- There is extensive literature on biologically plausible learning rules -- it is a field of study in its own right. As the paper is motivated in terms of biological plausibility, it would be good to include more general context on the different approaches taken to biological plausibility.

More detailed comments follow:

Thank you for including the glossary of symbols!

""Continuous Hopfield Network"" use lowercase for this (unless introducing acronym)

""is the set non-input"" -> ""is the set of non-input""

""$\alpha = ...$ ... $\alpha_j \subset ...$"" I could not make sense of the set notation here.

would recommend using something other than rho for nonlinearity. rho is rarely used as a function, so the prior of many readers will be to interpret this as a scalar. phi( ) or f( ) or h( ) are often used as NN nonlinearities.

inline equation after ""clamping factor"" -- believe this should just be C, rather than \partial C / \partial s.
Move definition of \mathcal O up to where the symbol is first used.

text before eq. 7 -- why train to approximate s- rather than s+? It seems like s+ would lead to higher accuracy when this is eventually used for inference.

eq. 10 -- doesn't the regularization term also decrease the expressivity of the Hopfield network? e.g. it can no longer engage in ""explaining away"" or enforce top-down consistency, both of which are powerful positive attributes of iterative estimation procedures.

notation nit: it's confusing to use a dot to indicate matrix multiplication. It is commonly used in ML to indicate an inner product between two vectors of the same shape/orientation. Typically matrix multiplication is implied whenever an operator isn't specified (eg x w_1 is matrix multiplication).

eq. 12 -- is f' supposed to be h'? And wasn't the nonlinearity earlier introduced as rho? Should settle on one symbol for the nonlinearity.

This result is very cool. It only holds in the neighborhood of the optimum though. At initialization, I believe the expected correlation is zero by symmetry arguments (eg, d L_2 / d s_2 is equally likely to have either sign). Should include an explicit discussion of when this relationship is expected to hold.

""proportional to"" -> ""correlated with"" (it's not proportional to)

sec. 3 -- describe nonlinearity as ""hard sigmoid""

beta is drawn from uniform distribution including negative numbers? beta was earlier defined to be positive only.

Figure 2 -- how does the final achieved test error change with the number of negative-phase steps? ie, is the final classification test error better even for init eq prop in the bottom row than it is in the top?

The idea of initializing an iterative settling process with a forward pass goes back much farther than this. A couple contexts being deep Boltzmann machines, and the use of variational inference to initialize Monte Carlo chains

sect 4.3 -- ""the the"" -> ""to the""","The review is generally positive in its assessment of the paper's contribution, highlighting the theoretical result as ""very cool."" However, the reviewer expresses significant concerns about the lack of comprehensive literature review and positions the paper's novelty as potentially limited without it. The reviewer provides a substantial list of related work for the authors to consider, indicating a desire to see the paper contextualized within the broader field. The remaining comments are largely constructive suggestions for improvement. Overall, the feedback aims to guide the authors towards strengthening the paper's foundation and impact. ",40.0,90.0
"The authors propose a number of methods to identify individual important neurons in a machine translation system. The crucial assumption, drawn from the computer vision literature, is that important neurons are going to be correlated across related models (e.g. models that are trained on different subsets of the data). This hypothesis is validated to some extent: erasing the neurons that scored highly on these measures reduced BLEU score substantially. However, it turns out that most of the activation of the important neurons can be explained using sentence position. Supervised classification experiments on the important neurons revealed neurons that tracked properties such as the span of parentheses or word classes (e.g., auxiliary verbs, plural nouns, etc).

Strengths:
* The paper is very well written and provides solid intuitions for the methods proposed.
* The methods seem promising, and the degree of localist representation is striking.
* The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed).
* There is a correlation between the neuron importance metrics proposed in the paper and the effect on BLEU score of erasing those neurons from the network (of course, it’s not clear what particular linguistic properties are affected by this erasure - the decrease BLEU may reflect inability to track specific word tokens more than any higher-level linguistic property).

Weaknesses:
* It wasn't clear to me why the neurons that track particular properties (e.g., being inside a parentheses) couldn't be identified using a supervised classifier to begin with, without first identifying ""important"" neurons using the unsupervised methods proposed in the paper. The unsupervised methods do show their strength in the more exploratory visualization-based analyses -- as the authors point out (bottom of p. 6), the neuron that activates on numbers but only at the beginning of the sentence does not correspond to a plausible a-priori hypothesis. Still, most of the insight in the paper seems to be derived from the supervised experiments.
* The particular linguistic properties that are being investigated in the classification experiments are fairly limited. Are there neurons that track syntactic dependencies, for example?
* I wasn't sure how the GMMs (Gaussian mixture models) for predicting linguistic properties from neuron activations were set up.
* It's nice to see that individual neurons function as knobs that can change the gender or tense of the output (with varying accuracy). At the same time, I was unable to follow the authors' argument that this technique could be used to reduce gender bias in MT.
* I wasn't sure what insight was gained from the SVCCA analyses -- this method seems to be a bit of a distraction given the general focus on localist vs. distributed representation. In general, I didn’t come away with an understanding of the pros and cons of each of the methods.","The review is mostly positive. It highlights the paper's strengths, such as its clear writing, promising methods, and interesting findings. While it raises valid weaknesses and questions, it does so constructively and with suggestions for improvement. The language is formal and objective, typical of academic peer reviews.",60.0,80.0
"In this paper, the authors studied zeroth order sign SGD. Sign SGD is commonly used in adversarial example generation. Compared to sign SGD, zeroth-order sign SGD does not require the knowledge of the magnitude of the gradient, which makes it suitable to optimize black-box systems. The authors studied the convergence rate of zeroth-order sign SGD, and showed that under common assumptions, zero-order sign SGD achieves O(sqrt(d/T)) convergence rate, which is slower than sign SGD by a factor of sqrt(d). However, sign SGD requires an unrealisitcally large mini-batch size, which zeroth-order sign SGD does not. The authors demonstrated the performance of zeroth-order sign SGD in numerical experiments.

Overall, this is a well written paper. The convergence property of the zeroth-order sign SGD is sufficiently studied. The proposal seems to be useful in real world tasks.

Weaknesses: 
1) out of curiosity, can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O(T)? This could help us better compare zeroth-order sign SGD and sign SGD.
2) Figure 2 is too small to be legible. Also, it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset. Is it true? If so, it would be beneficial to have a qualitative explanation of such behavior.","The reviewer states that the paper is ""well-written"" and the proposal ""seems to be useful in real-world tasks."" This indicates a positive sentiment. The weaknesses mentioned are framed as open questions and suggestions rather than harsh criticisms, suggesting a polite tone.",75.0,75.0
"This is a reasonable paper based on a simple intuition. The authors have noticed that some of the state of the art methods (they use Li et al - ICML18 as the main reference) are using only some simple normalization for improving the transfer learning and as such they propose preserving the outer layer output of the target network and aligning it with the one of the source network. On top of that they also propose modeling the difference of feature maps considering an attention mechanism obtain through supervised learning. 

The idea in itself is interesting and valuable. However, I have had some difficulty in understanding precisely how the ""behavior"" is really regularized. While I understand what is depicted in Figure 1 I'm not completely sure this really means that the network behavior is regularized rather than simply correlating the two outputs. In the evaluation, the authors present in Figure 4 some qualitative examples but I would have expected to see some quantitative evaluation of this. I would have liked to see experiments on some larger datasets that are commonly used in computer vision (e.g., Caltech 256 is rather old even if it has been used in Li et al.). The quantitative results in Table 1 and 2 indicate some slight improvement but I'm not completely convinced that this is really significant in the end. The results in Figure 4 tend to show that with the attention mechanism there is a central bias and most of the results tend to be concentrated on the center of the image (in this case the result might also be correct but the examples presented are not too eloquent). 
","The review starts with a positive note, acknowledging the paper's value and interesting idea. However, it then raises several concerns about the methodology, clarity, and significance of the results. The reviewer suggests improvements, indicating a desire to see the paper strengthened rather than rejected. The language used is professional and not dismissive, suggesting a neutral-to-slightly negative but constructive criticism.",20.0,60.0
"This paper introduced a new stochastic layer termed variance layer for Bayesian deep learning, where the posterior on weight is a zero-mean symmetric distribution (e.g., Gaussian, Bernoulli, Uniform). The paper showed that under 3 different prior distributions, the Gaussian Dropout layer can converge to variance layer. Experiments verified that it can achieve similar accuracies as conventional binary dropout in image classification and reinforcement learning tasks, is more robust to adversarial attacks, and can be used to sparsify deep models.

Pros:
(1)	Proposed a new type of stochastic layer (variance layer)
(2)	Competitive performance on a variety of tasks: image classification, robustness to adversarial attacks, reinforcement learning, model compression
(3)	Theoretically grounded algorithm

Cons:
(1)	My main concern is verification. Most of the comparisons are between variance layer (zero-mean) and conventional binary dropout, while the main argument of the paper is that it’s better to set Gaussian posterior’s mean to zero. So in all the experiments the paper should compare zero-mean variance layer against variational dropout (neuron-wise Eq. 14) and sparse variational dropout (additive Eq. 14), where the mean isn’t zero.
(2)	The paper applies variance layers to some specific layers. Are there any guidelines to select which layers should be variance layers?

Some minor issues:
(1)	Page 4, equations of Gaussian/Bernoulli/Uniform variance layer, they should be w_ij=…, instead of q(w_ij)= …
(2)	What’s the prior distribution used in the experiment of Table 1?

","The review acknowledges the merits of the paper, highlighting its novelty, performance, and theoretical grounding. This points towards a positive sentiment. However, it also raises a 'main concern' regarding the verification process, suggesting that the core argument lacks sufficient comparative analysis. This concern, along with other minor issues, moderates the overall positivity. The language used is constructive and professional, focusing on specific areas of improvement without resorting to harsh or disrespectful tones.",60.0,80.0
"This paper studied learning unsupervised node embeddings by considering the structural properties of networks. Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings. 

Strength:
- important problem and interesting idea
- the proposed approach seems to be effective according to the experiments
Weakness:
- some parts of the paper are quite unclear
- the complexity of the proposed algorithm seems to be very high
- the data sets used in the experiments are very small

Details:
-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right？
- In Table 2 and 3, how are the degree and block information leveraged into the model?
","The review acknowledges the strengths of the paper, such as the importance of the problem and the effectiveness of the proposed approach. However, it also points out significant weaknesses, including unclear explanations, high algorithm complexity, and limited experimental data. The use of phrases like ""quite unclear"" and ""seems to be very high"" suggests a negative inclination. While the reviewer provides constructive criticism, the overall tone leans towards the critical side. ",-10.0,60.0
"The authors study the problem of when the linear interpolant between two random variables follows the same distribution. This is related to the prior distribution of an implicit generative model. In the paper, the authors show that the Cauchy distribution has such a property, however due to the heavy-tails is not particularly useful. In addition, they propose a non-linear interpolation that naturally has this property.

Technically the paper in my opinion is solid. Also, the paper is ok-written, but I think it needs improvements (see comments).

Comments:

#1) In my opinion the motivation is not very clear and should be improved. In the paper is mentioned that the goal of shortest path interpolation is to get smooth transformations. So, in principle, I am really skeptical when the linear interpolant is utilized as the shortest path. Even then, what is the actual benefit of having the property that the linear interpolants follow the same distribution as the prior? How this is related to smoother transformations? What I understand is that, if we interpolate between several random samples, we will get less samples near the origin, and additionally, these samples will follow the prior? But how this induces smoothness in the overall transformation? I think this should be explained properly in the text i.e. why is it interesting to solve the proposed problem.

#2) From Observation 2.2. we should realize that the distribution matching property holds if the distribution has infinite mean? I think that this is implicitly mentioned in Section 2.2. paragraph 1, but I believe that it should be explicitly stated.

#3) Fig.1 does not show something interesting, and if it does it is not explained. In Fig. 2 I think that interpolations between the same images should be provided such that to have a direct comparison. Also, in Fig. 3 the norm of Z can be shown in order to be clear that the Cauchy distribution has the desired property. 

#4) Section 2.2. paragraph 6, first sentence. Here it is stated that the distribution ""must be trivial or heavy-tailed"". This refers only to the Cauchy distribution? Since earlier the condition was the infinite mean. How these are related? Needs clarification in the text.

#4) In Figure 4, I believe that the norms of the interpolants should be presented as well, such that to show if the desired property is true. Also in Figure 5, what we should see? What are the improvements when using the proposed non-linear interpolation?


Minor comments:

#1) Section 1.2. paragraph 2. For each trained model the latent space usually has different structure e.g. different untrained regions. So I believe that interpolations is not the proper way to compare different models.

#2) Section 1.3 paragraph 1, in my opinion the term ""pathological"" should be explained precisely here. So it makes clear to the reader what he should expect.

#3) Section 2.2. paragraph 2. The coordinate-wise implies that some Z_i are near zero and some others significantly larger? 

In generally, I like the presented analysis. However, I do not fully understand the motivation. I think that choosing the shortest path guarantees smooth transformations. I do not see why the distribution matching property provides smoother transformations. To my understanding, this is simply a way to generate less samples near the origin, but this does not directly means smoother transformations of the generated images. I believe that the motivation and the actual implications of the discussed property have to be explained better.","The reviewer acknowledges the technical soundness of the paper and finds the writing acceptable. However, they express significant concerns about the clarity of the motivation and the connection between the proposed property and smoother transformations. The numerous comments and questions regarding the paper's core arguments suggest a lack of enthusiasm and a need for substantial revisions. Therefore, the sentiment leans towards the negative side, although not extremely so. The language used is polite and professional, employing constructive criticism and suggestions for improvement.",-25.0,75.0
"This paper investigates speaker adaption with a few samples based on an existing (pre-trained) multi-speaker TTS system. The three approaches in this paper are almost the same as the voice cloning work in Arik et al. (2018). However, it is still very beneficial to demonstrate these approaches for linguistic feature conditioned WaveNet.

Detailed comments:

1) This manuscript is not self-contained, as it omits the important details for acquiring linguistic features (e.g., phoneme duration model) and fundamental frequency (F0) at training and test time. The only information is that it uses existing model (Zen et al., 2016) to predict linguistic features and F0. What type of linguistic features are used in this work? Is the existing model (Zen et al., 2016) trained on the same training set as WaveNet model?

2) It seems the only speaker-dependent part of the system is the embedding table for WaveNet. Actually, both linguistic features (e.g., phoneme duration) and fundamental frequency sequence are highly speaker-dependent. The authors normalize F0 to make it as speaker-independent as possible. What about the speaker-dependent linguistic features? Why not keep them as speaker dependent, and do speaker-adaption for the new speaker at inference?

3) In my opinion, it’s a bit superfluous to name fine tuning as non-parametric few-shot adaption, and auxiliary network (speaker encoding) as parametric few-short adaption. Both ideas are quite natural as in Arik et al. (2018).

4) The abbreviations SEA-ALL, SEA-EMB and SEA-ENC are appeared without explanation. 

5) It would be better to provide more details about early termination criterion in Section 3.1. Is it simply the validation loss?

6) In Table 1, the MOS from Arik et al. (2018) and Jia et al. (2018) are not comparable. The experimental settings are different. Perhaps more importantly, these MOS evaluations are done by different group of people.

7) In Section 5.3, Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.

Overall, this is a good work with limited novelty but solid results. However, it can be improved in many ways as detailed  in previous comments. I would like to raise my rating if these comments can be addressed properly.","The review acknowledges the work as ""good"" and the results as ""solid."" While it points out limitations in novelty, it also suggests the rating could be higher if the comments are addressed. This suggests an overall positive sentiment. The language used is formal, objective, and provides constructive criticism without resorting to harsh or disrespectful tones, indicating politeness.",60.0,80.0
"The paper proposes a method to classify vulnerable and non-vulnerable binary codes where each data instance is a binary code corresponding to a sequence of machine instructions. The contributions include the creation of a new dataset for binary code vulnerability detection and the proposition of an architecture based on a supervised adaptation of variational auto-encoder, built upon the result of a sequential information,  
and using a regularization term to better discriminate positive from negative data. An experimental evaluation on the data proposed is presented, including several baselines, the results show the good behavior of the method.

Pros:
-Presentation of new application of representation learning models
-Construction of a new dataset to the community for binary software vulnerability detection
-The proposed model shows a good performance
Cons:
-The presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.
-From the last remark, it is unclear for me if the dataset is representative of binary code vulnerability problem
-The proposed architecture is reasonable and maybe new, but I find it natural with respect to existing work in the literature.

Comments:

-If providing a new dataset is a key contribution, the authors should spend more time to present the dataset. What makes it interesting/novel/challenging must be clarified. 
This dataset seems actually built from the existing NDSS18 dataset for source code vulnerability detection. If I understood correctly, the authors have compiled (and sometimes corrected) the source to create binaries, then they use the labels in NDSS18 to label the binary codes obtained. 
This a good start and can be useful for the community.
However the notion of vulnerability is not defined and it is difficult for me to evaluate the interest of the dataset.
I am not an expert in the field, but I am not that convinced that vulnerability for binary codes is necessary related to vulnerability that can be detected from source codes.
Indeed, one can think that some vulnerability may appear in binary codes that cannot be detected from source codes: e.g. use of unstable libraries, problems with specific CPU architectures, problems du to different interpretation of standard.

The current version of dataset seems to be a data where one tries to find the vulnerability that can be detected from code. It would be interesting here to know if detecting the vulnerabilities are easier from source code or from binary code.

It could be good if the authors could discuss more this point.

-The architecture proposed by the authors seems to use a sequential model (RNN or other) as indicated in Fig.2, the authors should precise this point.
The architecture is general enough to work on other problems/tasks - which is good - but the authors focus on the binary vulnerability code dataset in the experiments.

If the authors think that their contribution is to propose a general method for sequence classification, it could be good to apply it on other datasets.
Otherwise, something maybe more specific to the task would be useful.
In particular, there is no clear discussion to justify that variational autoencoders are better models for the task selected, it coud be good to argue more about it.

That being said, having non fixed priors and trying to maximize the divergence between positive and negative distributions are good ideas, but finally rather natural.

","The review is mostly positive. The reviewer acknowledges the contributions of the paper, particularly the new dataset and the good performance of the proposed method. However, they also raise valid concerns and suggestions for improvement, especially regarding the dataset's description, representativeness, and the justification for using variational autoencoders. The language used is polite and professional, providing constructive criticism and suggestions.",60.0,80.0
"This paper proposes a new method to imitate expert efficiently. The paper first proposes a way to compute reward function from expert demonstration and uses the log probability to represent this reward function.  Then they find a form of bellman equation that can optimize the reward stably. After the 'Q learning without IRL', an off-policy RL off-pac is applied. So this paper achieves comparable results to GAIL but uses much less data amount. 

clarity:
This paper is clearly written.

originality:
This paper is original.

pros:
Comparable performance with GAIL.
Better performance than Behavioral Cloning
New way of using demonstrations

cons:
Although both the method and the experiments look promising, there is a very simple yet competitive baseline missing. This baseline is also mentioned in the original GAIL paper: you initialize GAIL with BC, and then train GAIL. That's the baseline for a set of fair comparison.
","The review starts with a neutral summary of the paper's contributions. It then praises the paper's clarity and originality, and lists both pros and cons. While it points out a missing baseline, the language remains constructive and suggests an improvement rather than harshly criticizing the work. Overall, the tone is balanced and leans towards the positive.",60.0,70.0
"This paper is built on a simple but profound observation: Frey's bits-back coding algorithm can be implemented much more elegantly when replacing arithmetic coding (AC) with asymmetric numerical systems (ANS), a much more recent development not known at the time, simply due to the fact that it encodes symbols in a stack-like fashion rather than queue-like.

This simple observation makes for an elegantly written paper, with promising results on MNIST. I truly enjoyed reading it, and I'm convinced that it will spark some very interesting further work in the field of compression with latent-variable models.

Having said that, I would like to point out some possible limitations of the proposed approach, which I hope the authors will be able to address/clarify:

1. At the beginning of section 2.1, the authors define the symbols as chained conditionals prod_n p(s_n | s_1 ... s_n-1), which is generally permissible in AC as well as ANS, as long as the decoding order is taken into account. That is, in AC, the symbols need to be encoded starting with the first symbol in the chain (s_1), while in ANS, the symbols must be encoded starting with the last symbol in the chain, because the decoding order is inverted.

In their description of BB-ANS, the authors omit the discussion of conditional chains. It is unclear to me if a conditioning of the symbols is feasible in BB-ANS due to the necessity to maintain a strict decoding order. It would be very helpful if the authors could clarify this, and update the paper accordingly, because this could present a serious limitation. For instance, the authors simply extrapolate the performance of their method to PixelVAE; however, this model is autoregressive, so a conditioning of symbols seems necessary. Similarly, in appendix A, the authors mention the work of Minnen et al. (2018), where the same situation would apply, albeit one probabilistic level higher (on encoding/decoding the latents with an autoregressive prior).

2. Furthermore, in both cases (PixelVAE and Minnen et al.), the symbols (s) and latents (y) are defined as jointly conditioned on each other (i.e., computing the posterior on one element of y requires knowledge of all elements of s, and computing the likelihood on one element of s requires knowledge of all elements of y). This seems to imply that all operations pertaining to one data vector (i.e. to one image) would have to be done in a monolithic fashion, i.e.: first sample all elements of y from the stack, then encode all elements of s, and then encode all elements of y. Hence, if the goal is to compress only one image, the algorithm would never get to the point of reusing the ""bits back"", and the overhead of BB-ANS would be prohibitive. It seems that in the MNIST experiments, the authors avoid this problem by always encoding a large number of images at a time, such that the overhead is amortized.

3. Similarly, although the compression of continuous-valued variables up to arbitrary precision is an exciting development and I do not wish to undermine the importance of this finding, it should be noted that the finer the quantization gets, the larger the potential overhead of the coding scheme will grow. In practice, this would make it necessary to encode more and more images together, in order to still benefit from the method. This would be a good point to make in the discussion.

4. The authors state in the appendix that learned compression methods like Ballé et al. (2018) and Minnen et al. (2018) could be improved by using BB-ANS. The potential gain of BB-ANS for these models seems rather small, though, as the entropy of y must be larger or equal to the entropy of y conditioned on s: H[y] >= H[y|s], the latter of which should represent the potential coding gain. Ballé et al. (2018), however, found that the bits used to encode the hierarchical prior (i.e. H[y]) is only a small fraction of the total bitrate, thus upper bounding the potential gains for this type of model.

Overall, I think this is a well-written, important and elegant paper, and I would like to see it accepted at this conference. If the authors can satisfactorily address some of the above potential limitations, it might turn out to be even better.
","The reviewer explicitly states ""This is a well-written, important and elegant paper, and I would like to see it accepted at this conference."" This clearly indicates a positive sentiment. The reviewer also provides constructive criticism and suggestions for improvement, which further supports a positive but not overly enthusiastic sentiment. Therefore, a sentiment score of 70 is justified. The language used throughout the review is polite and professional, with constructive criticism offered respectfully. Hence, a politeness score of 100 is appropriate.",70.0,100.0
"Summary. The paper is an improvement over (Balle et al 2018) for end-to-end image compression using deep neural networks. It relies on a generalized entropy model and some modifications in the training algorithm. Experimentals results on the Kodak PhotoCD dataset show improvements over the BPG format in terms of the peak signal-to-noise ratio (PSNR). It is not said whether the code will be made available.

Pros. 
* Deep image compression is an active field of research of interest for ICLR. The paper is a step forward w.r.t. (Balle et al 2018). 
* The paper is well written. 
* Experimental results are promising.

Cons.
* Differences with (Balle et al 2018) should be emphasized. It is not easy to see where the improvements come from: from the new entropy model or from modifications in the training phase (using discrete representations on the conditions).
* I am surprised that there is no discussion on the choice of the hyperparameter \lambda: what are the optimal values in the experiments? Are the results varying a lot depending on the choice? Is there a strategy for an a priori choice? 
* Also is one dataset enough to draw conclusions on the proposed method?

Evaluation.
As a non expert in deep learning compression, I have a positive opinion on the paper but the paper seems more a fine tuning of the method of (Balle et al 2018). Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results.

Some details.
Typos: the p1, the their p2 and p10, while whereas p3, and and figure 2 
p8: lower configurations, higher configurations, R-D configurations
","The reviewer acknowledges the paper's merits, such as being well-written, improving upon previous work, and showing promising results. However, they also express reservations about the novelty and significance of the contributions, suggesting it might be more of a refinement than a groundbreaking advancement. The reviewer's tone is critical but professional and maintains a neutral to slightly positive stance overall.",30.0,60.0
"In a distributed learning system where a parameter server maintains a full resolution copy of the parameters, communication costs can be reduced by (a) discretizing the weights that the server broadcasts to the workers, and (b) discretizing the gradients that the workers return to the parameter server. Following existing literature, the authors propose to discretize the parameters in a manner that limits its impact on the loss function by means of a diagonal approximation of the Hessian. This also means that one can bound the difference between the gradient for the full precision parameter and the gradient for the discretized parameter.  In contrast, they discretize the gradients stochastically so that the discretized version is an unbiased estimator of the full precision stochastic gradient. Since the stochastic gradient is itself an unbiased estimator of the gradient, this means we are dealing with an estimator whose variance has increased in a manner we can bound as well. The theoretical analysis consists in pushing these two bounds through classical analyses of the stochastic gradient algorithm, in this case, a regret-based version in the style of Zinkevich or Duchi.  Although i did not check the minute details of the proof, the argument feels correct and familiar.  They also give an interesting result in favor of clipping gradients, worth developing.

Although the title promises an analysis that holds for deep networks, this analysis strictly applies only to convex models. The author argue that the predictions made by this analysis also apply to deep networks, and support this argument with extensive experiments (which certainly represent a fair amount of work).  This result is believable but should not be construed as an analysis. Nevertheless, both results (the theoretical result for convex model and the empirical result for deep networks) are interesting and worth sharing.

The main caveat comes from the style the parallel learning algorithm they are considering.  In the data-parallel case (which they consider), parameter servers approaches have been displaced by setups where all workers update their copy of the weights using the allReduced gradients.  One could also use discretized gradients to speedup the allReduce operation (this is less of a win because latencies dominate) but this would only result in an increased variance and a much simpler analysis.

Finally I am not completely up-to-date with this line of work and cannot evaluate the novelty with confidence. This was not known to me, which is only a piece of evidence.

-- bumping down my score because the misleading title was not addressed by the author response.
-- bumping it up again because the authors have reacted.
","The review is mostly positive. The reviewer finds the theoretical analysis sound and the empirical results interesting. They praise the amount of work and acknowledge the novelty of the contribution. However, they point out a misleading aspect in the title regarding the applicability to deep networks, which is only empirical. The reviewer also mentions that the considered parallel learning algorithm might be outdated. The multiple score bumps highlight a dynamic interaction with the authors.",60.0,70.0
"The authors are proposing a method for allowing the generation of multiple objects in generated images given simple supervision such as bounding boxes and their associated labels. They control the spatial location of generated objects by the mean of an object pathway added to the architecture of both Generator and Discriminator within a GAN framework. They show generated results on Multi-MNIST, CLEVR with discussions of their model's abilities and properties. they also provide quantitative results on MSCOCO (IS and FID) using StackGAN and AttGAN models with the object pathway modifications and show some improvements compared to the original models. However it must be noted (as commented by the authors) that these models are using image captions only and do not have explicit supervision of bounding box and object labels.

This paper proposes a simple approach to generating requested objects in GAN-based image generation task, The method is supervised and requests (in its current form) the Bounding Boxes and Labels of the objects to integrate into the image generation. This task of controlling the nature (identity) and size of objects to integrate in a generated image is an important one and is significant to the GAN-base image generation community. In terms of originality, the approach is a nice simple architecture that takes care of the spatial location problem head-on. It seems like an obvious step but this does not take away from the merits of the proposed method.

The generator Global path is given a noise component. From the text, it does not seem that the Object path is given a noise component. Do you generate always the same object given the same label and Bounding Box then? Why not integrate some noise in this pipeline too?


Multi-MNIST:
The authors present results on Multi-MNIST 50K customed data to present the ability of the model to accurately put request images in the correct bounding box (BB) and do some ablation study. This is an interesting test as it shows that indeed the method proposed generates digits where it is expected to. Could you provide the ground truth labels for each/some image/s? For the failure cases it is often not clear what digit is what. For the Row E and F, 1s could be 7s and vice versa. Since it is a qualitative study, it would be nice to have the Ground Truth (GT) (which you provide to G at for generation). For the failure case of Row D (right) an interesting results would have been to have example of a digit bounding box from top to bottom with few pixel vertical shift to visualize when the model starts to mess-up the generation. This seems to point that your model (exposed to the location from BB for the object paths) is sensitive to what locations it has seen in training. How would you make the object path more robust to unseen location (overall you need to design an object of a given size, then locate it in your empty canvas prior to the CNN for generation)?

CLEVR:
The images resolution make it hard to really see the shape of the images (here too, the GT would be great). The bounding boxes make the images even harder to parse. I know the colors change but ""We can confirm that the model can control both location and object's shape"". For the location, it is true, for the shape is hard to completely tell at this resolution without GT. 

MS-COCO:
 Just a comment in passing on the fact that resizing images from COCO to 256x256 will inherently distort quite a bit of images, the median size (for each dim) for COCO is 640x480, if I am not mistaken. Most, if not all images in COCO are not 1.0 size ratio.
The quantitative results on COCO seem to confirm that the proposed method is generating ""better"" images according to IS and FID. This is a good thing, however the technique is strongly supervised (Bounding Box and Object Labels, caption compared to solely captions for StackGAN and AttGAN) so this result should be expected and really put into perspective as your are not comparing models w/ the same supervision (which you mention in the Discussion).

Discussion: 
I appreciate that the authors addressed the limitations of their approach in this section. The overlapping BBs seems to be an interesting challenge. Did you try to normalize the embeddings in overlapping area? A simple sum does not seem to be a good solution. In Figure 7 w/ overlapping zebras, the generation seems completely lost. 

In terms of clarity, the paper is well-written but would benefit *greatly* from using variables names when discussing 'layout embedding', 'generated local labels', etc. Variable names and equations, while not necessary, can go a long way to clearly express a model's internal blocks (most of the papers you referenced are using this approach). The paper employs none of this commonly used standard and suffers from it. I myself had to write down on the margin the different variables used at each step described in text to have an understanding of what was done (with help of Figure 1). You should reference Figure 1 in the Introduction, as you cover your approach there and the Figure is useful to grasp your contributions.

Another comment concerning clarity is, while it is fine to rely on previously published papers for description of our own work, you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures. If one uses GAN training, it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize. I am afraid that ""using common GAN procedures"" is not enough. When describing your experimental setup, pointing to another paper as ""hyperparameters remain the same as in the original training procedure"" should not be a substitution for covering it too, even if lightly in the Appendix. For instance: in the Appendix, it is mentioned that training was stopped at 20 epochs for Multi-MNIST, 40 for CLEVR... How did you decide on the epoch (early stopping, stopped before instabillity of GAN training, etc.) Did you use SGD? ADAM? Did you adjust the learning rate, which schedule? etc. for your GAN training. This information in the Appendix would make the paper overall stronger. 

Last comment: In terms of generation multiple objects. Have you had the chance to run an object detector on your generated image (you can build one on MSCOCO given the bounding box and label, finetune an ImageNet pretrained model). It would be interesting to see if the generated images are good enough for object detection.

Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7  ","The reviewer acknowledges the importance of the paper's contribution and finds the proposed method to be original and interesting. They provide constructive criticism and suggestions for improvement, but also highlight the strengths of the paper, such as the quantitative results on MSCOCO. The reviewer's decision to increase the rating after the authors addressed their concerns further demonstrates a positive sentiment. Overall, the tone is professional and polite, suggesting areas for improvement without resorting to harsh language.",60.0,70.0
"Summary:
The paper considers the problem of online stochastic convex optimization in a fully distributed topology. In particular, the authors focus on the synchronous setting and to avoid the slow progress that can be obtained by slow nodes, called stragglers, they propose an online distributed optimization method called Anytime Minibatch (AMB). In the update of AMB rather than fixing the minibatch size, they fix the computation time in each epoch. This characteristic prevents the stragglers from holding up the entire network, while allowing nodes to benefit from the partial work carried out by the slower nodes. 

A convergence analysis of AMB is provided showing that the online regret achieves the optimum performance. Numerical evaluations where a comparison of AMB and the ""Fixed MiniBatch"" method (FMB)are also presented.

Comments:
I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the numerical evaluation. 

1) In the title the word ""online"" is mentioned but never explained  in the main text. What is this mean? What are the differences compare to the ""static"" setting? See for example the work of [Tsianos, Rabbat (2016)] for more details on that. What are the related literature on this setting?

2) In the last paragraph of Introduction is highlighted that the algorithm AMB has the optimum performance?  The authors should add an appropriate reference there and explain why this is optimum for their setting. I believe that for the convenience of the reader current Section 5 called ""previous work"" can move immediately after introduction and more details of AMB with the existing literature should be provided. Probably rename the section ""Closely relate work"".

3) Section 2 is devoted mostly on the formal presentation of algorithm AMB. I strongly suggest the addition of a pseudocode of the algorithm in the appendix (or even in the main text if there is a space) where the reader can easily understand how the algorithm works.

4) On the Algorithm:  if some nodes are very slow and they do not make any update during the given time T what will happen? How this will affect the performance of the method? In this case does it make sense to increase the value of T.

5) On numerical evaluation:  A comparison of AMB and FMB  is presented both in synthetic and real data showing that AMB can be faster than FMB in terms of wall clock time. 
I am not sure if the performance of the AMB is as good as one should expect especially for the case of synthetic data. Will it be possible to construct a synthetic example with extremely slow nodes where the improvement of the performance is much better than 50%?

In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on control theory/signal processing and information theory.  Since the paper is focused on convex optimization I am not sure if it will be particularly interesting for a substantial fraction of the ICLR attendees.
","The reviewer provides constructive criticism, acknowledges the paper's merits (interesting idea, correct convergence analysis), and suggests specific improvements. They find the core idea promising and believe it will be of interest to a subset of the ICLR audience. The language is formal and professional throughout.",60.0,80.0
"The paper analyses the data collected from 6005 neurons in a mouse brain. Visual stimuli are presented and the responses of the neurons recorded. In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli. Results show a decent correlation between neuron responses and trained network. Moreover, the rotational equivariant architecture beats a standard CNN with similar number of feature maps. The analysis and discussion of the results is interesting. Overall, the methodological approach is good.

I have trouble understanding the plot in Figure 4, it also does not print well and is barely readable on paper.

I have a small problem Figure 6 where ""optimal"" response-maps are presented. From my understanding, many of those feature maps are not looking similar to feature maps that are usually considered. Given the limited data available and the non-perfect modeling of neurons, the computed optimal response-map might include features that are not present in the dataset. Therefore, it would be interesting to compare those results with the stimuli used to gather the data. E.g. for a subset of neurons, one could pick the stimulus that created the maximum response and compare that to what the stimulus with the maximum response of the trained neuron was. It might be useful to include the average correlation of the neurons belong to each of the 16 groups(if there are any meaningful differences), especially as the cut-off of ""correlation 0.2 on the validation set"" is rather low.

Note: I am not an expert in the neural-computation literature, I am adapting the confidence rating accordingly.","The reviewer provides a positive overview of the paper, highlighting the interesting analysis, discussion, and sound methodology. They do point out minor issues with figure clarity and suggest further analysis in Figure 6. However, these are constructive criticisms and the reviewer acknowledges their non-expertise in the specific field, suggesting an overall positive sentiment.",60.0,80.0
"This paper builds on the long recent tradition of analyzing deep linear neural networks. In addition to an ample appendix bringing the page total to 20, the authors went over the recommended eight pages, hitting the hard limit of 10 and thus per reviewing directions will be held to a higher standard than the other (mostly 8-page) papers. 

The recent literature on deep linear networks has explored many paths with the hope of producing insights that might help explain the performance of deep neural networks. A recent line of papers by Soudry and Srebro among others focuses on the behavior of stochastic gradient descent. This paper’s analysis comes from a different angle, following the work by Saxe et al (2013) whose analysis considers a (classic one hidden layer) linear teacher network that generates labels and a corresponding student trained to match those labels. The analysis hinges on the singular value decomposition of the composite weight matrix USV^T = W = W^{32} W^{21}.

One aim of the present work, that appears to be a unique contribution above the prior work is to focus on the role played by task structure, suggesting that certain notions of task structure may play a more significant role than architecture and that any bounds which consider architecture but not task structure are doomed to be excessively loose. 

To facilitate their analysis, the authors consider an artificial setup that requires some specific properties. For example, the number of inputs are equal to the input dimension of the network, with the inputs themselves being orthonormal. The labeling function includes a noise term and the singular values of the teacher model admit an interpretation as signal to noise ratios. Given their setup, the authors can express the train and test errors analytically in terms of the weight matrices of the teacher and student and the input-output covariance matrix. The authors then analyze the gradient descent dynamics in what appears to follow the work of Saxe 2013 although I am not an expert on that paper. The analysis focuses on the time dependent evolution of the singular values of the student model, characterized via a set of differential equations.

The next analysis explores a condition that the authors dub “training aligned” initial conditions. This involves initializing the student weights to have the same singular vectors as the training data input-output covariance but with all singular values equal to some amount epsilon. The authors show that the learning dynamics give rise to what they characterize as a singular value “detection wave”. Detecting the modes in descending order by their corresponding singular values.

A set of synthetic experiments show close alignment between theory and experiment.

Section 3.5 offers just one paragraph on a “qualitative comparison to nonlinear networks”. A few issues here are that aesthetically, one-paragraph subsections are not ideal. More problematic is that this theory presumably is building towards insights that might actually be useful towards understanding deep non-linear networks. Since the present material is only interesting as an analytic instrument, I would have hoped for greater emphasis on these connections, with perhaps some hypotheses about the behavior of nonlinear nets driven by this analysis that might subsequently be confirmed or refuted. 

The paper concludes with two sections discussing what happens when nets are trained on randomly labeled data and knowledge transfer across related tasks respectively. 

Overall I think the paper is well-written and interesting, and while I haven’t independently verified every proof, the technical analysis appears to be interesting and sound. The biggest weaknesses of this paper---for this audience, which skews empirical---concern the extent to which the work addresses or provides insight about real neural networks. One potential weakness in this line of work may be that it appears to rely heavily on the linearity of the deep net. While some other recent theories seem more plausibly generalized to more general architectures, it’s not clear to me how this analysis, which hinges so crucially on the entire mapping being expressible as a linear operator, can generalize. 

On the other hand, I am personally of the opinion that the field is in the unusual position of possessing too many tools that “work” and too few new ideas. So I’m inclined to give the authors some license, even if I’m unsure of the eventual utility of the work. 

One challenge in reviewing this paper is that it builds tightly on a number of recent papers and without being an authority on the other works, while it’s possible to assess the insights in this paper, it’s difficult to say conclusively which among them can rightly be considered the present paper’s contributions (vs those of the prior work).
","The reviewer provides a generally positive overview of the paper, highlighting its interesting analysis and well-written nature. They acknowledge the technical soundness and find the work interesting. However, they express concerns about the paper's applicability to real neural networks due to its reliance on linearity. Despite this, they lean towards a more positive sentiment, recognizing the need for new ideas in the field. The reviewer maintains a neutral and professional tone throughout the review.",60.0,80.0
"This paper explores the inevitability of adversarial examples with concentration inequalities. It is motivated by the difficulties of achieving adversarial robustness in literature. It derives isoperimetric inequalities on a cube, and then discuss the adversarial robustness of data distributed inside the cube, with the assumption that the data has bounded density. These inequalities are established on different norms. The authors then discuss limitation of the proposed bounds when analyzing practical data distribution and discussed the influence of dimensionality on adversarial robustness.


Novelty of the idea:
The idea of using concentration inequalities to explain vulnerability is novel in the field of adversarial examples and is a relevant/meaningful angle on understanding this phenomenon. (Although there are concurrent works also relating concentration inequalities to adversarial robustness, they don't diminish the novelty of this work.)



On technical contributions:
In summary, this paper applies / adapts previous results in concentration inequalities to develop bounds related to adversarial examples. The bounds in Lemma 3 are on any p>0, this seems to be new to my knowledge, but the technical contribution in the proof is limited.

Here are some detailed comments.

The authors claim that
""This question is complicated by the fact that simple, geometric isoperimetric inequalities fail to exist for the cube, and the shapes that achieve minimal \eps-expansion (if they exist) depend on the volume they enclose and the choice of \eps.""
This statement is at least misleading, if not wrong. It is well known that geometric isoperimetric inequality does exist for cube for the L2 case (see Ledoux, M., 2001. Proposition 2.8.), and the proof procedure the author used is also very similar to the proofs in Ledoux, M., 2001.

Theorem 5's proof is confusing, if not wrong. 
This is my brief recap on the first part of Thm 5, 
If there exists eps and p such that, for all classifiers on MNIST, a random image has eps-adv with probability at least p, then for all classifiers on b-MNIST, a random image has b*eps-adv with probability at least p.
The proof in Appendix E says b-MNIST images can be classified by first downsampling. These downsampled classifiers do not cover ""all classifiers on b-MNIST"", so I don't see how the proof stands.
Likewise, the proof of the second part has the similar problem.
Therefore, I'm not yet convinced that Thm 5 is correct.
Also I suggest the authors use more rigorous language to present Theorem 5, in a similar fashion to previous theorems.

Re: Lemma 4, my understanding is that it is from previous literature. The authors should point out exactly where is it from (with section# and theorem#), so that readers and reviewers can more easily check the correctness of it.

The authors mention that ""Intuitively, the concentration limit Uc can be interpreted as a measure of image complexity.""
I think this statement is problematic. It is, at best, oversimplifying the the problem. If we assume the data lies in low-dimensional space, the volume of the support will be 0, no matter how complex the shape of the manifold is. This lead to unbounded density in the ambient dimension.
Even when considering ""expanded dataset"" like the authors discussed in Section 7, it is not obvious that Uc can be interpreted as image complexity. To make such a claim, more assumptions need to made and more analyses need to be done.
Similar comments applies to the ""correlations between pixels"" and concentration.



On the significance:
As the author themselves have already mentioned, the bounds described in the paper all depends on the bounded density of the data distribution. In practice, the density of data distribution is difficult to understand, if not impossible. Therefore it is still inconclusive whether the ""inevitability"" exists. But to be fair, I believe this is mostly due to the difficulty of the problem being studied.



Clarity and writing:
The skeleton of the paper is well written and easy to follow. I've pointed out some problems in my previous comments.
I also appreciate that the authors made efforts to not overclaim.

here are a few more comments:
- I personally feel Section 3 as an ""warm-up"" section is redundant, and the authors can consider move them to the appendix.
- In Section 6 and 7, the authors talk about when is the bound ""meaningful"" and ""active"". This part is confusing/misleading. eps=sqrt(n) is actually the maximum possible perturbation and not falls into the common ""adversarial perturbation"" where the perturbation does not change the semantic meaning of the image. There should be a least an additional numerical examples on small eps, so the readers have better ideas on the tightness/looseness of the bound.



References:
Ledoux, M., 2001. The concentration of measure phenomenon (No. 89). American Mathematical Soc..

==========================
I change my rating on this paper to be 6, after the authors' response. 
","The review presents a mixed sentiment. While acknowledging the novelty and relevance of the paper's core idea, the reviewer raises significant concerns about the technical contributions, clarity, and significance of the findings. The reviewer points out potential errors and misleading statements, particularly questioning the correctness of Theorem 5 and the interpretation of the concentration limit Uc. Although the reviewer appreciates the paper's structure and efforts to avoid overclaiming, the numerous technical issues and the inconclusive nature of the results due to reliance on bounded density, which is impractical to determine in real-world scenarios, suggest a less than positive view. The reviewer's initial rating of 6 (out of 10, assuming a standard scale) further supports this assessment. The language used is generally polite and professional, employing constructive criticism and focusing on the scientific content rather than resorting to personal attacks. ",20.0,80.0
"The paper describes a new learning framework, based on generative
adversarial imitation learning (GAIL), that is able to learn sub-tasks
policies from unsegmented demonstrations. In particular, it follows
the ideas presented in InfoGAIL, that depends on a latent variable,
and extend them to include a sequence of latent variables representing
the sequence of different subtasks. The proposed approach uses a
pre-training step, based on a variational auto-encoder (VAE), to
estimate latent variable sequences. The paper is well written and
relates the approach with the Options framework. It also shows,
experimentally, its performance against current state-of-the-art
algorithms.  

Although the authors claim in the appendix that the approach is
relatively independent on the dimensionality of the context variable,
this statement needs further evidence. The approach is similar to HMMs
where the number f hidden states or latent variables can make a
difference in the performance of the system.

Also, it seems that the learned contexts do not necessarily correspond
to meaningful sub-tasks, as shown in the circle-world. In this sense,
it is not only relevant to determine the ""right"" size of the context
variable, but also how to ensure a meaningful sub-task segmentation. 
","The review starts with positive statements, highlighting the novelty, clarity, and experimental validation of the work. However, it raises concerns about the claims related to context variable dimensionality and the interpretability of learned sub-tasks. The language is professional and suggestive, not imposing changes but rather asking for clarification and further investigation.",60.0,80.0
"In this work, the authors propose Switchable Normalization (SN), which *learns* to switch / select different normalization algorithms (including batch normalization (BN), Instance Normalization (IN), Layer Normalization (LN)) in layers of the networks and in different applications. The idea is motivated by observations (shown in Fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. BN) are fragile to very small batch size.

The authors propose a general form for different normalization methods, which is a Gaussian normalization and then scale and shift by scalars. Different normalization methods utilize different statistics as the mean and the variance of the Gaussian normalization. The authors further propose to learn the combination weights on mean and variance, which is w_k and w'_k in Eqn (3). To avoid duplicate computation, the authors also do some careful simplification on computing mean and variance with all of the three normalization methods.

In the experiment part, the authors demonstrate the effectiveness of the proposed SN method on various kinds of tasks, including ImageNet classification, object detection and instance segmentation in COCO, semantic image parsing and video recognition. In all of the tasks tested, which also cover the common application in computer vision, SN shows superior and robust performance.

Pros:
+ Neat motivation;
+ Extensive experiments;
+ Clear illustration;

Cons
- There are still some experiment results missing, as the authors themselves mentioned in the Kinetics section (but the reviewer thinks it would be ready); 
- In Page 3 the training section and Page 4, the first paragraph, it mentioned Θ and Φ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative meta-learning style method. The authors attribute ""In contrast, SN essentially prevents overfitting by choosing normalizers to improve both learning and generalization ability as discussed below"". The reviewer does not see it is well justified and the reviewer thinks optimizing them jointly could lead to instability in the training (but it did not happen in the experiments). The authors should justify the jointly training part better.
- Page 5 the final paragraph, the reviewer does not see the point there. ""We would also like to acknowledge the contributions of previous work that explored spatial region (Ren et al., 2016) and conditional normalization (Perez et al., 2017). ""  Please make it a bit more clear how these works are related. ","The reviewer explicitly states several positive aspects of the paper, such as ""Neat motivation"", ""Extensive experiments"", and ""Clear illustration."" While they point out areas for improvement, the overall tone is constructive and suggests that the paper is promising. The reviewer acknowledges the strengths of the work and provides specific suggestions for improvement without resorting to harsh or disrespectful language. The critique is presented professionally and objectively, focusing on the scientific content and clarity of the work.",60.0,70.0
"Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines.

Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation.

Just above theorem 1 the sentence does not parse ""Further, for each s, let λs be the solution to "", firstly there is no 'solution' to an equation, secondly should it be λs or pi?

The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with.

The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously.

I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting.

Figure 2 is incomprehensible.

Two of the references are repeated (Schulman et al, Wang et al).

The appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.","The review starts with ""Overall this paper is ok."" which is a slightly positive statement but quickly moves to rather negative commentary. The reviewer lists several major issues with the paper, including poor exposition, lack of explanation, messy discussion, and incomprehensible figures. The language, while direct, avoids harsh or overly negative phrasing.",-10.0,60.0
"The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals. 
While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit. 
The problem of nonstationarity rsp. stability is an old one in non-invasive BCIs (shenoy et al JNE 2006 was among the first) and a large number of prior methods have been defined to robustify feature spaces, to project to stable subspaces etc. Clearly no Gans at that time. The least the authors could do is to make reference to this literature, some methods may even apply also for the invasive data of the paper.
While the authors did not clearly say that they present an offline analysis; one method, the GAN, gets 6% better results then the competitors. I am not sure whether this is practically relevant in an online setting. But this needs to be clearly discussed in the paper and put into perspective  to avoid wrong impression. Only an online study would be convincing. 

Overall, I think the paper could be accepted, the experiments are nice, the data is interesting, if it is appropriately toned down (avoiding statements about having done something for the first time) and properly references to prior work are given. It is an interesting application domain. I additionally recommend releasing the data upon acceptance. 

","The review starts with positive remarks, highlighting interesting aspects of the paper ('I like the paper and studied methods...'). However, it also criticizes the authors for overselling their contributions and not acknowledging prior work ('...the authors oversell a bit', '...a large number of prior methods have been defined...'). While the reviewer suggests acceptance, they recommend toning down claims and providing proper references. The language is critical but professional and not disrespectful.",50.0,50.0
"Pros:
- Paper proposes a somewhat complicated but easy to understand idea for open set classification. Formulation is quite intriguing.
- Outperforming recent baselines on most scenarios, despite being a linear classifier on fixed CNN features.

Cons:
- Experiment setup somewhat flawed (but the same flaw is in prior work too)
    To elaborate: DeCAF7 is trained on ImageNet, which gives the underlying network extra categorical information of the 1000 classes. Some of these clases are arguably in the ""unknown classes"" in the open set setting. This may jeopardize the premise since the feature knows those classes are semantically different from known classes. Unfortunately (Busto & Gall, 2017) and (Saito et al., 2018) do this too.
    This is especially problematic since DeCAF7 has a near-linear relationship to the final sigmoid logits, which are the 1000-way ImageNet class scores. This makes the authors formulation (separate subspaces for known and unknown classes) more easily exploit this leaked information. This is because the 1000-way scores obviously have subspaces for all 1000 ImageNet classes, and by extension, the ""known"" and ""unknown"" classes in the open set setting. 
    If this is true and is the main reason that the proposed method outperforms, I would not consider the conclusion of the paper very informative. Instead, its signifies the need of a better experiment setup for the problem.
    A way to strengthen the paper is to use a network pre-trained on other datasets (e.g. Places, or a subset of ImageNet) to verify the findings of the paper.
- Lacks clarity for what is being done at test time. 
    I cannot find whether the final SVM is trained on original DeCAF features, or S and T. If it is the latter, how are the representations of target domain data obtained at test time? Are they d dimentional or 2d dimentional?
    Can you clarify that the test samples are not used for unsupervised training?
- Experiment elaborate but feels incomplete.
    It feels like the authors are proposing 3 variations of the method, and there is not one of them that consistently outperform the others. If so, the paper would lack some ablation analysis that provides insights of what makes the FRODA-SVM outperform prior art. For example, how much do the hyperparameters matter? What happens if e.g. d or lambda1 is very large/small?

Clarity:
- Abstract spends too much time on defining problem setup
- ""Faster than prior work"" refers to the training time, and excludes the DeCAF feature extraction.

Originality:
I am not familiar with the related work.

Significance:
It is quite impressive that a linear model on fixed CNN activations outperforms prior art. However, see the first point in the cons.


-----------
Edit: most of the issues listed in ""cons"" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7.
","The reviewer starts with positive remarks, highlighting the paper's intriguing idea and strong performance. However, they raise a major concern about the experimental setup, suggesting it might invalidate the results. This concern, along with other issues like clarity and ablation analysis, pulls the sentiment down. The reviewer acknowledges the authors addressed some concerns in the edit but still doesn't seem fully convinced, as indicated by the moderate final rating of 7. The language used is critical and detailed but maintains a professional and respectful tone, suggesting a high level of politeness.",40.0,80.0
"The paper presents an approach for simultaneously learning policies and reward functions for reaching goals that are described by an instruction providing spatial relations among objects. The proposed platform, called Adversarial Goal-Induced Learning from Examples (AGILE), is composed of an off-the-shelf RL module like A3C and a separate module for learning a reward function, implemented using the NMN paradigm. The RL module is trained using the reward function learned by the reward module. The reward module is trained to map a given <instruction, state> into a score between 0 and 1 depending on how well the provided state satisfies the instructions provided in the instruction. The returned score is used as a reward function. The training of the reward function is performed by using a dataset of positive examples, and using the states visited by the agent while it's learning as negative examples. To account for the fact that the agent becomes better over time and its visited states can no longer be used as negative examples, the authors proposed a heuristic where the states visited by the agent are not all used as negative examples, but only those that have the lowest scores.
The paper also presents an empirical evaluation of the proposed approach on a synthetic task where the agent is tasked with move bocks of different shapes and colors to a desired final configuration. The AGILE approach was compared to the baseline A3C algorithm where a sparse binary reward signal was used only whenever the agent reaches the goal state. AGILE is also compared to A3C with an auxiliary task of reward prediction. 
The paper is clearly written and technically strong. However, I have two issues with this paper: 1) the proposed approach is a simple combination of A3C and the NMN architecture, 2) the experiments are performed on simple synthetic tasks that make learning spatial relations fairly easy, I would love to see more real images as it has been demonstrated in prior works on learning  spatial relations. It is not clear from these experiments if the proposed approach will scale up to higher-dimensional inputs. Moreover, there are several stability issues that can be caused by the proposed approach. For instance, the reward function is changing over time, how does that affect the learning rate? Also, instead of using the learned policy itself to generate negative examples and run into non IID data, instabilities, and increasingly good negative examples, why not use a fixed dataset of negative examples generated with a random policy? It would be interesting to do perform an experiment where you compare to the classical reward learning setup where you simply provided labeled positive and negative examples and classify them offline, then use the learned reward function online for RL. 
How did you tune the hyper-parameter \rho (percentage of negative examples to discard) for specific tasks? Do you have any guarantees for this approach?
In the generalization experiments, it is mentioned that 10% of the instructions are held out. Are these 10% randomized?","The reviewer acknowledges the paper's clarity and technical strength, which indicates a positive sentiment. However, they also raise significant concerns about the novelty, simplicity of the approach, and potential limitations. The reviewer suggests stronger experiments and points out specific issues like stability and hyperparameter tuning. This suggests the sentiment is not overly positive but rather leans towards cautious interest with a need for improvements. Therefore, the sentiment score is moderately positive. The language used is professional and polite throughout, without resorting to harsh or disrespectful language. The reviewer provides constructive criticism and suggestions for improvement.  ",40.0,80.0
"This manuscript introduces a computational method to speed up training and inference in deep neural networks: the method is based on dynamic pruning of the compute graph at each iteration of the SGD to approximate computations with a sparse graph. To select which neurons can be zeros and ignored at a given iteration, the approach computes approximate activations using random projections. The approach gives an overall decrease in run-time of 0.8 to 0.6. I believe that its largest drawback is that it does not lead to the same sparsity pattern in a full minibatch, and hence cannot be implemented using matrix-matrix multiplications (GEMM). As a result, the compute-time speed ups are not huge, though the decrease in memory is important. In my eyes, this is the largest drawback of the manuscript: the total computational speed-up demonstrated is not fully convincing.

The manuscript is overall well written and easy to understand, though I wish that the authors employed less acronyms which forced me to scan back as I kept forgetting what they mean.

The strength of the paper are that the solution proposed (dynamic approximation) is original and sensible. The limitations are that I am not sure that it can give significant speedups because I it is probably hard to implement to use well the hardware.

Questions and comments:

1. Can the strategy contributed be implemented efficiently on GPUs? It would have been nice to have access to some code.

2. Fig 8(b) is the most important figure, as it gives the overall convergence time. Is the ""dense baseline"" using matrix-vector operations (VMM) or mini-batched matrix-matrix operation (GEMM)?

3. Can the method be adapted to chose a joint sparsity across a mini-batch? This would probably mean worst approximation properties but would enable the use of matrix-matrix operations.

4. It is disappointing that figure 8 is only on VGG8, rather than across multiple architectures.

5. The strategy of zeroing inputs of layers can easily create variance that slows down overall convergence (see Mensh TSP 2018 for an analysis of such scenario). In stochastic optimization, there a various techniques to recover fast convergence. Do the authors think that such scenario is at play here, and that similar variance-reduction methods could bring benefits?

6. I could not find what results backed the numbers in the conclusion: 2.3 speed up for training. Is this compared to VMM implementations? In which case it is not a good baseline. Is this for one iteration? In which case, it is not what matters at the end.

7. Is there a link between drop-out and the contributed method, for instance if the sparsity was chosen fully random? Can the contributed method have a regularizing effect?

","The reviewer highlights both positive and negative aspects of the manuscript. While acknowledging the originality and clarity of the work, the reviewer expresses concerns about the computational speed-up, suggesting it's not substantial enough due to the inability to use matrix-matrix multiplications. The numerous questions and suggestions for improvement indicate a desire to see the work strengthened rather than rejected. The language used is professional and constructive, without resorting to harsh or disrespectful tones.",40.0,80.0
"This paper proposes a novel approach to explain neural network predictions by learning hierarchical representations of groups of input features and their contribution to the final prediction. The proposed method is a straightforward extension of the contextual decomposition work by (Murdoch et. al. 2018) which estimates feature interpretability for LSTMs. This work extends (Murdoch et. al. '18) to more general NN architectures and further employs agglomerative clustering to identify groups of features-- as opposed to individual features--that are predictive of the output. 

Results are shown using a LSTM trained on the standard Stanford sentiment task and a VCG DNN trained on ImageNet which show the superior performance of the proposed approach. In addition, the paper also provides some survey results where ""humans"" were asked to pick more interpretable models. 

The paper is nicely written and puts itself nicely in context of the previous work. Though, I have several concerns:

1). Biggest concern: Conditioning on the (Murdoch et. al. 18) paper, the methodological novelty of the proposed approach is minimal. Though, the experimental gains on the vision and NLP tasks are nice.

2). It was unclear to me how the agglomerative algorithm (Algorithm 1) was run. That is, was it run as part of the LSTM estimation for instance for the sentiment task OR was it run post-hoc after getting the model estimates from LSTM? If it was run post-hoc then I am unsure if we can assume that the ""agglomeratively grouped CD scores of individual features"" are the same as the ""CD scores for the groups/interactions of features"" in terms of their contribution to the final prediction.

3). Though, the paper mentions several times regarding generalizing (Murdoch et. al. 18) to architectures other than LSTMs but still the experimental results on the sentiment task uses an LSTM as the model. It would have been nice to show the comparative strength of the proposed approach on a different architecture even for the sentiment task. (I understand that the paper uses a different DNN architecture for the vision task).

4). The paper talks several times about diagnosing why a model went wrong e.g. the ""negation"" in the case of the LSTM model in Figure 2, but never discusses the bigger and more interesting problem. How can we build an improved LSTM model for the sentiment task which classifies that incorrect prediction correctly? 

","The reviewer acknowledges the paper is well-written and provides context. However, they express concerns about limited methodological novelty, lack of clarity in the agglomerative algorithm's application, and a missed opportunity to demonstrate model improvement based on the insights gained. The tone is critical but constructive, suggesting improvements rather than outright rejection. Therefore, the sentiment leans slightly negative due to the concerns, while the politeness remains positive due to the constructive tone.",-20.0,60.0
"The authors focus on the selection problem of k statistically significant features discriminating 2 probability distributions accessible via samples. They propose a non-parametric approach under the PSI (post selection inference) umbrella using MMD (maximum mean discrepancy) as a discrepancy measure between probability distributions. The idea is to apply (asymptotically) normal MMD estimators, rephrase the top-k selection problem as a linear constraint, and reduce the problem to Lee et al., 2016. The efficiency of the approach is illustrated on toy examples and in GAN (generative adversarial network) context. The technique complements the PSI-based independence testing approach recently proposed by Yamada et al., 2018. 

The submission is a well-organized, clearly written, nice contribution; it can be relevant to the machine learning community.

Below I enlist a few suggestions to improve the manuscript:
-Section 1: The notion of characteristic kernel (kernel when MMD is metric) has not been defined, but it was referred to. 'Due to the mean embeddings in RKHS, all moment information is stored.': This sentence is somewhat vague.
-Section 1: 'MMD can be computed in closed form'. This is rarely the case (except for e.g. Gaussian distributions with Gaussian or polynomial kernels). I assume that the authors wanted refer to the estimation of MMD.
-Section 1: 'K nearest neighbor approaches (Poczos & Schneider, 2011)'. The citation to this specific estimator can go under alpha-divergences. The Wasserstein metric could also be mentioned.
-Section 3.1: k is used to denote the number of selected features and also the kernel used in MMD. I suggest using different notations.
-Theorem 1: '\Phi is the CDF...'. There is no \Phi in the theorem.
-Section 3.2: The existence of MMD (mean embedding) requires certain assumptions: E_{x\sim p}\sqrt{k(x,x)} < \infty, E_{x\sim q}\sqrt{k(x,x)} < \infty.
-Section 3.2.: block estimator: 'B_1 and B_2 are finite'. 'fixed'?
-Section 3.2.: MMD_{inc}: 
   i) 'S_{n,k}': k looks superfluous.
   ii) 'l': it has not been introduced (cardinality of D).
-Section 3.3: typo: 'covraiance' (2x)
-Section 3.3: Fan et al. 2013: The citation can go to \citep{}.  
-Theorem 2: 
   i)'c' is left undefined.
   ii)Comma is missing before 'where'.
   iii)\xrightarrow{d} (Theorem 2, Corollary 3-4): Given that 'd' also denotes dimension in the submission, I suggest using a different notation for convergence in distribution.
-At the introduction of block-MMD the block size (B) was fixed, while in the experiments (e.g. Figure 3) it is growing with the sample size (B=\sqrt{n}). The assumption on B should be clearly stated.
-Section 5.1: (b) mean shift: comma is missing before 'where'.
-References: 
   i) Abbreviations and names in the titles should be capitalized (such as cramer, wasserstein, hilbert-schmidt, gan, nash). 
   ii) Scholkopf should be Sch\{""o}lkopf (in the ALT 2005 work).
   iii) 'Exact post-selection inference, with application to the lasso': All the authors are listed; 'et al.' is not needed.","The review is positive overall. The reviewer finds the submission to be a ""well-organized, clearly written, nice contribution"" and believes it to be ""relevant to the machine learning community."" The reviewer provides constructive feedback in the form of specific suggestions for improvement, indicating a desire to see the paper published. The language used is polite and professional throughout, focusing on the content and clarity of the work.",75.0,90.0
"The paper introduces an adaptation of the Scattering transform to signals defined on graphs
by relying on multi-scale diffusion wavelets, and studies a notion of stability of this representation
with respect to changes in the graph structure with an appropriate diffusion metric.

The notion of stability in convolutional networks is an important one, and the proposed notion of stability
with respect to diffusion distances seems like an interesting and relevant way to extend this to signals on graphs.
With this goal in mind, the authors introduce a scattering transform on graphs by relying on diffusion wavelets,
and provide an appropriate study of stability, which seems to highlight relevant properties of the graphs.
The proposed representation seems to provide benefits compared to the previous work of Zou & Lerman,
particularly regarding computational efficiency, as well as stability with respect to a metric that is perhaps more
useful, though there is a dependence on the graph topology through the spectral gaps.
In addition, the experiments on author attribution and source localization suggest that the
resulting representation remains discriminative, in addition to providing stability to changes in graph structure.

I find that these contributions provide an interesting advance in theoretical understanding of graph convolutional networks
from a stability perspective, in addition to introducing a useful non-learned representation,
and am thus in favor of acceptance.

Nevertheless, some parts of paper would benefit from further discussions and more clarity:

- other than empirically, one aspect that's missing compared to the original study of the scattering transform is energy preservation. The authors could at least provide a discussion of whether such a property can be obtained here as well (does it depend on the spectral gap through C(beta)?)

- what is the role of the spectral gap in the stability bounds? is this a drawback of the diffusion metric / choice of wavelets?

- Section 3.2 suggests that metric stability is a good way to characterize stability, by seeing deformations in Euclidian domains as a change to the ground metric. Yet, in Euclidian scattering, the same representation is applied to a deformed signal and the original signal, and stability is measured with the Euclidian metric.
Can the link be made more precise, by explaining what a deformation of a signal would be on a graph, or by applying arguments from the proposed construction to the Euclidian case?

- the paper is heavy on terminology from wavelets and harmonic analysis, a more detailed presentation of diffusion wavelets and related concepts such as localization would be beneficial. Also, it seems like the chosen wavelets in the construction favor spatial over frequential localization - is this due to a trade-off? if so, can it be avoided?


Some more detailed comments:
- Section 2, 'generally far weaker': what is meant by 'weaker'?
- Section 3.3:
  * 'calculus on T': T is used before being defined
  * clarify what norm is used (I assume operator norm?)
  * 'defines a distance', 'stronger than .. GH': this should probably be justified
- Section 4:
  * 'optimal spatial localization', 'temporal difference', 'favoring spatial over frequential localization': these could be clarified
  * 'amplify the signal': what does this mean?
  * the sentence about the choice of the appropriate J is not clear and should be further clarified
- Section 5.1:
  * the sentence about the choice pi/pi* = 1 should be clarified. Also, where is this assumption used?
  * epsilon_psi, epsilon_U should be defined
  * 'given that [..] by definition': this doesn't seem to be defined elsewhere
  * (16): isn't a factor m missing in the first term?","The review starts with positive statements, highlighting the importance and relevance of the paper's contributions. The reviewer explicitly states their favor of acceptance, indicating a positive sentiment. While the review lists points for improvement, these are constructive and presented politely with the aim of enhancing the paper. The language used throughout is formal, respectful, and encouraging.",75.0,90.0
"The authors suggest and analyse two types of preconditioners for optimization, a Newton type and a Fisher type preconditioner.

The paper is well written, the analysis is clear and the significance is arguably given. The authors run their optimizers on a synthetic benchmark data set and on imagenet.
The originality is not so high as the this line of research exists for long. 
The ""Lie"" in the title is (technically correct, but) a bit misleading, as only matrix groups were used.
","The review starts with positive statements: ""well written"", ""clear analysis"", ""significance is arguably given"". However, it also points out a lack of originality and a somewhat misleading title. Overall, the sentiment is more positive than negative, but the criticisms bring it down from highly positive. The language used is appropriate for academic discourse and doesn't contain any harsh or impolite phrasing.",60.0,80.0
"This paper proposes a method for stereo reconstruction using Deep Learning. Like some previous methods, a 'cost volume' is first computed by plane sweeping, in other words the cost volume is indexed by the 2D locations in the image plane, and the disparities for 3D planes parallel to the image plane. A network then predicts the disparities for each image location from this cost volume.

The contributions with respect to the state-of-the-art are:

- the cost volume is computed using differential warps, thus the network can be trained end-to-end;

- a better cost volume is computed from the original cost volume and the reference image.

The results look good, both quantitatively and qualitatively. The paper reads well, and related work is correctly referenced.

There is nothing wrong with the proposed method, it makes sense and I am convinced it works well. However, I found the contributions quite straightforward, and it is difficult to get excited about the paper.

More details would be welcome for Section 3.2","The reviewer acknowledges the merits of the paper, stating its clarity, good results, and proper referencing. However, they also express that the contributions feel ""straightforward"" and not particularly ""exciting."" This suggests a somewhat lukewarm reception overall, leaning slightly towards the positive due to the paper's technical soundness. ",20.0,70.0
"The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. 

One question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. 

In (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one ""the"" in ""minimize the the validation"" in the sentence above (5))","The reviewer explicitly states enjoying the paper and finding the idea cool, which points towards a positive sentiment. The reviewer also provides constructive feedback with concrete suggestions for improvement, indicating a polite and professional tone. There is no harsh language or negative framing used.",75.0,80.0
"Summary:
The authors present a novel adversarial attack scheme where a neural net is repurposed or ""reprogrammed"" to accomplish a different task than it the one it was originally trained on. This reprogramming from task1 to task2  is done through a given image from task2 additively enhanced with an adversarial program which is trained given the knowledge of the models parameters. A mapping from the repurposed output from task1 to relevant output for taks2 is also necessary (h_g function).

Review:
This approach seems quite novel as it enables the repurposing of ImageNet classifiers to be used for counting dots in images, MNIST and CIFAR10 classifications. This new type of ""adversarial attack"" by repurposing a model shows surprising efficacy at allowing an attacked models to change its task at hand. Some tasks being more difficult (CIFAR10) than MNIST or counting dots.

The paper is well-written and explains clearly the proposed technique. The proposed technique is simple in its formulation.
The assumption it is based on (access to model parameters) is acceptable for the sake of proof of concept.
Overall it is an interesting paper to read and seems of significance for the community working on adversarial attacks.

Few comments/questions come to mind though:
- The adversarial images are quite different from a common image as they embed the program around the new task images. This makes the technique itself quite susceptible to detection (just look at the statistics of the input images).
- How do you handle front end processing? Usually for ImageNet classification, a system will (for instance) resize its input to 256x256, center crop to 224x224 and renormalize the RGB features to match the statistics from the training data. It looks like the images generated are passed as inputs to the system. Do you assume that the front-end steps are not applied or do you assume it is (by including them in the network while training your program W).  My assumption is that you include those steps in the training network for W.
- The size of the program is disproportionately big compare to the task2 embedded image. This begs the question: what happens when you limit the size of the program to a smaller percentage of the whole image? When do you see a break in the reprogramming? Do you need that much extra programming W in your adversarial images?
- As the adversarial images seem to be quite easy to detect, would it be easy to integrate it into some task1 images? The equation (2) gives X_{adv} = \tilda{X} + P, could you use X_{adv} + w * X_{task1}, basically finding a way to hide the program and task2 image within a task1 image. This seems difficult, but have you thought of such approach?

Overall this is a paper that is a pleasant read and should be considered for publication.

Post Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8.  ","The reviewer explicitly states the paper is ""interesting"", ""well-written"", ""of significance"", and ""a pleasant read"" and suggests it should be published. They also upgraded their rating after rebuttal. This points to a positive sentiment. The reviewer does pose some critical questions, but these are constructive and aim to improve the paper. The language used is formal, respectful, and suggestive, indicating politeness.",80.0,90.0
"This paper presents a novel method for budgeted cost sensitive learning from Data Streams.
This paper seems very similar to the work of Contrado’s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively “purchasing” the most valuable features for the current datapoint under evaluation according to a budget. 

In this process, a sample (S_i) with up to “d” features arrives for evaluation.  A partially revealed feature vector x_i arrives at time “t” for consideration.  There seems to exist a set of “known features” that that are revealed “for free” before the budget is considered (Algorithm 1).  Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility.  When the stopping condition is reached, a prediction is made.  After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated.

The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado’s RADIN paper.   The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks.
The value of this paper is in the idea that we can learn online and in a cost sensitive way.  The most compelling example of this is the idea that a patient shows up at time “t” and we would like to make a prediction of disease in a cost sensitive way.  To this end I would have liked to have seen a chart on how well this algorithm performs across time/history.  How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).

Am I correct in assuming there are some base features that are revealed “for free” for all samples?  If so how are these chosen?  If so how does the number of these impact the results?  

In Contrado’s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset “cardio.”  Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset?  Did you actually re-implement RADIN or just take the numbers from their paper?  In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).

With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm.  For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost.  The second example uses computational cost vs relevance gain.  This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost.  With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). 

 In reality, these costs would be bundled.  You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience.  Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers.  Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront.  It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey.  The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage.  To show the value of your work, a better discussion of the cost savings would be appreciated.             
","The review is critical of the paper's novelty, methodology, and real-world applicability. The reviewer points out significant similarities with a previous work (RADIN) and questions the choice and justification of datasets and cost structures. While the language is direct and critical, it avoids personal attacks and maintains a professional tone.",-30.0,60.0
"In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances.

I think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising.

I have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained?

My another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for?
I am also not sure if it is appropriate to ""communicate"" through external repositories during the reviewing period.","The reviewer provides positive feedback, stating the idea is 'quite simple' and the results 'promising.' However, they also raise valid concerns and questions, indicating a neutral to slightly positive stance overall.",50.0,70.0
"
In this work, the authors considers a variation of GAN by consider simultaneously decrease the probability that real data is real for the generator. To include such a property, the authors propose a relativistic discriminator which estimate the probability that the given real data is more realistic than the fake data. Numerical results are performed to show that the proposed methods are effective, and the resulting GANs are relatively more stable and generate higher quality data samples than their non-relativistic counterparts.

Overall the paper is well written and the rationale behind the proposed modification is clear. In particular, the authors use three different perspective, (the prior knowledge, the divergence minimization, and the gradient expressions), to explain what they thought is missing in the state-of-the-art. By proposing to utilize the information about both real and fake data in the discriminator definition, the authors’ have (to some extent) alleviated the above shortcoming of the state-of-the-art.  Unfortunately, like almost all papers related to the field,  there has been no rigorously justification behind the proposed methods. 

The English of the paper has to be significantly improved. For example, grammar errors like “this mean….”, “didn’t converge, …”

Unfortunately, the codes of the paper is not released, I will encourage the authors to do so. 
","The review starts with a factual summary of the paper's content. It then praises the paper's clarity and the authors' approach (""well written"", ""rationale ... is clear""). While it points out a lack of rigorous justification, it acknowledges this is a common issue in the field. The criticism regarding English and code availability is constructive and not delivered harshly. Overall, the tone is balanced and leans positive.",60.0,70.0
"This paper proposes an algorithm to approximate kernel matrix based on the Taylor expansion of the element-wise functions. The authors provide a spectral norm based error bound for their method and the corresponding results for the special case, \epsilon-sparse matrix.

I have some comment as follow.

1. Can you provide some comparison with Nystrom methods? It is very popular for kernel approximation and looks more efficient than the proposed algorithm. 

2. The analysis relies on the Gaussian assumption on the input matrix. Can we extend it to more general case?

3. In section 5, the paper said “as our method consists in computing the sparse eigenvectors of a p \times p matrix which can be done by power method, the complexity of estimating the principal component is about O(ps) where s is the sparsity level”. The time complexity of the proposed algorithm is not clearly.
a) Is there any bound for the sparsity level s? Why the eigenvectors of p \times p matrix is sparse?
b) The convergence of power method is heavily affected by the eigen-gap of the matrix. Is there any theoretical or empirical result for the convergence behavior of power method on approximate matrix and original matrix?

","The review starts with a neutral summary of the paper's content, not expressing immediate negativity or positivity. This contributes to a neutral sentiment score. The reviewer then poses relevant questions and requests for clarification, indicating areas of potential improvement. However, the language used is constructive and not dismissive of the work, suggesting a polite and professional tone. The reviewer's use of ""Can you provide"" and ""Is there any"" are polite ways to phrase their suggestions.",0.0,50.0
"This work analyses the information bottleneck (IB) method applied to the supervised learning of a deterministic rule Y=f(X).

The idea as I understood it is as follows:
1) In a first section the authors discuss the relationship between supervised learning through minimization of the empirical cross entropy and the maximization of the empirical mutual information with an intermediate latent variable T. 
2) They show that in the case of a deterministic rule, the information bottleneck curve has a simple shape, piecewise linear, and is not strictly concave. 
3) They show that the optimization of the IB Lagrangian for different \beta does not lead to a point by point exploration of the IB curve.
4) They propose a cure to the previous issue by introducing the squared IB Lagrangian. 
5) They exhibit uninteresting representations (noisy versions of the output Y) that are on the IB curve.
6) They show that multiple successive representations (like in DNNs), have identical predicting power (mutual information with output Y) when they allow for perfect prediction. 
7) They use the IB method to train a neural net on MNIST, using the Kolchinsky estimate of the mutual informations. 
	- they show that the optimization of the squared IB reaches more different points on the IB curve,
	- but that these representations are possibly uninteresting (hard clustering of uneven numbers of grouped classes) 
	- they show that for large enough value of beta, zero error is reached. 

The necessity of noise in the IB theory has been already pointed out by (Gilad-Bachrach et al., 2003; Shwartz-Ziv et al.  2017), although the more thorough analysis proposed here is novel. In practice, besides a few recent propositions (Kolchinsky et al., 2017; Alemi et al., 2016; Chalk et al., 2016) the IB Lagrangian is not a usual objective function for supervised learning. The motivation and impact of this work studying deterministic rules is therefore not completely convincing. 

Further pros and cons:

Pros:
- The discussion is generally well written. 
- This work provides in depth clarification of the counter-intuitive behaviors of the IB method in the case to the learning of a deterministic rule. 
- These are demonstrated with experiments conducted on the MNIST dataset for concreteness.

Cons:
- The fact that multiple successive representations have identical predicting power when the prediction error is zero, was already observed for example in Shwartz-Ziv et al.  2017. It is not clear why this should be considered as an issue. It also seems to be a straightforward observation when restricting to the empirical measure on the training set. 
- The fact that the entire IB curve is not explored point by point by the IB Lagrangian is not necessarily an issue for learning. In the experiments of the present paper, the results seem to suggest that the interesting intermediate representations (separation in 10 compact clusters of the MNIST classes) is actually easier to obtain (large range of \beta) optimizing the IB Lagrangian rather than the proposed squared IB Lagrangian. 

Questions:
- Do the authors know of an application where the full probing of the IB curve would be necessary?
- In Section 2, when injecting the decomposition of the prediction density q(y|x) over the intermediate variable t in eq (3) was a Jensen inequality replaced by an equality?
","The review is mostly positive, acknowledging the paper's contributions (clarifying counter-intuitive behaviors of IB, thorough analysis, experiments on MNIST). However, it also points out limitations, arguing that some findings are not novel or impactful. The language is polite, employing constructive criticism and posing questions for clarification.",50.0,70.0
"The authors present a method for generating points clouds with the help of graph convolution and a novel upsampling scheme. The proposed method exploits the pairwise distances between node features to build a NN-graph. The upsampling scheme generates new points via a slimmed down graph convolution, which are then concatenated to the initial node features. The proposed method is evaluated on four categories of the ShapeNet dataset. Resulting point clouds are evaluated via a qualitative and quantitative comparison to r-GAN.

As far as I know, the paper introduces an overall novel and interesting idea to generate point clouds with localized operations.


The following questions could be addressed by the authors in a revised manuscript:

* The upsampling operation is not well motivated, e.g., neighboring node features are weighted independently, but root node features are not. What is the intuition besides reducing the number of parameters? Are there significant differences when not using diagonal weight matrices?
* As computation of pairwise node feature distances and graph generation based on nearest neighbors are expensive tasks, more details on the practical running time and theoretical complexity should be provided. Can the complexity be reduced by rebuilding graphs only after upsampling layers? How would this impact the performance of the proposed model?
* Although the evaluation on four categories is reported, Table 2 only gives results for two categories.
* How is the method related to GANs which generates graphs, such as GraphGAN or NetGAN?","The reviewer states that the paper presents a ""novel and interesting idea"", which points towards a positive sentiment. However, the reviewer also lists several questions and points that could be improved. Therefore, the sentiment is scored as moderately positive. The language used is objective and professional throughout the review, indicating a neutral politeness.",60.0,0.0
"This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks.

Pros:
1.	A novel algorithm that promotes the interpolation ability of AE
2.	A new synthesized line benchmark to verify the interpolation ability of different AE variants
3.	Strong results on downstream classification and clustering tasks

Cons: 
1.	The interplay of the adversarial network (between AE and critic) isn’t very clear and can be improved
2.	Eq. 1, should x be x_1 or a new data other than x1 and x2?
3.	The paper states that the 2nd term of Eq. 1 isn’t crucial. If x is a new data (other than x1 or x2), how can the critic infer \alpha without a reference to x1 or x2?
4.	The paper states that “encouraging this behavior also produce semantically smooth interpolation …”. Besides the empirical evidences from data, it would be better to any some theoretical justifications.
","The review starts by summarizing the paper's contributions and then lists both positive and negative aspects. While the reviewer acknowledges the novelty and strong results, they also raise several questions and suggest improvements. This suggests a generally positive but critical stance. The language used is objective and professional, without any personal attacks or overly negative phrasing.",50.0,75.0
"The paper proposes a method for inverse reinforcement learning based on AIRL. It's main contribution is that the shaping function is not learned while training the discriminator, but separately as an approximation of the empowerment (maximum mutual information). This shaping term aims to learn disentangled rewards without being restricted to learning state-only reward functions, which is a major restriction of AIRL.

The main weakness of the paper is, that it does not justify or motivate the main deviations compared to AIRL. The new objective for updating the policy is especially problematic because it does no longer correspond to the RL objective but includes an additional term that biases the policy towards actions that increase its empowerment. Although both terms of the update can be derived independently from an IRL and Empowerment perspective respectively, optimizing the sum was not derived from a common problem formulation. By combining these objectives, the learned reward function may lead to policies that fail to match the expert demonstration without such bias. This does not imply that the approach is not sound per se, however, simply presenting such update without any discussion is insufficient--especially given that it constitutes the main novelty of the approach. I think the paper would be much stronger if the update was derived from an empowerment-regularized IRL formulation. And even then, the implications of such bias/regularization would need to be properly discussed and evaluated, in particular with respect to the trade-off lambda, which--again--is hardly mentioned in the submission. I'm also not sure if the story of the paper works out; when we simply want to use empowerment as shaping term, why not use two separate policies for computing the empowerment and reward function respectively. Is the bias in the policy update maybe more important than the shaping term in the discriminator update for learning disentangled rewards?

Keeping these issues aside, I actually like the paper. It tackles the main drawback of AIRL and the idea seems quite nice. Having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions. The paper is also well written with few typos. The parts that are discussed are clear and the experimental results seem fine as well (although more experiments on the reward transfer would be nice).

Minor notes:
I think there is a sign error in the policy update
Typo in the theorem, grantee should be guarantee

Question:
Please confirm that the reward transfer was learned with a standard RL formulation. Does the learned policy change, when we use the empowerment objective as well?



Update (22.11)
I think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. 

- The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.

- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.

- The derivation could be a bit more rigorous.

As the presentation is now much more sound, I slightly increased my rating.","The reviewer acknowledges the paper's merits, finding the idea nice and the writing clear. They find the experimental results acceptable, though desiring more. However, they express significant concerns about the justification and motivation behind key deviations from AIRL, particularly the policy update. The lack of discussion regarding these choices and their implications, especially the trade-off lambda, forms the crux of their criticism. While initially finding the submission borderline, the reviewer acknowledges improvements in the revised version, particularly attributing improved generalization to an inductive bias. However, they still raise valid concerns about the necessity of empowerment for reward shaping and the need for further motivation and rigorous derivation. The language used is professional and provides constructive criticism.",40.0,80.0
"The paper presents a coupled deep learning approach for generating realistic liquid simulation data that can be useful for real-time decision support applications. While this is a good applied paper with a large variety of experimental results, there is a significant lack of novelty from a machine learning perspective. 

1. The primary novelty here is in the problem formulation (e.g., defining cost function etc.) where two networks are used, one for learning appropriate deformation parameters and the other to generate the actual liquid shapes. This is an interesting idea to generate the required training data and build a generalizable model. 

2. But based on my understanding, this does not really explicitly incorporate the physical laws within the learning model and can't guarantee that the generated data would obey the physical laws and invariances. So, this is closer to a graphics approach and deep learning has been used before extensively in a similar manner for shape generation, shape transformation etc.    

3. In terms of practical applications, to the best of my knowledge there are sophisticated physics-based and graphics based approaches that perform very fast fluid simulations. So, the authors need to provide accuracy and computation cost/time comparisons with such methods to establish the benefits of using a deep learning based surrogate model.   

xxxxxxxxxxxxxxxxxxx

I appreciate the rebuttals from the authors, updated my score, but I still believe (just like another reviewer) that this is better suited for a workshop or a conference like SIGGRAPH. ","The review acknowledges the applied value of the paper and finds the core idea interesting. However, it points out a lack of novelty in terms of machine learning and questions the practical benefits compared to existing methods. The reviewer also suggests the paper might be better suited for a different venue. All of this suggests a lukewarm response, leaning towards the negative. The language used is professional and polite throughout.",-20.0,50.0
"Summary:
The paper presents techniques for training a non expansive network, which keeps the Lipchitz constant of all layers lower than 1. While being non-expansive, means are taken to preserve distance information better than standard networks. The architectural changes required w.r.t standard networks are minor, and the most interesting changes are made to the loss minimized. The main claim of the paper is that the method is robust against adversarial attacks of a certain kind. However, the results presented show that a) such robustness comes at a high cost of accuracy for standard examples, and b) even though the network is preferable to a previous alternative in combating adversarial examples, the accuracy obtained in the face of adversarial attacks is too low to be of practical value. Other properties of the networks, explored empirically, are that the confidence of the prediction is indicative of robustness (to adversarial attacks) and that the networks learn better in the presence of high label noise. 
In short, this paper may be of interest to a sub-community interested in defense against certain types of adversarial attacks, even when the defense level is much too low to be practical. I am not part of this community, hence did not find this part very interesting. I believe the regularization results are of wider interest. However, to present this as the main contribution of L2NNN more work is required to find configuration which are resilient to overfit yet enable high training accuracy, and more diverse experiments are required.
Pros:
+ the idea of non expansive network is interesting and important
+ results indicate some advantages in fighting adversarial examples and label noise
Cons:
- the results for fighting adversarial examples are not significant from a practical perspective
- the results for copying with label noise are preliminary and require expansion with more experiments.
- the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention
- presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks. The choice to place the related work at the end also reduces presentation clarity.

More detailed comments:
Pages 1-3: In many places, small proofs are left to the reader as ‘straightforward’. Examples are: the claim in the introduction, in eq. 2, in section 2.2, section 2.3’ last line of page 3, etc.. While the claim are true (in the cases I tried to verify them long enough), this makes reading difficult and not fluent. For some of these claims I do not see the argument behind them. In general, I think proofs should be brought for claims, and short proofs (preferably) should be brought for small claims. Leaving every proof to the reader as an exercise is not a convenient strategy. 
Page 4: The loss is complex and its terms utility require empirical evidence. The third term is shown to be clearly useful, enabling a trade off between train accuracy and margin. However, the utility of terms 4) and 5) is not verified. Do we really need both these terms? Cannot we just stay with one?
The main claim is robustness w.r.t “white-box non targeted L2-bounded attacks”. This seems to be a very specific attack type, and it is not explained at all in the text. Hence it is hard to judge the value of this robustness. Explanation of adversarial attack kinds, and specifically of “white-box non targeted
L2-bounded attacks” is required for this paper to be a stand alone readable paper. Similarly ‘L_\infty’-bounded attacks, for which results are shown, should be explained.
Table 1,2: First, the model architecture used in these experiments is not stated. Second, the accuracy of the ‘natural’ baseline classifier, at least in the MNist case, is somewhat low – much better results can be obtained with CNN on MNist. Third, the accuracies of the suggested robust models are very low compared to what can be obtained on these datatsets. Forth, while the accuracies under attack of the proposed method are better than those of Madri et al., both are quite poor and indicate that the classifier is not useful under attack (from a practical perspective).
Page 6: The classifiers which share the work between an L2NNN network and a regular more accurate network may be interesting, as the accuracies reported for them are significantly higher than the L2NNN networks. However, the robustness scores are not reported for these classifiers, so it is not possible to judge if they lead to a practical and effective strategy.
Page 7: For me, the results with partially random labels are the most interesting in the paper. The resistance of L2NNN to overfit and its ability to learn with very noisy data are considerably better than the suggested alternatives.
Relevant work not mentioned “Spectral Norm Regularization for Improving the Generalizability of Deep Learning” - Yuichi Yoshida and Takeru Miyato, Arxiv, 2017.

I have read the rebuttal.
The discussion was interesting, but I do not see a need to change my assessment.
The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this.
I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. 

","The reviewer finds the core idea interesting and potentially important (+1 in 'pros') but ultimately is not convinced about the practical significance of the work. They find multiple issues with the experiments and results, deeming them preliminary at best and severely lacking in some aspects. They list four clear 'cons' against only two 'pros'. The language is quite direct, pointing out flaws and demanding more work, but it stays within the bounds of academic criticism and doesn't resort to personal attacks.",-25.0,50.0
"This paper shows that deep ""narrow"" neural networks (i.e. all hidden layers have maximum width at most the input dimension) with a variety of activation functions, including ReLU and sigmoid, can only learn functions with unbounded level set components, and thus cannot be a universal approximator. This complements previous work, such as Nguyen et. al 2018 which study connectivity of decision regions and Lu et. al 2017 on ReLU networks in different ways.

Overall the paper is clearly written and technically sound. The result itself may not be super novel as noted in the related work but it's still a strict improvement over previous results which is often constrained to ReLU activation function. Moreover, the proofs of this paper are really nice and elegant. Compared to other work on approximation capability of neural networks, it can tell us in a more intuitive way and explicitly which class of functions/problems cannot be learned by neural networks if none of their layers have more neurons than the input dimension, which might be helpful in practice. Given the fact that there are not many previous work that take a similar approach in this direction, I'm happy to vote for accepting this paper.  

Minor comments:
The proof of Lemma 3 should be given for completeness. I guess this can be done more easily by setting delta=epsilon, A_0=A and A_{i+1}=epsilon-neighborhood of f_i(A_i)?
page7: the square brackets in ""...g(x'')=[y-epsilon,y+epsilon]..."" should be open brackets.
page7:""By Lemma 4, every function in N_n has bounded level components..."" -> ""..unbounded...""","The reviewer explicitly states ""I'm happy to vote for accepting this paper"", which implies a positive sentiment. They find the paper well-written, technically sound, and an improvement on previous work. While they acknowledge the result may not be ""super novel"", they highlight the elegance of the proofs and the practical implications. The reviewer also provides constructive minor comments for improvement, indicating a polite and helpful approach.",75.0,100.0
"This paper proposed an approximation technique to learn the large-scale graph with the desired edge density. It was well-written and contains thorough experimental results and analysis.

A minor drawback is that while this work was motivated by the use of k-NN graph in graph convolution network (GCN), there was no evidence on how well A-NN performs in compare to k-NN with GCN.","The review starts with positive statements about the paper, highlighting its clarity, thoroughness, and experimental validation. This suggests a positive sentiment. The reviewer then points out a minor drawback, which indicates that the review is not entirely positive. However, the use of ""minor"" suggests that this drawback is not a major concern. The language used throughout is constructive and professional, indicating politeness. Therefore, the sentiment is moderately positive, and the language is polite.",60.0,80.0
"This work extends on [1] by constructing CNN filters using Fourier-Bessel (FB) bases for rotation equivariant networks. Additionally to [1] it extends the process with using SO(2) bases which allow to learn combination of rotated FB bases and ultimately achieve good performance with less parameters than standard CNN networks thanks to filter truncation.

In general, this work is well written and shows interesting results. However it lacks context with regards to other existing works. For example [2] also uses steerable filters for achieving rotation equivariance, however with different steerable bases (rotation harmonics instead of FB). It would be useful to clarify why FB bases are more appropriate for truncation, eventually providing empirical evidence (even though rotation harmonics would probably need more parameters). Authors mention [2], however disregard it due to computational complexity, which would be the same if the rotation harmonics bases were truncated as well.

Similarly, this work is not strong in evaluating against existing methods. It provides evaluation of the vanilla group equivariant networks in a similar configuration, but due to design choices in the training and test set, it is not possible to compare it against other algorithms and other steerable bases such as those from [2]. This degrades the results slightly as it does not allow to verify the baseline results from other works.

Additionally, it would be useful to provide an ablation study which would show how important the bases in SO(2) are important for the model accuracy. This would allow to compare the results against the [1] as the FB filters are steerable as well (Equation 4).

It is hard to reach a final rating for this submission. On one hand, it can be seen as an incremental improvement of [1] for a new domain of tasks, without a thorough comparison against existing methods. On the other hand, the paper is well written and the results look promising - evaluation verifies that the algorithm performs well in multiple tasks with a fraction of parameters.

Considering that authors plan to release the source code and that this conference aims for publishing novel ideas (and the goal of this work is to achieve rotation equivariance with less parameters, which hasn't been tackled before), I am inclined towards acceptance of this paper, even though the experiments can be significantly improved.

Unfortunately, I was not able to verify correctness of the provided proofs.

Additional minor issues:
* The paper does not specify what FB bases exactly are being used (such as in [table 1;1]), mainly it does not seem to specify the SO(2) bases.
* It would be useful to visualise K and K_\alpha in Figure 1.
* Citations, if not part of the sentence, should be in parentheses to improve readability (\citep for natbib).
* On page 8, end of first paragraph - wrong reference (see S.M.)
* L, in section 2.3 is not defined.

[1] Qiu, Qiang, et al. ""DCFNet: Deep Neural Network with Decomposed Convolutional Filters."", ICML 2018
[2] Weiler, Maurice, et al. “Learning Steerable Filters for Rotation Equivariant CNNs.” CVPR 2018
","The reviewer acknowledges the paper's merits, such as being well-written and presenting promising results with a novel approach to achieving rotation equivariance. However, they also point out significant weaknesses related to the lack of thorough comparison with existing methods and missing details in the experimental setup. The reviewer leans towards acceptance due to the novelty and potential but suggests significant improvements to the experimental evaluation. The language used is formal, polite, and constructive in its criticism.",50.0,80.0
"Summarization:
This paper presents a framework (called FX Network) of quantizing the weights and gradients of neural networks, based on five quantization criteria proposed in literature. The proposed framework can quantize the neural network obtaining a minimal or close-to-minimal error for a pre-specified precision level.


Pros:
- The proposed FX network can quantize all variables including both network weights and back-propagated gradients.
- Promising results have been obtained. Experimental results on CIFAR have shown that the proposed quantization framework had reduced the representational cost, computational cost, and the communication  by up to 6x, 8x, and 4x, respectively, compared to the 32-b FL baseline and related works.
- The paper is well written.




Cons:
- The experiment results showed in Figure 3 are quite confusing: why do the curves of the test error and loss suddenly drop at epoch 100? Explanation is needed. 

- This proposed quantization method require to pre-train a network with high precision in advance, similarly as the student-teacher framework or knowledge distillation. Different from BN and TG, FX network requires to pre-train a 32-b floating-point network, which requires more extra computational costs. 

- How does the quantization method compare with strategies like parameter pruning and sharing? It is better to see a discussion with them. It is also suggested to show the improvement of the proposed framework in terms of inference time during test. 

","The review acknowledges the merits of the paper, highlighting its contributions and clear writing. However, it also raises valid concerns and suggests areas for improvement, indicating a balanced perspective. Overall, the tone is constructive and helpful, aiming to guide the authors towards enhancing their work.",50.0,70.0
"This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties the identity STE,  by studying the properties of the orientation and norm of the course gradients for STE.

While the paper presents many theoretical results which might be useful for the community, they are not organized very well.  It is a bit hard for readers to quickly find the most important theoretical results.  Moreover, some symbols are used without definition, e.g. g_{relu} is used before being defined in sec 3.1. The discussions for most theoretical results are very short or not organized well, making the whole paper hard to follow,  e.g., ""the key observation ..."" after Lemma 4 is actually not about the Lemma 4 above, but Lemma 5 in the next Lemma.  Another major concern is that activation quantization is usually used in combination with weight quantization.  It would be more useful if weight and activation quantizations can be analyzed together.

Clarity in the experiment part can also be further improved. From Table 1, the clipped ReLU STE has the best performance, however, there is no theoretical analysis for it. For ResNet-20 with 2-bit activation, the training loss/accuracy results of vanilla ReLU is much worse than clipped ReLU, is there any explanation for this?  For the discussion in sec 4.2, what information does it want to convey?  What is the ""normal schedule of learning rate""? What if the small learning rate 1e-5 is kept after 20 epochs?

Typo: The last sentence on page 3, the definition of y*.

------------------------

The author response have addressed most of my concerns. Thus I have increased my score. ","The review starts with a positive sentiment, acknowledging the value of the theoretical analysis. However, it then lists several significant weaknesses related to organization, clarity, missing analysis, and unanswered questions. The reviewer also mentions increasing their score due to the author's response, indicating an overall more positive final view. Considering these points, the sentiment leans towards positive, but not overly enthusiastic due to the initially pointed criticism. The language used is constructive and professional, suggesting areas for improvement without resorting to harsh or disrespectful language.",60.0,75.0
"This paper introduces a method for handing input data that is defined on irregular mesh-type domains. If I understand it correctly, the core technique is  to perform Fourier analysis to transform the input data to the frequency domain, which is then transformed back to a regular domain before applying a standard neural network. The claimed result is that this is better than standard linear interpolations. The key technical contribution is to define FT on points, edges, and meshes (This reviewer appreciates these efforts). Explicit formula are given. However, the paper does not perform convolutions on the input irregular domain directly, which is quite disappointing. The experimental results are preliminary. It is expected to perform evaluation on more applications such as semantic segmentation. 


The major issue of the paper is that the goal was not stated clearly. Does it target for a neural network that is defined on irregular domains or simply a technique to handling irregular domains? Given the Fourier transform, it is possible to define convolutions directly as multiplications in the Fourier domain....the paper can be more interesting, if it follows this line.

Overall, it is hard to champion the paper based on the current technical approach and the experimental results. 
","The reviewer acknowledges the technical contributions and efforts in defining FT on different structures. However, they express disappointment about the limitations of the approach, particularly the lack of direct convolutions on irregular domains. The reviewer finds the experimental results to be preliminary and suggests exploring alternative directions. The overall tone suggests that the reviewer sees potential in the work but is not fully convinced by the current approach and results.",-10.0,60.0
"The paper presents several ways to regularize plain ReLU networks to optimize 3 things

- the adversarial robustness, defined as the fraction of examples for which adversarial perturbation exists
- the provable adversarial robustness, defined as the fraction of examples for which some method can show that there exists no adversarial example within a certain time budget
- the verification speed, i.e. the amount of time it takes some method to verify whether there is an adversarial example or not
 
Overall, the ideas are sound and the analysis is solid. My main concern is the comparison between the authors method and the 'certification' methods, both conceptually and regarding performance.

The authors note that their method falls under 'verification', whereas many competing methods fall under 'certification'. They point to two advantages of verification over certification: (1) the ability to provide true negatives, i.e. prove that an adversarial example exists when it does, and (2) certification requires that 'models must be trained and optimized for a specific certification method'. However, neither argument convinces me regarding the utility of the authors method. 

Regarding (2): The authors method also requires training the network in a specific way (with RS loss), and it is only compatible with verifiers that care about ReLU stability. 

Regarding (1): It is not clear that this would be helpful at all. Is it really that much better if method A has 80% proven robustness and 20% proven non-robustness versus method B that has 80% proven robustness and 20% unknown? One could make the case that method B is actually even better.

So overall, I think one has to compare the authors method and the certification methods head-to-head. And in table 3, where this is done, Dvijotham comes out on top 2 out of 2 times and Wong comes out on top 2 out of 4 times. That does not seem convincing. Also, what about the performance numbers form other papers discussed in section 2?

-------

Other issues:

At first glance, the fact that the paper only deals with (small) plain ReLU networks seems to be a huge downside. While I'm not familiar with the verification / certification literature, from reading the paper, I suspect that all the other verification / certification methods also only deal with that or highly similar architectures. However, I will defer to the other reviewers if this is not the case.

To expand upon my comment above, I think the paper should discuss true adversarial accuracy on top of provable adversarial robustness. Looking at table 1, for instance, for rows 2, 3 and 4, it seems that the verifier used much less than 120 seconds on average. Does that mean the verifier finished for all test examples? And wouldn't that mean that the verifier determined for each test example exactly whether an adversarial example existed or not? In that case, I would write ""true adversarial accuracy"" instead of ""provable adversarial accuracy"" as column header. If the verifiers did not finish, I would include in the paper for how many examples the result was ""adverarial example exists"" and for how many the result was ""timeout"". I would also include that information in table 3, and I would also include proving / certification times there. 

Based on the paper, I'm not quite sure whether the idea of training with L1 regularization and/or small weight pruning and/or ReLU pruning for the purpose of improving robustness / verifiability was an original idea of this paper. In either case, this should be made clear. Also, the paper seems to use networks with adversarial training, small weight pruning, L1 and ReLU pruning as its baseline in most cases (all figures except table 1). If some of these techniques are original contributions, this might not be an appropriate baseline to use, even if it is a strong baselines.

Why are most experiments presented outside of the ""experiments"" section? This seems to be bad presentation.

I would include all test set accuracy values instead of writing ""its almost as high"". Also, in table 3, it appears as if using RS loss DOES in fact reduce test error significantly, at least for CIFAR. Why is that?

While, again, I'm not familiar with the background work on verification / certification, it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures. If that is the case, one has to wonder whether that line of research should be encouraged to continue.

--------

Minor issues:

- ""our focus will be on the most common architecture for state-of-the-art models: k-layer fully-connected feed-forward DNN classifiers"" Citation needed. Otherwise, I would suggest removing this statement.
- ""such models can be viewed as a function f(.,W)"" - you also need to include the bias in the formula I think
- ""convolutional layers can be represented as fully-connected layers"". I think what you mean is ""convolutional layers can be represented as matrix multiplication""
- could you make the difference between co-design and co-training more clear?
- The paper could include in the appendix a section outlining the verification method of Tjeng","The reviewer expresses several concerns and doubts about the paper's approach and findings. They question the utility of the authors' method compared to certification methods, find the performance comparison unconvincing, and point out potential flaws in the baseline and presentation. The reviewer's tone is critical and questioning, but they maintain a professional and analytical approach without resorting to personal attacks or disrespectful language.",-20.0,60.0
"This paper presents an incremental extension to the Self-imitation paper by Oh, Junhyuk, et al. The previous paper combined self-imitation learning with actor-critic methods, and this paper directly integrates the idea into the generative adversarial imitation learning framework.

I think the idea is interesting, but there remains some issues very unclear to me. In the algorithms, when updating the good trajectory buffer, it is said ""We define ‘good trajectories’ as any trajectories whose the discounted sum of rewards are higher than that of the policy"". What does ""that of the policy"" mean? How do you know the reward of the policy?

Second, without defining good trajectories, I don't think Algorithm 1 would work. Algorithm ` 1 misses the part of how to update buffer B. After introducing their own algorithm, the author did not provide much solid proof or analysis for why this self-imitation learning works.

In the experiment section, the author implemented GASIL for various applications and presented reasonable results and compared them with other methods. Nevertheless, without theoretical proof, it is hardly convincing that the results could be consistently reproduced instead of being merely accidental for some applications.

Update:
The rebuttal resolves some of my concerns. However, I still think the contribution is incremental. The current version looks too heuristic, more theoretical analysis or inspirations need to be added.
","The reviewer finds the idea interesting but has several concerns about the clarity and theoretical grounding of the paper. They find the definition of ""good trajectories"" unclear and point out the lack of theoretical analysis to support the algorithm's effectiveness. While acknowledging the decent experimental results, the reviewer remains unconvinced due to the absence of solid proof, suggesting the results could be accidental. The update after rebuttal shows a slight improvement in the reviewer's perception, but they still consider the contribution incremental and call for more theoretical analysis. Therefore, the sentiment is more negative than positive, but not overly so. The language used is critical but professional and not rude.",-20.0,60.0
"In this paper the authors propose DL2 a system for training and querying neural networks with logical constraints

The proposed approach is intriguing but in my humble opinion the presentation of the paper could be improved. Indeed I think that the paper is bit too hard to follow. 
The example at page 2 is not clearly explained.

In Equation 1 the relationship between constants S_i and the variables z is not clear. Is each S_i an assignment to z?

I do not understand the step from Eq. 4 to Eq. 6. Why does arg min become min?

At page 4 the authors state ""we sometimes write a predicate \phi to denote its indicator function 1_\phi"". I’m a bit confused here, when is the indicator function used in equations 1-6?

What kind of architecture is used for implementing DL2? Is a feedforward network used? How many layers does it have? How many neurons for each layer? No information about it is provided by authors.

It is not clear to me why DL2/training is implemented in PyTorch and DL2/querying in TensorFlow. Are those two separate systems? And why implementing them using different frameworks?

In conclusion, I’m a bit insecure about the rating to give to this paper, the system seems interesting, but several part are not clear to me.

[Minor comments]
It seems strange to me to use the notation L_inf instead of B_\epsilon to denote a ball.

In theorem 1. \delta is a constant, right? It seems strange to me to have a limit over a constant.","The reviewer expresses that the proposed approach is ""intriguing"", but finds the paper hard to follow and unclear in several places. They list a number of concrete questions and concerns, indicating a lack of understanding of key elements. While not outright dismissive, the numerous issues raised suggest a significantly negative sentiment. The reviewer's language remains generally polite throughout, using phrases like ""in my humble opinion"" and ""It seems strange to me"" to soften criticism.",-50.0,70.0
"The authors present an architecture search method where connections are removed with sparse regularization. It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.

There are a few grammatical/spelling errors that need ironing out.

e.g. ""In specific"" --> ""Specifically"" in the abstract, ""computational budge"" -> ""budget"" (page 6) etc.

A few (roughly chronological comments).

- Pioneering work is not necessarily equivalent to ""using all the GPUs""

- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!

- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?

-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.

- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?

- You should add DARTS 1st order to table 1. 

- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?

- The ablation study is good, and the results are impressive.

I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time. However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.

------------
UPDATE: Score changed based on author resposne
------------
","The review starts with a positive sentiment, highlighting the merits of the work (good results, relatively quick). It then transitions into constructive criticism, pointing out areas for improvement in a professional manner. The reviewer acknowledges the impressive results and proposes a marginal acceptance, indicating overall positive sentiment but with reservations. The update message further suggests a positive shift in sentiment after the author's response. ",60.0,70.0
"This paper proposes to use GAN to address the image compression problem. It is shown to achieve superior results over the past work in two different settings (GC and SC). 

Novelty:

It has been well discovered in the literature of GANs that they can resolve the problem of blurriness in generation, compared to the traditional MSE loss. This paper proposes to combine a GAN loss with MSE, together with an entropy loss. However similar approaches were used such as video prediction [1] from 2016. The paper lacks a few references like this.

Major questions:

- How do the different loss terms play against each other? The entropy term and the MSE apparently conflict with each other. And how would this affect L_gan? I would like to request some more analysis of this or ablation study on different terms.

- How well does the GAN converge? A plot of G and D loss is often presented with GAN approaches.

- Discrete latent variable is in itself an interesting problem [2]. I see the image compression as a task to discover a discrete latent variable with minimal storage. Perhaps one most important problem is how to estimate the gradient through the discrete bottleneck. But the paper doesn't provide much insights or experiments on this. 

- I'm not fully convinced by the claim of the noise that this paper uses to combine the code can act as a regularizer. Adding the noise makes the decoder output stochastic, but the compression problem seems to be deterministic by nature, unlike many other generation problems.

[1] https://arxiv.org/abs/1511.05440
[2] https://arxiv.org/abs/1711.00937","The reviewer acknowledges the paper's goal and results but raises significant concerns about novelty, methodology, and analysis. They point out the lack of originality and raise several technical questions, suggesting further experiments and clarifications. The tone, while direct, maintains a professional and constructive approach.",-20.0,60.0
"Update: Lower the confidence and score after reading other comments. 
===

In this paper, the authors benchmark several RL algorithms on their abilities of generalization. The experiments show interpolation is somehow manageable but extrapolation is difficult to achieve. 

The writing quality is rather good. The authors make it very clear on how their experiments run and how to interpret their results. The experiments are also solid. It's interesting to see that both EPOpt and RL^2, which claim to generalize better, generalize worse than the vanilla counterparts. Since the success rates are sometimes higher with more exploration, could it be possible that the hyperparameters of EPOpt and RL^2 are non-optimal? 

For interpolation/extrapolation tasks, all 5 numbers (RR, EE, DR, DE, RE) are expected since the geometric mean is always 0 once any of the numbers is 0. 

What does ``""KL divergence coefficient"" in RL^2-PPO mean? OpenAI's Baselines' implementation includes an entropy term as in A2C. 
","The reviewer provides a generally positive overview, stating the writing is good, experiments solid, and results interesting. However, they also raise questions and ask for clarifications, indicating areas for improvement. Therefore, the sentiment is positive but not overly enthusiastic. The language used is entirely professional and courteous, suggesting a neutral to positive politeness level.",60.0,70.0
"This paper proposes an encoder-decoder model based on the graph representation of inputs and outputs to solve the multi-label classification problem. The proposed model considers the output labels as a fully connected graph where the pair-wise interaction between labels can be modelled.

Overall, although the proposed approach seems interesting, the representation of the paper needs to be improved. Below I listed some comments and suggestions about the paper.

- The proposed model did not actually use any graph structure of input and output, which can potentially mislead the readers of the paper. For instance, the encoder is just a fully connected feed-forward network with an additional attention mechanism. In the same sense, the decoder is also just a fully connected feed-forward network. Furthermore, the inputs and outputs used throughout the paper do not have any graph structure or did not use any inferred graph structure from data. I recommend using any graph-structured data to show that the proposed model can actually work with the graph-structured data (with proper graph notations) or revise the manuscript without graph2graph representation.

- I personally do not agree with the statement that the proposed model is interpretable because it can visualise the relation between labels through the attention. NN is hard to interpret because the weight structure cannot be intuitively interpretable. In the same sense, the proposed model cannot avoid the problem with the nature of black-box mechanism. Especially, multiple weight matrices are shared across the different layers, which makes it more difficult to interpret. Although the attention weights can be visualised, how can we visualise the decision process of the model from end-to-end? The question should be answered to claim that the model is interpretable.

- 2.2.1, 2.2.2, 2.3 shares the similar network layer construction, which can be represented as a new layer of NN with different inputs (or at least 2.2.2 and 2.3 have the same layer structure). It would be better to encapsulate these explanations into a new NN module which can be reused multiple parts of the manuscript for a concise explanation.

- Although the network claims to model the interactions between labels, the final prediction of labels are conditionally independent to each other, whereas the energy based models such as SPEN models the structure of output directly. In that sense, the model does not take into account the structure of output when the prediction is made although the underlying structure seems to model the 'pair-wise' interaction between labels.

- In Table1, if the bold-face is used to emphasise the best outcome, I found it is inconsistent with the result (see the output of delicious and tfbs datasets).

- Is it more natural to explain the encoder first followed by the decoder?","The reviewer finds the paper interesting but has significant concerns about the methodology and presentation. They find the core claim (graph-based representation) to be misleading and disagree with the interpretability assertion. While the reviewer offers constructive criticism and suggestions, the overall tone leans towards major revisions being needed. The language, however, remains professional and polite throughout.",-20.0,70.0
"Overview:

This paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.

Main strengths:

1. The paper is well written. The model description in Section 2 is very clear and precise.
2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.
3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.

Main weaknesses:

1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning are well-established). So this raises a concern about novelty (although the experimental results are new).

2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses ""far more resources."" The best ALL-CACO model also relies on several annotated but ""smaller"" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a ""similar"" amount of data from the specific task?

Overall feedback:

This is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.

Edit: I am changing my rating from 5 to 6 based on the authors' response.","The reviewer acknowledges the paper's strengths, such as clear writing, a simple yet effective approach, and extensive experiments. However, they raise concerns about novelty and suggest additional experiments to strengthen the paper. Despite the criticism, the reviewer increases their rating from 5 to 6 based on the authors' response, indicating a generally positive sentiment. The language used is constructive and professional, suggesting politeness.",60.0,80.0
"The biggest contribution is the setting part, where one seeks to adapt one source to multiple, but somewhat similar, target domains.  It is interesting to explore such direction since in many real-world applications, applying the model to many different target domains are required.   

It is also noted that there is one very related work ""Multi-target Unsupervised Domain Adaptation without Exactly Shared Categories"" available online (https://arxiv.org/pdf/1809.00852.pdf).  It is desirable to have a discussion and comparison with them since they are doing Multi-target Unsupervised Domain Adaptation. In their method, the exact shared category is even not required. 

For the algorithm part, authors basically adopt the information-theoretic approach to handle the proposed method. This part contribution is limited since the techniques involved are very common in the domain adaptation. 

","The review starts with positive remarks, highlighting the interesting and relevant contribution of the paper. However, it then points out a very related work that the authors failed to include, which is a significant oversight. Finally, the review states that the technical contribution is limited. Overall, the review leans towards the positive side, but with reservations due to the missing comparison and limited technical novelty.",40.0,50.0
"Strengths:
- clear explanation of the problem
- clear explanation of the model and its application (pseudocode)
- clear explanation of training and resulting hyperparameters

Weaknesses:
- weak experimental settings: 
-- (a) comparison against 'easy to beat' baselines. The comparison should also include as baselines the very relevant methods listed in the last paragraph of the related work section (Snow et a.l 2005, Sun and Grishman 2010, Liao et al. 2017, Cambria et al. 2018). 
-- (b) unclear dataset selection: it is not clear which datasets are collected by the authors and which are pre-existing datasets that have been used in other work too. It is not clear if the datasets that are indeed collected by the authors are publicly available. Furthermore, no justification is given as to why well-known publicly available datasets for this task are not used (such as CoNLL-YAGO (Hoffart et al. 2011), ACE 2004 (NIST, 2004; Ratinov et al. 2011), ACE 2005 (NIST, 2005; Bentivogli et al. 2010), and Wikipedia (Ratinov et al. 2011)).
- the coverage of prior work ignores the relevant work of Gupta et al. 2017 EMNLP. This should also be included as a baseline.
- Section 2 criticises Mikolov et al.'s skip-gram model on the grounds that it introduces noisy entities because it ignores context structure. Yet, the skip-gram model is used in the preprocessing step (Section 3.1). This is contradictory and should be discussed.
- the definition of synonyms as entities that are interchangeable under certain contexts is well known and well understood and does not require a reference. If a reference is given, it should not be a generic Wikipedia URL.
- the first and second bulletpoint of contributions should be merged into one. They refer to the same thing. 
- the paper is full of English mistakes. A proficient English speaker should correct them.
","The review points out several significant weaknesses, such as weak experimental settings, missing relevant baselines, and contradictory use of a prior model. While it acknowledges the clarity of the paper, the criticisms regarding the experimental setup and lack of strong baselines suggest a negative sentiment. However, the language used is generally constructive and not overtly harsh, indicating a level of politeness. Therefore, the sentiment leans negative, but the politeness remains relatively neutral.",-40.0,20.0
"The paper proposes to plan by taking an initial plan and improving it. The authors claim that 1) this will achieve results faster than planning from scratch and 2) will lead to better results than using quick, local heuristics. However, when starting with an initial solution there is always the danger of the final solution being overly biased by the initial solution. The authors do not address this adequately. They show how to apply tree and DAG-based LSTMs to job scheduling and shortening expressions. Since they are simply using previously proposed LSTM variants, I do not see much contribution here. The experiments show some gains on randomly generated datasets. More importantly, details are missing such as the definitions of SP and RS from section 4.4.","The review is mostly negative. The reviewer points out a major flaw in the paper's approach, stating the authors don't address the bias introduced by their proposed method. The reviewer doesn't see much contribution in the application of existing LSTM variants. While the reviewer acknowledges some gains, they note missing details and express concerns about the datasets used. The language used is critical but maintains a professional and academic tone. ",-50.0,50.0
"I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it.  I have three major complaints with this paper:                                                                         
                                                                                                     
1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering.
                                                                                                     
Question answering is not a unified phenomenon.  There is no such thing as ""general question answering"", not even for humans.  Consider ""What is 2 + 3?"", ""What's the terminal velocity of a rain drop?"", and ""What is the meaning of life?""  All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.
                                                                                                     
Question answering is a _format_ for studying particular phenomena.  Sometimes it is useful to pose a task as QA, and sometimes it is not.  QA is not a useful format for studying problems when you only have a single question (like ""what is the sentiment?"" or ""what is the translation?""), and there is no hope of transfer from a related task.  Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems.

We have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks).  I don't see any compelling justification for setting things up this way.
                                                                                                     
2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering.  There is nothing new in the transfer results that were presented here, however.  For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first).  For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly).  For the Czech task, fine tuning a pre-trained model has already been shown to help.  Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before.  The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label.  It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case.
                                                                                                     
3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice.  Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious.  Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior.
                                                                                                     
The three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results.  Any of these three could have been a single conference paper, had it been done well.  As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point.  Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?).  For MQAN, there's more than a page of the core new architecture that's pushed into the appendix.  And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere.","The reviewer expresses strong negative sentiment towards the paper, stating that they see ""little justification for accepting it."" They list three major complaints, focusing on the framing of the paper's central concept, lack of novelty in the results, and poor organization. The language used is critical and direct, but not overtly rude. They use strong phrasing like ""misguided"" and ""egregious"" but maintain a professional tone overall.",-75.0,-20.0
"The submission proposes to combine a tree2tree autoencoder with a sequence encoder for natural language. It uses the autoencoding objective to appropriately shape the latent space and train the decoder, and then uses a second training step to align the output of a sequence encoder with the input for the tree decoder. Experiments on a recent dataset for the natural language-to-code task show that the proposed model is able to beat simple baselines.

There's much to like about this paper, but also many aspects that are confusing and make it hard to tease out the core contribution. I'm trying to reflect my understanding here, but the authors could improve their paper by providing an explicit contribution list. Overall, there seem to be three novel things presented in the paper:
(1) (Pre)training the (program) tree decoder using an autoencoder objective
(2) The doubly-recurrent tree decoder, which follows a different signal propagation strategy from most other approaches.
(3) An ""attention"" mechanism over the point in latent space (that essentially rescales parts of the decoder input)

However, the experiments do not evaluate these contributions separately; and so their relative merits remain unclear. Primarily, I have the following questions (for the rebuttal, and to improve the paper):

Re (1):
 (a) Does the pre-training procedure help? Did you evaluate joint end-to-end training of the NL spec encoder and the tree decoder? 
 (b) The auto-encoder objective would allow you to train on a larger corpus of programs without natural language specifications. Arguably, the size of the dataset is insufficient for most high-capacity deep learning models, and as you use word embeddings trained on a much larger corpus...), you could imagine training the autoencoder on an additional corpus of programs without NL specs. Did you attempt this?

Re (2): 
 (a) The tree decoder is unusual in that (one) part of the recurrence essentially enforces a breadth-first expansion order, whereas almost all other approaches use a depth-first technique (with the only exception of R3NN, as far as I remember). You cite the works of Yin & Neubig and Rabinovich et al.; did you evaluate how your decoder compares to their techniques? (or alternatively, you could compare to the absurdly complex graph approach of Brockschmidt et al. (arxiv 1805.08490)))
 (b) Ablations on this model would be nice: How does the model perform if you set the horizontal (resp. the vertical) input to 0 at each step? (i.e., ablations to standard tree decoder / to pure BFS)

Re (3): This is an unusual interpretation of the attention mechanism, and somewhat enforced by your choice (1). If you run an experiment on end-to-training (without the autoencoder objective), you could use a standard attention mechanism that attends over the memories of the NL encoder. I would be interested to see how this would change performance.

As the experimental evaluation seems to be insufficient for other researchers to judge the individual value of the paper's contribution, I feel that the paper is currently not in a state that should be accepted for publication at ICLR. However, I would be happy to raise my score if (some) of the questions above are answered; primarily, I just want to know if all of the contributions are equally important, or if some boost results more than others.


Minor notes:
- There are many spelling mistakes (""snipped"" for ""snippet"", ""isomorhpic"", ...) -- running a spell checker and doing a calm read-through would help with these details.
- page1par2: Writing specifications for programs is never harder than writing the program -- a program is a specification, after all. What you mean is the hardness of writing a /correct/ and exact spec, which can be substantially harder. However, it remains unclear how natural language would improve things here. Verification engineers will laugh at you if you propose to ""ease"" their life by using of non-formal language...
- page1par3: This (and the rest of the paper) is completely ignoring the old and active field of semantic parsing. Extending the related work section to compare to some of these works, and maybe even the experiments, would be very helpful.
- page2par3 / page6par4 contradict each other. First, you claim that mostly normal english vocabulary is used, with only occasional programming-specific terms; later you state that ""NL vocabulary used in specifications is strongly related to programming"". The fact that there are only 281 (!!!) unique tokens makes it very doubtful that you gain anything from using the 1.9million element vocab of GLoVe instead of direct end-to-end training...
- page4par3: You state ""a reference to a previously used variable may require 'climbing up' the tree and then descending"" - something that your model, unlike e.g. the work of Yin & Neubig, does not support. How important is this really? Can you support your statement by data?
- page5, (14) (and (13), probably): To avoid infinite recursion, the $h_i^{(pred)}$ on the right-hand-side should probably be $h_{i-1}^{(pred)}$

","The review starts with positive sentiment, highlighting the interesting aspects of the paper. However, it then raises several concerns and questions about the clarity, experimental validation, and comparison with related work. The reviewer explicitly states that the paper is not ready for acceptance in its current form. This indicates a mixed sentiment leaning towards the negative side due to the significant revisions needed. The language used is polite and professional, providing constructive criticism and suggestions for improvement without resorting to harsh or disrespectful language.",-20.0,80.0
"Summary:
--------------
The paper considers the problem of constructing compositional robotic morphologies that can solve different continuous control tasks in a (multi-agent) reinforcement learning setting. The authors created an environment where the actor consists of a number of primitive components which interface with each other via ""linking"" and construct a morphology of a robot. To learn in such an environment, the authors proposed a graph neural network policy architecture and showed that it is better than the baselines on the proposed tasks.

I find the idea of learning in environments with modular morphologies as well as the proposed tasks interesting. However, the major drawback of the paper is the lack of any reasonable details on the methods and experiments. It's hard to comment on the novelty of the architecture or the soundness of the method when such details are simply unavailable.

More comments and questions are below. I would not recommend publishing the paper in the current form.


Comments:
----------------
- If I understand it correctly, each component (""limb"") represents an agent. Can you define precisely (ie mathematically) what the observations and actions of each agent are?

- Page 4, paragraph 2: in the inline equation, you write that a sum over actions equals policy applied to a sum over states. What does it mean? My understanding of monolithic agents is that observations and actions must be stacked together. Otherwise, the information would be lost.

- Page 4, paragraphs 3-(end of section): if I understand it correctly, the proposed method looks similar to the problem of ""learning to communicate"" in a cooperative multi-agent setting. This raises the question, how exactly the proposed architecture is trained? Is it joint learning and joint execution (ie there's a shared policy network, observation and action spaces are shared, etc), or not? All the details on how to apply RL to the proposed setup are completely omitted.

- Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?

- From the videos, it looks like certain behaviors are very unphysical or unrealistic (eg parts jumping around and linking to each other). I'm wondering which kind of simulator was used? How was linking defined (on the simulator level)? It would be nice if such environments with modular morphologies were built using the standard simulators, such as MuJoCo, Bullet, etc.


All in all, despite potentially interesting ideas and setup, the paper is sloppily written, has mistakes, and lacks crucial details.","The review starts with a positive sentiment, acknowledging the interesting aspects of the paper's idea and tasks. However, it quickly transitions into a highly critical tone, pointing out the lack of details and clarity as major drawbacks. The reviewer explicitly states they wouldn't recommend the paper in its current form, indicating an overall negative sentiment. Despite the criticism, the language remains professional and avoids personal attacks, suggesting a neutral politeness level.",-50.0,0.0
"The paper proposes to use a reward function to guide the learning of energy-based models for structured prediction. The idea is to update the energy function based on a random search algorithm guided by a reward function. At each iteration, the SPEN proposes a solution, then a better one is found by the search algorithm, and the energy function is updated accordingly.  Experiments are made on three use-cases and show that this method is able to outperform other training algorithms for SPENs. 

In term of model, the proposed algorithm is interesting since it can allow us to learn from weakly supervised datasets (i.e a reward function is enough). Note that in Section 3, the reward function R is never properly defined which would be nice. The algorithm is quite simple and well presented in the paper. The fact that it is based on a margin could be discussed a little bit more since the effect of the margin is not clear in the paper (the value of alpha). Moreover, the structured prediction problem has already been handled as the maximization of a reward function using RL techniques (see works by H. Daume, and works by F. Maes) and the interest of this approach w.r.t these papers is not clear to me. A clear discussion on that point (and experimental comparison) would be nice. 

The experimental section could be improved. First, the experiments on multi-label classification do not provide any comparison with SoTA methods while the two other use-cases provide some comparisons. Moreover, as far as I understand, the different use-cases could be fully supervised, and different reward functions could be defined. So investigating more deeply the consequences of the nature of the supervision/reward on these use-cases could be interesting and strengthen the paper.  Moreover, training sets are very small and it is difficult to know if this method can work on large-scale problems. 

Pro:
* interesting algorithm for structured prediction (base on reward)
* interesting results on some (toy) use-cases

Cons:
* Lack of discussion on the positive/negative point of the approach w.r.t SoTA, and on the influence of the reward function
* Lack of experimental comparisons 
* Only toy (but complicated) problems with limited training sets
","The review starts with a neutral summary of the paper's contributions. While acknowledging the interesting aspects of the proposed algorithm, the reviewer raises several concerns and suggests improvements, particularly regarding the clarity of the reward function, comparisons with existing methods, and the scale of experiments. The language used is formal and constructive, suggesting areas for improvement without resorting to harsh criticism.",40.0,80.0
"In this paper the authors introduce a new technique for softmax inference. In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert. Then, given the expert, output a particular category. The first level of sparsity comes from the first expert. The second level of sparsity comes from every expert only outputting a limited set of output categories.

The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. ""search right"" -> ""search for the right"", ""predict next word"" -> ""predict the next word"", ...) In section 3, can you be more specific about the gains in training versus inference time? I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?

Nits:
- it wasn't clear how the sparsity percentage on page 3 was defined?
- can you motivate why you are not using perplexity in section 3.2?
","The review starts with a positive note by acknowledging the novelty of the technique and the clarity of the paper. However, it also points out areas for improvement, primarily focusing on language and clarity in presenting results. The suggestions are specific and constructive, indicating a desire to see the paper improved rather than rejected. The tone throughout is professional and courteous, with no harsh language or dismissive remarks.",50.0,75.0
"This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones. This problem has a natural variational form that can be easily implemented from VIB. Experiments show good performance comparing to VIB. 

This paper is well-written and easy to read. The idea using KL divergence creating a deficiency channel to learn data representation is very natural. It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9). 

My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem. However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function. For example, how does the method compare with (variants of) Variational Autoencoder? A discussion on this or some empirical evaluations would be nice. ","The reviewer finds the paper well-written, the idea natural, and the results interesting. They commend the connection to statistical decision problems. However, their main concern, phrased as a suggestion, is the lack of comparison with other representation learning methods besides VIB. This indicates a positive but cautious stance.",60.0,90.0
"This paper presents a network compression method based on block-diagonal sparse structure for RNN. Two kinds of group mixing methods are discussed. Experiments on PTB and SQUAD have shown its superiority over ISS.
The idea present is interesting, and this paper is easy to follow. However, this paper can be improved from the following perspectives.
1.	The method of balancing the quantity of different parts in knowledge distillation is trivial. It is quite general trick.
2.	Details of experimental setup were unclear. For example, the optimization method used, the block size, and the hyper-parameters were unclear. In addition, it is also unclear how the block diagonal structure was used for the input-to-hidden weight matrix only or all weights. 
3.	In addition, the proposed method was compared with ISS only. Since there are many methods of compressing RNNs, comparison with other competitors (e.g., those presented in Related work) are necessary.  Moreover, more experiments with other tasks in addition to NLP will be better.  
4.	In Table 2, the comparison with ISS seems be unfair. The proposed methods, i.e., LGP-shuffle was obtained based on the distillation. However, ISS was trained without distillation. From Table 3, when Cmse and Ckl were set to zero, the result was much worse. The reviewer was wondering that how does ISS with distillation perform. 
","The review starts with positive remarks, highlighting interesting ideas and clarity. However, it then lists several areas for improvement, indicating a need for significant revisions. While the suggestions are constructive and not offensively phrased, the overall tone leans towards the critical side.",40.0,60.0
"This paper proposes a method for learning sentences encoders using artificially generated (fake) sentences. While the idea is interesting, the paper has the following issues:

- There are other methods that aim at generating artificial training data, e.g.:  Z. Zhao, D. Dua, S. Singh. Generating Natural Adversarial Examples. International Conference on Learning Representations (ICLR). 2018,  but no direct comparison is made. Also InferSent  (which is cited as related work) trains sentence encoders on SNLI: https://arxiv.org/pdf/1705.02364.pdf. Again a comparison is needed as the encoders learned perform very well on a variety of tasks. Finally, the proposed idea is very similar to ULMfit (https://arxiv.org/pdf/1801.06146.pdf) which trains a language model on a lot of unlabeled data and then finetunes it discriminatively. Finally, there should be a comparison against a langauge model without any extra training in order to assess the benefits of the fake sentence classification part of the model.

- It is unclear why the fake sentence construction method proposed by either swapping words or just removing them produces sentences that are fake and/or useful to train on. Sure it is simple, but not necessarily fake. A language model would be able to discriminate between them anyway, by assigning high probability to the original ones, and low probability to the manipulated ones. Not sure we need to train a classifier on top of that.

- I found the notation in section 2 confusing. What kind of distribution is P(enc(x,theta1)|theta2, theta3)? I understand that P(x|theta) is the probability of the sentence given a model, but what is the probability of the encoding? It would also be good to see the full derivation to arrive at the expression in the beginning of page 3. 

- An argument in favour of the proposed method is training speed; however, given that less data is used to train it, it should be faster indeed. In fact, if we consider the amount of time per million sentences, the previous method considered in comparison could be faster (20 hours of 1M sentences is 1280 hours for 64M sentences, more than 6 weeks). More importantly, it is unclear from the description if the same data is used in training both systems or not.

- It is unclear how one can estimate the normalization factor in equation 2; it seems that one needs to enumerate over all fake sentences, which is a rather large number due to the number of possible word swaps in the sentence,

- I am not sure the generator proposed generates realistic sentences only, ""Chicago landed in John on Friday"" is rather implausible. Also there is no generation method trained here, it is rule-based as far as I can tell. There is no way to tell the model trained to generate a fake sentence as far as I can tell.

- It is a bit odd to criticise other methods ofr using LSTMs with ""millions of parameters"" while the proposed approach also uses them. A comparison should calculate the number of parameters used in either case.

- what is the motivation for having multiple layers without non-linearity instead of a single layer?","The review is critical of the paper's novelty, methodology, and clarity. It points out several potential flaws and areas for improvement, suggesting the paper needs significant revisions. While the reviewer maintains a professional tone, the numerous concerns raised indicate a negative sentiment.",-60.0,60.0
"This submission sets out to taxonomize evasion-time attacks against deep RL and introduce several new attacks, including two heuristics for efficient evasion-time attacks and attacks that target the environment dynamics and RL system’s actions. The main limitation of this paper is probably its broad scope, which unfortunately prevents it in its current form from addressing each of the goals stated in the introduction systematically to draw conclusive takeaways. 

Taxonomizing the space of adversaries targeting deep RL at test time is a valuable contribution. While the existing taxonomy is a good start, it would be useful if you can clarify the following points in your rebuttal. Why were the “further categorization” items separated from adversarial capabilities? Being constrained to real-time or physical perturbations appears to be another way to describe the adversary’s capabilities. In addition, is there a finer-grained way to characterize the adversary’s knowledge beyond white-box vs. black-box? This binary perspective is common but not very informative. One way to move forward would be for instance to think about the different components of a RL system, and identify those that are relevant to have knowledge of when adversaries are mounting attacks. It would also be helpful to position prior work in the taxonomy. Finally, the taxonomy currently stated in the submissions is more a taxonomy of attacks (or adversaries) than a taxonomy of vulnerabilities, so the title of Section 3 could perhaps be updated accordingly. 

Section 4.1 gives a good overview of different attack strategies against RL based on modifying the observations analyzed by the agent. Many of these attacks are applications of known attack strategies and will be familiar to readers with adversarial ML background (albeit some of these strategies were previously introduced and evaluated against “supervised” classifiers only). One point was unclear however: why is the imitation learning based black-box attack not a transferability-based attack? As far as I could understand, the strategy described corresponds exactly to the commonly adopted strategy of transferring adversarial examples found on a substitute model (see for instance “Intriguing properties of neural networks” by Szegedy et al. and “Practical Black-Box Attacks against Machine Learning” by Papernot et al.). In other words, Section 4.1 could be rescoped to put emphasis on the attack strategies that have not been explored previously in the context of reinforcement learning: e.g., the finite difference approach with adaptive sampling or the universal attack with optimal selection of initial frames. It is unfortunate that the treatment of these two attacks is currently deferred to the appendix as they make the paper more informative. Similarly, Sections 4.2 and 4.3 would benefit from being extended to put forward the new attack threat model considered in these two sections. 

While the introduction claimed to make a systematic evaluation of attacks against RL, the presentation of the experimental section can be improved to ensure the analysis points out the relevant takeaways. For instance, it is unclear what the differences are between results on TORCS and other tasks included in the Appendix. Specifically, results on Enduro do not seem as conclusive as those presented on TORCS. Do you have some intuition as to why that is the case? In Figure 7, it appears that a large number of frames need to be manipulated before a drop on cumulative reward is noticeable. Previous efforts manipulated single frames only, could you stress why the setting is different here? Throughout the section, many Figures are small and it is difficult to infer whether the difference between the white-box and black-box variants of an attack is significant or not. Could you analyze this in more details in the text? In Table 2, how should the L2 distance be interpreted? In other words, when is the adversary successful? 

If you can clarify any of the points made above in your rebuttal, I am of course open to revise my review. 

Editorial details: 
Figures are not readable when printed. 
Figure 5 is improperly referenced in the main body of the paper. 
Figure 7: label is incorrect for Torcs and Hopper (top of figure)
","The review is constructive and provides concrete suggestions for improvement. While it acknowledges the paper's valuable contributions, it also points out limitations and areas for improvement. The reviewer's willingness to revise their opinion based on the authors' rebuttal suggests an openness to the paper's potential. Therefore, the sentiment is slightly positive, leaning towards neutral. The language used is polite and professional, focusing on the paper's content and providing constructive feedback.",20.0,80.0
"Paper Summary: 
The idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. 

Paper Strengths:
Unfortunately, there is not many positive points about the paper except that it explores an interesting direction. 

Paper Weaknesses: 

I vote for rejection of the paper due to the following issues:

- It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all.

- The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal.

- According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well?

- The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in ""On Evaluation of Embodied Navigation Agents"".

- Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work:
Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018

- The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics.

- What is the difference between this method and providing a large negative reward at a non-target object?

- The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. 

- It seems the same environment is used for train and test.

------------------------
Post rebuttal comments:

Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:

- I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic.

- The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given.

- The writing is still confusing. For instance, it is mentioned that ""Concretely, for each state s ∈ S, we define T as a teacher that gives an advice T(s)"", while that is not true since later it is mentioned that ""the teacher give advice based solely on the terminal state"". These statements are contradictory, and it is not trivial at all to provide an advice for each state.
","The reviewer acknowledges the interesting direction of the paper but raises several significant concerns about the methodology, results, and comparisons. They vote for rejection initially. Post-rebuttal, while some concerns are addressed, major issues remain regarding the simplicity of the approach, unfair comparisons due to extra annotations used, and confusing writing. The rating improves slightly to 5, but it's still far from a positive endorsement.",20.0,50.0
"This work shows that adding a simple blurring into max pooling layers can address issues of image classification instability under small image shifts. In general this work presents a simple and easy to implement solution to a common problem of CNNs and even though it lacks more thorough theoretical analysis of this problem from the signal processing perspective (such as minimal size of the blurring kernel for fulfilling the Nyquist-Shannon sampling theorem), it seems to provide ample empirical evidence.

Pros:
+ The introduction and motivation is really well written and Figure 3 provides a clear visualisation main max pooling operator issues.
+ The proposed method is really simple and shows promising results on the CIFAR dataset. With random shifts, authors had to tackle cropping with circular shifts. As it can cause artifacts in the data, authors also provide baseline performances on the original data (used for both training and testing).
+ Authors provide a thorough evaluation, ranging from comparing hidden representations to defining consistency metrics of the classified classes.

This work is lacking in the experimental section due to some missing details and few inconsistencies. I believe the most of my concerns can be relatively easily fixed/clarified in an update of this submission.

Major issues, which if fixed would improve the rating:
- It is not correct to average test accuracy and test consistency as both measures are different quantities, especially when using them for ranking. The difference between accuracy of different methods are considerably smaller than differences in the classification consistency. 
- It is not clear how many shifts are used for computing the ""Random Test Accuracy"" and the ""Classification Accuracy"". Also whether the random shifts are kept constant between evaluated networks and evaluation metrics.
- Authors do not address the question what is the correct order of operations for the blurring. E.g. would the method empirically work if blurring was applied before max pooling? Do the operations commute?
- The selection of the filters is rather arbitrary, especially regarding the 1D FIR filters. The separability of these filters should be discussed.
- I believe authors should address how this work differs to [1], as it also tests different windowing functions for pooling operators, even though in different tasks.

Minor issues, which would be nice to fix however which do not influence my rating:
* Section 3.1 - And L-Layer deep *CNN*, H_l x W x C_l -> H_l x W_l x C_l
* Section 3.1. Last paragraph - I would not agree with the statement that in CNNs the shift invariance must necessarily emerge upon shift equivariance. If anything, this may hold only for the last layer of a network without fully connected layers and with average pooling of the classifier output (ResNet/GoogleNet like networks).
* Explicitly provide the network architecture as [Simonyan14] does not test on CIFAR and cannot use Batch normalisation.
* It would be useful to add citation for the selected FIR filters.
* The flow of section 4.2. can be improved to help readability. The three metrics should be first motivated before their introduction. Metric 2. paragraph - the metric is defined below, not above. 
* It would be interesting to see what would be the performance if the blurring filters were trained as well (given some sensible initialisation).
* One future direction would be to verify that this approach generalises to larger networks as well. It might be worth to discuss this in the conclusions.

[1] Scherer, Dominik, Andreas Müller, and Sven Behnke. ""Evaluation of pooling operations in convolutional architectures for object recognition."" Artificial Neural Networks–ICANN 2010. Springer, Berlin, Heidelberg, 2010. 92-101.","The review starts with positive remarks, highlighting the relevance, simplicity, and promising results of the work. While it points out several areas for improvement, these are presented constructively with specific suggestions for clarification and expansion. The reviewer even acknowledges that most concerns can be easily addressed. The tone throughout is professional and encouraging, suggesting a positive outlook on the paper's potential.",60.0,80.0
"The paper improves upon the Real NVP/Glow design by proposing better dequantization schemes and more expressive forms of coupling layers. I really like Real NVP models, which I think are a bit underappreciated. Thus, I’m happy that there are papers trying to improve their performance.  However, I wish this was done with more rigour.

The paper makes 3 claims about the current flow models: (1) it is suboptimal to use additive uniform noise when dequantizing images, (2) affine coupling layers are not expressive enough, and (3) the architectures fail to capture global image context. I’ll comment on these claims and proposed solutions below.

(1) I agree with the reasoning behind the need for a better dequantization distribution. However, I think the authors should provide an evidence that the lower bound is indeed loose when q is uniform. For example, for the CIFAR-10 model, the authors calculated a gap of 0.025 bpd when using variational dequantization. What would this gap be when using uniform q?  Maybe, a clear illustration of the dequantization effect on a simpler dataset or a toy example would be more useful.

(2) My main concern about the mixture CDFs coupling layer is how much bigger the model becomes and how much slower it trains. I find this analysis crucial when deciding whether 0.05 bpd improvement as reported in Table 1 is worth the hassle.

(3) As a person not familiar with the Transformer, I couldn’t understand how exactly self-attention works and how much it helps the model to capture the global image context. Also, I think this problem needs a separate illustration on a dataset of larger images.  
  
The experiments section is very weak in backing up the identified problems and proposed solutions. Firstly, I think it is more clear if the ablation study is done in reverse: instead of making Flow++ and removing components, start with the vanilla model and then add stuff.  Secondly, it’s not clear if these improvements generalize across datasets, e.g. when images are larger than 32x32. Though, larger inputs may lead to huge models which are impossible to train when the resources are quite limited. That’s why I find it important to report how much complexity is added compared to the initial Real NVP. Also, I think it’s a well-known fact that sampling from PixelCNN models is slow unlike for Real NVPs, so I don’t find the results in Table 3 surprising or even useful. 

To conclude, I find this paper unfinished and wouldn’t recommend its acceptance until the analysis of the problems and their solutions becomes better thought out.  ","The reviewer expresses interest in the paper's topic and acknowledges the value of improving Real NVP models. However, they find the paper's claims weakly supported, the experiments lacking, and the analysis needing more depth. They suggest several improvements, indicating a desire to see the work strengthened rather than outright rejection. Overall, the tone is critical but constructive, suggesting a sentiment leaning slightly positive due to the interest shown and the detailed suggestions for improvement.",20.0,60.0
"This paper proposes an approach to model social influence in a scenario-independent manner by instantiating the concept of intrinsic motivation and combine various human abilities as part of a reinforcement learning function in order to improve the agent's operation in social dilemma scenarios. 

Agents are operationalised as convolutional neural network, linear layers and LSTM. Using these base mechanisms, different abilities (communication, models of other agents (MOA)), their causal influence is inferred based on counterfactual actions. The architecture is explored across two different sequential social dilemmas. 

The architecture is described in sufficient detail, with particular focus on the isolation of causal influence for communication and MOA influence. The experimental evaluation is described in sufficient detail, given the low complexity of the scenarios. While the agents with communicative ability and MOA show superior performance, a few results warrant clarification.

Figure 6a) highlights the performance of influencers in contrast to a visible actions baseline. This specific scenarios shows the necessity to run experiments for larger number of runs, since it appears that action observations may actually outperform influencer performance beyond 3 steps. Please clarify what is happening in this specific case, and secondly, justify your choice of steps used in the experimental evaluation. 

Another results that requires clarification is Figure 6f), which is not sufficiently discussed in the text, yet provides interesting patterns between the MOA baseline performance decaying abruptly at around 3 steps, with the influence MOA variant only peaking after that. Please clarify the observation. Also, could you draw conclusions or directions for a combination of the different approaches to maximise the performance (more generally, beyond this specific observation)? 

A valuable discussion is the exemplification of specific agent behaviour on Page 7. While it clarifies the signalling of resources in this specific case, it also shows shortcomings of the model's realism. How would the model perform if agents had limited resources and would die upon depletion (e.g. the de facto altruistic influencer in this scenario - since it only performs two distinct actions)? The extent of generalisability should be considered in the discussion. 

In general, the paper motivates and discusses the underlying work in great detail and is written in an accessible manner (minor comment: the acronym LSTM is not explicitly introduced). The quality of presentation is good. ","The reviewer provides constructive criticism, acknowledges the merits of the paper (detailed architecture description, good writing, interesting discussion), and raises specific questions for clarification. This suggests a positive attitude towards the work, but with room for improvement. Therefore, the sentiment is scored as moderately positive. The language is polite and professional throughout, using neutral phrasing and focusing on the scientific content.",60.0,80.0
"I like this paper. What the authors have done is of high quality. It is well written and clear. However, quite a lot of experiments are necessary to make this paper publishable in my opinion.

Strenghts:
- The idea to use a GAN for model compression is something that many must have considered. It is good to see that someone has actually tried it and it works well.
- I think the compression score is definitely an interesting idea on how to compare GANs that can be of practical use in the future.
- The experimental results, which are currently in the paper, largely support what the authors are saying.

Weaknesses:
- The authors don't compare how good this technique is in comparison to simple data augmentation. My suspicion is that the difference will be small. I realise, however, that the advantage of this method over data augmentation is that it is harder to do it for tabular data, for which the proposed method works well. Having said that, models for tabular data are usually quite simple in comparison to convnets, so compressing them would have less impact.
- The experiments on image data are done with CIFAR-10, which as of 2018 is kind of a toy data set. Moreover, I think the authors should try to push both the baselines and their technique much harder with hyperparameter tuning to understand what is the real benefit of what they are proposing. I suspect there is a lot of slack there. For comparison, Urban et al. [1] trained a two-layer fully connected network to 74% accuracy on CIFAR-10 using model compression.

[1] Urban et al. Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? 2016.","The reviewer starts with positive remarks, highlighting the quality and clarity of the work. They also see value in the idea and the proposed compression score. However, they call for more experiments and express concerns about the method's advantage over simpler techniques. The reviewer also points out limitations in the current experimental setup (using CIFAR-10) and suggests pushing the techniques further. Overall, the feedback is constructive and encouraging further development rather than outright rejection. Therefore, the sentiment is mildly positive. The language used is polite and professional throughout, with no personal attacks or disrespectful remarks.",60.0,80.0
"This paper proposes an approximate second-order method with low computational cost. A common pitfall of second-order methods is the computation (and perhaps inversion) of the Hessian matrix. While this can be avoided by instead relying on Hessian-vector products as done in CG, it typically still requires several iterations. Instead, the authors suggest a simpler approach that relies on one single gradient step and a warm start strategy. The authors points out that the resulting algorithm resembles a momentum method. They also provide some simple convergence proofs on quadratics and benchmark their method to train deep neural networks.

While I find the research direction interesting, the execution is rather clumsy and many details are not sufficiently motivated. Finally, there is a lot of relevant work in the optimization community that is not discussed in this paper, see detailed comments and references below.

1) Method
The derivation of the method is very much driven on a set of heuristics without theoretical guarantees. In order to derive the update of the proposed method, the authors rely on three heuristics:
a) The first is to reuse the previous search direction z as a warm-start. The authors argue that this might be beneficial if If z does not change abruptly. In the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature where the direction of z might change quickly from one iteration to the next.
The ""warm start"" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?

b) The second step interleaves the updates of z and w instead of first finding the optimum z. This amounts to just running one iteration of CG but it is rather unclear why one iteration is an appropriate number. It seems one could instead some adaptive strategy where CG with a fixed accuracy. One could potentially see if allowing larger errors at the beginning of the optimization process might still allow for the method to converge. This is for instance commonly done with the batch-size of first-order method. Gradually increasing the batch-size and therefore reducing the error as one gets close to the optimum can still yield to a converging algorithm, see e.g. 
Friedlander, M. P., & Schmidt, M. (2012). Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3), A1380-A1405.

c) The third step consists in replacing CG with gradient descent.
""If CG takes N steps on average, then Algorithm 2 will be slower than SGD by a factor of at least N, which can easily be an order of magnitude"".
First, the number of outer iterations may be a lot less for the Hessian-free method than for SGD so this does not seem to be a valid argument. Please comment.
Second, I would like to see a discussion of the convergence rate of solving (12) inexactly with krylov subspace methods. Note that Lanczos yields an accelerated rate while GD does not. So the motivation for switching to GD should be made clearer.

d) The fourth step introduces a factor rho that decays z at each step. I’m not really sure this makes sense even heuristically. The full update of the algorithm developed by the author is:
w_{t+1} = w_t - beta nabla f + (rho I - beta H) (w_t - w_{t-1}).
The momentum term therefore gets weighted by (rho I - beta H). What is the meaning of this term? The -beta H term weights the momentum according to the curvature of the objective function. Given the lack of theoretical support for this idea, I would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.
This is especially important given that you claim to decay rho therefore giving more importance to the curvature term.
Finally, why would this be better than simply using CG on a trust-region model? (Recall that Lanczos yields an accelerated linear rate while GD does not).

2) Convergence analysis
a) The analysis is only performed on a quadratic while the author clearly target non-convex functions, this should be made clear in the main text. Also see references below (comment #3) regarding a possible extension to non-convex functions.
b) The authors should check the range of allowed values for alpha and beta. It appears the rate would scale with the square root of the condition number, please confirm, this is an important detail. I also think that the constant is not as good as Heavy-ball on a quadratic (see e.g. http://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf), please comment.
c) Sub-sampling of the Hessian and gradients is not discussed at all (but used in the experiments). Please add a discussion and consider extending the proof (again, see references given below).

3) Convergence Heavy-ball
The authors emphasize the similarity of their approach to Heavy-ball. They cite the results of Loizou & Richtarik 2017. Note that they are earlier results for quadratic functions such as 
Lessard, L., Recht, B., & Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1), 57-95.
Flammarion, N., & Bach, F. (2015, June). From averaging to acceleration, there is only a step-size. In Conference on Learning Theory (pp. 658-695).
The novelty of the bounds derived in Loizou & Richtarik 2017 is that they apply in stochastic settings.
Finally, there are results for non-convex functions such convergence to a stationary point, see
Zavriev, S. K., & Kostyuk, F. V. (1993). Heavy-ball method in nonconvex optimization problems. Computational Mathematics and Modeling, 4(4), 336-341.
Also on page 2, ""Momentum GD ... can be shown to have faster convergence than GD"". It should be mentioned that this only hold for (strongly) convex functions!

4) Experiments
a) Consider showing the gradient norms. 
b) it looks like the methods have not yet converged in Fig 2 and 3.
c) Second order benchmark:
It would be nice to compare to a method that does not use the GN matrix but the true or subsampled Hessian (like Trust Region/Cubic Regularization) methods given below.
Why is BFGS in Rosenbrock but not in NN plots?
d) ""Batch normalization (which is known to improve optimization)"" 
This statement requires a reference such as
Towards a Theoretical Understanding of Batch Normalization
Kohler et al… - arXiv preprint arXiv:1805.10694, 2018

5) Related Work
The related work should include Cubic Regularization and Trust Region methods since they are among the most prominent second order algorithms. Consider citing Conn et al. 2000 Trust Region,  Nesterov 2006 Cubic regularization, Cartis et al. 2011 ARC.
Regarding sub-sampling: Kohler&Lucchi 2017: Stochastic Cubic Regularization for non-convex optimization and Xu et al.: Newton-type methods for non-convex optimization under inexact hessian information.

6) More comments

Page 2
Polyak 1964 should be cited  where momentum is discussed.
""Perhaps the simplest algorithm to optimize Eq. 1 is Gradient Descent"". This is technically not correct since GD is not a global optimization algorithm. Maybe mention that you try to find a stationary point
rho (Eq. 2) and lambda (Eq. 4) are not defined

Page 4: 
Algorithm 1 and 2 and related equations in the main text: it should be H_hat instead of H.

Background
“Momemtum GD exhibits somewhat better resistance to poor scaling of the objective function”
To be precise the improvement is quadratic for convex functions. Note that Goh might not be the best reference to cite as the article focuses on quadratic function. Consider citing the lecture notes from Nesterov.

Section 2.2
This section is perhaps a bit confusing at first as the authors discuss the general case of a multivalue loss function. Consider moving your last comment to the beginning of the section.

Section 2.3
As a side remark, the work of Dauphin does not rely on the Gauss-Newton approximation but a different PSD matrix, this is probably worth mentioning.

Minor comment: The title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small e.g. in Fig 1.
","The review is critical of the paper, pointing out a lack of theoretical rigor and missing comparisons with relevant work. The reviewer uses strong language like ""clumsy"", ""not sufficiently motivated"", ""does not seem to be a valid argument"", and calls for major revisions. However, the criticism is presented in a professional and constructive manner, suggesting specific improvements and relevant literature. Therefore, the sentiment is negative but the politeness level is neutral.",-40.0,0.0
"The paper claims to propose a novel generative probabilistic neural network model such that its encoder (classifying an image) can be approximated by a convolutional neural network with ReLU activations and MaxPooling layers. Besides the standard parameters of the units (weights and biases), the model has two additional latent variables per unit, which decide whether and where to put the template (represented by the weights of the neuron) in the subsequent layer, when generating an image from the class. Furthermore, the authors claim to derive new learning criteria for semi-supervised learning of the model including a novel regulariser and claim to prove its consistency. 

Unfortunately, the paper is written in a way that is completely incomprehensible (for me). The accumulating ambiguities, together with its sheer length (44 pages with all supplementary appendices!), make it impossible for me to verify the model and the proofs of the claimed theorems. This begins already with definition of the model. The authors consider the latent variables as dependent and model them by a joint distribution. Its definition remains obscure, let alone the question how to marginalise over these variables when making inference. Without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.

In my view, the presented material exceeds the limits of a single conference paper. A clear and concise definition of the proposed model accompanied by a concise derivation of the basic inference and learning algorithm would already make a highly interesting paper.

Considering the present state of the paper, I can't, unfortunately, recommend to accept it for ICLR.

","The reviewer finds the paper interesting but ultimately incomprehensible and overly long. They criticize the clarity of the model definition and the feasibility of verifying the proofs. While acknowledging the potential of the work, the reviewer leans towards rejection due to these issues.",-50.0,50.0
"The authors present a method for fine-tuning neural networks so inference can be performed in a quantized low bit data format down to 3 bits. The authors achieve this through a combination of three techniques:
1. Noise injection to fine-tune the weights before quantization. The effect of noise injection can model that of quantization, but rather than being stuck in a quantization bin, fine grained weight updates are still possible
2. A schedule that quantizes layer by layer, rather than all layers at the same time
3. Clipping weights and activations within a learned range to obtain finer grained bins within that range. 


The main contribution is a novel combination of mostly existing techniques. Clipping (or clamping as the authors call it) has been proposed by Zhang et al. 2018, but it's an interesting contribution to have the clipping learned directly via backpropagation with a straight-through estimator. Treating the quantization as noise has been proposed in a different form in McKinstry et al. 2018. Gradual quantization appears novel, but is also the least interesting of the techniques. Therefore, novelty on ideas/methods is somewhat limited, and the contribution is mostly the in the impressive experimental results, which appear to be outperforming previous methods. The main weaknesses are poor writing, and that some details of the implementation required to reproduce the results are missing. For example, the training schedule is not given, e.g. how many epochs to train the clean model, how many with noise, how many quantized. Details on the gradual quantization are also missing. Block based quantization is completely heuristic and not well motivated. If this is the main novel ingredient, more details on the mechanics would be needed. Is both the noise injection and the quantization done in blocks? If the motivation is in ""the opportunity to adapt”, then what does the adaptation look like? 

As above, my other main issue is with the writing, there are many examples where I would suggest improvements:

This work could be improved greatly by copy editing for English grammar. There are many typos (including ones that can be caught by autocorrect, missing punctuation, or using similar but unrelated words, e.g. ""token"" instead of ""taken""). The manuscript appears hastily put together and not ready for publication. 

The acronym NICE already has a meaning in the DL literature: Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: Non-linear Independent Components Estimation. It confusing to reuse it. 

The term clamping is only explained on page 4 but used since the abstract. It’s used in a nonstandard way to mean “constrained to lie within a range” which should be explained earlier. I think “clipped” would be a better term, following the related Choi et al. 2018. Clamping usually means ""constrained to a fixed value"" (not a range), so it is not a good term to use in this context. 

Are the results shown in table 2 and table 3 from a single trial or averaged across reruns? If single trial, it's misleading to have 2 figures after the decimal. Even non-quantized ResNet tends to have 0.5% or so run to run variability, which is much larger than the differences between some of the methods shown here. In fact, a lot of the results could just be due to picking a lucky random seed. 

Comparisons are shown against methods JOINT (Jung et al), LQ-Nets (Zhang et al), FAQ (McKinstry et al). It would be helpful to present them with the same names in the ""related work"" section, and explain why they were picked out for the comparison. For someone not familiar with the literature it's hard to see why these 3 would be the obvious picks. 

Readability would increase if table 2 and 3 were moved to section 4 where they are referenced, rather than after the discussion. Fig 2 font size too small and hard to read. 
","The review acknowledges the authors' work and contributions but points out limitations in novelty and writing. While impressed by the experimental results, the reviewer finds the writing poor and lacking details, impacting reproducibility. The criticism, though direct, maintains a professional and constructive tone.",20.0,60.0
"PROs
-seemingly reasonable approach to polyphonic music generation: figuring out a way to splitting the parts, share parameters appropriately, measuring entropy per time, all make sense
-the resulting outputs tend to have very short-term harmonic coherence (e.g. often a ‘standard chord’ with some resolving suspensions, etc), with individual parts often making very small stepwise motion (i.e. reasonable local voice leading)
-extensive comparison of architectural variations
-positive results from listening experiments

CONs
-musical outputs are *not* clearly better than some of the polyphonic systems described; despite the often small melodic steps, the individual lines are quite random sounding; this is perhaps a direct result of the short history
-I do not hear the rhythmic complexity that is described in the introduction
-the work by Johnson (2015) (ref. provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and has rhythmic and polyphonic complexity and sounds quite good (better, in my opinion) 
-some unclear sections (fixable, especially with an appendix; more detail below)
-despite the extensive architectural comparisons, I was not always clear about rationale behind certain choices, eg. if using recurrent nets, why not try LSTM or GRU? (more questions below)
-would like to have heard the listening tests; or at least read more about how samples were selected (again, perhaps in an appendix and additional sample files)

 quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).

Quality -- In this work, various good/reasonable choices are made. The quality of the actual output is fine. It is comparable to-- and to my ears not better than-- existing polyphonic systems such as the ones below (links to sample audio are provided here):

-Bachbot - https://soundcloud.com/bachbot (Liang et al 2017)
- tied parallel nets - http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/ (Johnson 2015, ref below)
-performanceRNN - https://magenta.tensorflow.org/performance-rnn - (Simon & Oore 2017)
..others as well..


Clarity -- Some of the writing is ""locally"" clear, but one large, poorly-organized section makes the whole thing confusing (details below). It is very helpful that the authors subsequently added a comment with a link to some sample scores; without that, it had been utterly impossible to evaluate the quality. There are a few points that could be better clarified:
	-p5”a multi-hot vector of notes N”. It sounds like N will be used to denote note-numbers, but in fact it seems like N is the total number of notes, i.e. the length of the vector, right? What value of N is used?
-p5 “a one-hot vector of durations D”. It sounds like D will be used to denote durations, but actually I think D is the length of the 1-hot vector encoding durations right? What value of D is used, and what durations do the elements of this vector represent?
-similarly, does T represent the size of the history? This should really be clarified.
	-p5 Polyphonic models.
		-Eq (2), (3), (4): Presumably the h’s are the hidden activations layers?
		-the networks here correspond to the blue circles in Fig 1, right? If so, make the relationship clear and explicit 
		-Note that most variables in most equations are left undefined       
		-actually defining the W’s in Eq(2-4)  would allow the authors to refer to the W’s later (e.g. in Section 5.2) when describing weight-sharing ideas. Otherwise, it’s all rather confusing. For example, the authors could write, “Thus, we can set W_p1 = W_p2 = W_p3 = W_p4” (or whatever is appropriate). 
	-Generally, I found that pages 5-7 describe many ideas, and some of them are individually fairly clearly described, but it is not always clear when one idea is beginning, and one idea is ending, and which ideas can be combined or not. On my first readings, I thought that I was basically following it, until I got to Table 5, which then convinced me that I was in fact *not* quite following it. For example, I had been certain that all the networks described are recurrent (perhaps due to Fig1?), but then it turned out that many are in fact *not* recurrent, which made a lot more sense given the continual reference to the history and the length of the model’s Markov window etc. But the reader should not have had to deduce this. For example, one could write, 
	“We will consider 3 types of architectures: convolutional, recurrent, .... In each architecture, we will have [...] modules, and we will try a variety of combinations of these modules. The modules/components are as follows:”. It’s a bit prosaic, but it can really help the reader. 
-Appendices, presented well, could be immensely helpful in clarifying the exact architectures; obviously not all 22 architectures from Table 5 need to be shown, but at least a few of them shown explicitly would help clarify. For example, in Fig1, the purple boxes seem to represent notes (according to the caption), but do they actually represent networks? If they really do represent notes, then how can “notes” receive inputs from both the part-networks and the global network? Also, I was not entirely clear on the relationship of the architecture of the individual nets (for the parts) to that of the global integrating network. E.g. for experiment #20, the part-net is an RNN (with how many layers?? with regular or LSTM cells?) followed by a log-linear predictor (with one hidden layer of 300 units right? or are there multiple layers sometimes?), but then what is the global network? Why does the longest part-history vector appear to have length 10 based on Table 5, but according to Table 3 the best-performing history length was 20? Though, I am not sure the meaning of the “bottom/top” column was explained anywhere, so maybe I am completely misunderstanding that aspect of the table? Etc.
-Many piano scores do not easily deconstruct into clean 4-part polyphony; the example in Appendix A is an exception. It was not clear to me how piano scores were handled during training. 
-Terminology: it is not entirely clear to me why one section is entitled “homophonic models”, instead of just “monophonic models”. Homophonic music usually involves a melody line that is supported by other voices, i.e. a sort of asymmetry in the part-wise structure. Here, the outputs are quite the opposite of that: the voices are independent, they generally function well together harmonically, and there is usually no sense of one voice containing a melody. If there’s some reason to call it homophonic, that would be fine, but otherwise it doesn’t really serve to clarify anything. However, the authors do say that the homophonic composition tasks are a “minor generalization of classic monophonic composition tasks”, so this suggests to me that there is something here that I am not quite understanding.

The last sentence of Section 5.3 is very confusing-- I don’t understand what lin_n is, or 1_n is, or how to read the corresponding entries of the table. The first part of the paragraph is fairly clear. 

Table 4: “The first row” actually seems like it is referring to the second row. I know what the authors mean, but it is unnecessarily confusing to refer to it in this way. One might as well refer to “the zeroth row” as listing the duration of the clip :)

The experimental evaluation: I would like to hear some of the paired samples that were played for subjects. Were classical score excerpts chosen starting at random locations in the score, or at the beginning of the score? It is known that listening to a 10-second excerpt without context can sometimes not make sense. I would be curious to see the false positives versus the false negatives. Nevertheless, I certainly appreciate the authors’ warning to interpret the listening results with caution.




Originality & Significance -- So far, based both on the techniques and the output, I am not entirely convinced of the originality or significance of this particular system. The authors refer to “rhythmically simple polyphonic scores” such as Bachbot, but I cannot see what is rhythmically fundamentally more sophisticated about the scores being generated by the present system. One nice characteristic of the present system is the true and audible independence of the voices.

One of the contributions appears to be the construction of models that explicitly leverage with shared weights some of the patterns that occur in different “places” (pitch-wise and temporally) in music. This is both very reasonable, and also not an entirely novel idea; see for example the excellent work by Daniel Johnson, “Generating Polyphonic Music Using Tied Parallel Networks” (paper published 2017, first shared online, as far as I know, in 2015: links to all materials available at http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/  )
Another now common (and non exclusive) way to handle some of this is by augmenting the data with transposition. It seems that the authors are not doing this here. Why not? It usually helps. 

Another contribution appears to be the use of a per-time measure of loss. This is reasonable, and I believe others have done this as well. I certainly appreciated the explicit justification for it, however.

Note that the idea of using a vector to indicate metric subdivision was also used in (Johnson 2015).

Playing through some of the scores, it is clear that melodies themselves are often quite unusual (check user studies), but the voices do stay closely connected harmonically, which is what gives the system a certain aural coherence. I would be interested to hear (and look at) what is generated in two-part harmony, and even what is generated-- as a sort of baseline-- with just a single part. 

I encourage the authors to look at and listen to the work by Johnson:
-listening samples: http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
-associated publication: http://www.hexahedria.com/files/2017generatingpolyphonic.pdf

Overall, I think that the problem of generating rhythmically and polyphonically complex music is a good one, the approaches seem to generally be reasonable, although they do not appear to be particularly novel, and the musical results are not particularly impressive. The architectural choices are not always clearly presented.
			
		
","The review acknowledges some positive aspects of the work, such as the approach and the effort put into architectural comparisons. However, it also expresses significant concerns about the originality, the quality of the generated music, and the clarity of the paper. The reviewer finds the generated music not significantly better than existing systems and points out a lack of clarity in the paper's structure and explanations. The language used is quite direct and critical in some places, but it maintains a professional and academic tone. Therefore, the sentiment leans towards the negative side, although it's not extremely negative, and the politeness score is slightly on the positive side of neutral.",-20.0,20.0
"Summary: This paper proposes an integration of active learning for multi-task learning with policy search. This integration is built on an existing framework, EPOpt, which each time samples a set of models and a set of trajectories for each model. Only trajectories with the bottom \epsilon percentile returns will be used to update the multi-task policy. This paper proposes a way to improve the sample-efficiency so that fewer trajectories will be sampled and fewer trajectories will be loss. 

In general, the paper presentation is easy to follow. The idea is well motivated of why an active learning integration is needed. The related work is a bit too narrow, e.g. work [1] on the same approach like EPOpt or meta-learning (for model adaptation) [2] (and others more on this topic)

[1] T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-Ensemble Trust-Region Policy Opti
mization. In ICLR, 2018.

[2] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML, 2017.

In overall, I have major concerns regarding to the proposed framework.
- Active learning is a method that is in general known to be an optimal trade-off between exploration vs. exploitation in finding a global optimal solution. That means, the proposed use of linear stochastic bandits is trying to find an optimal arm \theta^* (the worst trajectory) that gives the highest reward (the lowest return). In my opinion, integrating this idea naively into EPOpt to sample a set of trajectories would only aim to find the worst trajectory among all trajectories from all models. This is clearly not enough to say ""finding ALL the WORSE regions among trajectory space"" to improve the policy. Therefore, a new way of integration or a new objective should be used in order to make a principled framework. 

- The statement over sample-efficiency gain vs. EPOpt in Section 4 is too loose which is not based on any detailed analysis or further theoretical results.

- The experiment results are not well presented: there is no results for EPOpt in Fig. 1; 


Minor comments:

- Algorithm 1: argument of GetTrajectory (in LEARN) should be \theta_i, instead of \pi_\theta_i?.


In conclusion, the proposed framework is not yet principled. Experiment results are too preliminary and not well presented. ","The reviewer raises major concerns about the paper's core idea and methodology, stating that the integration of active learning is naive and lacks a principled framework. They find the claim of sample-efficiency gain to be loosely stated without sufficient analysis or theoretical backing. The presentation of experimental results is also criticized for being incomplete and not well-structured. While the reviewer acknowledges the clarity of presentation and motivation behind the idea, the significant concerns about the core proposal lead to a negative sentiment. The language used is professional and maintains a neutral politeness level, directly addressing the shortcomings without resorting to personal attacks or overly negative phrasing.",-50.0,0.0
"The paper addresses the incremental few-shot learning problem where a model starts with base network and then introduces the novel classes, building a connection between novel and base classes via an attention module.

Strengths:
+ clear writing. 
+ the experiments are compared with related work and the ablation studies can verify the effectiveness of the proposed (or ""introduced"" would be a precise term) recurrent BP.

Weakness:

- [Novelty]
The paper title is called attention attractor network, which shares very relevance to previous CVPR work (Gidaris & Komodakis, 2018). So the first thing I was looking for is the clear description of the difference between these two. Unfortunately, in related work, authors mention the CVPR work without stating the difference (last few lines in Section 2). As such, I don't see much novelty in the paper compared with previous work. Eqn. (7)-(10) explicitly describes the attention formula. What's the distinction from the CVPR work?

- [Motivation of the regularizer using Recurrent BP is not clear]
The use of recurrent BP is probably the most distinction from previous work. However, I don't see a clear description on why such a technique is necessary.

Starting from the first line in Section 3.3, ""since there is no closed-form of the regularizer in Eqn (13)"", E needs BPTT or the introduced recurrent BP. This part is simply a re-adaption of other algorithms. A very simple question is, how about use other regularizers to replace Eqn (13)? 

- [Some experiments missing]
The experiments section 4.6 uses a case of None and ""best WD"" to address some of my concerns. This is good. Does the ""gamma random"" indicates only E is used without the ||W||^2? why the best WD for one-shot is zero? This implies the model is best for applying no weight decay?

What's the effect of using the recurrent BP technique to the CVPR work? Is there some similar improvement? If yes, then the paper makes some contribution by the regularization. If not, what's the reason?

How about using the truncated BPTT with a larger T?

In general, I think the recurrent BP part should be the highlight of the paper and yet authors fail to spread such a spirit in the abstract or title. And there are some experiments missed as I mentioned above.
","The reviewer raises several significant concerns about the paper, particularly regarding the novelty of the work and the motivation behind the use of recurrent backpropagation. They find the paper's contribution potentially incremental and suggest further experiments to justify the proposed method's effectiveness compared to existing techniques. The tone, while direct and critical, maintains a professional and constructive approach by providing specific suggestions for improvement. ",-20.0,60.0
"To the best of my understanding the paper proposes some methodological ideas for visualizing and analyzing representations. 
The paper is unclear mainly because it is a bit difficult to pinpoint the contribution and its audience. What would help me better understand and potentially raise my rating is an analysis of a classical model on a known dataset as a case study and some interesting findings would help make it more exciting and give the readers more incentives to try this out. Like train an AlexNet and VGG imagenet model and show that the embeddings are better aligned with the wordnet taxonomy in one of the other. This should be possible with their approach if i understand it correctly. 

pros:
- visualization and analysis is a very exciting and important topic in machine learning
- this is clearly useful if it worked
cons:
- not sure what the contribution claim for the paper is since these types of plots existed already in the literature (is it a popularization claim ?)","The reviewer acknowledges the paper's potential and importance of the topic (visualization and analysis) but expresses concerns about the clarity and contribution. They suggest a concrete case study to better demonstrate the value and excite the readers. The language is mostly neutral, with constructive criticism, but lacks strong positive affirmations.",10.0,50.0
"General:
In general, this is a well-written paper and I feel pleasant to read the paper. The paper proposed a model named Pseudo Invertible Autoencoder(PIE) which combines invertible architecture and inference model.

Strength:
1. The explanation of the paper is very clear and consistent.
2. The idea is interesting. A lot of papers related to the inverse problem focus on perfect invertibility, but the author(s) emphasize the importance of invertible compression and relate PIE to the inference model.

Possible Improvements:
1. The experiments could have been more convincing: 1) The only competitors are VAE and WAE. 2)The only data set has been tested was MNIST data set. There are many great works mentioned in the paper and those works should also be compared in a way.
2. The content could be more compact so that more experiments can be shown to support the paper. It seems to me there is too much explanation to previous works in the paper. 
3. The paper has 9 pages which exceed the suggestion a little bit.
4. I am not sure if the author(s) checked the grammar of the paper carefully. I found quite few typos in the paper. Page 3: 'Rather then' should be 'Rather than' and 'As we are interested' should be as 'As we are interested in'; Page 4: 'Can me' should be 'Can be'; Page 6: 'Better then' should be 'Better than'; Fig.6 (b): Should it be '0' or 'g(z)'?

Conclusion:
This is a good and clean paper in general. It explains the related work and presents PIE with necessary details. My biggest concern is that empirical validation(experiment) is poor. As a conclusion, I tend to vote for weak rejection.

Minor Suggestion:
Refer to the conference instead of arXiv if the paper was already published.","The reviewer explicitly states a ""weak reject"" recommendation. While they find the paper interesting and well-written, they point out a significant flaw in the limited experimental validation. The tone, while suggesting improvements, remains constructive and not harshly critical.",30.0,70.0
"This paper proposes a Bayesian extension to knowledge base embedding methods, which can be used for hyperparameter learning. My rating is based on following aspects.

Novelty. 
Applying Bayesian treatment to embedding methods for uncertainty modelling and hyperparameter tuning is not new (examples include PMF [1] and Bayesian PMF [2]), and Sec 3 can be regarded as a knowledge base extension of them with a different likelihood (MF considers user-item pairs while knowledge base considers head-edge-tile triplets). However, it seems that there is little work considering the hyperparameter tuning problems for knowledge base embeddings.

Quality & Clarity.
This paper makes two arguments. 1. Small data problems exist, and needs parameter uncertainty; 2. Bayesian treatment allows efficient optimization over hyperparameters. However, as mentioned in Sec 4 and Sec 5, they still use MAP estimations with tuned hyperparameters instead of variational distribution directly. This does not support the parameter uncertainty argument (since there is no uncertainty in parameters of the final model, i.e., those re-trained in line 10 of algorithm 1). More analysis, both theoretically and experimentally, is needed to address this argument. The hyperparameter tuning argument is well-supported by both theoretical analysis and experiments. 

My questions are mainly about experiments. Overally, I think current experiments cannot support the claims well and further experiments are needed.
1.	As mentioned above, the parameter uncertainty issue hasn’t been well verified (Figure 3 demonstrates the advantages of hyperparameter tunning instead of uncertainty in parameters).
2.	Table 1 & 2 demonstrates that hyperparameter tunning using algorithm 1 introduces performance improvement on ComplEx and DistMult. Since the Bayesian treatment is general, such an improvement should be found for other knowledge base embedding methods. 
3.	Time complexity is not analyzed (since Algorithm 1 requires re-train the models).
4.	Algorithm 1 is a one-EM-step approximation for optimizing the ELBO. How well such a algorithm approximates the optimal solution of ELBO. For example, what will happens if running line 4-10 for multiple times? Does the performance increase or decrease?

[1] Salakhutdinov and Minh, Probabilistic Matrix Factorization, NIPS 2007.
[2] Salakhutdinov and Minh, Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo, ICML 2008.
","The reviewer acknowledges the potential of the paper's approach for hyperparameter tuning in knowledge base embeddings, but expresses concerns about the clarity and experimental validation of the parameter uncertainty argument. The reviewer finds the hyperparameter tuning aspect well-supported, but suggests additional experiments to address the parameter uncertainty claims and other aspects like time complexity and the approximation quality of the proposed algorithm. The language used is formal, objective, and provides constructive criticism without resorting to personal attacks or disrespectful language.",30.0,70.0
"The paper tackles the problem of semi-supervised classification using GAN-based models. They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference. The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations. The idea is to use GAN to learn the manifold. The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs. They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \delta to z directly (||f(z) - f(g(z+\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small. The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \bar r(z) and then adding a tunable magnitude of \bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\epsilon \bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.

Pros:
- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.
- The idea is simple and easy to implement based on a standard GAN.
- The authors conduct various experiments to show the interaction of the regularization and the generator.

Cons:
- For semi-supervised classification, the paper did not report the best results in other baselines. E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels). The performance of the proposed method is worse than the previous work but they claimed ""state-of-the-art"" results. The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2]. The experimental results are not very convincing because many importance baselines are neglected.
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018). 

I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3]. It would be better to compare with them.

References:
[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018
[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018
[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018","The reviewer acknowledges the merits of the paper, such as clear writing, simple idea, and comprehensive experiments. However, they point out a major flaw: the paper's claim of ""state-of-the-art"" results is inaccurate as it overlooks stronger baselines. The reviewer also questions the novelty of the proposed method. The tone is critical but professional and provides constructive feedback with specific recommendations. Therefore, the sentiment leans towards the negative side due to the significant concerns raised.",-30.0,60.0
"The authors present a study on multi-agent communication.
Specifically, they adapt communication to be targeted and multi-staged.
Experiments on  2 synthetic datasets and 1 3D visual dataset confirm that both additions are beneficial

Overall, this paper was somewhat clear and more importantly includes experiments on House3D, a more realistic dataset.

My main concern is the following: the method is not about targeting, but about selectively hearing.
If agents are sharing the reward then why should targeted communication be beneficial at all? Isn't the optimal strategy to just communicate everything to everyone? I understand that they should be selective at the listening side to properly integrate only the relevant information (so, attend over all received messages), but why should we expect the speaker to apriori know who this message should go to? Moreover, I don't really understand how targeted communication can even work (in the way the authors explain it) since the agents have partial information (e.g., in shapes they only see 5x5 around them), so they don't really know who is where --  but I could potentially see this working should the agents put information about their own identity and location.  So, given the positive results that the authors get, my understanding is that the signature doesn't have information about who should the recipient of the information be but more about what where the properties of the sender of this information.  So, based on my understanding, I don't feel that the flow of the story quite matches what is really happening and this might be very confusing for prospective readers. Can the authors elaborate on this, aim i getting things wrong?

There is literally no information about model size (or at least I wasn't able to find any). Is there any weight-sharing across agents? Do you obtain CommNets by using the implementations of the authors or by ablating the signature-part of your model? Moreover, why do agents have a limited view window on the SHAPES -- is (targeted) communication redundant when agents have full observability? The part about how multi-staged communication is implemented is quite cryptic at the moment -- is multi-staged the fact that the message is out-putted by processing with a recurrent unit? The messages is factorized into two parts k and u leading to a vector of size D -- what happens should we have one message of size D (rather than factorizing into 2), something like this would control for any improvements obtained from increases the parameters of the model.

Finally,  if the premises of the paper is to define more effective communication protocols, evident in the use of continuous communication, (rather than studying what form can multi-agent communication etc etc), a necessary baseline  (especially in cases where agents share reward), is to communicate the full observation (rather than a function of it).  This baseline is not presented here and it's absolutely necessary.
","The reviewer expresses several concerns about the paper's core ideas and methodology. They question the effectiveness of targeted communication in a shared reward scenario and find the explanation lacking clarity. The reviewer also points out missing details about the model and the absence of a crucial baseline comparison. While acknowledging the positive results and the use of a realistic dataset, the numerous concerns and the call for significant revisions suggest a lukewarm reception overall.",20.0,60.0
"The paper proposes a new ""sorting"" layer in neural networks that offers
some theoretical properties to be able to learn network which are 1-Lipschitz
functions.

The paper contains what seems to be a nice contribution but the manuscript
seems to have been written in a rush which makes it full of typos
and very hard to read. This unfortunately really feels like unfinished work.

Just to name a few:

- Please check the use of \citep and \citet. See eg Szegedy ref on page 3.

- Unfinished sentence ""In this work ..."" page 3.

- ""]"" somewhere at the bottom of page 4.

- ""Hence, neural network has cannot to lose Jacobian norm... "" ???

etc...

Although I would like to offer here a comprehensive review I consider
that the authors have not done their job with this submission. ","The reviewer acknowledges a potentially ""nice contribution"", which is positive. However, they express strong disappointment with the writing and typos, indicating it feels ""unfinished"" and that the authors ""have not done their job."" This suggests overall negative sentiment due to the poor presentation overshadowing the potential. The language, while direct, avoids personal insults and maintains a professional tone.",-60.0,40.0
"This submission is an great ablation study on the capabilities of modern reinforcement learning to discover the causal structure of a synthetic environment. The study separates cases where the agents can only observe or they can also act, showing the expected gains of active intervention.

The experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments. It would also be very nice to have a sequence of increasingly complex synthetic environments where causal inference is the task of interest, such that we can compare the performance of different RL algorithms in this task (the authors only used one).

I would change the title to ""Causal Reasoning from Reinforcement Learning"", since ""meta-learning"" is an over-loaded term and I do not clearly see its prevalence on this submission.","The review starts with very positive statements about the submission, using terms like ""great"" and ""interesting"". It also suggests further research directions, indicating the reviewer sees value in the work. While there are suggestions for improvement, they are framed constructively. The language throughout is professional and courteous.",80.0,90.0
"Update: see comments ""On revisions"" below.

This paper essentially introduces a label-dependent regularization to the VIB framework, matching the encoder distribution of one computed from labels. The authors show good performance in generalization, such that their approach is relatively robust in a number of tasks, such as adversarial defense.

The idea I think is generally good, but there are several problems with this work.

First, there has been recent advances in mutual information estimation, first found in [1]. This is an important departure from the usual variational approximations used in VIB. You need to compare to this baseline, as it was shown that it outperforms VIB in a similar classification task as presented in your work.

Second, far too much space is used to lay out some fairly basic formalism with respect to mutual information, conditional entropy, etc. It would be nice, for example, to have an algorithm to make the learning objective more clear. Overall, I don't feel the content justifies the length.

Third, I have some concerns about the significance of this work. They introduce essentially a label-dependent “backwards encoder” to provide samples for the KL term normally found in VIB. The justification is that we need the bottleneck term to improve generalization and the backwards encoder term is supposed to keep the representation relevant to labels. One could have used an approach like MINE, doing min information for the bottleneck and max info for the labels. In addition, much work has been done on learning representations that generalize using mutual information (maximizing instead of minimizing) [2, 3, 4, 5] along with some sort of term to improve ""relevance"", and this work seems to ignore / not be aware of this work.

Overall I could see some potential in this paper being published, as I think the approach is sensible, but it's not presented in the proper context of past work.

[1] Belghazi, I., Baratin, A., Rajeswar, S., Courville, A., Bengio, Y., & Hjelm, R. D. (2018). MINE: mutual information neural estimation. International Conference for Machine Learning, 2018.
[2] Gomes, R., Krause, A., and Perona, P. Discriminative clustering by regularized information maximization. In NIPS, 2010.
[3] Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017.
[4] Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670.
[5] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. ""Representation learning with contrastive predictive coding."" arXiv preprint arXiv:1807.03748 (2018).","The reviewer outlines some positive aspects: ""The idea I think is generally good"" and ""Overall I could see some potential in this paper being published, as I think the approach is sensible"". However, they also list several significant concerns, indicating that the paper in its current form is not ready for publication. They find problems with the positioning against related work, the structure and clarity of the paper, and the significance of the contribution. Therefore, the sentiment is rather negative, although there is potential for improvement. The language used is direct and critical, typical of academic peer review, but maintains a professional and respectful tone.",-30.0,50.0
"Summary:
The role of auxiliary tasks is to improve the generalization performance of the principal task of interest. So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest. The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.  The key components of the method are: (1) meta-generator; (2) multi-task evaluator. These two models are trained using the gradient-based meta-learning technique (for instance, MAML).  The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well. 

Strengths:
- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel. 
- The paper is well written and easy to read.
- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning. 

Weakness:
- The performance gain is not substantial in experiments. I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks. You can refer to the state-of-the-arts performance on CIFAR.
- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.
","The review acknowledges the novelty of the paper's idea and praises its writing. While it points out a weakness in the performance gain and suggests improvements, the overall tone is constructive and encouraging. Therefore, the sentiment leans towards the positive side, but not overwhelmingly so, due to the constructive criticism. The language used is polite and professional throughout, suggesting concrete ways to improve the work without resorting to harsh or negative phrasing.",60.0,80.0
"The paper proposes a novel adversarial attack on deep neural networks. It departs from the mainstream literature in two points: 
1. A 'federated' learning setting is considered, meaning that we optimize a DNN in parallel (imagine a map-reduce approach, where each node performs SGD and then a central server (synchronously) updates the global parameters by averaging over the results of the nodes) and an attacker has control over one of the nodes.
2. The treat model is not the common data poisoning setting, but 'model poisoning' (the attacker can send an arbitrary parameter vector back to the server).

The paper, which is well written, starts with proposing a couple of straightforward (naive) attacks, which are subsequently used as a baseline. Since there (apparently) is no direct related work, these baselines are used in the experimental comparisons. Then the authors propose a more sophisticated attacks, based on alternatingly taking a step into the attack direction (to get an effective attack) and minimizing the loss (to Camouflage the attack), respectively. They add also the feature of restricting the solution being not to far away from the usual benign SGD step.

All in all, I am acknowledging that his paper introduces the federated learning paradigm to 'adversarial examples' subcommunity of ICLR and would make for good discussions at a potential poster. I find the used method slightly oversimplistic, but this is maybe fine for a proof of concept paper. 

Final judgement: For me this paper is a 6-7 rating paper; a nice addition to the program, but not a must-have.

A have a question to the authors that is important to me: it seems that the baseline attack could be very very simply detected by checking on the server the norm of the update vector of the attacked node. Since the vector has been boosted, the norm will be large. While your distance-based regularization somewhat takes that effect away, it remains unclear to what amount. Can you give me some (empirical) details on this issue? / or clarify if I am completely off here?  thank you","The reviewer acknowledges the novelty of the paper and its potential to spark discussion (positive). They find the methods somewhat simplistic but acceptable for a proof-of-concept (slightly negative). They rate the paper as 6-7, which is above average but not a 'must-have' (slightly positive). The reviewer also asks a critical question, but the tone is inquisitive, not accusatory.",50.0,70.0
"The paper proposes a framework for training generative models that work on composed data. The models are trained in an adversarial fashion. The authors apply it to decompose foreground/background parts on MNIST images, and to perform sentence composition/decomposition.

High level comments:
* Clarity: In terms of language and writing style, the paper is written very clearly and easy to follow. In terms of presentation, there are some details that are omitted which would have made understanding easier and the work more reproducible.
* Quality: The idea that is introduced seems intuitive and reasonable, but the experiments does not have enough details to prove that this method works (i.e. no quantitative results presented).  Moreover, the presentation of the method is not very well done (missing details), especially since the authors used the upper limit of 10 pages.
* Originality: I am not familiar with the literature of generative models to judge this precisely, but according to the related work section it sounds like an original idea that is worth sharing.
* Significance: I believe the idea of modeling data composition explicitly sounds intuitive and interesting, and it is worth sharing. However, the experimental section does not have enough evidence that it is actually possible to learn this, so it is not clear whether the contribution is significant.

Pros:
-	interesting new problem formulation 
-	simple and clear language
-	the theoretical analysis in the last section could be interesting more generally in the context of GANs
-	the framework is applied on 2 different modalities: images and text.

Cons:
-	hard to tell whether this approach works since the metrics for evaluation are not specified and there are no quantitative results in the experimental section (only 1 qualitative example per task)
-	the work is not reproducible due to the lack of details (see more explanations below)
-	the theoretical analysis is a standalone piece of the paper, without any discussion about the implications, or making connections to the previous sections.

Detailed comments:
1.	I believe the weakest part of this paper is the evaluation section. The authors run their framework on 4 tasks of increasing difficulty. While the MNIST examples make for a nice and intuitive qualitative analysis, the are no quantitative results at all. The only result that is reported for each task is one qualitative picture. The authors make statements such as “The decomposition network learns to decompose the digits and backgrounds correctly” , “Given one component, decomposition function and the other component can be learned.” but there is not mention for how these conclusion are made (no metrics, no numbers). Indeed, it is difficult in general to quantify the results of generative models, but most other GAN papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset. If the authors manually inspected the results, they should at least report how many images they inspected and how many looked correct. 
2.	Aside from evaluation, there are some other details missing from the presentation. The individual details may not be major, but because all of these are missing together, it really affects the overall quality of the paper. For example:
    	 the authors state: “To train discriminator(s), a regularization is applied. For brevity, we do not show the regularization term (see Petzka et al. (2017)) used in our experiments.”. For reproducibility purposes, I believe it is important to at least mention the type of regularization, at least in the appendix. 
    	There is a parameter alpha used to balance the losses. What values was used in the experiments?
    	Choices of models are often not explained. Why did you choose that form for c(o1, o2) in section 3.3? Why DCGAN for component generators, and U-net for decomposition?
    	It is not explained in detail how the Yelp-reviews dataset is altered to achieve coherence. The authors mention that “As we sample a pair independently, the input sentences are not generally coherent but the coherence can be achieved with a small number of changes.”. However, the specific algorithm by which these changes are made is not specified, and thus it can’t be reproduced.
3.	The theoretical section is an interesting contribution, but the paper just states a list of theorems without making any connections to the applications used before, or a broader discussion about how these fit in the context of GANs more generally.
4.	My understanding is that both datasets used are created by the authors by making alterations to MNIST and Yelp-reviews dataset, thus making them to some extent synthetic datasets suited to fit this problem formulation. I would have like to see how this composition/decomposition works on existing datasets with no alterations. Does it still work? 
5.	In section 2.3, in the coherent sentence experimental setting, I don’t fully understand the design of the task. Figure 2 shows an example where composition and decomposition are not symmetric (i.e. composing then decomposing does not go back to the input sentences), although one of your losses is supposed to ensure exactly this cyclic consistency. Why not choose another problem that doesn’t directly violate your assumptions?

Minor issues: 
6.	From the related work section, it is not clear how your approach is different from Azadi et al. (2018). Please include more details.
7.	In section 2.4, you mention using Wasserstein GANs, with no further details about this model (not even a one line description). Without reading their paper, the readers of your paper could not easily follow through this section. The losses further introduced are also not explained intuitively (e.g. what do the two expectation terms in l_g_i represent?).
8.	I believe there are some errors in which tasks reference which figures in section 3.3. Should Task 2 refers to Figure 6, and Task 3 to Figure 7?
9.	What exactly is range(.) in section 4? If this refers to the interval of values that a variable can take, the saying “is a matrix of size |range(Z)| × |range(Y )|” doesn’t exactly make sense. Please define formally. 

Final remarks and advice: 
Overall, I believe the paper introduces some interesting ideas. There is definitely value in the problem definition and theoretical analysis. However, I believe the paper needs more work on presentation and evaluation, especially since the authors opted for 10 pages and according to ICLR guidelines “Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.”. Hopefully the above comments will help the authors improve this work!","The reviewer acknowledges the paper's interesting ideas, particularly the problem definition and theoretical analysis. However, they express significant concerns about the lack of quantitative results, missing details hindering reproducibility, and a standalone theoretical section. The tone is critical but professional, suggesting improvements rather than outright rejection. Thus, the sentiment leans slightly negative due to the substantial concerns, while the politeness remains positive due to the constructive and respectful language.",-20.0,80.0
"This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks.

This paper represents an impressive amount of experimentation. The study and the experimental results will be useful and interesting to the community. The result that some tasks' performance are negatively correlated with each other is surprising. The paper is clearly written. 

One clarification question I have is about what the ""Single-task"" pre-training means. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. I'm confused by what this means, and how this is different from just training on that task. ","The review starts with positive statements, highlighting the thoroughness and usefulness of the research. The reviewer finds the results interesting, particularly highlighting a surprising finding. They also praise the clarity of the writing. While the reviewer does have a clarification question, it is posed constructively and does not detract from the overall positive sentiment. ",75.0,100.0
"This paper presents a new approach in network quantization. The key insights of this paper is quantizing different layers with different bit-widths, instead of using fixed 32-bit width for all layer weights and activation in previous works. At the same time, this paper adopted the idea form both DARTS and ENAS with parameter sharing, and introduces a new differentiable neural architecture search framework. As the authors proposed, this DNAS framework is able to search efficiently and effective through a large search space.  As demonstrated in the Experiment section of the paper, it achieves better validation accuracy than ResNet with much smaller model size and lower computational cost.

1. An improved gradient method in updating the network architecture and parameters compared to DARTS and ENAS. It applies the Gumbel softmax to refine the sub-graph structure without training the entire super-net through the whole process. The work is able to obtain the same level of validation accuracy on Cifar-10 as ResNet while reduce the model parameters by a large margin. 
2. The work is in the middle ground of two previous works: ENAS by Pham et al. (2018) and DARTS by Liu et al. (2018). However, there is no comparison with ENAS and DARTS in experiments. ENAS samples child networks from the super net to be trained independently while DARTS trains the entire super net together without decoupling child networks from the super net. By using Gumbel Softmax with an annealing temperature, The proposed DNAS pipeline behaves more like DARTS at the beginning of the search and behaves more like ENAS at the end. 
","The review acknowledges the paper's contributions (new approach, better accuracy, smaller model size) and uses positive language. However, it also points out a missing comparison with related works, which slightly lowers the overall positivity. Therefore, the sentiment is scored as moderately positive. The language used is neutral, lacking strong positive or negative connotations.",60.0,0.0
"The paper presents a method for training a probabilistic model for Multitask Transfer Learning. The key idea is to introduce a latent variable ""z"" per task which to capture the commonality in the task instances. Since this leads to an intractable likelihood the authors use the standard ELBO with a Variational Distribution over ""z"" defined as a Gaussian + Inverse Autoregressive Flow. For classification, the authors also show that they can combine the model with the main idea in Prototypical Networks. 

The experiments evaluate on three different task, the comparison against MAML on the toy problem is quite interesting. However, the results on the Mini-Imagenet suggest that the main contributors to the better performance are the Prototypical Networks idea and the improved ResNet. Additionally, the authors compare against MAML only on the toy task and not on their synthetic dataset. I think that the experiments need better comparisons (there have been published an improved version of MAML, or even just add results from your own implementation of MAML with the same ResNet on the 3rd task as well). 

A major issue is that the model presented is not really a Hierarchical Bayesian model as being strongly presented. It is much more a practical variational algorithm, which is not bad by no means, but I find its ""interpretation"" as a Hierarchical Bayesian method as totally unnecessary and making the paper significantly harder to read and follow than it needs to be. This is true for both the base model and the model + ProtoNet. I think that the manuscript itself requires more work as well as a better comparison of the method to baseline algorithms.


Section 2.2:

The authors start by introducing a ""Hierarchical Bayes"" model over the parameters of a Neural Network for multi-task learning. By defining the model parameters to be an implicit function of some low-dimensional noise and the hyper-parameter they shift the inference to the noise variable ""z"". One issue, which I won't discuss further, is that this defines a degenerate distribution over the parameters (a fact well known in the GAN literature), which seem counter-intuitive to call ""Bayesian"". Later, since the parameters ""w"" has vanished from the equation the authors conclude that now they can change the whole graphical models such that there is actually no distribution over the parameters of a Neural Network, while the hyper-parameter IS now the parameters of a Neural Network and the latent variable is an input to it. Mathematically, the transformation is valid, however, this no longer corresponds to the original graphical model that was described earlier. The procedure described here is essentially a Variational Model with latent variable ""z"" for each task and the method performs a MAP estimation of the parameters of the Generative Model by doing Variational Inference (VAE to be exact) on the latent ""z"". There is nothing bad about this model, however, the whole point of using a ""Hierarchical Bayes"" for the parameters of the Network serves no purpose and is significantly different to the actual model that is proposed. 

In section 2, the prior term p(a) in equation 7 and Algorithm 1 is missing.

Section 3:

The authors argue that they add yet another level of hierarchy in the Graphical Model with a further latent variable ""v"", which is unclear fundamentally why you need it as it can be subsumed inside ""z"" (from a probabilistic modelling perspective they play similar roles). Additionally, they either do not include a prior or on ""v"" or there is a mistake in the equation for p(S|z) at the bottom of page 4. The main motivation for this comes from the literature where for instance if we have a linear regression and ""v"" represents the weights of the last linear layer with a Gaussian Prior than the posterior over ""v"" has an analytical form. After this whole introduction into the special latent variable ""v"", the authors actually use the idea from Prototypical Networks. They introduce a valid leave-one producer for training. However, the connection to the latent variable ""v"" which was argued to be the third level of a Hierarchical Bayes model is now lost, as the context c_k is no longer a separate latent variable (it has no prior and in the original Prototypical Network although the idea can be interpreted in a probabilistic framework it is never presented as a Hierarchical Bayes).  

","The reviewer raises several major issues with the paper, including misleading framing as a Hierarchical Bayesian model, lack of strong comparisons in the experiments, and potential theoretical flaws. While the reviewer acknowledges some positive aspects (interesting results on the toy problem), the overall tone is critical, suggesting significant revisions are needed. The language, while direct, maintains a professional and academic tone.",-30.0,60.0
"Focus on navigation problems, this paper proposes Q-map, a neural network that estimates the number of steps (in terms of the discount factor gamma) required to reach any position on the observable screen/window. Moreover, it is shown that Q-map can be applied for exploration, by trying to reach randomly selected goal.

Pros
1. Novel goal-based exploration scheme

Cons
1. Similar idea has been proposed before
For example, Dayan (1993) estimates the number of steps to reach any position on the map using successor representations. Discussion about this field (successor representations/features) is completely missing in the paper.
Ref:
- Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613–624, 1993.
- Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4058–4068, 2017.
- Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 510–519, 2018.

2. Comparison to existing methods is only vaguely discussed
For example, it is claimed multiple times that UVFA requires the goal coordinates, but Q-map also requires coordinates when doing the exploration.

3. The network architecture is not clearly presented
For example, the output of the network needs to be clipped, which suggests that there is no output transform. Since the predicted output is in [0,1], it would make sense to use Sigmoid transform for each pixel and use logistic loss.

4. The proposed exploration scheme could be unnecessarily complicated
Sec.3.1 provides lengthy discussion about the drawback of eps-greedy exploration. Then in Sec.3.2, \epsilon_r is basically the same as the eps-greedy algorithm, using to randomly select an action. Isn't this a ""bad"" thing as suggested in Sec.3.1? Moreover, the new exploration scheme requires two more hyper-parameters (min/max distance threshold), which will add more complication to the already very complicated deep RL learning procedure.

5. Experiment results are limited
For the toy experiment in Sec.2.3, the map are relatively simple. The example of Dayan (1993) with an agent surrounded by walls is an interesting scenario and should be included. The proposed Q-map (ConvNet) could fail because it is hard to learn geodesic distance with only local information. More importantly, there is no comparison to similar methods in Sec.3. UVFA can replace Q-map to do similar exploration.

6. Writing can be greatly improved
There are many grammar errors. To name a few, ""agent capable to produce"", ""the gridworld consist of"", ""in the thrist level"".

Minors
- UFV should be UVF in the introduction
- Citation in Sec.3 is not consistent with the rest of the paper. Use \citep or \citet properly.","The review acknowledges the novelty of the paper's proposed method but points out several flaws, especially the lack of comparison with existing similar methods and limited experimental validation. The reviewer lists six major concerns, indicating a rather negative sentiment. However, the language remains technical and avoids harsh or disrespectful wording, suggesting a professional and polite tone.",-30.0,50.0
"
Pros:
I also study some related tasks and suspect that Wasserstein is helpful for measuring co-occurrence-based similarity. It is nice to see the effort in this direction. 

Cons:
The methods are either not very novel or not very well-motivated. The experiment results are interesting but mixed. If the doubts about the experiments are clarified and the methods are motivated better (or the strengths/weaknesses are better analyzed), I will vote for acceptance.

Related work:
In addition to the work in the related work section, some other work also studied the NLP applications of Wasserstein, especially the ones (such as [1,2,3]) which are related to similarity measurement. The authors should include them in the related work section. 

Question about experiments:
1. Why are the SIF scores reported in Table 1 much lower than the results reported in Arora et al., 2017 and in [4]?
2. If we compare CMD with DIVE + C * delta S, the proposed method wins in EVALution and Weeds, loses in Baroni, Kotlerman, BLESS, and Levy. If you compare DIVE + delta S (Chang et al. 2017) with DIVE + C * delta S, delta S also wins in EVALution and Weeds, loses in Baroni, BLESS, Kotlerman, and Levy (although CMD seems to be better than DIVE + delta S). 
Based on the fact that your method has a high correlation with DIVE + delta S (Chang et al. 2017), I guess that CMD does not work very well when the dataset contains random negative samples, but work well when all the negative samples are similar to the target words. If my guess is right, the performance should be improved on average if you multiply the scores from CMD with the word similarity measurement.
3. To make it efficient, CMD seems to sacrifice some resolutions by using the K representative context. Does this step hurt the performance? Could you provide some performance comparison with different numbers of K to let readers know whether there is a tradeoff between accuracy and efficiency?
4. Since the results are mixed, I suppose readers would like to know when this method will perform better and the reasons for having worse results sometimes.

Writing and presentation suggestions/questions:
1. If the proposed method is a breakthrough, I am fine with the title but I think the experiment results tell us that Wasserstein is not all you need. I understand that everyone wants to have an eye-catching title for their paper. The title of this paper indeed serves this purpose. Since the strategy is effective, more and more people might start to write papers with a title like this. However, having lots of paper called ""XXX is all you need?"" or ""Is XXX all you need?"" is definitely not good for the whole community. Please use a more specific title such as Context Mover Distance or something like that.
2. The last point in the contribution is not supported by experiments. I suggest that the authors move this point to the future work section.
3. It is good to see some negative results like Baroni in Table 2. Results on other datasets should not be put into Table 4 in Appendix.
4. Using Wasserstein barycenter to measure sentence similarity seems to be novel, but the motivation is not very clear. Based on A.6, we could see that for each sentence, authors basically find the representative word which is most likely to co-occur with every word in the sentence (has the highest average relatedness rather than similarity) and measure the Wasserstein distance between the co-occurrence probability distribution. I suppose sometimes relatedness is a better metric when measuring sentence similarity, but I think authors should provide some motivative sentence pairs to explain when that is the case.
Using Wasserstein to detect hypernym seems to also be novel, but the motivation is also not clear. Again, a good example would be very helpful.
This point is also related to the last question for experiments.

Minor writing suggestions:
1. In section 3, present the full name of CITE
2. If you put some important equations to the appendix (e.g., the definition of SPPMI_{alpha,gamma}), remember to point readers to the appendix. 
3. In the second paragraph of section 7, Nickel & Kiela, 2017 is a method supervised by a hierarchical structure like WordNet rather than a count-based or word embedding based methods. 
4. In Chang et al., the training dataset is not Wikipedia dump from 2015. This difference of evaluation setup should be mentioned somewhere (e.g., in the caption of Table 2).
5. The reference section is not very organized. For example, the first name of Benotto is missing for the PhD thesis ""Distributional Models for Semantic Relations: A Study on Hyponymy and Antonymy"". The arXiv papers are cited using different formats. Only some papers have URL. The venue's names are sometimes not capitalized. Gaussian embedding is cited twice, etc.


[1] Kusner, M., Sun, Y., Kolkin, N., & Weinberger, K. (2015). From word embeddings to document distances. In International Conference on Machine Learning (pp. 957-966).
[2] Xu, H., Wang, W., Liu, W., & Carin, L. (2018). Distilled Wasserstein Learning for Word Embedding and Topic Modeling. NIPS 
[3] Rolet, A., Cuturi, M., & Peyré, G. (2016, May). Fast dictionary learning with a smoothed wasserstein loss. In Artificial Intelligence and Statistics (pp. 630-638).
[4] Perone, C. S., Silveira, R., & Paula, T. S. (2018). Evaluation of sentence embeddings in downstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.
","The review is mostly critical of the paper, pointing out flaws in the methodology, results, and presentation. While it acknowledges some positive aspects (e.g., novelty in certain areas), the overall tone suggests significant revisions are needed. The reviewer's willingness to reconsider if doubts are addressed indicates openness but also highlights the review's negative leaning. The numerous detailed questions and suggestions further reinforce this critical but constructive stance. ",-30.0,60.0
"This paper presents a laudable attempt to generalize the learning of active learning strategies to learn general strategies that apply across many different datasets that have variables of different, not pre-determined, types, and apply the learned active learning strategies to datasets that are different from what they have been learned with. The paper is written quite clearly and is clear in its discussion of what its advance is beyond the current state of the art.

Unfortunately, the motivation of the details of the algorithm and the experiment analysis leave the paper short of what is needed to truly assess the value of this area of work and; therefore, short of what is needed for publication in ICLR. The most notable shortcoming is on page 4, at the bottom, where the actions are described. Among the components of the actions are statistics related to the dataset---the average distance from the chosen point to all the labeled data, and the average distance from the chosen point to all the unlabeled data. The authors do not provide a motivation for the use of these particular statistics. Additionally, the authors did not explore any other statistics. I should think that statistics relevant to the sparsity of the data (e.g., how well they cluster). Additionally, what distance measure is being used? A variety of distance metrics should be explored, such as d-separation for continuous variables and Hamming distance for discrete variables, should be tested, as they intuitively seem likely to affect the results. Additionally, many values are chosen for the experiments without motivation and without testing a variety of values (e.g., 30 for the size of the dataset used to calculate the reward, 1000 RL iterations, and others).

In the experiments, there needs to be discussion of how much variety there is in the different datasets in terms of their statistical properties that are relevant to active learning, such as how well the data cluster? That would help in understanding why the new algorithm performs as it does relative to the baseline.

One relatively minor point: The authors state on page 3, ""For example, the probability that the classifier assigns to a datapoint suits this purpose because most classifiers estimate this value."" This is a bit misleading---only generative classifiers would do this, not discriminative classifiers.

Pros:
1. Very clear writing.
2. Good motivation for the general problem.
3. Precise description of algorithm.

Cons:
1. Poor motivation for the particular algorithm implementation---features used in the actions, parameter values chosen.
2. Lack of experiments with different choices for features and parameter values.
3. Lack of assessment of the dataset characteristics and how they relate to algorithm performance.","The reviewer acknowledges the paper's clear writing, relevant topic, and well-defined algorithm as positive aspects. This points towards a somewhat positive sentiment. However, the review heavily criticizes the lack of justification for design choices (features, parameters) and the absence of experiments exploring these choices. The reviewer also points out a lack of analysis regarding dataset characteristics and their impact on performance. This suggests significant revisions are needed. The criticism, while direct, maintains a professional and constructive tone, indicating politeness. Overall, the sentiment leans slightly positive due to the acknowledged merits, but the numerous and substantial concerns pull it back from a highly positive score.",20.0,60.0
"
-- Contribution, Originality, and Quality --

This paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5).

These two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting.

-- Clarity --

I have two major complaints about the clarity of this paper. 

1) Section 4 of the paper is not well written and is hard to follow.

2) Some notations in the paper are not well defined. For instance

2a) In page 3, the notation \delta has not been defined.
2b) In page 6, both notation V_{\theta'_V} and V'_{\theta_V} have been used. I do not think either of them has been defined. 

-- Pros and Cons --

Pros:

1) The proposed approaches and the experiment results are interesting.

Cons:

1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference.

2) The paper is not very well written, especially Section 4.

3) For Theorem 3.2, why not prove a variant of it for the general multi-task case?

4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.","The reviewer acknowledges the interesting aspects of the work but raises significant concerns about novelty and clarity. Phrases like ""neither of them seems to contain *significant* novelty,"" ""derivations... are also relatively straightforward,"" and ""the paper is not very well written"" point towards a negative sentiment. However, the reviewer also provides constructive feedback and suggestions for improvement, indicating a professional and polite tone overall.",-30.0,60.0
"The paper introduces RL based approach to prune layers in a DenseNet. This work extends BlockDrop to DenseNet architecture making the controller independent form the input image. The approach is evaluated on CIFAR10 and CIFAR100 datasets as well as on ImageNet showing promising results.

In order to improve the paper, the authors could take into consideration the following points:

1. Given the similarity of the approach with BlockDrop, I would suggest to discuss it in the introduction section clearly stating the similarities and the differences with the proposed approach. 
2. BlockDrop seems to introduce a general framework of policy network to prune neural networks. However, the authors claim that BlockDrop ""can only be applied to ResNets or its variants"". Could the authors comment on this? 
3. In the abstract, the authors claim: ""Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives"". It is hard to asses if the statement is correct given the evidence presented in the experimental section. It is not clear if the method is more efficient and compact than others, e. g.  CondenseNet. 
4. In the experimental section, addressing the following questions would make the section stronger: What is more important FLOPs or number of parameters? What is the accuracy drop we should allow to pay for reduction in number of parameters or FLOPs?
5. For the evaluation, I would suggest to show that the learned policy is better than a random one: e. g. not using the controller to define policy (in line 20 of the algorithm) and using a random random policy instead.
6. In Table 1, some entries for DenseNet LWP are missing. Is the network converging for this setups? 
7. \sigma is not explained in section 3.3. What is the intuition behind this hyper parameter?
8. I'd suggest moving related work section to background section and expanding it a bit.
9. In the introduction: ""... it achieved state-of-the-art results across several highly competitive datasets"". Please add citations accordingly.

Additional comments:
1. It might be interesting to compare the method introduced in the paper to a scenario where the controller is conditioned on an input image and adaptively selects the connections/layers in DenseNet at inference time.
2. It might be interesting to report the number of connections in Table 1 for all the models.

Overall, I liked the ideas presented in the paper. However, I think that the high degree of overlap with BlockDrop should be addressed by clearly stating the differences in the introduction section. Moreover, I encourage the authors to include missing results in Table 1 and run a comparison to random policy. In the current version of the manuscript, it is hard to compare among different methods, thus, finding a metric or a visualization that would clearly outline the ""efficiency and compactness"" of the method would make the paper stronger.","The review starts with a positive note, acknowledging the novelty and promising results. However, it raises several important questions and suggests significant improvements, indicating a need for major revisions. While the suggestions are constructive and the language is polite, the call for major revisions suggests the sentiment is more nuanced than overly positive.",50.0,75.0
"Unfortunately, while this is interesting work, the authors emails are listed on the first page and the acknowledgments are very revealing. I am a big fan of Google, UCL, and the Royal Society, and this strongly biases my view of the work. 

My biased review:

- the paper is interesting, and should go to another venue. I do not think the authors will get benefit from presenting this work at ICLR (there is a tiny quantum focus).

- how is the cost function justified? I'd be curious to see how the authors derived it. Right now above Eq 2.4 it seems like it is heuristic to balance successful/erroneous/inconclusive rates. If it is a heuristic, the paper should clearly state this. 

- using simple examples of quantum data and quantum states would go a long way towards helping me understand the problem setup (Eq 2.1). It took me a while to grok this.

- The acronym POVM is never defined. ","The reviewer expresses reservations about the paper due to potential bias stemming from the authors' affiliations. They find the work ""interesting"" but suggest a more suitable venue. The reviewer raises valid technical questions about the cost function and clarity of the problem setup, indicating a need for improvement. However, the language remains professional and constructive throughout.",20.0,60.0
"The paper proposes a method for using auxiliary tasks to support the optimization with respect to a main task. In particular, the method assumes the existence of a loss function for the main task that we are interested in, and a loss function for an auxiliary task that shares at least some of the parameters with the main loss function. When optimizing for the main loss function, the gradient of the auxiliary loss function is also used to update the shared parameters in cases of high cosine similarity with the main task. The method is demonstrated on image classification and a few reinforcement learning settings.

The idea of the paper is simple, and the method has a nice property of (if ignoring some caveats) guaranteeing steps that are directionally correct with respect to the main task. In that sense it is useful in practice, as it limits the potential damage the auxiliary task does to the optimization of the main task.

As the authors also note, the method suffers from some drawbacks. Although the method limits the negative effect of the auxiliary task on the optimization of the main loss function, it can still slow down optimization if the auxiliary task is not well chosen. In that sense, the method is no silver bullet. In addition, the method seems fairly computationally expensive (it would be interesting to understand how much it slows down an update, I would assume the added complexity is roughly a constant multiplier). However, as an alternative to naively adding an auxiliary task, the proposed method is a welcome addition to the tool box of practitioners.

Although the experiments presented in the paper are quite different from each other, I would have wished for even more experiments. The reason is that as the method does not guarantee faster convergence, its applicability is mainly an empirical question. Especially experiments where auxiliary tasks have been used before would be interesting to test with the only addition being introducing the method proposed.

The paper is generally well written and the results are fairly clearly presented. As a minor comment, the authors might want to check that articles (such as ""the"") are not missing in the text.

All in all, the main merit of the proposed method is its conceptual simplicity and easy to understand value in practical applications where an auxiliary loss function is available. The method also seems to work well enough in the experiments presented.","The reviewer acknowledges the merits of the proposed method, such as its simplicity and practical value. They find it useful in limiting potential damage from auxiliary tasks. However, they also point out drawbacks like potential slow down and computational cost. While they don't deem it a breakthrough, they see it as a valuable addition to practitioners' toolbox. The reviewer's desire for more experiments, especially comparisons with existing methods, suggests room for improvement rather than a negative view. The writing tone is constructive and professional throughout.",60.0,80.0
"The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. 

The lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed “less” important if a notion of coefficient importance can be derived? 

Can you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? 

For the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. 

On the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more ""rounds"" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper's core idea (federated dropout). While they express skepticism about the lossy compression aspect, they do so respectfully and with a clear desire to understand the rationale behind the approach. The use of phrases like ""it would certainly help me better appreciate the contributions"" and ""adding clarity on this would certainly help"" suggests a collaborative and helpful intent.",60.0,80.0
"pros:

- Clearly written and sound paper.
- Addresses interesting problem. 
- Improves existing methods used for this learning scenario.  

cons:

- The core contribution is a special case of previously published more general framework which is not cited in the paper.

It is clearly written paper with a good motivation. The major problem is that the core contribution, namely, the risk reformulation in Theorem 1 and the derived loss (6), are special cases of more general framework published in 
   Jesus Cid-Sueiro et al. Consistency of Losses for Learning from Weak Labels. ECML 2014.

The work of [Cid-Sueiro2014] proposes a general way how to construct losses for learning from weak labels. They require that the distribution of weak labels is a linear transformation of the true label distribution, i.e. the assumption (3) of the paper under review. According to [Cid-Sueiro2014], the loss on weak labels is constructed by $weak_loss = L*original_loss$, where $L$ is the left inversion of the ""mixing matrix"" $T$ in (3). [Cid-Sueiro2014] also shows that such weak loss is classification calibrated which implies statistical consistency of the method. 

Learning from complementary labels is a special case when the mixing matrix is $T=(E-I)/(K-1)$ (E is unitary matrix, I is matrix of ones, K is number of labels). In this case, the left inversion of $T$ is simply $L=- E*(K-1) + I$ and so the weak loss is $weak_loss=L*loss$ which corresponds to the loss (5) proposed in the paper under review (in fact, the loss (5) also adds a constant term (Y-2)/(Y-1) which however has no effect on the minimizer). 

The novel part of the paper is the non-negative risk estimator proposed in sec 3.3 and the online optimization methods addressed in sec 3.4. These extensions, although relatively straightforward, are empirically shown to significantly improve the results.","The review acknowledges the paper's clarity, interesting problem, and improvement upon existing methods, indicating positive aspects. However, it highlights a significant flaw: the core contribution being a special case of a previously published framework not cited. This omission is a serious criticism. While the reviewer acknowledges the novel part of the paper, the overall tone suggests that the uncited work significantly impacts the paper's originality and contribution. Therefore, the sentiment leans towards the negative side. The language remains technical and professional throughout, without resorting to personal attacks or harsh wording, indicating a neutral politeness level.",-30.0,0.0
"In this work, the authors explore different ways to pre-train contextualized word and sentence representations for use in other tasks. They propose two main methods: a straight-forward extension of the ElMO model for hierarchical uni-directional language models, and a de-noising auto-encoder type method which allows to train bi-directional representations. The learned contextual representations are evaluated on three downstream tasks, demonstrating the superiority of the bi-directional training setting, and beating strong baselines on extractive summarization.

The method is clearly presented and easy to follow, and the experiments do seem to support the author's claims, but their exposition misses several important details (or could be presented more clearly). For the document segmentation task, are the articles taken from a held-out set, or are they contained in the pre-training set? For passage retrieval, is the representation the same or are the representations re-trained from scratch using paragraph blocks? What exactly are the other features (those can go in the appendix)? And for the extractive summarization task, how many sentences are selected? Is pre-training also done on Wikipedia, or are those representations trained on news text?

A comparison to non-contextualized sentence representations would also be welcome (SkipThought, InferSent, ElMO-pool for settings other than passage retrieval). Note also that the local pre-training is not equivalent to ElMO, as the later sees context form the whole document rather than just the current sentence.

It is interesting to see that contextualized sentence representations can be used and that the Mask-LM objective yields better results than L+R-LM, but these points would be better made if the above questions were answered.","The review starts with a positive sentiment, highlighting the clarity and support for the authors' claims. However, it then raises several questions and suggests improvements, indicating a desire for more clarity and completeness. Overall, the tone is constructive and helpful, suggesting a positive outlook with room for improvement.",50.0,75.0
"Major Contribution:
The paper introduces a method that combines the advantage and of model-based RL and imitation learning and offset their weakness. The method proposes a probabilistic inference approach to analyze the action of the model.

Organization/Style:
The paper is well written, organized, and clear on most points. 

Technical Accuracy:
I'm not an expert in RL. The method is obscure to me, but from my point of view, the experiments are done quite thoroughly and the results look good.

Presentation:
Good. 

Adequacy of Citations: 
The author should consider adding the related works include:
Bojarski, Mariusz, et al. ""End to end learning for self-driving cars."": using CNNs to implement imitation learning for self-driving cars

Multimedia:
Videos are helpful to understand the method and are well composed.","The review is overall positive. The reviewer explicitly states that the paper makes a ""Major Contribution"" and finds the method interesting although it is outside their main area of expertise. The reviewer also praises the writing, organization, presentation, and experimental validation. There is only one suggestion for improvement regarding related work, which doesn't detract from the positive sentiment. The tone throughout is neutral and professional, without any negative or overly critical language.",75.0,75.0
"This paper gives a model for understanding locally connected neural networks. The main idea seems to be that the network is sparsely connected, so each neuron is not going to have access to the entire input. One can then think about the gradient of this neuron locally while average out over all the randomness in the input locations that are not relevant to this neuron. Using this framework the paper tried to explain several phenomena in neural networks, including batch normalization, overfitting, disentangling, etc.

I feel the paper is poorly written which made it very hard to understand. For example, as the paper states, the model gives a generative model for input (x,y) pairs. However, I could not find a self-contained description of how this generative model works. Some things are described in Section 3.1 about the discrete summarization variables, but the short paragraph did not describe: (a) What is the ""multi-layer"" deterministic function? (b) How are these z_\alpha's chosen? (c) Given z's how do we generate x? (d) What happens if we have z_\alpha and z_\beta and the regions \alpha and \beta are not disjoint? What x do we use in the intersection?

In trying to understand the paper, I was thinking that (a)(b) The multilayer deterministic function is a function which gives a tree structure over the z_\alpha's, where y is the root. (I have no idea why this should be a deterministic function, intuitively shouldn't y be chosen randomly, and each z_\alpha chosen randomly conditioned on its parent?)  (c) there is a fixed conditional distribution of P(x_\alpha|z_\alpha), and I really could not figure out (d). The paper definitely seems to allow two receptive fields to intersect as in Figure 1(b).

Without understanding the generative model, it is impossible for me to evaluate the later results. My general comments there is that there are no clear Theorems that summarizes the results (the Theorems in the paper are all just Lemmas that are trying to work towards the final goal of giving some explanations, but the explanations and assumptions are not formally written down). Looking at things separately (as again I couldn't understand the single paragraph describing the generative model), the Assumption in Theorem 3 seems extremely limiting as it is saying that x_j is a discrete distribution (which is probably never true in practice). I wouldn't say ""the model does not impose unrealistic assumptions"" in abstract if you are going to assume this, rather the model just makes a different kind of unrealistic assumptions (Assumptions in Theorem 2 might be much weaker, but it's hard to judge that).

==== After reading the revision

The revised version is indeed more clear about how the teacher network works, and I have tried to understand the later parts of the paper again. The result of the paper really relies on the two assumptions in Theorem 2. Of the two assumptions, the first one seems to be intuitive (and it is OK although exact conditional independence might be slightly strong). The second assumption is very unclear though as it is not an assumption that is purely about the model/teacher network (which are the x and z variables), it also has to do with the learning algorithm/student network (f's and g's). It is much harder to reason about the behavior of an algorithm on a particular model and directly making an assumption about that in some sense hides the problem. The paper mentioned that the condition is true if z is fine-grained, but this is very vague - it is definitely true if z is super fine-grained to satisfy the assumption in Theorem 3, but that is too extreme.

Overall I still feel the paper is a bit confusing and it would benefit from having a more concrete example. I like the direction of the work but I can't recommend for recommendation at this stage.","The reviewer finds the paper's core idea interesting but criticizes its clarity and the strength of its assumptions. They find the writing difficult to follow, particularly the explanation of the generative model, making it hard to evaluate the results. While acknowledging improvements in a revised version, they still find it confusing and reliant on a strong, unclear assumption in Theorem 2. The language is critical but professional, focusing on content rather than personal attacks.",-20.0,60.0
"This paper formulates a new method called human-guided column networks to handle sparse and noisy samples. Their main idea is to introduce human knowledge to guide the previous column network for robust training.

Pros:

1. The authors find a fresh direction for learning with noisy samples. The human advice can be viewed as previledged information.

2. The authors perform numerical experiments to demonstrate the efficacy of their framework. And their experimental result support their previous claims.

Cons:

We have three questions in the following.

1. Motivation: The authors are encouraged to re-write their paper with more motivated storyline. The current version is okay but not very exciting for idea selling. For example, human guidance should be your selling point, and you may not restrict your general method into ColumnNet, which will limit the practical usage.

2. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1], estimating noise transition matrix [2,3], and explicit and implicit regularization [4]. I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.

3. Experiment: 
3.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.
3.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data. Besides, the authors are encouraged to conduct 1 NLP dataset.

By the way, if your human guidance is totally wrong, how your model handle such extreme cases? Could you please discuss this important point in your paper?

References:

[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.

[2] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

[3] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

[4] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.","The reviewer provides both positive feedback (finding the direction fresh and experiments supportive) and constructive criticism with specific suggestions for improvement.  While they offer substantial recommendations, the tone is encouraging and aims to guide the authors towards a stronger paper. The requests for more motivation, deeper discussion of related works, and additional experiments are standard expectations in peer review. ",50.0,70.0
"The paper presents and evaluates different common inductive biases in Deep RL. These are systematically evaluated on different experimental settings.

The paper is easy to read and the authors explain well the setting and their findings. The comparison and evaluations is well conducted and valuable contribution to the literature.  I would have liked some more details on the motivating example in section 3.1, maybe with a figure supporting the explanation of the example. ","The reviewer explicitly states the paper is well-written, easy to follow, and presents a valuable contribution. They offer a minor suggestion, not a criticism, indicating a positive stance overall.",80.0,100.0
"After rebuttal, I adapted the score. See below for original review.
--------------------------------------------


The authors implement a two-stage multi-objective optimization scheme to optimize neural network architectures with several conflicting goals.
I can not accept the paper in its current form.

In short, I have the following main criticisms:
1. use of crowding distance(CD) instead of hypervolume-contribution.
CD is not consistent with the HV estimator, especially CD might remove solutions that have a large HV-contribution and thus HV will not increase monotonically. The effect is even visible in Figure 8c) as in iteration 22, HV is decreasing as crowding distance removes a good offspring. In short: Crowding distance should not be used as long as the number of objectives does not prohibit computing the HV-contribution.

2. No good justification of BN. It is unclear to me why BN should be used instead of more iterations at stage 1. In 4.4 BN is only compared to the uniform initialization, but this comparison has no meaning given that we already have an optimized front that improved on the uniformly sampled distribution. To be honest, the samples shown from BN do not look very convincing as a lot of very poor architectures are created.

A proper comparison would be comparing the 2-step approach with only the first step and the same budget. Then we could compare samples from both distributions (either sampling from the front using mutation/crossover or sampling from BN). Also we would have a fair comparison of the obtained fronts and HV-values.

3. Ablation study cross-over
I am not convinced by the results presented. The paper says this is a ""small scale"" study but does not give the number of iterations/samples. It is clear that in the setup of the mutation operator cross-over might help, simply because it can change many more connections in a single iteration than mutation alone, which is limited to max 1 change. Allowing up to two mutations and no crossover could already proof to be better (orsmaller size of offspring population, see below)


Smaller concerns:

1. The results suggest that the uniform distribution might not be tuned well, as it only covers the ""expensive"" networks but not the ""cheap"" networks. A better initialization scheme that covers the x-axis better might already show vastly different results. As the Flop-objective is cheap to compute and does not require simulation, one could expect to tune this offline before initialization.

2. No handling of Noise.
During optimization, the chosen starting point and SGD algorithm will introduce noise into the process. Thus, the final test accuracy will be noisy. As an elitist dominance scheme is used, one might easily end up with an architecture that has a large variance when trained, i.e. when performing a final training pass on the full dataset, the performance might be very different. Moreover, the algorithm might stop convergence towards the true pareto front as it is held back by noisy ""good"" results. This should be discussed in the paper

3. A single-offspring approach might be better than sampling a full population (or offspring size in the order of parallel instances one can expend to run). 40 sounds excessive given that the sampling distribution is only improved through selection and given that the pareto front approximation appears to include less than 40 elements. This might also affect the results in the ablation study for cross-over: more iterations with reduced offspring size allows for more mutations of successful offspring.

4. Some unclear or wrong wordings:
page 4: ""As a consequence[...] the best solution encountered [...] will always be present in the final population. "" What do you consider ""best"" in a 2-objective problem? Do you mean: the best in each objective?
page 6, footnote1: this is not true. even without crossover the selection operator ties the solutions together, an offspring has to beat any point in the population, not necessarily its direct parent.

5. Figure 8a) does not include the state of the art result for CIFAR10, see for example 
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130","The review starts with a clear statement of rejecting the paper in its current form, which highly suggests a negative sentiment. While the reviewer lists several major and minor concerns, they provide concrete reasons and suggestions for improvement. The language, while direct, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-50.0,50.0
"The paper proposes a class of Evolutionary-Neural hybrid agents (Evo-NAS) to take advantage of both evolutionary algorithms and reinforcement learning algorithms for efficient neural architecture search. 

1. Doesn't explain how exactly the mutation action is learned, and missing the explanation of how RL acts on its modification on NAS (Evo-NAS). 
2. Very poor explanation on LEARN TO COUNT experiment. The experiment contains difficult setups on a toy data, which makes it difficult to repeat. In figure 3, the paper says that the sample efficiency of the Evo-NAS strongly outperforms both the evolutionary and the neural agent. However, where the strength comes from is not discussed in detail. In figure 2, the paper claims that PQT outperforms Reinforce for both the Neural and the Evo-NAS agent. For the Evo-NAS agent, the gain is especially pronounced at the beginning of the experiment. Thus, the paper concludes that PQT can provide a stronger training signal than Reinforce. However, how much stronger training signal can obtain of the proposed method is not discussed. Because the experiments of 5.1 is setup on a toy data with complicated parameters. The conclusions based on this data set is not convincing. It would be better to add comparative results on the CIFAR and Imagenet data for convenient comparisons with state-of-the-art. 
3. Confusing notation and experimental setup. In 5.1, the sequence a is first defined as <a1, a2, .., an>. Then, after eq.2, the sequence a is given as a=<1, 2, ..., n>. It would be better to use different symbols here. ","The review is critical of the paper, pointing out several weaknesses such as lack of clarity in explaining concepts, poorly explained experiments with difficult setups, confusing notations, and a lack of comparison with state-of-the-art on standard datasets. The language used, while direct and critical, maintains a professional and academic tone. ",-40.0,60.0
"CAML seems an interesting meta-learning algorithm. I like the idea that the context parameters are used to modulate the whole network during the inner loop of meta-learning, while the rest of the network parameters are adaped in the outer loopand shared across tasks. Also, it is good to see that CAML is competitive with on few shot CNNs.

The paper is very well presented. Experiments are reasonably solid.

If I understood correctly, although CAML has achieved better accuracy it seems CAML still requires a decent amount of parameter/network structure optimisation. Would be good if the paper has a section talking about practical tricks of how to find the best CAML hyperparameter quickly.","The review starts with positive statements, highlighting strong points like the interesting algorithm, good presentation, and solid experiments. While it points out an area of improvement regarding hyperparameter optimization, it frames it as a suggestion rather than a harsh criticism. The language used is constructive and typical of academic discourse.",75.0,80.0
"This paper suggests to rethink about the bias-variance tradeoff from statistical machine learning in the context of neural networks. Based on some empirical observations, the main claims in this work are that (1) it is not always the case that the variance will increase when we use bigger neural network models (particularly, by increasing the network width); (2) the variance should be decomposed into two parts: one part accounts for the variance caused by random initialization of network parameters/optimization and the other part is caused by ""sampling of the training set"".

For the first claim is based the empirical observation that increasing the number of hidden units did not cause the incrase of variance (as in figure 1). However, to my understanding, it only means increasing the number of hidden units is probably not a good way to increase the network capacity. In other words, this cannot be used as an evidence that the bias-variance tradeoff is not valid in neural network learning.

For the second claim, I don't like the way that they decompose the variance into two parts. To be clear, the classical bias-variance tradeoff doesn't consider the optimization error as an issue. For a more generic view of machine learning errors, please refer to ""The Tradeoffs of Large Scale Learning"" (Bottou and Bousquet, 2008). In addition, if the proposed framework wants to include the optimization error, it should also cover some other errors caused by optimization, for example, early stopping and the choice of a optimization algorithm.

Besides these high-level issues, I also found the technical parts of this paper is really hard to understand. For example,

- what is exactly the definition of $p(\theta|S)$? The closely related case I can think about is in the Baysian setting, where we want to give a prior distribution of model (parameter). But, clearly, this is not the case here. 
- similar question to the ""frequentist risk"", in the definition of frequentist risk, model parameter $\theta$ should be fixed and the only expectation we need to compute is over data $S$
- in Eq. (5), I think I need more technical detail to understand this decomposition.","The reviewer raises several significant concerns about the paper's core claims, methodology, and clarity. They find the evidence for the first claim unconvincing and disagree with the approach taken for the second claim. The reviewer also points out a lack of clarity in the technical details, making it difficult to understand the paper's contributions fully.  The tone is critical, but the reviewer provides concrete examples and suggestions for improvement. ",-50.0,50.0
"Authors establish a connection between communication reduction in distributed optimization and dithered quantization. This allows us to understand prior approaches in a new perspective, and also motivates authors to develop two new distributed training algorithms which communication overhead is significantly reduced. The first algorithm, DQSG, uses dithered quantization to reduce the communication bits. The second algorithm, NDQSG, uses nested dithered quantization to further reduce the amount of needed communication. The usefulness of these algorithms are empirically validated by computing the raw communication bits and average entropy of them. Therefore, dithered communication seems to provide both theory and algorithm which are useful.

The paper is clearly written. It provides a succinct review of dithered quantization and previous works, and figures provide a good insight into why the algorithm works, especially Figure 3.

Theorems in this paper are mostly about plugging in properties of dithered quantization into standard results in stochastic optimization, but they are still useful. The analysis of NDQSG does not seem to be as complete as that of DQSG, however. With NQSG, now workers are divided into two groups, and there would be an interesting tradeoff between assignments to these two: how should we balance two groups? This might be tricky to analyze, but it is still useful to clarify limitations and provide conjectures. At least, this could be analyzed empirically.

pros:
* establishing a connection to other topic of research often facilitates productive collaboration between two fields
* provides a new perspective to understand prior work
* provides new useful algorithms

cons:
* experiments were conducted on small models and small datasets
* unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear wall-time would actually be reduced with these algorithms.","The review is overall positive. The reviewer highlights the novelty of the connection the paper makes, the clarity of the writing, and the usefulness of the algorithms. While it points out limitations such as limited experimental validation and some theoretical aspects needing further clarification, it frames them constructively as areas for improvement rather than outright flaws. ",75.0,75.0
"This paper presents an approach to evaluate the quality of segmentations. To achieve this, a variational auto-encoder (VAE) is trained on the ground truth masks to extract shape-relevant information in the feature space, assuming that incorrect segmentations will be far from the normal distribution. Then, a regression model is trained to predict the quality of the segmentation based on the shape-learned features. The authors use several datasets focusing on pancreas segmentation to evaluate their quality-assessment approach, showing competitive performance with respect to other approaches.

The paper is well written, easy to follow in general, and the methodology is sound. Nevertheless, I have some concerns related to the applicability of this approach.

- Closely related works in the literature are missing:

There is a closely related recent work that used auto-encoders on the sets of ground-truth masks to build representations of shape and constrain the outputs of deep networks: Otkay et al., Anatomically Constrained Neural Networks (ACNN): Application to Cardiac Image Enhancement and Segmentation, IEEE TMI 2017

This work does not focus directly on quality assessment. However, I believe the loss in this work, which evaluates the difference between the obtained segmentation (characterized by the outputs of a deep network) and an auto-encoder description of shape, can be used directly as a criterion for evaluating the quality of segmentation (on validation data) in term of consistency with the shape prior. I think this work is very closely related and should be discussed. 

Also, a quick google search provided some missing references related to this work. I think including comparisons to the recent work in [1], for example, would be appropriate. As the focus is on quality assessment of medical image segmentation, I would suggest a deeper review of the literature.

[1] Vanya V Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos Kamnitsas, Eric O Aboagye, Andrea G Rockall, Daniel Rueckert, and Ben Glocker. Reverse classification accuracy: Predict- ing segmentation performance in the absence of ground truth. IEEE Transactions on Medical Imaging, 2017. 
[2] S. Chabrier, B. Emile, C. Rosenberger, and H. Laurent, “Unsupervised performance evaluation of image segmentation,” EURASIP Journal on Applied Signal Processing, vol. 2006, pp. 217–217, 2006. 
[3] Gao H, Tang Y, Jing L, Li H, Ding H. A Novel Unsupervised Segmentation Quality Evaluation Method for Remote Sensing Images. Sensors. 2017 Oct 24;17(10):2427.

- The proposed quality assessment uses the learned shape features.  Even though it is strong prior information, there  might be situations where the predicted segmentation might be plausible in terms of shape, but is not a good segmentation. 

-  I wonder how this approach works in problems with a high size/shape variation. For example, in the case of tumors, where their shape is unpredictable and each unknown case can be seen as a ‘rare’ example.

- To better capture the shape in the proposed approach, images need to be aligned, which limits the applicability of this approach to aligned volumes only. 

- This approach gives a global hint about a given segmentation result, as a whole. I think it would be more interesting to provide local information on a segmentation, as it may happen that a predicted contour is generally correct, but there are some crispy borders in some points due to low contrast, for example. Even though the quality assessment would say that the prediction is correct, the contour may be unusable for certain applications, where a minimal surface distance is required (e.g., radiotherapy).

- As the quality assessment is based on shape and not in image information, it would be interesting to see how accurately it predicts the performance on different image modalities (for example, the method is trained on ground truth masks corresponding to CT images and quality is assessed in segmentations performed in MRI).

If I understood correctly, comparison with other methods is done with the same dataset under the same conditions (i.e., all the images are pre-aligned). As the other methods might not have the limitation of requiring aligned images, it would be interesting to compare also the performances in this situation.

How the training (or the VAE) is adapted for DeepLab-3, as it is based on 2D convolutions?

Minor: The paper needs a proof-read to fix some issues (e.g. ‘the properties of F is encoded’)
","The review starts with positive remarks, highlighting the paper's clarity and sound methodology. However, it then raises several concerns about the approach's limitations, suggesting missing references and questioning its applicability in various scenarios. The reviewer provides constructive criticism and suggestions for improvement, indicating a desire to see the work strengthened rather than rejected. Therefore, the sentiment is more nuanced than extremely positive or negative.",20.0,80.0
"The paper describes a cache side-channel attack on a deep learning model. In a cache side-channel attack, the attacker sets up a process on the same machine where the victim process (that is running the training or evaluation job for the DNN model) is running. It is assumed that the victim process uses a common shared library for DNN computations as the attacking process. The attacking process flushes the cache, then observes access times for key functions. The paper shows that, based on the speed of accessing previously flushed functions, the attacker can discover the high-level network architecture, namely the types of layers and their sequence. The paper shows that, by spying on such cache access patterns in the Tensorflow library, this method can reliably extract the above high-level information for 11 different network architectures. It also describes a few counterattack alternatives whereby the victim can obfuscate its cache access patterns for self-protection.

The significance of the results is not clear to me. The extracted information is very high level. What realistic attacks can be constructed from such a coarse-grained fingerprinting? The experimental results show that the fingerprint can be used to map the architecture to one of the 13 well-known architectures (VCC16, ResNet, DenseNet, Inception, etc.). But so what? What does the victim lose by revealing that it's using one of a few very well known types of DNNs (the ones tested in this paper). There may very well be a good reason why this is very dangerous, but that is not explained in the paper. Not being familiar with this line of research and its significance, I looked up several of the related papers (Suciu et al., 2018, Tramer et al., 2017, Papernot et al., 2017, Yan et al., 2018). None of them could explain why this particular type of fingerprinting is dangerous.

Of the cited previous work, Yan et al., 2018 seems to present the most closely related approach. The method described in that paper is very similar: cache side attack on a shared library through a co-located attacker process. They monitor at a finer grain -- Generalized Matrix Multiplications -- and are thus able to infer more details such as the size of the layers. This also makes the inference problem harder -- they were able to narrow down the search space of networks from >4x10^35 to 16 (on VGG16). On the surface, the results presented in this paper seem stronger. But they are actually solving a much easier problem -- their search space is one of 13 well-known networks. To me, Yan et al.'s approach is a much more powerful and promising setup.

Overall, while the paper is clearly written and presents the idea succinctly, it is derivative of previous research, and the results are not stronger. I'm not an expert in this area, so it's possible that I missed something. Based on my current understanding, however, I recommend reject.",The reviewer finds the paper's contribution not significant and the results not stronger than previous work. They question the practical implications of the attack and find the approach less powerful than a cited paper. They recommend rejection but acknowledge they might lack expertise in the specific area.,-50.0,50.0
"This interesting paper tackles the problem of joint source-channel coding, by means of learning.

From 100kft heights, especially given the choice of VIMCO gradient estimates, this is effectively a ""let's embed a source-channel-decoder simulator and differentiate through it"", and find a solution that is better than source|channel factorized classic methods, or hand-tuned approaches.

The method and results are good. The authors also show some interesting results about the representations learned, about how decoded samples (images) change smoothly when the (discrete) embedding (the-codes) changes over deltas of hamming_d()=1bit. This is very good results IHMO. One limitation of this method is the fixed-code-length.

Jumping straight to my main main issue with this paper: no code was made available, at least not at this time.

While the authors do provide an extensive appendix with hyper-parameter specs, usually in my experience when dealing with discrete / monte-carlo methods, it's usually rather hard to reproduce results. I really strongly advise the authors to provide fully reproducible code for this paper, to help further research on this topic.

Besides that I have three technical comments / request regarding this paper:

1// the choice of BSC channel - while this is the easiest most natural choice, and we should certainly have results on BSC, I am left wondering why the authors didn't try other more complex / more realistic channels? The authors only mention this as potential area of future research in the last sentence of the conclusions. 

There are several reasons for this comment: first of all, it is well known that even classic joint source-channel coding methods do shine on complex channels, such fading/erasure channels and/or in general channels with correlated error sequences. Such channels are indeed key in modern wireless communications, and are easy to simulate. Given that more-complex channels could be introduced in the channel model p(y_hat|y) -  it would not change the rest of the method - it would be particularly interesting to see what results this method achieve in these more complex environments.

2// I would like to hear more about the choice of VIMCO. Understood the authors statement to ""preserve the hard discreteness"" ~ that said methods like Gumbel-SM and several others also referenced in the paper ~ have been used  successfully to solve for propagating gradients through discrete units. This is where, in my opinion, experiments comparing VIMCO approximation results to at least one other method could allow to decide / validate the best architecture. 

This is also because, in my previous experience, this type of networks with discrete units may be hard to train. I would like to hear from the authors about how stable the training was under different hyper-parameters, and perhaps see some convergence curves for the loss function(s).

3// it's not 100% clear to me where the limitation of fixed code-length come into play from the architecture. Could the authors please point this out clearly?

Thank you!
","The review starts with positive remarks, highlighting the interesting aspects and good results of the paper. However, the reviewer identifies a major issue: the lack of provided code, which is crucial for reproducibility. While the reviewer acknowledges the detailed appendix, they express concerns about the difficulty of reproducing results without code, especially given the use of discrete and Monte Carlo methods. The reviewer also raises valid technical questions and suggests areas for improvement and further investigation, indicating a desire to see the work strengthened. Overall, the tone is constructive and professional, suggesting a positive disposition towards the paper's core ideas but with clear expectations for improvement.",60.0,80.0
"In this paper, the authors consider CNN models from the lens of kernel methods. They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm. These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs. They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization. They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.

Overall, the paper is a little confusing. A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper. It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models). It might be nice to carefully delineate the authors' work from the former, and present their contributions. 

Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?

Page 4 last paragraph: ""One advantage ... complex architectures in practice"" : True, but the tightness of the bounds *do* depend on ""f"" (specifically the RKHS norm). It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?

eqn (8): use something else to denote the function 'U'. You used 'U' before to denote the set. 

eqn (12): does \tilde{O} hide polylog factors? please clarify. 



","The review is critical of the paper, pointing out a lack of clarity in contributions and potential overlap with prior work. It doesn't express outright negativity but calls for significant revisions and clarifications. The language is direct and critical, but maintains a professional and constructive tone. It avoids harsh language or personal attacks, focusing on the scientific content and presentation.",-30.0,60.0
"The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items.

A bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better.

In their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach.

The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.","The reviewer acknowledges the authors' work and results (""the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures"") but raises several concerns and questions. They find some architectural choices questionable and point out limitations in the experimental setup and results. However, the reviewer also highlights the authors' positive response to the initial review, stating that the revisions ""go a long way towards lending the insights that were missing"" and ultimately upgrading their rating to a 7. This suggests an overall positive sentiment, despite the critical remarks. The language used is formal, academic, and devoid of personal attacks, indicating politeness.",60.0,80.0
"Quality: The overall quality of this paper is good. It adopts a simple but novel idea on SISR and shows clear improvement against existing method (e.g., SRGAN). 

Clarify: This paper is well written and easy to follow. It shows a clear motivation for adopting the implicit probabilistic model.

Originality: To the best of my knowledge, this paper is the first work to learn multi-modal probabilistic model for SISR.

Significance: While the results can be further improved (still look a bit blurred), this paper shows an interesting and important direction to learn better mappings for SISR.

Pros:
+ The writing is clear.
+ The proposed method is well motivated and easy to understand.
+ The experimental results include both objective and subjective evaluations.

Cons:
- The two-stage architecture is similar to the following generative models and SR methods. It’s suggested to discuss them as well.
[a] Denton, E. L., Chintala, S., & Fergus, R. “Deep generative image models using a￼ laplacian pyramid of adversarial networks”. NIPS, 2015.
[b] Karras, T., Aila, T., Laine, S., & Lehtinen, J. “Progressive growing of gans for improved quality, stability, and variation”. ICLR 2018.
[c] Lai, W. S., Huang, J. B., Ahuja, N., & Yang, M. H. “Deep laplacian pyramid networks for fast and accurate super-resolution.” CVPR 2017.
[d] Wang, Y., Perazzi, F., McWilliams, B., Sorkine-Hornung, A., Sorkine-Hornung, O., & Schroers, C. “A Fully Progressive Approach to Single-Image Super-Resolution.”. CVPR Workshops 2018.

- In the hierarchical sampling (section 2.4), it’s not clear how to generate the upper noise vector “conditioned on the lower noise vector”. 

- The hierarchical sampling seems to improve the efficiency of training. I wonder does it affect the results of testing?

- In the implementation details (section 2.5), I don’t understand why you need to transfer the the feature activations from GPU to CPU? I think all the computation can be done on GPU for most common toolboxes. Projecting the activations to a lower dimension with a “random Gaussian matrix” sounds harmful to the results.

- How do you generate the low-resolution images? Are you using bicubic downsampling or other approaches? This detail should be clarified.

- While the evaluation with PSNR and SSIM is a reference to show the quality, many literatures already show that PSNR and SSIM are not correlated well with human perception. It is suggested to also evaluate with some perceptual metrics, e.g., LPIPS [e].
[e] Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. “The unreasonable effectiveness of deep features as a perceptual metric.” CVPR 2018.

- In Figure 7, how do you generate different results from the same input image for SRGAN? From my understanding, SRGAN doesn’t take any noise vector as input and cannot generate multi-modal results.

- I feel that the comparison with only SRGAN is not enough. There are some GAN-based SR methods [f][g]. It’s also suggested to compare with MSE-based state-of-the-art SR algorithms [h][i].

[f] Sajjadi, M. S., Schölkopf, B., & Hirsch, M. “Enhancenet: Single image super-resolution through automated texture synthesis.“ ICCV 2017.
[g] Wang, X., Yu, K., Dong, C., & Loy, C. C. “Recovering realistic texture in image super-resolution by deep spatial feature transform.” CVPR 2018.
[h] Lim, B., Son, S., Kim, H., Nah, S., & Lee, K. M. “Enhanced deep residual networks for single image super-resolution.” CVPR Workshops 2017.
[i] Zhang, Y., Tian, Y., Kong, Y., Zhong, B., & Fu, Y. “Residual dense network for image super-resolution.” CVPR 2018.

","The review is positive overall. The reviewer finds the paper well-written, the idea novel, and the results promising. While there are many constructive suggestions for improvement, they are presented in a neutral and helpful manner.",60.0,80.0
"This  seems like a very interesting concept, creating adversarial agents for each class that essentially compete with each other.  It seems like this might be a very promising method for arguing for even more abstract classes like ""circus"" vs ""zoo"" 

I wise more had been said about why the Honest Advocate outperformed the standard Advocate on the MIMIC dataset.  

The authors state:

""Advocates can effectively compete to generate higher quality evidence, though this effect was
largely localized to a few class-pairs (e.g. shirts v.s. pullovers). ""

Does it do this on things that are essentially very similar?  

Overall, I think this is a great idea. I have been looking for some similar work and consider this work to be similar in the multi-generative aspect: ""MEGAN: Mixture of Experts of Generative Adversarial Networks for Multi-modal Image Generation"" - Park, Yoo, Bahng, Choo and Park, IJCAI 2018, but I cannot find similar work using the generative experts as collective adversaries for discrimination.

The paper is clear and well written.  Improvements for the paper would be going into more detail about why the method works.  It would have been great to have seen a data set on which the method performs poorly - that would give additional insight into its strengths and weaknesses.

","The reviewer explicitly states ""Overall, I think this is a great idea"" and ""The paper is clear and well written."" They offer constructive criticism and suggestions for improvement rather than harsh critiques. The reviewer also shows interest in the work and its potential. All of this points to a positive sentiment with polite and constructive language.",80.0,90.0
"Summary:
This paper proposes a novel optimization strategy regarding softmax cross-entropy loss, to extract the effective features of well generalization in the framework of metric learning.
The authors focus on the ""temperature"" parameter in the softmax and through analyzing the role of the temperature in terms of gradient, propose the approach of heating-up softmax in which the temperature is varied from low to high in training.
And, the effects of normalization such as by l2 and BatchNorm are discussed in the framework of heated-up softmax.
The experimental results on metric learning tasks demonstrate the effectiveness of the proposed method in comparison with the other methods.

Comments:
Pros:
+ The idea of heating up the temperature in softmax is interesting, and seems novel in the literature of metric learning.
+ The performance improvement, especially produced by batchNorm-based normalization, is shown.

Cons:
- The formulation of tempered softmax with normalization is already presented in [Wang et al., 2017].
- The reason why the heating-up approach contributes to better metric learning is not clearly provided in a well convincing way.
- It lacks an important ablation study to fairly validate the method.
- The discussion/comparison is limited to the simple softmax function.

Although the reviewer likes the idea of heating up softmax, this paper can be judged as a borderline slightly leaning toward reject, due to the above weak points, the details of which are explained as follows.

- Formulation
The softmax equipped with temperature for the normalized features and weights are shown in [Wang et al., 2017]. The only difference from that work is the way to deal with temperature; in [Wang et al., 2017], the temperature is ""optimized"" as a trainable parameter, while it is dealt with in a hand-crafted way of heating up in this work. Honestly speaking, it is unclear which approach is better, though the optimization in [Wang et al., 2017] seems elegant as stated in that paper. The only way to validate this work compared to [Wang et al., 2017] is to empirically evaluate those two methods in the experiments. Such a comparison experiment is not found and it is a main flaw of this paper.

- Justification of the method
The gradients of the softmax cross-entropy loss parameterized with a temperature T are well analyzed in Sections 3.1&3.2. But, in Section 3.3, the reviewer cannot find the clear and convincing explanation for why the temperature T should be increased in the training. My question is: why don't you use alpha=4 consistently throughout the training?
 It might be related to the process of simulated annealing (though ""temperature"" is usually cooled down in SA), and more interestingly, it would also be possible to find connection with the work of [Guo et al., 2017]. In [Guo et al., 2017], the temperature in the softmax is optimized as a post processing for calibrating the classifier outputs. Though the calibration task itself is a little bit apart from the metric learning of the authors' interest, we can find in that paper an interesting result that the temperature is heated up to increase the confidence of the classifier outputs, which is quite similar to the process of fine-tuning by heating up softmax as done in this work. Therefore, the reviewer guesses that the effectiveness of heating up softmax can also be interpreted from the viewpoint of [Guo et al., 2017].

There is also less description about Figure 1; in particular, the reviewer cannot understand what Figure 1(d) means.

- Ablation study
To empirically resolve the above concerns, it is necessary to present the empirical comparison with the ""static"" softmax.
Namely, the methods of HLN/HBN should be carefully compared to LN/BN of ""alpha=4"", not only those of alpha=16 shown in Table 1&2; the comparison in Table 3 seems unfair since the authors apply the static softmax without normalization.
And, it would be better to show the performance of heated-up softmax ""without"" normalization to show the important role of the normalization, as done in [Wang et al., 2017].
In summary, since the proposed method is composed of a heating-up approach and feature normalization, the authors are required to validate the method from those two aspects, respectively, for increasing the significance of this paper.

- Other loss function
For achieving a compactness in feature representation, the simple softmax requires both temperature and normalization. It, however, is also conceivable to employ the other types of loss function for that purpose, such as [a] which is based on the (Mahalanobis) distance with taking into account the margin between categories. The distance based loss also embeds features into localized clusters, which satisfies the authors' objective in this work. To validate the proposed method, it is required to compare the method with such a different types of loss function.

[a] Wan, W., Zhong, Y., Li, T., & Chen, J. (2018). Rethinking Feature Distribution for Loss Functions in Image Classification, In CVPR2018, pp. 9117–9126.","The review starts with positive remarks, highlighting novelty and performance improvement. However, it quickly delves into significant concerns regarding novelty, justification, and lack of comprehensive analysis. The reviewer suggests the paper is ""borderline slightly leaning toward reject."" The language, while critical, maintains a professional and analytical tone, avoiding personal attacks or disrespectful phrasing.",-20.0,60.0
"
Summary:
This paper proposes MetaMimic, an algorithm that does the following:
(i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task.
(ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task.

Overall Evaluation:
This is a good paper. In my opinion however, it does not pass the bar for ICLR.

Pros:
- The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive.
- The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful.
- Interesting pipeline of learning policies that can use demonstrations without actions.
- The results on the simulated robot arm (block stacking task with two blocks) are good.

Cons:
- The abstracts oversells the contribution a bit when saying that MetaMimic can learn ""policies for high-fidelity one-shot imitation of diverse novel skills"". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better.
- Experimental results are shown only for one task; block stacking with a robot arm in simulation.
- Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks.

Questions:
- Where does the ""task stochasticity"" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic?
- The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time?
- It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.)
- The x-axis in the figures says ""time (hours)"" - is that computation time or simulated time?

Other Comments:
- In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? 
- An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum.

Nitpick (no influence on score):
[1. Introduction]
- I find the first sentence, ""One-shot imitation is a powerful way to show agents how to solve a task"" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like ""Expert demonstrations are a powerful way to show agents how to solve a task."" works better?
- Second sentence, the chosen example is ""manufacturing"" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations.
- Add note that with ""unconditional policy"" you mean not conditioned on a demonstration.
[2. MetaMimic]
- [2.1] Third paragraph: write ""Figure 2, Algorithm 1"" or split the algorithm and figure up so you can refer to them separately.
- [2.1] Last paragraph, second line: remove second ""to""","The reviewer provides both positive and negative feedback, but leans towards the negative side, ultimately suggesting the paper is not a good fit for ICLR. While they acknowledge the paper's merits (well-written, clear contributions, good results in the specific task), they point out limitations in the scope of the work and its applicability to a broader range of problems. The language used is constructive and professional throughout the review.",-20.0,80.0
"Overall this paper contributes many interesting insights into the specific application of empathetic dialog into chatbot responses.  The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot.

While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders.

There is a lot in this paper, and I think it could have been better organized.
I am more familiar with emotion related research and not language to language translation, so I would have appreciated a better explanation of the rationale for using BLEU scores.  I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity, it is unclear how they capture ”relevance” at least according to the brief tutorial that I read (https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).  I did not see the paper describing the use of this score in the references but perhaps I missed it – could you please clarify why this is a good metric for relevance?  It seems that these scores are very sensitive to sentence variation.  I am not sure if you can measure empathy or appropriateness of a response using this metric.
For your data collection you have 810 participants and 24,850 conversations.  Are the 810 participants all speakers or speakers and listeners combined?  How many conversations did each speaker/listener pair perform 32?  (one for each emotion) or 64? (two for each emotion) Was the number variable?  If so what is the distribution of the contribution – e.g. did one worker generate 10,000 while several hundred workers did only three of four?  Was it about even?  Just for clarity – how did you enroll participants?  Was it through AMT?  What were the criteria for the workers?  E.g. Native English speaker, etc.

In your supplemental material, I found the interchanging of the words “context” and “emotion” confusing.  The word context is used frequently throughout your manuscript: “dialog context,” “situational context” - emotions are different from situations, the situational utterance is the first utterance describing the emotion if I read your manuscript correctly.  Table 6 should use “Label” or “Emotion” instead of the more ambiguous “Context.”  

My understanding is that speakers were asked to write about a time when they experienced a particular feeling and they were given a choice of three feelings that they could write about.  You then say that workers are forced to select from contexts they had not chosen before to ensure that all of the categories were used.  From this I am assuming that each speaker/listener worker pair had to write about all 32 emotions – is this correct?  Another interpretation of this is that you asked new workers to describe situations involving feelings that had not been chosen by other workers as data collection progressed to ensure that you had a balanced data set.  This would imply that some emotional situations were less preferred and potentially more difficult to write about.  It would be interesting if this data was presented.  It might imply that some emotion labels are not as strong if people were forced to write about them rather than being able to choose to write about them.  
Were these dialogs ever actually annotated?  You state in section 2, Related Work “we train models for emotion detection on conversation data that has been explicitly labeled by annotators” – please describe how this was done.  Did independent third party annotators review the dialogs for label correctness?  Was a single rater or a majority vote used to decide the final label.  For example, in Table 1, the label “Afraid” is given to a conversation that could also have reasonable been generated by the label “Anxious” a word explicitly used in the dialog.  I am guessing that the dialogs are just labeled according to the label / provocation word and that they were not annotated beyond that, but please make this clear.  
In the last paragraph you state “A few works focus..” and then list 5.  This should rather be “several other works have focused on “ …  
Conversely, you later state in section 3 “Speaker and Listener”, “We include a few example conversations from the training data in Table 1,” this should more explicitly be “two.”
Also in section 3 when you describe your cross validation process, you state “We split the conversations into approximately 80/10/10 partitions.  To prevent overlap of <<discussed topics>> we split the data so that all the sets of conversations with the same speaker providing the prompt would be in the same partition.  
In your supplemental material you state that workers were paired.  Each worker is asked to write a prompt, which also seems to be the first utterance in the dialog they will start.  You state each worker selects one emotion word from a list of three which is somehow generated (randomly?) form your list of 32 .  I am assuming each worker in the pair does this, then the pair has a two “conversations” one where the first worker is the speaker and another where the second worker is the speaker – is this correct?  It is not entirely clear from the description. Given that you have 810 workers and 24,850 conversations, I am assuming that each worker had more than one conversation.  My question is  - did they generate a new prompt / first utterance for each conversations.  I am assuming yes since you say there are 24,850 prompts/conversations.  For each user are all of the situation/prompts they generate  describing the same emotion context?  E.g. would one worker write ~30 conversations on the same emotion.  This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. “fear” several times.  If the same worker was allowed to select the same emotion context multiple times was it found that they re-used the same prompt several times?  I am assuming that this is the case and that this is what you mean when you say that you “prevent overlap of discussed topics” between sets when you exclude particular workers.  Is this correct?  Or did you actually look and code the discussed topics to ensure no overlap even across workers (e.g. several people might have expressed fear of heights or fear of the dark).

In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt.  Calling these both prompts makes the statement about 24,850 prompts/conversations a bit ambiguous.  A better statement would be 24,850 conversations based on unique situational prompts/descriptions (if they are in fact unique situational prompts.  I am assuming they are not if you are worried about overlapping “discussed topics” which I am assuming are the situational prompts since the dialogs are very short and heavily keyed off these initial situational prompts)

In your evaluation of the models with Human ratings you describe two sets of tests.  In one test you say you collect 100 annotations per model.  More explicitly, did you select 100 situational prompts and then ask workers to rate the response of each model?  Was how many responses was each worker shown?  How many workers were used?  Are the highlighted numbers the only significant findings or just the max scores?  Annotations is probably not the correct word here.

Please also describe your process for assigning workers to the second human ratings task.     

Since the two novel aspects of your paper are the new dataset and the use of this dataset to create more empathetic chatbot responses (""I know the feeling"") I have focused on these aspects of the paper in my review.

I found the inclusion of Table 7 underexplained in the text.  The emotion labels for all these datasets are not directly comparable so I would have liked to have seen more explanation around how these classifications were compared.  It would also be helpful to know how more similar emotions such as ""afraid"" and ""anxious"" were scored vs ""happy"" and ""sad"" confusions 
","The review is largely positive, highlighting the paper's valuable contributions (new dataset, insights into empathetic chatbot responses). However, it also presents numerous constructive criticisms and requests for clarification, indicating areas for improvement. The reviewer's tone is professional and inquisitive, seeking better understanding and suggesting enhancements rather than expressing negativity.",60.0,80.0
"This is a paper about sentence embedding based on orthogonal decomposition of the spanned space by word embeddings. Via Gram-Schmidt process, the sequence of words in a sentence is regarded as a sequence of incoming vectors to be orthogonalized. Each word is then assigned 3 scores: novelty score, significance score, and uniqueness score. Eventually, the sentence embedding is achieved as weighted average of word embeddings based on those scores. The authors conduct extensive experiments to demonstrate the performance of the proposed embedding. I think the idea of the paper is novel and inspiring. But there are several issues and possible areas to improve:

1. What if the length of the sentence is larger than the dimension of the word embedding? Some of the 3 scores will not be well-defined.

2. Gram-Schmidt process is sensitive to the order of the incoming vectors. A well-defined sentence embedding algorithm should not. I suggest the authors to evaluate whether this is an issue. For example, if by simply removing a non-important stop word at the begging of the sentence and then the sentence embedding changes drastically, then it indicates that the embedding is problematic.

3. I’m confused by the classification between training-free sentence embedding and unsupervised sentence embedding? Don’t both of them require training word2vec-type embedding?

4. The definition of the three scores seems reasonable, but requires further evidence to justify. For example, by the definition of the scores, do we have any proof that the value of \alpha indeed demonstrated the related importance level?","The reviewer provides both positive feedback (""novel and inspiring"") and constructive criticism with specific areas for improvement. This suggests an overall positive sentiment, but not overly enthusiastic. The language used is formal, respectful, and focused on the scientific content, indicating politeness.",60.0,80.0
"The main idea of this paper is that a 'realistic' way to attack GCNs is by adding fake nodes. The authors go on to show that this is not just a realistic way of doing it but it can done in a straightforward way (both attacks to minimize classification accuracy and GAN-like attacks to make fake nodes look just like real ones). 

The idea is neat and the experiments suggests that it works, but what comes later in the paper is mostly rather straightforward so I doubt whether it is sufficient for ICLR. I write ""mostly"" because one crucial part is not straightforward but is on the contrary, incomprehensible to me.  In Eq (3) (and all later equations) , shouldn't X' rather than X be inside the formula on the right? Otherwise it seems that the right hand side doesn't even depend on X' (or X_{fake} ). 
But if I plug in X', then the dimensions for weight matrices  W^0 and W^1 (which actually are never properly introduced in the paper!) don't match any more. So what happens? To calculate J you really need some extra components in W0 and W1. Admittedly I am not an expert here, but I figure that with a bit more explanation I should have been able to understand this. Now it remains quite unclear...and I can't accept the paper like this.

Relatedly, it is then also unclear what exactly happens in the experiments: do you *retrain* the network/weights or do you re-use the weights you already had learned for the 'clean' graph? 

All in all: 
PRO:
- basic idea is neat 
CON:
- development is partially straightforward, partially incomprehensible.

(I might increase my score if you can explain how eq (3) and later really work, but the point that things remain rather straightforward remains). ",,,
"The authors propose a method for image restoration, where the restored image is the MAP estimate. A pretrained GAN is utilized to approximate the prior distribution of the noise-free images. Then, the likelihood induces a constraint which is based on the degradation function. In particular, the method tries to find the latent point for which the GAN generates the image, which if gets degraded will match the given degraded image. Also, an optimization algorithm is presented that solves the proposed constrained optimization problem.

I find the paper very well written and easy to follow. Also, the idea is pretty clean, and the derivations are simple and clear. Additionally, the Figures 2,3 are very intuitive and nicely explain the theory. However, I think that there are some weaknesses (see comments):

Comments: 

#1) I do not understand exactly what the ""general method"" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?

#2) Clearly, the efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.

#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details. For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.

#4) I think that the function F has to be differentiable, and this should be mentioned in the text. Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments. The same holds for the p(Omega). This parameter Omega is estimated individually for each degraded image?

#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.

#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown. Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.

#7) The optimization steps seem to be intuitive, however, there is not any actual proof of converge. Of course, the example in the Figure 3 is very nice and intuitive, but it is also rather simple. I would suggest, at least, to include some empirical evidences in the experiments that show convergence.

#8) In the experiments I think that at least one example of F and p(Omega) should be presented. Also, what the numbers in Table 4 show? Which is the best value that can be achieved? These numbers correspond to several images, or to a unique image? 
#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN. I believe that a more challenging experiment should be conducted e.g. using celebA dataset.

Minor comments:

#1) In the paragraph after Eq. 4 the equality p_r(x)=p_G(x) is very strong assumption. I would suggest to use the \simeq symbol instead.

#2) After Eq. 6 the ""nonnegative"" should be ""nonzero"".

#3) Additional density estimation models can be used e.g. VAEs, GMM. Especially, I believe that the VAE will provide a way to approximate the prior easier than the GAN.

#4) In Section 2 paragraph 2, the sentence ""However, they only ... and directly"" is not clear what means.

In general, I find both the proposed model and optimization algorithm interesting. Additionally, the idea is nicely presented in the paper. Most of my comments are improvements which can be easily included. The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments. The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems. Also, I think that additional methods to compute the image prior should be included in the experiments.","The reviewer provides generally positive feedback, praising the paper's clarity, the idea's novelty, and the intuitiveness of the figures. They find the model and optimization algorithm interesting. However, they also raise valid concerns and suggest improvements, particularly regarding the convergence proof, more challenging experiments beyond MNIST, and exploring alternative methods for image prior computation. The tone is constructive and polite, suggesting areas for improvement without being dismissive.",65.0,80.0
"The paper investigates the use of multi-objective optimization techniques in GAN-setups where there are multiple discriminators. Using multiple discriminators was proposed in Durugkar et al, Arora et al, Neyshabur et al and others. The twist here is to focus on the Pareto front and to import multiple gradient descent and hypervolume-maximization based methods into GANs. 

The results are decent. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. However, just scaling up (and carefully optimizing), can yield extremely impressive samples, https://arxiv.org/abs/1809.11096. It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. 

From my perspective, the paper is interesting because it introduces new methods into GANs from another community. However, the results themselves are not sufficient for publication. 
","The reviewer acknowledges the paper's merit in introducing new methods from another community (""interesting"") but expresses reservations about the significance of the results. They find the results ""decent"" but not groundbreaking, especially compared to scaling up GANs. The statement ""the results themselves are not sufficient for publication"" clearly indicates a negative sentiment overall.",-25.0,50.0
"In this paper the authors propose an extension to successor features (SF). Akin to UVFAs, they condition on some goal state by concatenating to the current state after some shared preprocessing. The authors claim three contributions: 1) introducing the USF, 2) proposing an appropriate deep learning architecture for it, and 3) showing experimentally that USFs improve transfer both within a goal set and to novel goals.

Claims 1) and 2) don't seem particularly noteworthy. Extending SF to be goal-conditioned is very straightforward, doesn't leverage anything unique to the SF formalism (e.g. the reward weights w already encode a goal in some sense), and doesn't attempt to extend its theoretical grounding. The architecture is likewise unsurprising, and the lack of ablations or alternatives make it seem rather unmotivated.

The usage of a Q-learning loss instead of a reward-prediction loss for updating phi is mentioned without citation. This seems quite novel, and could be a significant contribution if its advantage was demonstrated experimentally.

The experiments appear to show a significant advantage for USFs. For the training-goal-set advantage, it would be useful to know the architecture of multi-goal DQN. One hypothesis is that the extra weight-sharing is what is giving USFs an edge, and this should be ruled out. It is briefly mentioned that UVFAs weren't considered due to their stated instability, but its unclear how they differ from the multi-goal DQN.

The novel-goal results are impressive at first glance, but there is a glaring omission. Hindsight experience replay (HER) is mentioned but not evaluated, and would very likely trivialise the train/test goal-set distinction (unless the test goals were never previously visited). As these results are the primary contribution of this paper, this must be addressed prior to publication acceptance.

Edit: The addition of HER experiments push this up a bit (5-->6). I'm still concerned about how significant the contribution is (as it is a straightforward extension to SFs), but the empirical results are now quite strong.","The review is critical of the novelty and significance of the paper's contributions, questioning the originality of the proposed method and pointing out a significant experimental flaw. However, the reviewer acknowledges the potential of the approach and the impressiveness of the results, particularly after the addition of HER experiments. The language, while direct and critical, maintains a professional and constructive tone. Therefore, the sentiment leans slightly towards the negative side due to the concerns raised, but the politeness remains relatively neutral.",-20.0,40.0
"UPDATE:
Thanks for your response. As you mentioned, methods like [1] and [2] do perform open-ended recombination. Note that these methods perform not only texture transfer but also color transfer, while the proposed method seems to perform mostly only color transfer. As shown in Figure 6, essentially what the method does is transfer the color of the style image to the content image, sometimes with a little tweak, making the image distorted. One could say that in terms of image style transfer, the proposed method actually underperforms [1] and [2]. 

Hence I agree with R2 that comparison is still necessary for the submission to be more convincing and complete.

------------------------------

This paper proposed to use a mechanism of leakage filtering to separate styles and content in the VAE encoding, and consequently enable open-ended content-style recombination. Essentially the model tries to maximize the similarity between images in S^+ and minimize the similarity between those in S^-.

I have several questions:

One concern that I have is the relationship/difference between this work and previous work on style transfer, especially universal/zero-shot style transfer as in [1,2]. In the introduction and related work sections, the authors argue that most previous work assumes that content classes in testing are the same as those in training, and that they are not general purpose. Note that various works on style transfer already address this issue, for example in [1, 2]. For those models, content is represented by high-level feature maps in neural networks, and style is represented by the Gram matrix of the feature maps. The trained model is actually universal (invariant to content and styles). Actually these methods use even less supervision than STOC since they do not require labels (e.g., digit labels in MNIST).

This brings me to my second concern on proper baselines. Given the fact that previous universal/zero-shot style transfer models focus on similar tasks, it seems necessary to compare STOC to them and see what the advantages of STOC is. Similar experiments can be conducted for the data augmentation tasks.

In Sec. 4, the authors mentioned that U-Net skip connection is used. Does it affect the effectiveness of the content/style separation, since the LF objective function is mostly based on the encoding z, which is supposed ‘skipped’ in STOC. Will this lead to additional information leakage?

It is not clear how the last term of L_{LF} is computed. Could you provide more details?

The organization and layout of figures could be improved. The title/number for the first section is missing.

Missing references:

[1] Universal style transfer via feature transforms, 2017
[2] ZM-Net: Real-time zero-shot image manipulation network, 2017
[3] Structured GAN, 2017","The review is critical of the paper, pointing out flaws and suggesting comparisons with existing methods are missing. The language, while direct, is professional and typical of academic peer reviews. There's no negativity, just constructive feedback.",-20.0,60.0
"This paper discusses conditions under which  the convergence of training models with low-precision weights do not rely on model dimension. Extensions to two kinds of non-linear quantization methods are also provided. The dimension-free bound of the this paper is achieved through a tighter bound on the variance of the quantized gradients.  Experiments are performed on synthetic sparse data and small-scale image classification dataset MNIST.

The paper is generally well-written and structure clearly. However, the bound for linear quantization is not fundamentally superior than previous bounds as the ""dimension-free"" bound in this paper is achieved by replacing the bound in other papers using l2 norm with l1 norm. Note that l1 norm is related to the l2 norm as: \|v\|_1 <= \sqrt{d}\|v\|_2, the bound can still be dependent on  dimension, thus the title may be misleading. Moreover, the assumptions  1 and 2 are much stronger than previous works, making the universality of the theory limited. The analysis on non-linear quantization is interesting, which can really theoretically improve the bound. It would be nice to see some more empirical results on substantial networks and  larger datasets which can better illustrate the efficacy of the proposed non-linear quantization.

Some minor issues:
1. What is HALP in the second contribution before Section 2?
2. What is LP-SVRG in Theorem 1?
3. What is \tilde{w} in Theorem 2?","The review starts with positive remarks, highlighting the paper's clarity and structure. However, it then raises significant concerns about the main claim and assumptions, suggesting the title might be misleading. While acknowledging the interesting aspects of non-linear quantization analysis, the reviewer calls for more empirical evidence. The tone remains professional and constructive throughout, focusing on specific improvements. Therefore, the sentiment leans slightly negative due to the critical points raised, but the politeness remains positive due to the constructive and respectful tone.",-10.0,80.0
"In the paper, the authors try to propose an adaptive learning rate method called predictive local smoothness.  They also do some experiments to show the performance. 

The following are my concerns:

1. The definition of the L(x_t) is confusing. In (8), the authors define L(x_t), and in (10), the authors give another definition.  Does the L(x_t) in (10) always guarantee that (8) is satisfied? 

2. In theorem 1, \mu^2 = \frac{1}{n} \sum_{i=1}^n L_i^2(x_t) + \frac{2}{n^2}  \sum_{i<j}^n L_i(x_t) L_j(x_t) > v. It looks like that \mu > (1-\rho^2) v, no matter the selection of \rho.  Why?

3. How do you compute L_i(x_t)  if x is a multi-layer neural network?

4. The experimental results are too weak. In 2018, you should at least test your algorithm using a deep neural network, e.g. resnet. The results on a two-layer neural network mean nothing. 

5. sometimes, you algorithm even diverge. for example, figure 3 second column third row.  


","The review starts with a neutral statement but lists several significant concerns about the paper, including confusing definitions, potential flaws in the theorem, lack of clarity in application, weak experimental setup, and even algorithm divergence. The language used is quite direct and critical, particularly terms like ""too weak"", ""mean nothing"", and pointing out divergence as ""sometimes"" happening, which comes across as slightly dismissive of the authors' work.",-40.0,-10.0
"This paper proposes runs variational inference with discrete mean-field distributions. The paper claims the proposed method is able to give a better estimation of uncertainty from the model. 

Rating of the paper in different aspects ( out of 10)
Quality 6, clarify 5, originality 8, significance of this work 5 

Pros: 

1. The paper proposes a generic discrete distribution as the variational distribution to run inference for a wide range of models. 

Cons:

1. When the method begins to use mean-field distributions, it begins to lose fidelity in approximating the posterior distributions. Even the model is able to do a good job in approximating marginal distributions, it is hard to evaluate whether the model is gaining benefit overall. 

2. I don't see a strong reason for using discrete distributions. In one dimensional space, a distribution can be approximated in different ways. Using discrete distributions only increases the difficulty of reparameterization. 

3. In the experiment evaluation, the algorithm seems only marginally outperforms competing methods. 


Detailed comments: 

In the motivation of the paper, it cites low-precision neural networks. However, low-precision networks are for a different purpose -- small model size and saving energy. 

equation 6 is not clear to me.

In equation 10, how are these conditional probabilities parameterized? Is it like: z ~ Bernoulli( sigmoid(wz) ) ?

It is nice to have a brief introduction of the evaluation measure SGR. 

In table 3, 1st column, the third value seems to be the largest, but the fourth is bolded. 
","The reviewer raises several valid concerns about the paper, questioning the necessity and effectiveness of the proposed method. They point out marginal performance improvements and a lack of strong justification for using discrete distributions. While the reviewer provides constructive criticism and detailed comments, the overall tone suggests skepticism towards the paper's contributions. Therefore, the sentiment leans towards the negative side. The language used is polite and professional, focusing on the scientific aspects and providing specific suggestions for improvement.",-20.0,80.0
"# Paper summary
This paper advances a method for accelerating semantic segmentation on video content at higher resolutions. Semantic segmentation is typically performed over single images, while there is un-used redundancy between neighbouring frames. The authors propose exploiting this redundancy and leverage block motion vectors from MPEG H.264 video codec which encodes residual content between keyframes. The block motion vectors from H264 are here used to propagate feature maps from keyframes to neighbouring non-keyframe frames (in both temporal directions) avoiding thus an additional full forward pass through the network and integrate this in the training pipeline. Experimental results on CamVid and Cityscapes show that the proposed method gets competitive results while saving computational time.


# Paper strengths
- This paper addresses a problem of interest for both academic and industrial purposes.
- The paper is clearly written and the authors argument well their contributions, adding relevant plots and qualitative results where necessary.
- The two-way interpolation with block motion vectors and the fusion of interpolated features are novel and seem effective.
- The experimental results, in particular for the two-way BMV interpolation, are encouraging.


# Paper weaknesses

- The idea of using Block Motion Vectors from compressed videos (x264, xvid) to capture motion with low-cost has been previously proposed and studied by Kantorov and Laptev [i] in the context of human action recognition. Flow vectors are obtained with bilinear interpolation from motion blocks between neighbouring frames. Vectors are then encoded in Fisher vectors and not used with CNNs as done in this paper. In both works, block motion vectors are used as low-cost alternatives to dense optical flow. I would suggest to cite this work and discuss similarities and differences.


- Regarding the evaluation of the method, some recent methods dealing with video semantic segmentation, also using ResNet101 as backbone, are missing, e.g. low latency video semantic segmentation[ii]. Pioneer Clockwork convnets are also a worthy baseline in particular in terms of computational time (results and running times on CityScapes are shown in [ii]). It would be useful to include and compare against them.

- In Section 4.1.2 page 7 the authors mention a few recent single-frame models ((Yu et al. (2017); Chen et al. (2017); Lin et al. (2017); Bilinski & Prisacariu (2018)) as SOTA methods and the current method is competitive with them. However I do not see the results from the mentioned papers in the referenced Figures. Is this intended?

- On a more general note related to this family of approaches, I feel that their evaluation is usually not fully eloquent. Authors compare against similar pipelines for static processing and show gains in terms of computation time. The backbone architecture, ResNet-101 is already costly for high-resolution inputs to begin with and avoiding a full-forward pass brings quite some gains (though a part of this gain is subsequently attenuated by the latency caused by the batch processing of the videos). There are recent works in semantic segmentation that focus on architectures with less FLOPs or memory requirements than ResNet101, e.g. Dilated ResNets [iii], LinkNet[iv]. So it could be expected that image-based pipelines to be getting similar or better performance in less time. I expect the computational gain on such architectures when using the proposed video processing method to be lower than for ResNet101, and it would make the decision of switching to video processing or staying with frame-based predictions more complex. 
The advantage of static image processing is simpler processing pipelines at test time without extra parameters to tune. It would be interesting and useful to compare with such approaches on more even grounds.


# Conclusion 
This paper takes on an interesting problem and achieves interesting results. The use of Block Motion Vectors has been proposed before in [i] and the main novelty of the paper remains only the interpolation of feature maps using BMVC. The experimental section is missing some recent related methods to benchmark against.
This work has several strong and weak points. I'm currently on the fence regarding my decision. For now I'm rating this work between Weak Reject and Borderline  

# References

[i] V. Kantorov and I. Laptev, Efficient feature extraction, aggregation and classification for action recognition, CVPR 2014
[ii] Y. Li et al., Low-Latency Video Semantic Segmentation, CVPR 2018
[iii] F. Yu et al., Dilated Residual Networks, CVPR 2017
[iv] A. Chaurasia and E. Culurciello, LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation, arXiv 2017
","The review acknowledges the paper's strengths, such as addressing a relevant problem, clear writing, and promising results. However, it also points out significant weaknesses, including lack of novelty, missing relevant baselines in the evaluation, and questioning the overall impact of the approach. The reviewer's conclusion of being ""on the fence"" and rating the work as ""between Weak Reject and Borderline"" suggests a somewhat negative leaning sentiment. The language used is professional and polite, providing constructive criticism and suggestions for improvement.",-20.0,80.0
"This paper presents a method for generating 3D objects. They train a VAE to generate voxel occupancy grids. Then, they allow a user to generate novel shapes using the learned model by combining latent codes from existing examples. 

Pros:
- The idea of linking affordances to 3D object generation is interesting, and relevant to the machine learning and computer vision communities.

- They propose to evaluate the quality of the shape based on a physical simulation (Section 4.4.3), which is an interesting idea.

Cons:
- This paper is not well written. The method is described in too much detail, and the extra length (10 pages) is unnecessary. Cross entropy, VAEs, and many of the CNN details can usually just be cited, instead of being described to the reader.

- The paper uses suggestive terminology, like ""functional essence"" and ""functional arithmetic"" for concepts that are fairly mundane (see Lipton and Steinhardt, 2018 for an extended discussion of this issue). For example, the ""functional essence"" of a class is essentially an average of the VAE latent vectors (Section 3.3.1). The paper claims, without sufficient explanation, that this is computation is motivated by the idea that ""form follows function"".

- The results are not very impressive. There is no rigorous evaluation. They propose several nice metrics to use (eg. affordance simulation), but the results they present for each metric are quite limited. The qualitative results are also not particularly compelling.

- The paper should more thoroughly evaluate the importance weighting that is described in Section 3.3.2.

 - The technical approach (combining VAE vectors to make new shapes) is not particularly novel[

Overall:

The paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation.
","The review is quite negative overall. The reviewer points out several flaws in the paper, including confusing writing, lack of rigorous evaluation, and an unimpressive novelty. While the reviewer acknowledges some interesting ideas, the cons outweigh the pros, leading to a recommendation for rejection. The language used is critical but professional and avoids personal attacks, staying within the bounds of academic discourse.",-50.0,50.0
"Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset.

The paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model?

Additionally, think about a pseudo-code for improved understandability. 

Some minor concerns/ notes to the authors:
1.	At page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters?
2.	Page 6; baseline model: Why do you removed the pooling layers?
3.	I’m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I’m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar.
4.	Why is the magnitude of lambda1 so different between the two dataset that you used?
5.	Could you add the inference times to your tables and discuss that in addition?
6.	What kind of noise is added to MNIST?
7.	What is the state-of-the-art performance on the Hippocampus dataset?
8.	What would be the performance in your experiments with a MaskRCNN segmentation network?
9.	I’m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. 
10.	For both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix…
	
My major concern is that both datasets are not dealing with real background noise. I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete.
To make your results transparent you could think about to publish the code somewhere.
","The reviewer provides constructive criticism, acknowledges the paper's strengths (well-written, well-explained), and offers specific suggestions for improvement. While they express concerns about the datasets and generalizability, the tone remains professional and focused on enhancing the paper. The numerous questions and requests for clarification indicate a desire to understand the work thoroughly, rather than simply dismissing it.",50.0,75.0
"Partially observable Markov decision processes (POMDPs) are a widely-used framework to model decision-making with uncertainty about the environment and under stochastic outcome. In conventional POMDP models, the observations that the agent receives originate from fixed known distribution. However, in a variety of real-world scenarios the agent has an active role in its perception by selecting which observations to receive. Due to the combinatorial nature of such a selection process, it is computationally intractable to integrate the perception decision with the planning decision. 

The author proposes a new form of POMDPs called AP2-POMDP, which takes active perception into account. The AP2-POMDP problem restricts the maximum number of sensors that can be selected by an agent. The agent also faces the planning problem to select the sensors. To prevent such expansion of the action space, the authors propose a greedy strategy for observation selection and obtain a near optimal bound based on submodular optimization.

The author also proposes a greedy-based scheme for the agent to find an almost optimal active perception action by minimizing the uncertainty of beliefs. The author also uses theories to prove the near-optimal guarantees of this greedy method. The author also proposes a novel perception-aware point-based value iteration to calculate the value function and obtain the policy. The author also operates an interesting simulation experiment, which shows less uncertainty of the robot when taking planning actions when using the proposed solver.

The contribution is significant in reinforcement community. The writing is in general clear. It can be improved with minor modifications, for example, explaining math equations better in English. 

My main comment for the authors is whether they have considered the scenario where the perception and the planning actions are connected. I agree with the authors that the best strategy for perception is to reduce uncertainty (and indeed, the greedy approach yields a near-optimal performance), given the restricted situation that the perception and planning are two separated processes. Nonetheless, in most real-world applications, the two processes are coupled, and therefore, we face, immediately, the trade-off between exploration and exploitation. I wonder if the authors have considered how they can extend their approach to such scenarios. 

A few minor comments:

	
(i) The authors should add a legend and perhaps, more explanation in the captions of Figure 5. The colors of the heat-map are confusing. If dark blue and dark red represent lowest and highest frequency, what about other colors? Are there obstacles placed in the grid? If so, are they placed as shown in Figure 3(b)?
	
(ii) What is the effect of k, the maximum number of sensors to be placed? Can the authors provide a figure showing the change of performance with varying k?
	
(iii) It will be more convincing if the author deploys this algorithm to real-world robots and demonstrate its effectiveness. 	

","The review starts with a positive note, acknowledging the significance of the contribution and clarity of writing. While it raises a valid concern and suggests improvements, the overall tone remains constructive and encouraging. The reviewer acknowledges the value of the work ('The contribution is significant') and uses polite phrasing like 'It can be improved with minor modifications' and 'It will be more convincing if...' rather than direct criticism.",60.0,75.0
"This paper presents a new adversarial defense based on ""cleaning"" images using a round trip through a bidirectional gan.  Specifically, an image is cleaned by mapping it to latent space and back to image space using a bidirectional gan.  To encourage the bidirectional gan to focus on the semantic properties, and ignore the noise, the gan is trained to maximize the mutual information between z and x, similar to the info gan.

Pros:
	1. The paper presents a novel (as far as I am aware) way to defend against adversarial attacks by cleaning images using a round trip in a bidirectional gan

Cons:
	1. The method performs significantly worse than existing techniques, specifically adversarial training.
		a. The authors argue ""Although better than FBGAN, adversarial training has its limitation: if the attack method is harder than the one used in training(PGD is harder than FGSM), or the perturbation is larger, then the defense may totally fail. FBGAN is effective and consistent for any given classifier, regardless of the attack method or perturbation.""
		b. I do not buy their argument, however, because one can simply apply the strongest defense (PGD 0.3 in their results) and this outperforms their method in *all* attack scenarios.  And if someone comes out with a new stronger attack there's no guarantee their method will be strong defense against that method
	2. The paper is not written that well.  Even though the technique itself is very simple, I was unable to understand it from the introduction, and didn't really understand what they were doing until I reached the 4th page of the paper. 
	

Missing citation:
PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples  (ICLR 2018)
","The reviewer acknowledges the novelty of the proposed method, which is a positive point. However, they point out a significant flaw: the method's performance is considerably worse than existing techniques. The reviewer also criticizes the clarity of the paper, stating they struggled to understand the method initially. The tone, while critical, maintains a professional and respectful tone, using phrases like ""I do not buy their argument"" rather than overtly aggressive language. ",-20.0,60.0
"This is a well-written paper studying the important problem of dynamic network embedding. Please find below some pros and cons of this paper.
Pros:

* Studies the important problem of network embedding under a more realistic setting (i.e., nodes & edges evolve over time).
* Introduces an interesting architecture that uses two forms of attention: structural and temporal.
* Demonstrated the effectiveness of the temporal layers through additional experiments (in appendix) and also introduced a variant of their proposed approach which can be trained incrementally using only the last snapshot.

Cons:

* The authors compared against several dynamic & static graph embedding approaches. If we disregard the proposed approach (DySAT), the static methods seem to match and even, in some cases, beat the dynamic approaches on the compared temporal graph datasets. The authors should compare against stronger baselines for static node embedding, particularly GAT which introduced the structural attention that DySAT uses to show that the modeling of temporal dependencies is necessary/useful. Please see [1] for an easy way to train GCN/GAT for link prediction.
* There are actually quite a number of work done on network embedding on dynamic graphs including [2-4]. In particular, [2-3] support node attributes as well as the addition/deletion of nodes & edges. The author should also compare against these work.
* The concept of temporal attention is quite interesting. However, the authors do not provide more analysis on this. For one, I am interested to see how the temporal attention weights are distributed. Are they focused on the more recent snapshots? If so, can we simply retain the more relevant recent information and train a static network embedding approach? Or are the attention weights distributed differently?

[1] Modeling Polypharmacy Side Effects with Graph Convolutional Networks. Zitnik et. al. BioInformatics 2018. 
[2] Attributed Network Embedding for Learning in a Dynamic Environment. Li et. al. In Proc. CIKM '17. 
[3] Streaming Link Prediction on Dynamic Attributed Networks. Li et. al. In Proc. WSDM '18. 
[4] Continuous-Time Dynamic Network Embeddings. Nguyen et. al. In Comp. Proc. WWW '18. ","The review starts with a positive sentiment, highlighting the significance of the paper's topic and praising its clarity, interesting architecture, and demonstration of effectiveness. However, it then delves into several 'cons,' suggesting improvements and pointing out limitations in the comparisons and analysis. The cons, while constructive, bring down the overall positivity. The language remains polite throughout, employing phrases like 'The authors should consider' and 'It would be interesting to see,' which are suggestive rather than demanding.",50.0,75.0
"Summary: The paper proposes a new architecture to defend against adversarial examples. The authors propose a network with new type of hidden units (RBFI units). They also provide a training algorithm to train such networks and evaluate the robustness of these models against different attacks in the literature. 

Main concern: I think the idea  proposed here of using RBFI units is very interesting and intuitive. As pointed out in the paper, the RBFI units make it difficult to train networks using standard gradient descent, because the gradients can be uninformative. They propose a new training algorithm based on ""pseudogradients"" to mitigate this problem. However, while evaluating the model against attacks, only gradient based attacks are used (like PGD attack of Madry et al., or Carlini and Wagner). It's natural to expect that since the gradients are uninformative, these attacks might fail. However, what if we considered similar ""pseudogradient"" based attacks? In particular, just use the same training procedure formulation to attack (where instead of minimizing loss like in training, we maximize loss)?
I think this key experiment is missing in the paper and without this evaluation, it's hard to claim whether the models are more robust fundamentally, or it's just gradient masking. 


Revision: After the authors revision, I change my score since they addressed my main complaint about results using pseudogradient attacks 
","The review starts with positive remarks, highlighting the interesting and intuitive nature of the proposed RBFI units. However, it then raises a significant concern about the evaluation methodology, suggesting a potential flaw that needs to be addressed. The reviewer's tone is critical but professional and offers constructive criticism by suggesting a specific experiment to strengthen the paper. The update at the end indicates that the reviewer's main concern was addressed, leading to a positive sentiment overall.",60.0,70.0
"This paper proposes two gated deep learning architectures for sensor fusion. They are all based on the previous work 
Naman Patel et al's modality fusion with CNNs for UGV autonomous driving in indoor environments (IROS). By having the grouped features, the author demonstrated improved performance, especially in the presence of random sensor noise and failures.

#Organization/Style:
The paper is well written, organized, and clear on most points. A few minor points:
1) The total length of the paper exceeds 8 pages. Some figures and tables should be adjusted to have it fit into 8 pages.
2) The literature review is limited.
3) There are clearly some misspellings. For example, the ""netgated"" is often written as ""negated"".

#Technical Accuracy:
The two architecture that the author proposes all based on the grouped features, which to my point of view, is a very important and necessary part of the new model. However, the author failed to rigorously prove or clearly demonstrated that why this is effective to our new model.  Moreover, how to make groups or how many groups are needed are not clearly specified. The experiments used only two completely different datasets, none of them are related to the previous sensor fusion method they are trying to compete. I'm afraid this method cannot generalize to a common case.

In addition, if we look at Table 4 and Table 5, we can find the first Group-level Fusion Weight actually increases, which seems contradictory to the result shown in Table 6.

#Adequacy of Citations: 
Poor coverage of literature in sensor fusion. There are less than 10 references are related to sensor fusion.

Overall, it is not an ICLR standard paper.","The review acknowledges the paper's strengths, such as clear writing and organization, but highlights significant weaknesses. These weaknesses include limited literature review, potential flaws in the technical approach, and questions about the generalizability of the findings. The presence of factual errors and the comparison to ICLR standards further suggest a negative sentiment. However, the language used remains professional and constructive throughout the review, indicating a neutral to polite tone.",-40.0,60.0
"Statistics based on KNN distances are ubiquitous in machine learning. In this paper the authors propose to apply the existing LID metric to GANs. The metric can be decomposed as follows: (1) Given a point x in X, compute the k-nearest neighbors KNN(x, X) and let those distances be R1, R2, …, Rk. Now, rewrite LID(x, X) = [max_over_i (log Ri) - mean_over_i (log Ri)] to uncover that the distribution of (log-)distances is summarized as a function of the max distance and the mean distance. (2) To extend the metric to two sets, A and B, define CrossLID(A; B) = E_(x in A) [LID(x, B)]. To see why CrossLID is useful, let X be the observed data and G the generated data. First consider CrossLID(A, B) where A=B=X which determines a lower-bound which is essentially the average (over elements of A) LID statistic determined by the underlying KNN graph of X. Now, keep A=X, and progressively change B to G (say by replacing some points from X with some points from G). This will induce a change of the distance statistics of some points from A, which will be detected on the individual LID scores of those points, and will hence be propagated to CrossLID. As a result, LID close to the baseline LID detects both sample quality issues as well as mode dropping/collapse issues. In practice, instead of computing this measure in the pixel space, one can compute it in the feature space of some feature extractor, or in some cases directly in the learned feature space of the generator. Finally, given some labeling of the points, one can keep track of the CrossLID statistic for each mode and use this during training to oversample modes for which the gap between the expected CrossLID and computed one is large.

Clarity: I think the clarity can be improved -- instead of stating the (rather abstract) properties of LID, the readers might benefit from the direct discussion of the LID estimator and a couple of examples, derive the max - mean relationship for the MLE estimator and provide some guiding comments. In a later section one might discuss why the estimator is so powerful and generally applicable. Secondly, the story starts with “discriminability of the distance measure” and the number of latent variables needed to do it, but I felt that this only complicated matters as many of these concepts are unclear at this point. 
Originality: Up to my knowledge, the proposed application is novel, albeit built on an existing (well-known) estimator. Nevertheless, the authors have demonstrated several desirable properties which might be proven useful in practice.
Significance of this work: The work is timely and attempts to address a critical research problem which hinders future research on deep generative models.

Pro:
- Generally well written paper, although the clarity of exposition can be improved. 
- Estimator is relatively easy to compute in practice (i.e. the bottleneck will still be in the forward and backward passes of the DNNs).
- Can be exploited further when labeled data is available
- Builds upon a strong line of research in KNN based estimators.
- Solid experimental setup with many ablation studies.

Con:
- FID vs CrossLID: I feel that many arguments against FID are too strong. In particular, in “robustness to small input noise” and “robustness to input transformation” you are changing the underlying distribution *significantly* -- why should the score be invariant to this? After all, it does try to capture the shift in distribution. In the robustness to sample size again FID is criticized to have a high-variance in low-sample size regime: This is well known, and that’s why virtually all work presenting FID results apply 10k samples and average out the results over random samples. In this regime it was observed that it has a high bias and low variance (Figure 1 in [1]). In terms of the dependency of the scores to an external model, why wouldn’t one compute FID on the discriminator feature space? Similarly, why wouldn’t one compute FID in the pixel space and get an (equally bad) score as LID in pixel space? Given these issues, in my opinion, Table 1 overstates the concerns with FID, and understates the issues with CrossLID. 
- FID vs CrossLID in practice: I argue that the usefulness comes from the fact that relative model comparison is sound. From this perspective it is critical to show that the Spearman’s rank correlation between these two competing approaches on real data sets is not very high -- hence, there are either sample quality or mode dropping/collapsing issues detected by one vs the other. Now, Figure 1 in [1] shows that this FID is sensitive to mode dropping. Furthermore, FID is also highly correlated with sample quality (page 7 of [2]).
- A critical aspect here is that in pixel space of large dimension the distances will tend to be very similar, and hence all estimators will be practically useless. As such, learning the proper features space is of paramount importance. In this work the authors suggest two remedies: (1) Compute a feature extractor by solving a surrogate task and have one extractor per data set. (2) During the training of the GAN, the discriminator is “learning” a good feature space in which the samples can be discriminated. Both of these have significant drawbacks. For (1) we need to share a dataset-specific model with the community. This is likely to depend on the preprocessing, model capacity, training issues, etc.. Then, the community has to agree to use one of these. On the other hand, (2) is only useful for biasing a specific training run. Hence, this critical aspect is not addressed and the proposed solution, while sensible, is unlikely to be adopted.
- Main contributions section is too strong -- avoiding mode collapse was not demonstrated. Arguably, given labeled data, the issue can be somewhat reduced if the modes correspond to labels. Similarly, if the data is well-clusterable one can expect a reduction of this effect. However, as both the underlying metric as well as the clustering depends on the feature space, I believe the claim to be too strong. Finally, if we indeed have labels or some assumptions on the data distribution, competing approaches might exploit it as well (as done with i.e. conditional GANs).
- In nonparametric KNN based density estimation, one often uses statistics based on KNN distances. What is the relation to LID?

With respect to the negative points above, without having a clear cut case why this measure outperforms and should replace FID in practice, I cannot recommend acceptance as introducing yet another measure might slow down the progress. To make a stronger case I suggest:
(1) Compute Spearman's rank correlation between FIDs and CrossLIDs of several trained models across these data sets.
(2) Compute the Pearson's correlation coefficient across the data sets. Given that your method has access to dataset specific feature extractors I expect it perform significantly better than FID.
 
[1] https://arxiv.org/pdf/1711.10337.pdf
[2] https://arxiv.org/pdf/1806.00035.pdf

========
Thank you for the detailed responses. I have updated my score from 5 to 6.","The review acknowledges the novelty and potential of the proposed method but raises several concerns regarding its practical advantages over existing metrics like FID. The reviewer finds some arguments against FID to be overstated and suggests additional experiments to strengthen the paper's claims. While the reviewer increased the score from 5 to 6 after the authors' response, the tone suggests a leaning towards the critical side, indicating a slightly positive sentiment. The language used is formal and professional, reflecting typical academic discourse.",20.0,80.0
"This paper investigates a meta-learning approach for the contextual bandit problem. The goal is to learn a generic exploration policy from datasets, and then to apply the exploration policy to contextual bandit tasks. The authors have adapted an algorithm proposed for imitation learning (Ross & Bagnell 2014) to their setting. Some theoretical guarantees straightforwardly extracted from (Ross & Bagnell 2014) and from (Kakade et al 2008) are presented. Experiments are done on 300 supervised datasets.

Major concerns:

1 This paper investigates a problem that does not correspond to the real problem: how to take advantage of a plenty of logs generated by a known stochastic policy (or worst unknown deterministic policy) for the same (or a close) contextual bandit task? 
Most of companies have this problem. I do not know a single use case, in which we have some full information datasets, which are representative of contextual bandit tasks to be performed. If the full information datasets does not correspond to the contextual bandit tasks, it is not possible to learn something useful for the contextual bandit task. 

2 The experimental validation is not convincing.

The experiments are done on datasets, which are mostly binary classification datasets. In this case, the exploration task is easy. May be it is the reason why the exploration parameter \mu or \epsilon = 0 provides the best results for MELEE or \epsilon-greedy?

The baselines are not strong. The only tested contextual bandit algorithm is LinUCB. However a diagonal approximation of the covariance matrix is used when the dimension exceeds 150. In this case LinUCB is not efficient. There are a lot of contextual bandit algorithms that scale with the dimension.


3 The theoretical guarantees are not convincing. 

The result of Theorem 1 is a weak result. A linear regret against the expected reward of the best policy is usually considered as a loosely result. Theorem 2 shows that there is no theoretical gain of the use of the proposed algorithm: the upper bound of the expected number of mistakes obtained when Banditron is used in MELEE is upper than the one of Banditron alone.

Minor concerns:

The algorithms are not well written. POLOPT function has sometimes one parameter, sometimes two and sometimes three parameters. The algorithm 1 is described in section 2, while one of the inputs of the algorithm 1 (feature extractor function) is described in section 3.1. The algorithm 1 seems to return all the N exploration policies. The choice of the returned policy has to be described.

In contextual bandits, the exploration policy is not handcrafted. The contextual bandit algorithms are designed to be optimal or near optimal in worst case: they are generic algorithms.
","The review is highly critical of the paper, pointing out major flaws in the problem definition, experimental validation, and theoretical contributions. The reviewer questions the practical relevance of the problem addressed and finds the experiments unconvincing due to weak baselines and potentially flawed setup. The theoretical results are also deemed weak. While the language is direct and critical, it maintains a professional and academic tone without resorting to personal attacks or disrespectful language.",-70.0,60.0
"This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency.
 
In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\del and \pi^\del) and vol \pi^\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. 
I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. 

Minor concerns:
1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary.
2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1.
3. In Page 7, “Figure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.”  I think, the subfigure shows that the ratio approaches 1 when d and k are all increased.
","The reviewer explicitly states that they find the paper ""very interesting and enlightening."" They commend the authors for their analysis and the novelty of their approach. While they raise valid concerns and suggestions for improvement, these are presented constructively and aim to enhance the paper's quality. The tone throughout is professional and respectful, indicating a positive yet discerning review. Therefore, the sentiment leans strongly positive, and the politeness is high.",80.0,85.0
"This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.

Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.

I have several further concerns about this work:

* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.

* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?

* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.

I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.

Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.



Other comments:

* In the introduction, an adversarial criterion is referred to as a ""discriminative objective"", but ""adversarial"" (i.e. featuring a discriminator) and ""discriminative"" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.

* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.

* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.

* Introduction, top of page 2: should read ""does not learn"" instead of ""do not learns"".

* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.

* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.

* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.

* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").

* Section 3.1, ""amounts to optimizing"" instead of ""amounts to optimize""

* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to ""Generating Sentences from a Continuous Space"" by Bowman et al. (2016). This should probably be cited instead.

* ""circle-consistency"" should read ""cycle-consistency"" everywhere.

* MMD losses in the context of GANs have also been studied in the following papers:
- ""Training generative neural networks via Maximum Mean Discrepancy optimization"", Dziugaite et al. (2015)
- ""Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"", Sutherland et al. (2016)
- ""MMD GAN: Towards Deeper Understanding of Moment Matching Network"", Li et al. (2017)

* The model name ""FILM-poi"" is only used in the ""implementation details"" section, it doesn't seem to be referred to anywhere else. Is this a typo?

* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?

* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here.","The review starts with a negative sentiment, stating the results are ""disappointing.""  It lists numerous concerns and questions the paper's claims. While acknowledging some positives (incremental comparisons, Figure 1), the overall tone is critical, leading to a recommendation against acceptance. The language, while direct and critical, maintains a professional and academic tone. There's no personal attack or disrespectful language used.",-50.0,50.0
"This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring. The proposed method performs much better than baseline sentence embedding methods. Another contribution is on using negative pre-training to deal with an imbalanced training dataset. 

To me this paper is just not good enough - the method essentially i) use ""a professor and two teaching assistants"" to build a ""rule-based concept extractor"" for problems, then ii) map problems into this ""concept space"" and simply treat them as words. There are several problems with this approach. 

First, doing so does not touch the core of the proposed application. For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper. Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.

Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words. I am not sure how this will generalize to a larger number of problem spanning many different domains.

I also had a hard time going through the paper - there aren't many details. Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.","The reviewer starts with a relatively neutral acknowledgment of the paper's contributions. However, the tone quickly turns negative, using phrases like ""just not good enough"" and outlining several significant concerns. The reviewer questions the fundamental approach and methodology, highlighting a lack of practical evaluation and scalability issues. While the critique is direct, it avoids personal attacks and maintains a professional tone.",-60.0,40.0
"Paper summary:

Given a pre-trained VAE (e.g. over images), this paper is about inferring the distribution over missing variables (e.g. given half the pixels, what is a plausible completion?). The paper describes an approach based on variational inference with normalizing flows: given observed variables, the posterior over the VAE's latents is inferred (variationally) and plausible completions for missing variables are sampled from the VAE decoder.

Technical quality:

The presented method is technically correct. The evaluation carefully compares different types of normalizing flow and HMC, and seems to follow good practices.

I have a suggestion for improving the GVI method. The way it's described in the paper, GVI requires computing the determinant of a DxD matrix, which costs O(D^3), and there is no guarantee that the matrix is invertible. However, this approach over-parameterizes the covariance matrix of the modelled Gaussian. Without losing any flexibility, you can use a lower triangular matrix with strictly positive diagonal elements (e.g. the diagonal elements can be parameterized as the exp of unconstrained variables). That way, the determinant costs O(D) (it's just the product of diagonal elements) and you ensure that the matrix is invertible (because the determinant is strictly positive), without hurting expressivity. You can think of this as parameterizing the Cholesky decomposition of the covariance matrix.

Also, there are more flexible normalizing flows, such as Inverse Autoregressive Flow, that can be used instead of the planar flow used in the paper.

Clarity:

The paper is written clearly and in full detail, and the mathematical exposition is clear and precise.

Some typos and minor suggestions for improvement:
- It'd be good to move Alg. 1 and Fig. 1 near where they are first referenced.
- Page 2: over to \theta --> over \theta
- Eq. 3: p_\theta appears twice in the middle.
- one can use MCMC to attempt sampling --> one can use MCMC to sample
- Eq. 5: should be q_\psi as subscript of E.
- Fig. 7, caption: should be GVI vs. NF.
- In references, should be properly capitalized: Hamiltonian, Langevin, Monte Carlo, Bayes, BFGS
- Lemma 1: joint divergence is equivalent to --> joint divergence is equal to
- Lemma 1: in the chain rule for KL, the second KL term should be averaged w.r.t. its free variables.

Originality:

In my opinion, there is little original contribution in this paper. The inference method presented (variational inference with normalizing flows) is well-known and already in use. The paper applies this method to VAEs, which is a straightforward application of a well-known inference method to a relatively simple graphical model (z -> {x, y}, with x, y independent given z).

I don't see the need for introducing a new term (cross-coder). According to the paper, a cross-coder is precisely a normalizing flow (i.e. an invertible smooth transformation of a simple density). I think new terms for already existing ideas add cognitive load to the community, and are better avoided.

Significance:

In my opinion, constructing generative models that can handle arbitrary patterns of missing data is an important research direction. However, this is not exactly what the paper is about: the paper is about inference in a given generative model. Given that there is (in my opinion) no new methodology in the paper, I wouldn't consider this paper a significant contribution.

I would also suggest that in a future version of the paper there is more motivation (e.g. in the introduction) of why the problem the paper is concerned with (i.e. missing data in generative models) is significant. Is it just for image completion / data imputation, or are there other practical problems? Is it important as part of another method / solution to another problem?

Review summary:

Pros:
- Technically correct, gives full detail.
- Well and clearly written, precise with maths.
- Evaluation section interesting to read.

Cons:
- No original contribution.
- Could do a better job motivating the importance of the problem.

Minor points:
- I don't completely agree with the way VAEs are described in sec. 2.1. As written, it follows that VAEs must have a Gaussian prior and a conditionally independent decoder. Although these are common choices in practice, they are not necessary: for example, one could take the prior to be a Masked Autoregressive Flow and the decoder a PixelCNN.
- Same for observation 1. This is not an observation, but an assumption; that is, the paper assumes that the decoder is conditionally independent. This is of course an assumption that we can satisfy by design, but it's a design choice that restricts the decoder in a specific way.","The review acknowledges the technical correctness and clarity of the paper, which contributes to a positive sentiment. However, the reviewer finds the contribution to be insignificant due to the lack of novelty. The reviewer also suggests improvements and provides constructive criticism, indicating a neutral to slightly negative sentiment. The language used is polite and professional throughout, with specific suggestions and explanations provided.",-10.0,80.0
"In this paper, the authors study the problem if learning for observation, a reinforcement learning setting where an agent is given a data set of experiences from a potentially arbitrary number of demonstrators. The authors propose a method which deploys these experience to initialize a place. Then estimate the value of this policy in order to improve it.

The paper is well written and it is easy to follow. 

Most of the theoretical results are interesting and the derivations are kinda straightforward but not fully matching the main claim in the paper. Mainly the contribution in this paper heavily depends on an assumption that Q^D and Q^\beta are close to each other. This assumption simplifies the many things resulting in a simple algorithm. But this assumption is too strong while the main challenge in the line of learning from observation comes from the fact that this assumption does not hold. Under this assumption and the similarity in distributions mentioned in proposition 4.2 make the contribution of this paper significantly weak.

Please let me know if you do not actually use this assumption in your results and justification.","The review starts with positive remarks, praising the paper's clarity and interesting theoretical results. However, it then raises a significant concern about a strong assumption made by the authors, arguing that this assumption weakens the paper's contribution. The reviewer's tone remains professional and polite throughout, even when expressing disagreement. Therefore, the sentiment is moderately negative due to the critical concern, and the politeness remains positive.",-30.0,70.0
"The submission presents an extension to the Allamanis et al ICLR'18 paper on learning from programs as graphs. The core contribution is the idea of introducing extra nodes and edges into the graph that correspond to (potentially rare) subwords used in the analyzed program code. Experiments show that this extended graph leads to better performance on two tasks, compared to a wide range of baseline methods.

Overall, this is a nice paper with a small, incremental idea and substantial experiments that show its practical value. I only have minor comments / questions on the actual core content. However, the contribution is very incremental and of interest to a specialized subsegment of the ICLR audience, so it may be appropriate to reject the paper and redirect the authors to a more specialized venue.

Minor comments:
- There's a bunch of places where \citep/\citet are mixed up (e.g., second to last paragraph of page 2). It would make sense to go through the paper one more time to clean this up.
- Sect. 4: I understand the need to introduce context, but it feels that more space should be spent on the actual contribution here (step 3). For example, it remains unclear why this extra nodes / edges are only introduced for subwords appearing in variables - why not also for field names / method names?
- Sect. 5: It would be helpful if the authors would explicitly handle the code duplication problem (Lopes et al., OOPSLA'17), or discuss how they avoided these problems. Duplicated data files occurring in several folds are a significant risk to the validity of their experimental findings, and very common in code corpora.
- Table 1: It is unclear to me what the ""Pointer Sentinel"" model can achieve. Without edges connecting the additional words to where they occur, it seems that this should not be performing different than ""Closed Vocab"", apart from noise introduced by additional nodes.
- Table 1: Do Pointer Sentinel/GSC use a CharCNN to embed node labels of nodes that are not part of the ""cache"", or a closed vocabulary? [i.e., what's the embedding of a variable ""foo""?] If not, what is the performance of the GSC model with CharCNN-embeddings everywhere? That would be architecturally simpler than the split variant, and so may be of interest.
- Page 6: When truncating to 500 nodes per graph: How many graphs in your dataset are larger than that?
- Page 7: Do you really use attention over all nodes, instead of only nodes corresponding to variables? How do you deal with results where the model picks a non-variable (e.g., a corresponding cache node)? Does this happen?
","The reviewer finds the paper to be ""nice"" and the experiments ""substantial"", which points towards a positive sentiment. However, they also mention the contribution is ""incremental"" and suggest a more specialized venue might be more appropriate. This suggests the sentiment is not overly positive. Therefore, a sentiment score of 50 seems appropriate. The language used throughout is polite and professional, with constructive criticism and suggestions for improvement. Thus, a politeness score of 100 is justified.",50.0,100.0
"Summary: 
This paper presents three small improvements for training binarized neural networks: (1) a modified straight-through estimator, (2) a novel regularizer to push weights to +/- 1, and (3) the use of scaling factors for the binarized weights. Using the methods presented, the validation accuracies on ImageNet and CIFAR-10 are improved by just under 2 percentage points.

  Pros:
    - Decent improvement in the performance of the binarized network in the end
    - The presented regularizers make sense and seem effective. The modified straight-through estimator seems reasonable as well, although the authors do not compare to recent work with a similar adjustment. 

  Cons:
    - The paper is poorly written and confusing. It reads as if it was written in one pass with no editing or re-writing to clarify contributions or key points, or ensure consistency.
    - While the final numbers are acceptable, the experiments themselves could be stronger and could be presented more effectively.
   - The novelty of the scale factors is questionable.


Questions and comments:

1. How exactly is the SS_\beta activation used? It is entirely unclear from the paper, which contradicts itself in multiple ways. Is SS_\beta used in the forward pass at all for either the weight or activation binarization? Or is only its derivative used in the backward pass? If the latter, then you are not replacing the activation anywhere but are simply using a different straight-through estimator in place of the saturated straight-through estimator (e.g., see [1]).
   (a) At the beginning of Section 3.3, you say that you modify the training procedure by replacing the sign binarization with the SS_\beta activation. This sounds like it is referring to the activation function at each layer; however, the pseudocode says that you are using sign() as the per-layer activation. 
   (b) Further, Figure 4 shows that you are using the SS_\beta function to do weight binarization. However, again, the pseudocode shows that you are using sign() to do the weight binarization. 

2. In [1], the authors used a similar type of straight-through estimator (essentially, the gradient of tanh instead of hard_tanh) and found that to be quite effective. You should compare to their method. Also, it's possible that SS_\beta reduces to tanh for some choice of \beta -- is this true?

3. The use of scale factors seems to greatly increase the number of parameters in the network and thus greatly decrease the compression benefits gained by using binarization, i.e., you require essentially #scale_factors =  a constant factor times the number of actual parameters in the network (since you have a scale factor for each convolutional filter and for each column of each fully-connected layer). As a result of this, what is the actual compression multiplier that your network achieves relative to the original network?

4. For the scale factor, how does yours differ from that used in Rastegari et al. (2016)? It seems the same but you claim that it is a novel contribution of your work. Please clarify.

5. Why did learning \beta not work? What was the behavior? What values of \beta did learning settle on? 

6. I assume that for each layer output y_i = f(W_i x_i), the regularizer is applied as R(y_i) while at the same time y_i is passed to the next layer -- is this correct? The figures do not clearly show this and should be changed to more clearly show how the regularizer is computed and used, particularly in relation to the activation.

7. In the pseudocode:
   (a) What does ""mostly bitwise operations"" mean? Are some floating point?
   (b) Is this the shift-based batchnorm of Hubara et al. (2016)?

8. For Table 1:
   (a) I assume these are accuracies? The caption should say.
   (b) Why are there no comparisons to the performance of other methods on this dataset?
   (c) Any thoughts as to why your method performs better than the full-precision method on this dataset for VGG?

8. For Table 2:
   (a) Does Table 2 show accuracies on ImageNet? You need to make this clear in the caption.
   (b) What type of behavior do the runs that do not converge show? This seems like a learning rate problem that is easily fixable. Are there no hyperparameter values that allow it to converge?
   (c) What behavior do you see when you use SS_1 or SS_2, i.e., \beta = 1 or \beta = 2? Since lower \beta values seem better.
   (d) The regularization seems to be the most useful contribution -- do you agree?
   (e) Why did you not do any ablations for the scale factor? Please include these as well.

9. For Table 3, did you compute the numbers for the other approaches or did you use the numbers from their papers? Each approach has its own pros and cons. Please be clear.

10. Are there any plots of validation accuracy versus epoch/time for the different algorithms in order to ensure that the reported numbers were not simply cherry-picked from the run? I assume that you simply used the weights from the end of the 50th epoch -- correct? 

11. Is there evidence for your introductory claims that 'quantizing weights ... make neural networks harder to train due to a large number of sign fluctuations' and 'maintaining a global structure to minimize a common cost function is important' ? If so, you should cite this evidence. If not, you should make it clear that these are hypotheses. 

12. Why are there not more details about the particular architectures used? These should be included in the appendices to aid those who would like to rerun your experiments. In general, please include more experiment details in the body or appendices.


Detailed comments:
- R(l) is not defined in Figure 1 and thus is confusing. Also, its replacement of 'Error' from the original figure source makes the figure much more confusing and less clear.

- Typos:
   - 'accustomed' (p.1)
   - 'the speed by quantizing the activation layers' doesn't make sense (p.1)
   - 'obtaining' (p.4)
   - 'asymmetric' doesn't make sense because these are actually symmetric functions across the y-axis (p.4)
   - 'primary difference is that this regularization ...' --> 'primary difference is that their regularization ...' (p.4)
   - 'the scales with 75th percentile of the absolute value ... ' is very confusing and unclear (p.7)
   - 'the loss metric used was the cross-entropy loss, the order of R_1.' I do not know what you're trying to say here (p.8)

- Citations: Fix the capitalization issues, typos, and formatting inconsistencies.


[1]  Friesen and Domingos. Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. ICLR 2018.


-------------------

After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not.

If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication.
","The review is highly critical of the paper, pointing out major flaws in clarity, experimental setup, and novelty. While it acknowledges some improvements in accuracy, the numerous concerns and the final paragraph stating the paper is ""not ready for publication"" indicate a negative sentiment. However, the language, while direct and critical, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",-60.0,60.0
"In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.

Strength:

1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. 

2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.

Weakness:

1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.

2, Authors motive their work by saying in the abstract that “other graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters”. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.

3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.

4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?

5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.

Typos:

1, There are a few references missing (question mark) in the first and second paragraphs of section 2.

2, Methods in the experiment section are given without explicit reference, like GCPN.

3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. 

Overall, I do not think this paper is ready for publishing and it could be improved significantly.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Update:

Thanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. 

However, I still found some claims made by authors problematic. 
For example, it reads in the abstract that ""...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters..."". 
Clearly, Li et al. 2018b has a differentiable formulation which falls under your description.

Besides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. 
Also, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.
Directly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.

Therefore, I would like to keep my original rating. 
","The reviewer acknowledges the importance of the problem and finds the proposed model interesting. However, they raise several significant concerns regarding missing comparisons with existing methods, lack of ablation studies, unclear explanations, and potential flaws in the authors' claims. The reviewer maintains their initial rating even after the authors' response, indicating a negative sentiment overall.",-20.0,50.0
"In this paper, the effect of batch normalization to the maximum eigenvalue of the Fisher information is analyzed. The techinique is mostly developed by Karakida et al. (2018). The main result is an informal bound of the maximum eigenvalue, which is given without proof. Though, the numerical result corresponds to the derived bound.

The paper is basically well written, but the technical part has several notational problems. For example, there is no definition of ""\otimes"", ""\odot"", and ""Hess"" operators.

The use of the mean-field theory is an interesting direction to analyze batch normalization. However, in this paper, it seems failed to say some rigorous conclusion. Indeed, all of the theoretical outcomes are written as ""Claims"" and no formal proof is given. Also, there is no clear explanation of why the authors give the results in a non-rigorous way, where is the difficult part to analyze in a rigorous way, etc. 

Aside from the rigor issue, the paper heavily depends on the study of Karakida et al. (2018). The derivation of the bound (44) is directly built on Karakida's results such as Eqs. (7,8,20--22), which reduces the paper's originality.

The paper also lacks practical value. Can we improve an algorithm or something by using the bound (44) or other results?","The review is critical of the paper, pointing out a lack of rigor, originality, and practical value. While it acknowledges that the paper is well-written and finds the use of mean-field theory interesting, the negative points outweigh the positive. The language used is professional and not overtly negative, but the criticism is clear and direct.",-30.0,60.0
"Authors propose a new method called TriMap, which captures higher orders of structure with triplet information, and minimize a roust loss function for satisfying the chosen triplets.
 
The proposed method is motivated by the misleading selection approach for a dimensionality reduction method using local measurements. And then, authors resort to an evaluation based on visual clues based on a number of transformations. Authors then claim that any DR method preserving the global structure of the data should be able to handle these transformations.  An example on MNIST data illustrate these properties, but it is still not clear what are the visual clues as the criterion to select a good DR method and what are the global structures.
 
Authors discussed the results in Figure 4 for six real-world datasets, but there is no convincing evidence from the corresponding domains or reference researches for the support of the global structure in the learned embedding space.  It will be good to add some convincing evidences for the conclusion.
 
As the method highly depends on the subset of sampled triplets, it is interesting to see how the global structure changes if a different set of triplets is used.  In addition, it is unclear why sampled triplets can achieve a global structure of data instead of pairwise relations. From the experiments, triplets are also sampled according to the pairwise nearest neighbor graph.","The review is mostly neutral, pointing out both positive aspects (e.g., the method's motivation) and areas for improvement (e.g., lack of clarity on visual clues and global structure, dependence on triplet sampling). It acknowledges the proposed method and its rationale but raises several questions and requests for clarification, indicating a need for more convincing evidence and explanations. The language is polite and professional, employing constructive criticism and suggestions for improvement rather than harsh language.",-10.0,50.0
"This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. The model is a simple combination of GCN and existing framework for community detection. The proposed algorithm is compared to baselines on various datasets, and demonstrated to be accurate in many cases.

I think the paper does not deal with one of the most important aspects of network modeling - the degree heterogeneity of nodes. Many works reported that lack of degree corrections would result in bad estimates of community structures [1,2,3]. Probably including the degrees as feature of nodes would be helpful. 

Regarding the stochastic gradient descent by edge subsampling, I think the authors should mention [4], where the idea of edge subsampling in stochastic gradient descent setting was introduced before this work. Also, it is worth noting that we may lose some important distributional properties in graphs if we naively subsample from it [5]. For instance, sampling from positive and negative pairs to balance the class contribution may distort the sparsity and degree distributions of subsampled graphs. 

If we choose to use Bernoulli-Poisson link function, we can reduce the time complexity of likelihood and gradient computation to O(N + E), where N is the number of nodes and E is the number of edges, with the auxiliary variable trick introduced in [6]. In that case we don't really have to worry about subsampling. Why didn't you consider applying this to your model?

Regarding the experiments, I think some important baselines are missing [3, 6]. Also, I wonder whether the proposed algorithm would scale to the graphs with more than 100,000 nodes. 

References
[1] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Physical Review E, 83(1):016107, 2011.
[2] P. K. Gopalan, C. Wang, and D. Blei. Modeling overlapping communities with node popularities. NIPS 2013.
[3] A. Todeschini, X. Miscouridou and F. Caron. Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities. CoRR 2016.
[4] J. Lee, C. Heakulani, Z. Ghahramani, L. F. James, and S. Choi. Bayesian inference on random simple graphs with power law degree distributions. ICML 2017.
[5] P. Orbanz. Subsampling large graphs and invariance in networks. CoRR 2017.
[6] M. Zhou. Infinite edge partition models for overlapping community detection and link prediction. AISTATS 2015","The review is critical of the paper, pointing out several limitations and suggesting improvements. While the reviewer acknowledges the paper's contributions, the tone is rather negative due to the focus on missing aspects and comparisons to potentially superior methods. The language, however, remains polite and professional throughout, offering constructive criticism and suggestions rather than direct attacks.",-30.0,70.0
"This paper presents an a particular architecture for conditional discriminators in the cGAN framework. Different to the conventional approach of concatenating the conditioning information to the input, the authors propose to process them separately with two distinct convolutional networks fusing (by element-wise addition) intermediate features of the conditioning branch into the input branch at each layer.

Pros:
+ The writing is mostly clear and easy to follow.
+ I feel that exploring better conditioning strategies is an important direction. Quite often the discriminator discards additional inputs if no special measures against this behaviour are taken.
+ The proposed method seem to outperform the baselines

Cons:
- I’m generally not excited about the architecture as it seems a slight variation of the existing methods. See, for example, the PixelCNN paper [van den Oord et al., 2016] and FiLM [Perez et al., 2017].
- Theoretical justification of the approach is quite weak. The paper shows that the proposed fusion method may result in higher activation values (in case of the ReLU non-linearity, other cases are not considered at all) but this is not linked properly to the performance of the entire system. Paragraph 3 of section 3.1 (sentence 3 and onward) seems to contain a theoretical claim which is never proved.
- It seems that the authors never compare their results with the state-of-the-art. The narrative would be much more convincing if the proposed way of conditioning yielded superior performance compared to the existing systems. From the paper it’s not clear how bad/good the baselines are.

Notes/questions:
* Section 3.1, paragraph 1: Needs to be rephrased. It’s not totally clear what the authors mean here.
* Section 3.1, paragraph 4: “We observed that the fusion …” -  Could you elaborate on this? I think you should give a more detailed explanation with examples because it’s hard to guess what those “important features” are by looking at the figure.
* Figure 4: I would really want to see the result of the projection discriminator as it seems to be quite strong according to the tables. The second row of last column (which is the result of the proposed system) suspiciously resembles the ground-truth - is it a mistake?
* Figure 5: It seems that all the experiments have not been run until convergence. I’m wondering if the difference in performance is going to be as significant when the model are trained fully.

In my opinion, the proposed method is neither sufficiently novel nor justified properly. On top of that, the experimental section is not particularly convincing. Therefore, I would not recommend the paper in its present form for acceptance.","The reviewer acknowledges some positives like clear writing and exploration of an important direction. However, they express significant concerns about the novelty and theoretical justification of the proposed method. They find the experimental section unconvincing and ultimately do not recommend the paper for acceptance. The tone is critical but professional and not rude.",-50.0,50.0
"This method deals with compressing tiny videos using an end-to-end learned approach. However, the paper has a significant number of limitations, which I will discuss below.

1. The method has only been trained on very small videos due to the fact that fully connected layers are used. I don't really understand why was this necessary, and it's not explained in the paper at all. Just this fact makes it completely infeasible for any ""real"" application.
2.  The evaluation was done on very limited domains. Of huge concern to me is the fact that very good results are presented on the sprites dataset. However, that dataset can be literally encoded by providing an index in a lookup table of sprites, so it's absolutely ludicrous to compare learned methods on that set to general video compression methods. The results look a lot less exciting when looking at the Kinetics 64x64 dataset. 
3. The evaluation (again) is problematic because the results refer to PSNR. PSNR for video is a very overloaded term. In fact, just the way to compute PSNR is not very clear for video. Video compression papers in general compute it in one of two ways: take the mean squared error over all the pixels in the video, then compute PSNR; or compute per frame PSNR then average. Additionally, none of the papers in this domain use RGB, because the human visual system is much more sensitive to detail preservation (the Y/luminance channel) than they are to chroma (color) changes. When attempting to present results for video, I would recommend to use PSNR-Y (and explain which type it is!), while also mentioning which ITU recommendation is used for defining the Y channel (there are multiple recommendations). 
4. It is not very clear how the global code is obtained. It is implied that all frames get processed in order to come up with f, but does this mean that they're processed via an LSTM model, or is there a single fully connected layer which takes as input all frames? In terms of modeling f, it sounds like the hyperprior model from Balle et al is employed, but again it's not clear to me how (is it modelling an entire video or a sequence?). I would really like to see a diagram for the network structure that computes f.

Ont he positives of the paper: I applaud the authors with respect to the fact that they made an effort to explain how the classical codecs were configured and being explicit about the chroma sampling that's employed. 

I think all the problems I mentioned above can be fixed, so I don't want to reject the paper per se. If possible, should the authors address my concerns (i.e., add more details), I think this could be an interesting ""toy"" method. ","The review starts with a negative sentiment, pointing out significant limitations. While it acknowledges some positives and doesn't outright reject the paper, the overall tone is critical, suggesting major revisions are needed. The language is quite direct, using terms like ""ludicrous"" and ""completely infeasible,"" which, while not personally insulting, are harsh in a scientific context.",-50.0,-20.0
"The paper aims at studying the setting of perturbed rewards in a deep RL setting. Studying the effect of noise in the reward function is interesting. The paper is quite well-written. However the paper studies a rather simple setting, the limitations could be discussed more clearly and there are one or two elements unclear (see below).

The paper assumes first the interesting case where the generation of the perturbed reward is a function of S*R into the perturbed reward space. But then the confusion matrix does *not* take into account the state, which is justified by ""to let our presentation stay focused (...)"". I believe these elements should at least be clearly discussed. Indeed, in that setting, the theorems given seem to be variations of existing results and it is difficult to understand what is the message behind the theorems.

In addition, it is assumed that the confusion matrix C is known or estimated from data but it's not clear to me how this can be done in practice.  In equation 4, how do you have access to the predicted true rewards?

Additional comments:
- The discount factor can be 0 but can not, in general, be equal to 1. So the equation in paragraph 2.1 ""0 < γ ≤ 1"" is wrong.
- The paper mention that ""an underwhelming amount of reinforcement learning studies have focused on the settings with perturbed and noisy rewards"" but there are some works on the subject (e.g., https://arxiv.org/abs/1805.03359) and a discussion about the differences with the related work would be interesting.","The review starts with positive remarks, highlighting the interesting topic and good writing. However, it then raises several concerns about the simplicity of the setting, lack of clarity, and potential repetition of existing results. The reviewer also points out specific issues with assumptions and methodology. While the reviewer provides constructive criticism and suggestions for improvement, the overall tone suggests a need for significant revisions. Therefore, the sentiment leans towards the negative side. The language used is polite and professional, employing a respectful and constructive tone throughout.",-20.0,80.0
"This work proposes to use duality gap and minimax loss as measures for monitoring the progress of training GANs. The authors first showed a relationship between duality gap(DG) and Jensen-Shannon divergence and non-negativeness on DG. Then, a comprehensive discussion was presented on how to estimate and efficiently compute DG. A series of experiments were designed on synthetic data and real-world image data to show 1) how duality gap is sensitive to capture non-convergence during training and 2) how minimax loss efficiently reflects the sample quality from generator. 


I was not very familiar with GANs, thus I'm not sure on the significance of paper and would like to see opinions from other reviews on this. For reviewing this paper, I also read the cited works such as Salimans (2016), Heusel (2017). Compared with them, the theoretical contribution of this work seems less significant. Also, I'm not quite impressed by the advantages of proposed metrics. However, this work is nicely written, the ideas are delivered clearly, experiments are nicely designed. I kind of enjoying reading this paper due to its clarity.


Other concerns:

There are two D_1 in Equation Mixed Nash equilibrium.
","The review starts with a neutral summary of the paper's contributions. While the reviewer acknowledges the paper is well-written and easy to follow, they express uncertainty about the significance of the work and are not impressed by the proposed metrics. They also point out a minor error. Overall, the tone is neutral to slightly positive, leaning towards constructive criticism rather than harsh negativity.",20.0,70.0
"The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.
In summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable.","The review is highly critical of the paper, pointing out significant flaws in the methodology and originality. The reviewer uses terms like ""very simple approaches,"" ""too simple,"" ""implicit assumption,"" ""low complexity,"" ""poor,"" and ""low"" which all point towards a negative sentiment. While the review is critical, the language used is professional and avoids personal attacks, suggesting a neutral politeness.",-75.0,0.0
"The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. 
The paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. 

I incline to my current score after reading the response and other reviews.","The reviewer provides a positive assessment of the paper, using phrases like ""well-motivated"", ""nicely presented"", ""easy-to-understand"", ""properly discussed"", ""convincing results"". The reviewer also explicitly states their inclination towards a positive score. There is no use of harsh language, and the critique is presented constructively.",85.0,90.0
"This paper provides an algorithm that excludes the bad training data in the training process and obtain a more accurate model for both supervised and unsupervised learning problem. The paper gives the theoretical guarantee for mixed linear regression and Gaussian mixture model, and also conducts the experiments for deep image classification and deep generative models.

Major Concerns:
1, As said in related work, a soft version of this paper’s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.

2, In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  “strict inequality” property in the 3rd line in Theorem 2, because it should correspond to the  “strict inequality” property in the 2nd line in Theorem 1.

3. Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to 

A Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017

The comparison to these type of methods need to be included. 

Minor Concerns:
1, Theorem 2 does not give the probability, only mentioning “high probability”. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that

2, In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.","The review is critical of the paper, pointing out significant limitations and lack of novelty. While the language is mostly factual, the phrasing of the concerns, particularly ""too few innovations,"" leans towards a negative sentiment. The reviewer also points out missing information and comparisons, indicating a lack of thoroughness in the paper. However, the critique is presented in a professional and constructive manner, suggesting areas for improvement rather than resorting to personal attacks.",-50.0,60.0
"In this paper, the authors proposed 'defensive distinction' to address questions: Are adversarial examples distinguishable from natural examples? Are adversarial examples generated by different methods distinguishable from each other?

I have some major concerns about this submission.

1) The presentation of this work should further be improved. It contains many vague sentences. For example, ""Unfortunately, even state-of-the-art defense approaches such as adversarial training and defensive distillation still suffer from major limitations and can be circumvented."" I really hope I can see some justifications based on authors' approach for this argument. Also, the definition of 'AdvGen-Model' is not clear. Do you mean Adversarial attack generator knows the network model (i.e., white-box attack)? It is also not clear that how representative scenarios and cases in Table 1 affect the implementation of the proposed experiments (implementation details rather than results). 

2) The technical contribution of this paper is weak, and the experiments are not enough to support its main claim. MNIST is a simple dataset, please try larger and more complex datasets. The contribution of the current version is limited. 

 ","The reviewer expresses ""major concerns"" about the submission, stating that the technical contribution is ""weak"" and the experiments are ""not enough."" They also point out issues with clarity and presentation. This suggests a negative sentiment. However, the reviewer provides constructive criticism and specific recommendations for improvement, indicating a polite tone overall.",-60.0,60.0
"The paper deals with a problem of expressiveness of a piecewise linear neural network, characterized by the number of linear regions of the function modeled. This is one of the widely accepted measure of expressiveness of a linear model. As such, it has been studied before. The main contributions of the paper are:
1) Different algorithms are proposed that allow to compute the bounds faster, leveraging probabilistic algorithms
2) Tighter bounds are obtained 
I find the results somewhat interesting. However, I do not think there is a lot of practical value in having faster algorithms for obtaining the bounds, as they are not used in practice anyway. I am also not convinced that the quest for tighter-and-tighter bounds in this approach is the right scientific direction. I find the paper to be an interesting contribution, but of a marginal value to the progress of the domain and for the improvement of our understanding of the models.","The reviewer acknowledges the contributions of the paper, stating them as ""interesting"" and the paper as ""an interesting contribution"". However, they express doubts about the practical value and the chosen scientific direction, ultimately deeming the value ""marginal"". This suggests a slightly negative sentiment overall. The language used is polite and professional throughout, without resorting to harsh or disrespectful vocabulary.",-10.0,80.0
"Authors propose a supervised method for predicting adjacency matrix for a set of points. Loss function consists of 4 terms: intersection over union loss with respect to target adjacency, cross entropy loss, symmetry penalty and L2 regularization of parameters. Learning process consists of alternating node feature updates parametrized by GCN-like layers and updates of the adjacency matrix (different across layers).

My main concern is the heuristic nature of the method without any successful real data application. I do not see this work as impactful or of interest to ICLR community.

Directly regarding the content I have following comments and questions:

Word ""structure"" seems to be used in several meanings. For example ""We consider the problem of predicting the structure of a given set of points (which we assume are the nodes of a graph) and an initial structure (connections of the points)."" It only becomes somewhat clear later what is actually the learning problem studied in this paper.

""The learned convolutions"" - convolution is a particular mathematical operation. I believe authors should refer to the weights of their architecture instead.

Symmetry penalty of equation 14 seems unnecessary. When optimizing for symmetric matrix it should be recognized that corresponding symmetric entries are identical variables. Hence it is sufficient, and mathematically appropriate, to correct the gradient computed without symmetric consideration. Correction is simply sum of the gradient with itself transposed (without diagonal entries).

""We compare against traditional generative models for graphs: mixed-membership stochastic block models (MMSB) "" - could you please elaborate on how you use MMSB for graph generation. The use-case I am familiar with is community detection.
","The reviewer expresses strong concerns about the paper's impact and lack of real-world application. They call the method ""heuristic"" and state it's not of interest to the ICLR community, which indicates a negative sentiment. However, they provide detailed and specific feedback, suggesting ways to improve the paper. The language used is formal and focused on the scientific content, without resorting to personal attacks or overly negative phrasing.",-60.0,60.0
"This paper proposes a specific measure of difficulty for training examples called “easiness”. Easiness is based on training the model N times and counting the number of times an example is classified correctly. Based on this measure, they introduce “matching rate” as a measure of similarity of two architectures. Two architectures are suggested to be similar if the set of easy and hard examples is similar. The rest of the paper presents comparisons of architectures. Considering the problems below, I don’t see any reliable contribution in this paper.

- Why this specific definition of easiness? Can you compare to simply using “loss” as a measure for the difficulty of an example?
- e_t seems to be measuring the variance of training on a single example. If there is only one example that is always classified correctly, the denominator can be simplified to K. It doesn’t tell us how many training iterations it takes to fit that example.
- Why this specific formulation for “matching rate”? Why not a more common measure of similarity between sets such as intersection over union (IoU)? Can you suggest any references using a similar similarity score?
- Numbers in Table 1 do not seem particularly big to support the claim in section 4 that “...CNNs start learning from the *same* examples even if CNN architectures are different”. 0.20 is definitely bigger than random 0.1 for the matching rate but it still means only a 20% match.
- Random 0.1 is redundant in table 1.
- In section 4, define “contradicted patterns”.
- Are all images in Figure 1 for one model? How does it compare to visualizing examples according to their loss?
- The conclusion in section 5 says “... different CNNs start learning from similar patterns”. As mentioned above, “easiness” and consequently “matching rate” do not provide information about the progress of training and only final trained models. Regardless, this conclusion does not seem particularly unexpected or informative.
- Section 6 proposes to test a model on data with a different structure from data provided in training. This is a distribution mismatch and the model is not trained to handle.","The review is highly critical of the paper's core concepts and contributions. The reviewer finds the proposed definitions and methods questionable, unsupported by evidence, and ultimately not novel. The numerous pointed questions and challenges to the authors' claims indicate a negative sentiment. However, the language remains professional and avoids personal attacks, suggesting a neutral politeness.",-75.0,0.0
"The paper addresses the robustness of deep neural networks to adversarial example attacks. It uses a bilateral filtering as a preprocessing step to recover clean data from adversarial ones. It can also get combined with adversarial training and be trained end-to-end. The paper is well written, the background and introduction is clear. However I have comments about their implementation and experimental results:

1. They are claiming to have a very high distortion while their model can still perform well.  They have to make sure obfuscated gradients has not happened and they have implemented the back-propagation from attack to defense correctly. 

2. Also if they had consider black-box threats as well as white-box one, it would have been more informative of how their method actually performs. Specially, in order to check whether obfuscated gradients has happened, the black-box threats are better choice, which they have not tried.  

So considering the above, there might be an issue in what they are claiming, so they might reconsider their method. Maybe reviewing their codes by some experts in this topic can give a good evaluation. 

(I have to mention that I am not an expert in Adversarial networks)



","The review starts with positive remarks, praising the paper's clarity and writing. However, it raises serious concerns about the methodology and results, suggesting potential flaws. The reviewer's lack of expertise in adversarial networks adds uncertainty but emphasizes the need for careful re-evaluation. The tone is professional and polite throughout, suggesting areas of improvement rather than outright dismissal.",20.0,70.0
"Based on my understanding, this paper describes a novel approach for addressing the large batch training problem. The authors propose increasing the batch size based on reductions in the largest eigenvalue of the Hessian. This is combined with adversarial training using the fast gradient sign method to reduce the total number of iterations required for training and improve generalization performance. Unfortunately, although the numerical results seem quite promising, the algorithm and its explanation and details are not described clearly in the paper, which makes me lean towards rejection. I describe this more fully below:

1. Description of the Algorithm

The description of the algorithm in Section 3.1 is simply not clear, and lacks clear exposition motivating why the algorithm ought to work. To add to this confusion, there appear to be some inconsistencies between the (brief) description of the method and the description given in the Main Contributions and Limitations section in the Introduction. 

As an example, in Section 3.1, the approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance <= 10^-2), but this should be elaborated on in Section 3.1. In fact, the limitations should not be discussed until a clear description of the algorithm is given.

The introduction makes this even more confusing by claiming the second order information is computed by “backpropagating the Hessian operator”. This seems to imply that the 3rd derivative information is computed for second-order information. Later in the Introduction, the authors claim to use Hessian matvecs to perform the power iteration. I believe that the authors mean that the Hessian-vector product is obtained by differentiating the product g’v (a scalar quantity). 

In addition, it was not described how the learning rate is changed in the algorithm. Later in the experiments, none of the additional hyperparameters in the procedure are given, such as the duration factor, kappa, the hyperparameters in the adversarial training, and more. This all ought to be included for completeness.

2. Questions about Details of the Algorithm

If it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? In addition, if the intention was to compute the largest eigenvalue of the Hessian, one must be wary that the power iteration/Lanczos method computes the eigenvalue with largest magnitude (the absolute value of lambda), which may mean that it’s possible that the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima. This needs to be addressed.

Secondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian. This is certainly not true for all optimization problems. Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region? This intuition and explanation needs to be expanded on (and argued for) in order for the algorithm to make any conceptual sense.

Lastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? How is the duration factor tuned?

3. Inconsequential Theoretical Results

The authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2] and does not utilize any of the algorithmic mechanisms described in the paper. Hence, the theoretical result is not novel, does not provide any additional insight on the algorithm, and could be applied to any adaptive/changing batch size SG algorithm. In my opinion, this ought to be removed. (Assumption 2 is also mentioned in the main paper, but is only described in the Appendix.)

4. Additional Considerations

The paper is missing much work done by Nocedal’s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3].

Other relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. 

Lastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. It may make sense to plot test loss/error against epochs and batch size against iterations for clarity.

Typos/Grammatical Errors:
- Page 2: Should not state “(We refer to this method as ABS)”, easier to include by including (ABS) after Adaptive Batch Size in the beginning of the bullet point.
- Page 6: Section 4: “information” not “informatino”
- Page 6: Section 4: “the” not “teh”
- Page 7: Section 4.1: “confirms” not “confirming”
- Page 7: Section 4.1: no “a” in “a very consistent performance”

Summary:

Overall, although the paper presents some promising numerical results, it lacks a detailed description and explanation of the algorithm to be worthy of publication. It leaves many aspects of the algorithm open to the reader’s interpretation, and I do not believe I could reproduce the results with the information provided. The manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.

References:
[1] Bollapragada, Raghu, et al. ""A progressive batching L-BFGS method for machine learning."" arXiv preprint arXiv:1802.05374(2018).
[2] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. ""Optimization methods for large-scale machine learning."" SIAM Review 60.2 (2018): 223-311.
[3] Byrd, Richard H., et al. ""Sample size selection in optimization methods for machine learning."" Mathematical programming134.1 (2012): 127-155.
[4] Chaudhari, Pratik, et al. ""Entropy-sgd: Biasing gradient descent into wide valleys."" arXiv preprint arXiv:1611.01838(2016).
[5] Jastrzębski, Stanisław, et al. ""DNN's Sharpest Directions Along the SGD Trajectory."" arXiv preprint arXiv:1807.05031(2018).
[6] Sagun, Levent, et al. ""Empirical Analysis of the Hessian of Over-Parametrized Neural Networks."" arXiv preprint arXiv:1706.04454 (2017).
[7] Zhu, Zhanxing, et al. ""The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent."" arXiv preprint arXiv:1803.00195 (2018).","The review starts with a polite but clear statement that the reviewer leans towards rejection. This indicates a negative sentiment. However, the reviewer acknowledges the promising results, which balances the negativity slightly. The tone throughout is critical but professional and provides concrete examples and suggestions for improvement. Therefore, it's not overly negative or rude.",-40.0,60.0
"Summary:

This paper proposes layer wise training of neural networks using classification auxiliary tasks for training each layer. Experiments are presented on CIFAR10 and Imagenet. Accuracies close to end to end training are obtained. 

The layer wise training is repeated for J steps, the auxiliary tasks are trained on top of the shallow one layer (of width M ) with a network  of depth k and width tilde{M}. Layerwise training is done using sgd with momentum, and the learning rate is decayed through epochs. 

Note that the layer wise training is done with large width M than typical end to end networks in use. 

The authors argue and test the hypothesis that auxiliary tasks  encourage the linear separability of CNN features. 

To reduce the size of the learned network the author propose a layer wise compression using the filter removal technique of Molchanov et al .

Reproducibility:

This empirical  work has been investigated for a while with mild success, the authors should make their code available to the community to confirm and reproduce  their findings.  I encourage the authors to make their code available during the review/discussion period. 


Significance of the work:

From reading the paper it is not clear what is the main ingredient that makes this layer wise training  successful, negative results would help in understanding what is important for the accuracy. 

Some more ablation studies and negative results will be insightful and here are few suggestions in that direction:

- Authors claim that they used invertible downsampling as max or average pooling  lead to information loss. Does the layer wise training give worst results with average or max pooling? If so please report those numbers to know what is the implications of this choice of pooling.  

- On the width of the networks, seems it is key for the success of the approach.  What if  you train wider networks with J that is small? (  J=3 for instance but much  wider networks, instead of J=8 now for imagenet.)

- To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks (smaller M )?

- Would the accuracy  with the layer wise training reach a  plateau if one uses an architecture with J higher than 8? 

- Transferability of the learned features: end to end features are know to be transferable. It would be good to see if this still holds using the network layer wise trained on imagenet for CIFAR10 or other datasets. 

Other Questions:
- Section 3.2 is vague. In Proposition 3.1 and  Proposition 3.2 can you add some text to explain what are the implicitions of the claims? “Thus our training permits to extend some results on shallow CNN to deeper CNNs …” which shallow results ?

- “For k>1 batch normalization was useful “ is this only on the auxiliary problems networks  or you used also batch norm for the layer ?

- The ensemble used is Z=\sum_{j=1}^J 2^j z_j , this uses the network of J layers ,  also the O(J) auxiliary networks  of depth k. Please report the number of parameters for all models (single and ensembles) in Table 1 and Table 2. 

- In the conclusion: “The framework can be extendable to the parallel training …” how would this possible since one needs the output of the first training to do the training of the next layer. can you elaborate on what is meant here?

Minor:
page 2 bottom have competitorsand -> have competitors and 
the non linearity rho in equation 1 and throughout the paper put a bracket for its argument \rho(x) not \rho x
Page 6 , Imagenet paragraph : W —> We
section 4.2 we define linear separability etc… a space is missing before Further 
section 4.3 we report our results .. (Imagenet) a space is missing after ImageNet)

Overall Assessment: 
This is a good paper, making the code available and adding more ablation studies and explanations of width versus depth and the choice of pooling will make the contributions easier to understand. ","The reviewer provides constructive criticism and suggestions for improvement, indicating a positive attitude towards the paper. They highlight the strengths of the work, such as achieving accuracies close to end-to-end training. However, they also point out areas for improvement, like the need for more ablation studies and clearer explanations. The language used is formal, professional, and polite throughout the review.",60.0,80.0
"The paper proposes a hybrid model-free and model-based RL agent for the task of navigation. Reaching the target is decomposed into a set of sub-goals, and the plan is updated as the agent explores the environment. The method has been tested in the House3D environment for the task of RoomNav, where the goal is to navigate towards a certain room. 

The idea of integrating RL agents with semantic knowledge is interesting. However, the paper has several major issues that should be addressed in the rebuttal:

(1) The experiment results in Figure 3 and Figure 4 are based on groundtruth room information. The only experiment that is fully automatic is the one in Figure 5. However, there is no difference between the proposed method and the baselines in that case. So the proposed method is not effective without groundtruth information.

(2) The only evaluation metric that is used is ""Success Rate"". That metric is not sufficient for evaluation of navigation agents since it does not include episode length information. All of the results should be based on the protocol mentioned in ""On Evaluation of Embodied Navigation Agents"", arXiv 2018. 

(3) There is no termination action according to Appendix B. So the agent does not know if it is at the target or not. It seems the agent will stop if it issues ""stay still"" three times. That is different from termination action. Also, it is confusing what 450 pixels means for a scene classifier that works on the image.

(4) The paper is written in a convoluted way:
   (a) It is not clear if the semantic model is trained along with the RL model end-to-end or not.
   (b) Regarding multi-target sub-policies, is there a separate policy for each pair of intermediate targets? 
   (c) Regarding inference and planning on M, what is \tau exactly? How is the length of the plan determined? 
   (d) Why is the model updated only after a fixed number of steps? That increases the episode length. 

(5) The number of T_i's is manually set to 8. That causes serious generalization issues. How do we know how many T_i's exist in a new environment?


Minor comments:
- The paper mentions ""An example of such environments is House3D which contains 45k real-world 3D scenes"". House3D includes only synthetic scenes. They should not be called real-world scenes.
- How is the reward shaping done?

****
Final comments after reading the response and the reviews:

Regarding the fairness of the review, success rate is not sufficient to evaluate navigation agents. A random agent can achieve 100% success if it is given enough time. So it is totally fair to ask for a metric (such as SPL) that is a function of both success rate and episode length. 

I am going to increase the rating to 5 since some of my concerns have been addressed. There are still a number of issues:

- The authors did not run the experiments with the termination action. I disagree that this is orthogonal to the focus of the paper. This is not just an additional action. It indicates whether the agent has learned anything or it is just a combination of better obstacle avoidance and luck. The SPL numbers are so low (maximum SPL is 6.19%) so adding the termination action will probably make the method similar to random.

- There is a huge gap between success rate and SPL numbers. For instance, success rate is 66.4, while SPL is 5.84 (note that for some reason the SPL numbers are multiplied by 10 in the table). I doubt that the agent has learned anything meaningful in comparison to the baseline. I understand that the task is hard, but this gap is so huge.

- A separate policy is trained for each sub-target. This doesn't scale. There should be one policy for all targets. 

","The review starts with a neutral statement, acknowledging the interesting aspects of the paper. However, it quickly lists several major issues, indicating a negative sentiment. While the reviewer provides constructive criticism and suggestions for improvement, the tone is quite direct and critical, especially in pointing out flaws and questioning the effectiveness of the proposed method. The final comments, even after acknowledging the authors' response, maintain a skeptical and somewhat negative tone. The language used is formal and professional, avoiding any personal attacks or disrespectful remarks. However, the directness in pointing out flaws and the use of strong words like ""serious generalization issues"" or ""huge gap"" contribute to a less polite tone.",-30.0,40.0
"In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true. As the authors state, their notion of ""state-of-the-art"" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1. The question is, why one would exlude the mixture-of-softmax approach here? This is clearly misleading.

The authors introduce the idea of past decoding for the purpose of regularization. It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.

The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank. Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets. The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
","The review is highly critical of the paper. It points out a misleading claim about state-of-the-art results and questions the significance and scalability of the proposed method. The reviewer uses strong language like ""clearly misleading"" and questions the validity of the evaluation. While the criticism is direct, it avoids personal attacks and maintains a professional tone.",-60.0,20.0
"
Summary:
The paper proposes an unsupervised method to learn a vector representation for short genomic sequences, so-called kmers (like n-grams in natural language processing). The method learns a representation that will result in a good predictor of kmer counts from the kmer sequence itself. The idea is that neighbouring kmers (from the same gene) would have similar counts (same gene expression), and hence would be embedded near to each other. The paper shows some small empirical experiments for 3 tasks: showing similar genes are close, able to distinguish different genes, and able to detect genomic structural variation.

This is an interesting idea, and would have large impact if done well. However the current approach has multiple weaknesses which leave the proposal less strong that it could be. The paper is written clearly, and while the idea is motivated from word2vec and derivatives, the application to kmers is original.


Overall comments:
- There are two issues conflated in the word scalability:
  1. computational scalability, where the authors need to run the method on a more realistic dataset and show that the LSTM converges.
  2. statistical scalability, which I will expand in the next point.
- The design of finding an embedding that will identify the count given a kmer has several weaknesses, which the paper did not address:
  1. Two genes could have similar expression, hence similar kmer counts, but different kmers.
  2. A kmer can appear in multiple genes, and the total count is the sum of all of them.
  3. Copy number variation (since the paper is interested in cancer) would affect counts
  4. Two kmers with only one or two differences could be due to SNPs. Should they be near or far?
  5. Should we learn a representation for each individual, or a representation for the population? Depending on how the sample id (and hence vector v_i) is used, one can get different effects.
- It seems wasteful that there is no representation learning for each individual, but instead it is just a fixed (arbitrary?) vector in a look up table.
- The choice of embedding dimension 2 seems to be driven by the fact that the authors wanted to visualize. This is tied up with a weakness that the paper does not measure the quality of the embedding, e.g. using reconstruction error. A good approach is to show that the resulting embedding is useful for some other prediction task (which is usually the reason we want to find an embedding). Reporting mean squared error for Figure 4C would also be helpful.

Minor typos/issues:
- page 3, Section 3: does j range over k-mers in x_{ij}? You also use it r_j in the first sentence.
- page 3, Section 3: read length = 100. kmer length = 24. This should be put in the experimental section. Furthermore due to reverse complements, it would be better to have an odd number for k, e.g. 23.
- page 3,4: using angle brackets to mean a pair is uncommon. Suggest tuple (u,v).
- page 4: The notation U in the description of the LSTM can be confused with two other things:
U is the kmer embedding space, and u_{ij} is the embedding vector.
- page 6: In the text you refer to Figure 3A, I assume you mean Figure 3.
- page 6, Figure 3: Unclear what the three columns are. I assume similar to Figure 4, they are three individuals.
- page 6, task 2: It is unclear how the reader can see that the information recovered by kmer2vec is the same information recovered by standard RNA-Seq analysis.
- page 8, first word: Not sure how Figure 4B shows what the sentence is trying to say.
","The review starts with positive remarks, highlighting the interesting idea and clear writing. However, it then delves into multiple weaknesses, suggesting the current approach is lacking. While the language is constructive and provides specific recommendations, the sheer number of concerns raised indicates a negative sentiment. The tone remains professional and polite throughout, focusing on the scientific aspects and offering suggestions for improvement.",-30.0,70.0
"This work tackles the problems encountered by bounded memory storage mechanisms when faced with abundant data, of which much may be irrelevant or redundant. Such a problem is faced in lifelong learning settings, where a limitless data stream must somehow be encoded and stored so as to be useful at later points in time. 

The researchers propose a solution based on “learning what to remember”. That is, rather than encode every observation (which can quickly become problematic), the model learns to replace less important memories. The importance of a memory is determined by its correlation with future reward; a “memory retention policy” is learned via reinforcement learning, wherein the model learns to retain or discard memories based on these actions’ (i.e., retentions) impact on future reward. Experiments to show the effectiveness of this mechanism include gridworld IMaze and Random Mazes, bAbI question answering (task 2), and Trivia QA. 

Altogether the work does well to clearly describe an interesting approach to an important problem. The model is motivated and explained well, and there were no issues with understanding its inner workings. 

Regarding the work’s novelty, there is a precedent for using RL-based write schemes (DNTM from Gulcehre et al, 2016), which the authors point out. I am not entirely convinced that the proposed writing scheme is a substantial addition over this past work, but I am not overly concerned about this since proper due credit is assigned in the paper. Perhaps a bit more discussion about the advantages of the proposed writing scheme could go a long way, since as it stands now, the paper simply claims that this past work “only considers the pairwise relationships between the current data instance and each individual memory”, and I’m not sure how much substance actually underlies this difference.

Unfortunately I think there is a fundamental problem with the work. The model is a proposed solution for problems with vast amounts of streaming data; problems that, presumably, current memory models would struggle with. However, the tasks in the paper do not fall in this domain. Instead, the authors chose to artificially cripple the size of their memory (using, for example, just a handful of memory “slots”) and demonstrate its performance on tasks that are otherwise completely within the realm of being solved by conventional memory models. This is fine as a jumping off point for the research, but for the model to be taken seriously as a valid solution to problems involving such a scale of data that current models cannot even cope, then it needs to show its worth on problems involving such a scale of data that current models cannot cope. 

Demonstrating success here is important for a few reasons. First, such high-data scenarios may involve situations where many, many memories need to be encoded and considered for the future, since they are all useful or necessary for future performance. The experiments do not show whether the model can scale to, say, 100 or 1000 memories, which is within the realm of being “reasonable” for current memory architectures. Second, high-data scenarios may involve an abundant amount of distracting, irrelevant data. This places particularly tough demands on the RL-based writing mechanism, which will undoubtedly face problems with temporal credit assignment if: (a) the time between encoding and retrieval is long, and (b) there is high reward noise in the intermediate time. Thus, the authors should stress-test the components of their model, since these stresses will undoubtedly exist in the problems that the model is proposed to solve.

Some other minor considerations include the following. (1) The use of a single bAbI task is questionable. Why not run the model on the full suite? (2) How do conventional memory models perform on the tasks? Why are the baselines only variants of the proposed model? 

To conclude and summarize, as a proposed solution to scenarios with streams of abundant data -- which the authors claim is a domain that current memory models may struggle -- the proposed model should tackle problems that: 1) have characteristics more reminiscent of these scenarios, and 2) are problems on which current memory models struggle, for the reasons claimed in the paper. In particular, it would be valuable to see model performance on tasks wherein very long stretches of time need to be considered. This is important because it can address questions with memory scaling (how does the model cope with more than a handful of memories?), and issues that would crop up in a reinforcement learning-based approach to memory retention over long time intervals (namely, long-term temporal credit assignment). 
","The review starts with positive sentiment, praising the clarity and motivation of the work. However, it identifies a ""fundamental problem"" in the experimental setup, arguing that the chosen tasks don't reflect the intended use case of the model. The reviewer suggests more challenging experiments and provides constructive criticism. While the criticism is substantial, the tone remains professional and offers pathways for improvement, making it lean towards the positive side overall.",20.0,70.0
"Summary of the paper:
This paper studies using a three-layer convolutional neural network for the XOR detection problem. The first layer consists of 2k 2 dimensional filters, the second layer is ReLU + max pooling and the third layer are k 1s and k (-1)s. This paper assumes the input data is generated from {-1,+1}^{2d} and a margin loss is used for training. 
The main result is Theorem 4.1, which shows to achieve the same generalization error, defined as the difference between training and test error, the over-parameterized neural network needs significantly fewer samples than the non-over-parameterized one. 
Theorem 5.2 and 5.3 further shows randomly initialized gradient descent can find a global minimum (I assume is 0?) for both small and large networks. 


Major Comments:
1.  While this paper demonstrates some advantages of using over-parameterized neural networks, I have several concerns.
This is a very toy example, XORD problem with boolean cube input and non-overlapping filters. Furthermore, the entire analysis is highly tailored to this toy problem and it is very hard to see how it can be generalized to more practical settings like real-valued input. 
2. The statement of Theorem 4.1 is not clear. The probabilities p_+ and p_- are induced by the distribution D. However, the statement is given p_+ and p_-, there exists one D satisfies certain properties. 
3. In Theorem 5.1 and 5.2, the success probability decreases as the number of samples (m) increases. 


Minor Comments:
1. The statement of Theorem 4.1 itself does not show the advantage of over-parameterization because optimization is not discussed. I suggest also adding discussion on the optimization to Sec.4 as well.
2. Page 5, last paragraph: (p1p-1)^m -> (p_+p_-)^m.
3. There are many typos in the references, e.g. cnn -> CNN, relu -> ReLU, xor -> XOR.

","The reviewer raises several significant concerns about the paper, particularly its reliance on a toy example (XOR problem) and the lack of generalizability to more practical settings. They find the main theorem's statement unclear and point out counterintuitive results in other theorems. While the reviewer offers constructive criticism and suggestions for improvement, the overall tone suggests significant weaknesses in the paper. The language used is formal and professional, without resorting to harsh or disrespectful language.",-30.0,70.0
"The authors hypothesize that, under appropriate conditions, neural networks without specific architectural biases trained by model-free reinforcement learning algorithms are capable of learning procedures that are analogous to planning. This is certainly an important area of research in reinforcement learning.

Unfortunately, the approach employed to demonstrate this hypothesis seems flawed, which is why this submission should be rejected in its present form.

The authors suggest that the presence of planning should be accompanied by three observable characteristics: generalization of desired behavior to radically different situations, learning of desired behavior from small amounts of data, and ability to benefit from additional ""thinking"" time. Instead of trying to identify how an environmental model is represented by a network and how it is used for planning, the authors focus on checking for the aforementioned characteristics.

Even after conceding their strong claim despite weak argumentation provided by the authors, there are fundamental experimental issues that make the conclusions of this study unwarranted. Regarding the first two characteristics, the concepts of ""radically different situations"" or ""small amounts of data"" are extremely vague. Basically the authors assume that their problems are difficult enough to require planning. Having solved these problems with their proposed architecture, they conclude that planning must have occurred. Regarding the use of additional ""thinking time,"" the authors claim that the improvement in performance caused by providing additional micro-steps to a recurrent neural network is clear evidence that something analogous to planning is happening, which is obviously not the case.

While it would not be surprising if there was indeed something analogous to planning happening inside the networks under consideration, this paper presents no stronger evidence for this claim than most other deep reinforcement learning papers that tackle complex environments.

Perhaps the most important contribution of this submission is the architecture based on ConvLSTMs proposed by the authors, which apparently surpasses many alternatives, including some biased towards planning. However, surpassing planning models is not strong evidence of planning. When stripped of unwarranted claims made by the authors regarding implicit planning, the proposed architecture does not seem sufficiently novel to warrant acceptance.

The authors should be commended for what was certainly very demanding experimental work, even though it does not support their core claims. Their second most important contribution is the experimental comparison between several recent architectures in a diverse selection of environments. 

The writing is clear and accessible, except possibly for the architectural details described in Section 2.1.2, which do not seem very important. There are also several typos in Appendix D.2.

Regarding related work, the authors mention that Pang and Werbos [1] ""advanced the approach."" But they do not explain how they advanced this approach. In fact, we could not find much about this in the 1998 paper. Also, to our knowledge, ""additional thinking time"" was first proposed in the context of reinforcement learning and planning with two interacting RNNs by Schmidhuber [2, Section: ""more network ticks than environmental ticks""].

[1] Xiaozhong Pang & Paul J. Werbos (1998): Neural Network Design for J Function Approximation in Dynamic Programming
[2] J.  Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. TR FKI-126-90, TU Munich, November 1990.

Perhaps a strongly revised version the paper might become more acceptable if the authors addressed the issues above and especially toned down their claims about having experimentally identified the emergence of planning. Instead they should be extremely careful here, perhaps present this as ""intriguing results,"" and address all possible counter arguments.
","The review starts with a polite but clear statement that the paper's approach is flawed and should be rejected. The reviewer then provides detailed and specific criticism of the methodology and conclusions, using strong language like ""unwarranted"" and ""obviously not the case."" However, they also acknowledge the authors' effort and point out some positive aspects, such as the architecture and experimental work. The overall tone is critical but professional, suggesting the reviewer is trying to be helpful despite the negative assessment. The final paragraph offers a path forward, suggesting a major revision with toned-down claims could make the paper more acceptable. 

Therefore, the sentiment is negative due to the strong criticism and rejection recommendation, but not extremely so due to the constructive elements. The politeness remains relatively high despite the directness of the critique.",-60.0,60.0
"# overview
This paper leverages a consensus based approach for computing and communicating approximate gradient averages to each node running a decentralized version of stochastic gradient descent.

Though the PushSum idea isn't new, its application to distributed SGD and corresponding convergence analysis represents a valuable contribution, and the experimental results indicate a potentially large speedup (in highly variable or latent networks) without substantially sacrificing model accuracy.

The paper itself is reasonably comprehensive but does miss out on comparisons with more recent but equally promising approaches, namely AD-PSGD. 

# pros
* Empirically shown to be significantly faster than SGD, D-PSGD in high-latency, communication bound configurations which is a fairly common real-world setup. There is an accuracy tradeoff at work here, but performance doesn't seem to suffer too much as the node count scales.
* introduces and proves theoretical guarantees for SGP approximate distributed average convergence for smooth, non-convex case, including upper bounded convergence rates.

# cons
* biggest criticism is that AD-PSGD from Lian et al 2018 is not included in experimental comparisons even though the paper is referenced. Authors state that asynchronous methods typically generalize worse than their synchonous counterparts but that isn't what Lian et al found in their comparison with D-PSGD (see table 2 and 3 from their paper). This comparison would be particularly interesting as AD-PSGD also performs well in the high network latency regime that SGP is touted for.
* would've liked to see comparison on other tasks beyond just image classification on ResNet.

# other comments
* Didn't see mention of specific iteration count value(s) K used in experiments or hyperparameters A.3. Since it bounds the convergence rate, this would be important to include.
* Found a few small typos:
  * pg. 5: Relatively -> Relative
  * pg. 7, fig. 2: part -> par
  * pg. 8, sec. 5.3. par. 2: achieves -> achieved
  * pg. 8, sec. 5.3, par. 2: ""neighbors also to increases"" (drop ""to"")
  * pg. 12, sec. A.2: ""send-buffer to filled"" -> ""send-buffer to be filled""","The review is overall positive. The reviewer finds the paper to be a ""valuable contribution"" and highlights the practical benefits and theoretical soundness of the approach. While there are constructive criticisms regarding missing comparisons and minor typos, these are presented in a neutral and professional manner. The reviewer even acknowledges the comprehensiveness of the paper. ",65.0,75.0
"This paper is clearly written and explains everything in a good detail. I have a few questions about the design of the algorithm and experiments that I will explain next. Most importantly, I am confused why the communication actions are modeled with continuous actions. Also, the communicating agent idea is incorporated in MADDPG paper, and the contribution of the proposed network is unclear to me.  Right now, I am leaning toward weak reject now but I might update my evaluations after seeing the authors' feedback.

1) First, your construction of communication medium simply seems to be learning a method for graph sparsification and this deserves some explanation.  Also, I think that using the graph terminology for describing the communication medium structure will significantly improve the clarity of the paper. For example, I assume that by saying that m^t = ...=m^{t+C-1} you mean that you simply fix the communication graph structure for C steps, not the communicating observations. Based on your notations, it is a little bit confusing -- in your notations $m^t$ is the set of observation that flows through the graph which should be different than $m^{t+1}$.

2) Even MADDPG is very challenging to train! Now, this paper utilizes two MADDPG, and that is something that concerns me a lot. I don't think that replicating the results of this paper is possible by other people. How much was the cost of the hyper-parameters search? 

3) Why the decision of where to send the observations is modeled with a continuous control action? It can be simply modeled with discrete action in a more efficient way, right? What I mean is that $c_i$ can be a binary which tells whether send an observation or not. Am I missing anything?

4) In section 2, you argue that in the original MADDPG paper, there is no inter-agent communication. As far as I remember, they have some experiments for cooperative communication or covert communication in which the communication is allowed between the agents. I would like to know more about this statement; maybe I am missing something. Why you are not designing the communication network which is a differentiable medium such as Foerster 2015? Isn't that efficient?

5) In alternating case, I don't see (intuitively) why the communication should help to improve upon MADDPG. My intuition is that each agent will be the gifted one 1/3 on average. This means that the agents cannot perceive who gives the correct information and the policy should converge to a point where the communication does not give any new information.

6) I would like to see what will happen with C=1? I think this hyper-parameter deserves some analysis to see how it affects the performance of the proposed method.

7) In section 5, you say that in original DDPG, there is no real need for inter-agent communication"". This is a little bit strict statement, I guess. For example in the case with full observability, the agents can send messages which conveys their intention and help each other.



Minor:
* I would suggest using partially observability terminology instead of saying noisy observation because I think it includes a more general class of the problems to solve.
* ""that a coupled through a communication medium"" -> ""that are coupled through a communication medium""
* In section 4.2, it is unclear to me what is the exploration strategy. Please explain more.
* section 5.2: Using the term lower bound is not accurate. Try changing it to something else or use with quotations: ""lower bound""
* What will happen you choose the top-k rule for sending the information? For example, does top-2 (two-to-one) rule improve the results? The experiments might be added in future.
-----------------------------------
post rebuttal: the authors have responded to my main questions, and I would like to increase my score, but I cannot agree with them on possile future extensions of this work, e.g. in learning from pixels.","The reviewer provides constructive criticism, acknowledges the paper's clarity, and expresses interest in the research. While they have concerns and lean towards a weak reject initially, their willingness to revise the evaluation after the authors' feedback indicates an openness to persuasion. The tone is professional and inquisitive, suggesting a positive attitude towards improvement rather than outright rejection.",20.0,80.0
"If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments.
---
The paper uses a Seq2Seq network to re-rank candidate items in an information retrieval task so as to account for inter-item dependencies in a weakly supervised manner. The gain from using such a re-ranker is demonstrated using synthetic experiments as well as a real-world experiment on live traffic to a recommender system.

Paragraph below eqn2: for *any* fixed permutation. Figure1 and notation indicates that, at each step of decoding, the selected input x_j is fed to the decoder. The text suggests the embedding of the input e_j is fed to the decoder (which is consistent with ""go"" being a d-dimensional vector, rather than the dimensionality of x).

Single step decoder with linear cost: Is there a missing footnote? If not, why call it p^1? Simpler notation to just call it p.
Eqn7: Role of baseline. In REINFORCE, b(x) is typically action-independent (e.g. approximating the state value function induced by the current policy). L_pi(theta) is action dependent (depends on the permutation sampled from P_theta). So, I'm unclear about the correctness of Eqn7 (does it yield an unbiased policy gradient?)

Eqn5: Expected some discussion about the mismatch between training loss (per-step cross entropy) vs. testing loss (e.g. NDCG@k). Does a suitable choice of w_j allow us to recover standard listwise metrics (that capture interactions, e.g. ERR)?

Expt implementation: Why was REINFORCE optimizing NDCG@10 not regularized?
Expt cascade click model: Expected an even simpler experiment to begin with; [Is the Seq2Slate model expressive enough to capture listwise metrics?] Since the relevances are available, we can check if Seq2Slate trained to the relevance labels yields NDCG performance comparable to LambdaMART, and whether it can optimize metrics like ERR.

Table1: On the test set, is NDCG&MAP being computed w.r.t the ground truth relevances? So, is the claim that Seq2Slate is more robust when clicks are noisy in a one-sided way (i.e. relevant items may not receive clicks)? Not clear how much of this benefit comes from a more expressive model to predict relevances (see suggested expt above) vs. Seq2Slate from clicks. NDCG & MAP definitely don't account for inter-item dependencies, so unclear what is being tested in this experiment.

For diverse-clicks, eta=0 while for similar-clicks, eta>0 (in a dataset dependent way). Why? Can expt numbers for the 3 choices of eta be added to the appendix? [Seems like cherry-picking otherwise]

Can Ai et al, 2018 be benchmarked on the current expt setup? Is it identical to the single-step decoder proposed in the paper?

Comment: Would be more accurate to call seq2slate a re-ranker throughout the text (in the abstract and intro, the claim is seq2slate is a ranker).

Expected to see training time and inference time numbers. Since Seq2Slate does extra computation on top of, e.g. LambdaMART, it is useful to know how scalable it can be during training, and when the extra perf is worth the O(n^2) or O(n) [for single-step decoding] during inference.

General comments:
Clarity: The paper is well written and easy to follow. There are a few notation choices that can be improved/clarified.
Originality: This work seems closely related to Ai et al, SIGIR 2018. Going from a single-shot decoding to sequential decoding is an incremental step; the real-world experiment seemed the most novel and compelling contribution (however, it is unclear how one can reproduce it).
Significance: The paper addresses a significant real-world problem. Many high-impact applications of ranking rely on being able to model inter-dependencies well.
Quality: The paper has interesting contributions, but can be substantially stronger (see some of the specific comments above). For instance, A careful study of the computational vs. performance trade-off, fine-grained comparison of different decoding architectures, better understanding of which architectural choices allow us to model any arbitrary ranking metric more effectively vs. which ones are more robust to click noise vs. which ones capture inter-item dependencies.
","The reviewer explicitly states a positive sentiment: ""If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments."" They provide constructive criticism and suggestions for improvement, indicating a desire to see the paper succeed. The language used is formal, respectful, and focused on the scientific content of the paper.",75.0,90.0
"The problem addressed in this paper is worth the attention of the community. Not so much for it being of strong interest to the majority of ICLR attendees, but due to the fact that it deals with data of origin (finance) and properties (high-order Markov chain dependencies) that have never been considered in the past.

However, apart from this merit, the paper as is stands now suffers from major prohibitive shortcomings. Specifically, the authors have failed to provide a detailed account of the novel network architecture introduced in this paper. Their description is too vague, and misses crucial details. For instance, they use a convolutional structure (layer?) at some point. How is this configured? Why do they need a convolutional layer, since they present it with a vector output from a preceding dense layer? What about the CDA network (both its configuration and its training procedure)? These are crucial aspects that the authors have failed to describe at all.

In the same vein, technical details concerning the selection of the training algorithm hyper-parameters are also missing from the experimental section. Although not as glaring an omission as the previously described ones, these are also crucial for the replicability of the presented research results. 

Finally, the authors have failed to provide comparisons to alternative baselines. For instance, why not train a simple LSTM and use to generate new samples. Why not use a recurrent variational autoencoder? Eventually, since the time-series we are dealing with are governed by a high-order Markov chain, we not fit and sample from a high-order hidden Markov model? These are crucial questions that must be adequately addressed by the authors. ","The review starts with a mildly positive note, acknowledging the relevance of the problem. However, it quickly transitions into a highly critical tone, pointing out major shortcomings. The language used, while direct, maintains a professional and academic tone. The reviewer criticizes the authors for lack of clarity, missing details, and the absence of crucial comparisons, which are considered serious flaws in scientific work. ",-50.0,50.0
"The work releases a large-scale multimodal dataset recorded from the X-Plane simulation, as a benchmark dataset to compare various representation learning algorithms for reinforcement learning. The authors also proposed an evaluation framework based on some simple supervised learning tasks and disentanglement scores. The authors then implemented and compared several representation learning algorithms using this dataset and evaluation framework. 

pros:
1.  Releasing this dataset as a benchmark for comparing representation learning algorithms can potentially impact the community greatly;
2. The authors combined several existing work on measuring representation learning algorithms and proposed an evaluation framework to evaluate the quality of learned representation using supervised learning tasks and disentanglement scores;
3. The authors implemented an extended list of representation learning algorithms and compared them on the dataset;

cons:
1. the paper lacks clarification and guideline to convince the readers of the usefulness of the dataset and the evaluation framework. The authors spent almost half of the space explaining different existing representation learning algorithms. A more convincing story would be to find a few well-established representation learning algorithms to corroborate on the reliability of the dataset and the evaluation metrics;
2. More details should be put into describing the dataset. It is not clear why this dataset is particularly suited for evaluating representation learning in the context of reinforcement learning. Do the authors have insight on the difficulty of the task? While having multi-modality is appreciated, it might worth thinking a separate dataset focusing on a single modality, e.g., image;
3.  Given that the authors designed the dataset for evaluating representation learning for reinforcement learning, it is worth evaluating these algorithms on solving the main task using some standard RL techniques on top of the learned representations.
4. Table 4 is difficult to parse. ","The review acknowledges the potential impact of the dataset and the effort put into the evaluation framework. However, it also raises several concerns regarding clarity, focus, and the need for further validation. The language is constructive and suggestive, not dismissive or aggressive.",20.0,70.0
"Summary
This paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.

Strong points
- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).
- Good performance.
- The paper is generally well written.

Concerns
- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.
- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.
- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?
- Too many hyperparameters to tune.

Minor points
- I think the line above Eq (4) should be like \tilde w_ij = w_ij / sum_k w_ik.
- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.
- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.
","The review acknowledges the strengths of the paper, such as novelty, performance, and writing quality. This points towards a positive sentiment. However, it also raises several concerns, questioning design choices and results. While the concerns are substantial, they are presented constructively, suggesting improvements rather than outright flaws. Therefore, the sentiment is moderately positive, and the language is polite and professional.",60.0,75.0
"This paper presents a multiview framework for sentence representation in NLP tasks. Authors propose two architectures, one using a generative objective, while the other uses a discriminative objective. Both combine a recurrent based encoding function and a linear model. Large experiments have been conducted on several NLP tasks and datasets, showing improvement of the introduced frameworks compared to baselines.

The paper is globally well written and has a clear presentation. But I'm not sure to understand why authors motivate their work on the asymmetric information processing in the two hemispheres of the human brain. It sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation..

My main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation). This is far from significant works on multiview learning, see: ""Multi-view learning overview: Recent progress and new challenges"".

Table 3, where G and D refer respectively to Generative and Discriminative models. But what differences between G1, G2, G3 ; D1, D2, D3 ?

Invertible constraint is a nice idea for using inverse of the decoder as the encoder. Is it really to take advantages of decoder information on the encoder/representation part? Or also to reduce the amount of parameters learnt in the model? Moreover, it is unclear on the ablation study: did you consider the original encoder ; or still the inverse of decoder but without the constraint? Unfortunately, it seems to not give significant gain, according to ablation study in table 6.

In this current form, I feel this paper does not give sufficient novelty to be accepted at ICLR.","The reviewer acknowledges the paper's clarity and experimental rigor but expresses reservations about the novelty and justification for certain aspects. They find the ""multiview"" aspect not sufficiently substantial and question the connection to human brain asymmetry. While they find the ""invertible constraint"" interesting, its impact seems limited based on the ablation study. Overall, the feedback leans towards the negative due to concerns about the paper's contribution to the field.",-25.0,50.0
"The paper considers sparse kernel design in order to reduce the space complexity of  a convolutional neural network. In specifics, the proposed procedure is composed of following steps: 1) remove repeated layers, 2) remove designs with large degradation design, and 3) further remove design for better parameter efficiency.

The paper proposed the composition of group convolution, pointwise convolution, and depthwise convolution  for the sparse kernel design, which seems pretty promising. In addition, the authors discussed the efficiency of each convolution compositions.

I failed to appreciate the idea of information field, I didn't understand the claims that ""For one output tensor, sizes of information fields for all activations are usually the same"". When introducing a new concept, it's very important to make it clear and friendly. The author could consider more intuitive, high level, explanation, or some graphic demonstrations. Also, I couldn't see why this notion is important in the rest of the paper.

Personally I'm so confused by the theorem. It looks like a mathematical over-claim to me. It claims that the best efficiency is achieved when M N = C. However, is it always the case? What is M N \neq C? What does the theorem mean for real applications?

All the reasoning and derivation are assuming the 3 x 3 spatial area and 4 way tensor. I would assume these constant are not important, the paper could be much stronger if there is a clear notion of general results.","The reviewer provides a mixed review. They find the core idea promising and interesting. However, they also express confusion and raise critical questions about the clarity of the ""information field"" concept and the generalizability of the theorem presented. The language used doesn't express strong negativity but highlights areas of significant improvement needed.",20.0,50.0
"This paper presents an iterative approach to separate unobserved distribution signal from a mixture with observed distribution. The proposed approach looks reasonable to me, however, the experiment and analysis are insufficient.
1. At test time, does the input also go through the same number of iterations (10)? I would like to see how the separated results evolve over iterations.
2. It is not clear what is the quality of samples generated by GLO. In the image separation task, GLOM performs better than GAN, but worse in other tasks. Analysis is needed here.
3.  I noticed that only in the music separation task, finetuning is significantly better than vanilla NES. Is it because generative models can synthesize more realistic data samples? For example, would the generator learn to synthesize X+B with temporal synchronization? More analysis is also needed here.

============================

I think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6.","The reviewer finds the approach reasonable, which indicates a positive starting point. However, they raise several concerns and questions, suggesting the paper needs significant improvement.  The final score of 6 (while not directly translatable to the -100 to 100 scale) confirms this - it's above average but not enthusiastic. The language is constructive and professional throughout.",40.0,80.0
"The paper starts by establishing that biases play an important, negligible role in existing DNNs.
Specifically, they help improve classification performance, and networks trained with biases do make use of biases.

Then, the authors recognize that the state of the art DNNs use ReLU and variants, which are a piece-wise linear function.
Over the linear regions, the entire DNN can be collapsed into a single linear model f(x) = Wx + b.

Then the authors argue that the existing gradient-based attribution methods (for interpreting DNNs) often ignore the attribution of the `b` terms in the heatmap.
That is, when backpropagating the DNN outputs back to the input, the gradient of (Wx + b) wrt x is exactly W only (ignoring the contribution of b).

The paper then proposes a method for backpropagating biases.
From the presented results, I only can conclude that bias backpropagation does show a different heatmap compared to regular gradient-based methods.
However, it is unclear how much this BBp result is advancing our understanding of DNNs.
The result for this is still preliminary.

- Clarity
Research is well motivated, and paper presentation shows a nice, coherent story.

- Originality
AFAIK, the direction of looking at bias attribution is novel.

- Significance
The significance of the paper is limited because (1) the paper only considers the positive region of ReLUs; (2) the empirical results are preliminary and do not show a convincing usefulness of BBp.
Suggestions: authors may design a toy dataset or find a dataset that has some inherent biases (e.g. data imbalance) to show that DNNs do capture interesting information in the biases. From there, hopefully the impact of BBp can be clearer.

At the moment, the paper appears not ready for publication.","The reviewer acknowledges the novelty of the approach and clarity of the paper but raises significant concerns about the impact and preliminary nature of the results. They suggest further experiments to strengthen the paper. The language is critical but professional and suggestions are constructive, indicating a slightly negative but not overly harsh sentiment.",-20.0,60.0
"I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter ""drop rate"" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. 

======
The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.

Major concerns:
An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.

Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.

Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.","The reviewer initially leans towards rejection (-20 sentiment) due to the ad-hoc nature of the approach and lack of strong justification. However, the upgraded review and acknowledgment of ""interesting findings"" after rebuttal pull it back towards neutrality. The language remains professional throughout, indicating politeness.",-20.0,60.0
"This work introduces a novel defense method ""Neural Fingerprinting"" against adversarial examples.
In the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern.  The defender can detect if a given query is adversarial or not by checking the pattern at test time.

Strong point:
The strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors.  The organization is good and the idea is clearly stated. 
  
Weak point:
One question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability.

Comments:
The presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example:
https://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf
It would be interesting to mention to these methods in the related work section.

","The review acknowledges the strengths of the work, such as originality, technical soundness, and clear presentation ('The strong point is...', 'The organization is good and the idea is clearly stated'). While it raises a valid question about the defense against a specific attack, it's presented as a point for clarification rather than a flaw ('One question is...', 'If this is correct...'). Additionally, the suggestion to include related work is constructive and helpful. Overall, the tone is neutral and professional, leaning towards the positive due to the praise of the work's quality.",60.0,80.0
"This paper proposes a new architecture for adversarial training that is able to improve both accuracy and robustness performances using an attention-based model for feature prioritization and L2 regularization as implicit denoising. The paper is very clear and well written and the contribution is relevant to ICLR. 

Pros:

- The background, model and experiments are clearly explained. The paper provides fair comparisons with a strong baseline on standard datasets.
- Using attention mechanisms to improve the model robustness in an adversarial training setting is a strong and novel contribution 
- Both quantitative and qualitative results are interesting. 

","The review starts with positive statements such as ""very clear and well written"", ""contribution is relevant"", and ""strong and novel contribution"". It also exclusively lists 'Pros' which are all positive aspects.  Therefore, the sentiment is highly positive. The language used is constructive, professional, and void of any negative or rude phrasing, indicating a high level of politeness.",80.0,90.0
"Summary:
The authors present an empirical analysis of how the size of SGD batches affects neural networks' training time.   

Strengths:
As mini-batches training is highly popular nowadays, the problem emphasized by the authors may have a high impact in the community. Together with recent analysis on the generalization properties of over-parametrized models, the paper may help understand more general open problems of neural networks' training. A nice contribution of the paper is the observation that different phases of scaling behaviour exist across a range of datasets and architectures.  

Weaknesses:
Based on empirical evaluation, the paper cannot make any claim about the generality of the obtained results. Even if the authors' analysis is based on a large set of benchmarks, it is hard to asses whether and how the results extend to cases that are not included in Section 4. In particular, it is not clear how the definition of different training phases can help the practitioner to tune the training parameters, as the size and range of the different regimes depend so strongly on the model's architecture and dataset at hand.  

Questions:
- have the properties of mini-batches training been explored from a formal/theoretical perspective? do those results match and confirm the proposed empirical evaluation?
- how are the empirical results obtained in the experiment section expected to depend on the specific dataset/benchmark? For example,  given a particular architecture, what are the key features that define the three training phases (shape of the nonlinearity, number of layers, underlying distribution of the dataset)?
- what is a  batch size that does not allow one to 'fully utilize our available compute'?
- does the amount of over-parameterization in the model have any effects on the definition of the training phases? How are the results obtained in the paper linked to the generalization gap phenomenon?","The review acknowledges the paper's strengths, such as its relevant topic and interesting observations. However, it also points out significant weaknesses regarding the paper's limited generalizability and lack of practical advice. The reviewer poses several critical questions, indicating a need for further clarification and analysis. Overall, the feedback is constructive but highlights considerable areas for improvement. Therefore, the sentiment leans slightly towards the negative side.",-20.0,70.0
"
[Summary]
This paper proposed an algorithm for zero-shot translation by using both dual learning (He et al, 2016) and multi-lingual neural machine translation (Johnson et al 2016). Specially, a multilingual model is first trained following (Johnson et al 2016) and then the dual learning (He et al 2016) is applied to the pre-trained model using monolingual data only. Experiments on MultiUN and WMT are carried out to verify the proposed algorithm. 

[Details]
1.	The idea is incremental and the novelty is limited. It is a simple combination of dual learning and multilingual NMT. 

2.	Many important multilingual baselines are missing. [ref1, ref2]. At least one of the related methods should be implemented for comparison.

3.	The Pseudo NMT in Table 3 should also be implemented as a baseline for MultiUN experiments for in-domain verification.

4.	A recent paper [ref3] proves that using more monolingual data will be helpful for NMT training. What if using more monolingual data in your system? I think using $1M$ monolingual data is far from enough.

5.	What if using more bilingual sentence pairs? Will the results be boosted? What if we use more language pairs?

6.	Transformer (Vaswani et al. 2017) is the state-of-the-art NMT system. At least one of the tasks should be implemented using the strong baseline.

[Pros] (+) A first attempt of dual learning and multiple languages; (+) Easy to follow.
[Cons] (-) Limited novelty; (-) Experiments are not enough.

References
[ref1] Firat, Orhan, et al. ""Zero-resource translation with multi-lingual neural machine translation."" EMNLP (2016).
[ref2] Ren, Shuo, et al. ""Triangular Architecture for Rare Language Translation."" ACL (2018).
[ref3] Edunov, Sergey, et al. ""Understanding back-translation at scale.""EMNLP (2018). 

I am open to be convinced.

==== Post Rebuttal ===
Thanks the authors for the response. I still have concerns about this work. Please refer to my comments ""Reply to the rebuttal"". Therefore, I keep my score as 5.

","The review is critical of the paper, pointing out limited novelty and insufficient experiments. While it lists some positive aspects like the attempt to combine dual learning with multilingual models and the paper's readability, the numerous concerns and the final score of 5 (likely on a 10-point scale) suggest a negative sentiment. The language, however, remains polite and professional throughout, focusing on constructive criticism and suggestions for improvement.",-30.0,70.0
"This paper proposed a multi-Level framework for learning node embeddings for large-scale graphs. The author first coarsens the graphs into different levels of subgraphs. The low-level subgraphs are obtained with the node embeddings of the higher-level graphs with a graph convolutional neural network. By iteratively applying this procedure, the node embeddings of the original graphs can be obtained. Experimental results on several networks (including one network with ~10M node) prove the effective and efficiency of the proposed method over existing state-of-the-art approaches.   

Strength:
- scaling up node embedding methods is a very important and practical problem
- experiments show that the proposed methods seems to be very effective. 
Weakness:
- the proposed method seems to be very heuristic
- some claims in the papers are wrong according to existing literatures

Overall, the paper is well written and easy to follow. The proposed method is simple but heuristic.  However, the performance seems to be quite effective according to the experiments. The reasons that why the method works need to be better explained, which can significantly the quality of the paper and its impact in the future.

Details:
-- In the introduction part, ""However, such methods rarely scale to large datasets (e.g., graphs with over 1 million nodes) since they are computationally expensive and often memory intensive"". This is not TRUE! In the paper of LINE (Tang et al. 2015). It shows the LINE model can easily scale up to networks with one million nodes with a few hours. 
-- The authors use Equation (7) to learn the parameters of the graph convolutional neural network. I am really surprised that this method works. Especially the learned parameters are shared across different layers. 
-- Have you tried and compared different approaches of graph coarsening?
-- In Figure 2. (a), according to Equation (1), in the second step, the weight of the edge between A and DE should be 2/sqrt(3)*sqrt(4)?","The reviewer acknowledges the importance of the problem tackled and the effectiveness of the proposed method. However, they also point out significant weaknesses, such as the heuristic nature of the approach and the presence of incorrect claims. The detailed comments highlight potential flaws in the methodology and suggest areas for improvement. Overall, the review leans towards the positive side due to the promising results, but it's not overwhelmingly positive because of the identified issues.",30.0,70.0
"The paper proposes a new way of defining CNNs for omnidirectional images. The method is based on graph convolutional networks, and in contrast to previous work, is applicable to other geometries than spherical ones (e.g. fisheye cameras). Since standard graph CNNs are unable to tell left from right (and up from down, etc.), a key question is how to define anisotropic filters. This is achieved by introducing several directed graphs that have orientation built into the graph structure.

The paper is fairly well written, and contains some new ideas. However, the method seems ad-hoc, somewhat difficult to implement, and numerically brittle. Moreover, the method is not equivariant to rotations, and no other justification is given for why it makes sense to stack the proposed layers to form a multi-layer network. 

The results are underwhelming. Only experiments with small networks on MNIST variants are presented. A very marginal improvement over SphericalCNNs is demonstrated on spherical MNIST. I'm confused by the dataset used: The authors write that they created their own spherical MNIST dataset, which will be made publicly available as a contribution of the paper. However, although the present paper fails to mention it, Cohen et al. also released such a dataset [1], which raises the question for why a new one is needed and whether this is really a useful contribution or only results in more difficulty comparing results. Also, it is not stated whether the 95.2 result for SphericalCNNs was obtained from the authors' dataset or from [1]. If the latter, the numbers are not comparable.

The first part of section 3.2 is not very clear. For example, L^l is not defined. L is called the Laplacian matrix, but the Laplacian is not defined. It would be better to make this section more self contained.

In the related work section, it is stated that Cohen et al. use isotropic filters, but this is not correct. In the first layer they use general oriented spherical filters, and in later layers they use SO(3) filters, which allows anisotropy in every layer. Estevez et al. [2] do use isotropic spherical filters.

In principle, the method is applicable to different geometries than the spherical one. However, this ability is only demonstrated on artificial distortions of a sphere (fig 3), not practically relevant geometries like those found fisheye lenses.

In summary, since the approach seems a bit un-principled, does not have nice theoretical properties, and the results are not convincing, I recommend against acceptance of this paper in its current form.


[1] https://github.com/jonas-koehler/s2cnn/tree/master/examples/mnist
[2] Estevez et al. Learning SO(3) Equivariant Representations with Spherical CNNs","The reviewer raises several serious concerns about the paper, including its ad-hoc nature, numerical brittleness, lack of strong results, and clarity issues. While acknowledging some new ideas and good writing, the overall tone is quite negative due to the fundamental flaws pointed out. The language remains professional and not directly offensive, indicating a level of politeness despite the negative criticism.",-60.0,60.0
"This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of “interesting” states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.

Clarity:
I found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? It’s not clear from the algorithm box. Some important details are also left out. The section on Montezuma’s Revenge mentioned that the goal set was initialized using a “custom edge detection algorithm”. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I can’t find it anywhere in the paper. Omissions like this make the method completely unreproducible. 

Novelty:
The idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of “Dynamic Abstraction in Reinforcement Learning via Clustering” by Mannor et al. or of “Learning Options in Reinforcement Learning” by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.

Significance:
I was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezuma’s Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, it’s not clear how this method could address scalability issues. Repeated clustering could become expensive and it’s not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.

Overall quality:
The proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) I’m not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.

Minor comments:
- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.
- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I don’t understand what it means for a problem to have scaling issues.
- Please check your usage of \cite and \citep. Some citations are in the wrong format.
- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\gamma max Q) are held fixed in Q-learning.","The reviewer highlights several shortcomings in the paper, including clarity issues, lack of novelty, limited significance, and potential brittleness in the proposed approach. They find the paper difficult to follow and question the reproducibility of the method. While the reviewer acknowledges some aspects might be novel, they generally perceive the contributions as limited. The tone is critical and points out flaws directly, but maintains a professional and analytical approach without resorting to personal attacks or disrespectful language.",-50.0,50.0
"The authors present an OOD detection scheme with an ensemble of generative models. When the exact likelihood is available from the generative model, the authors approximate the WAIC score. For GAN models, the authors compute the variance over the discriminators for any given input. They show that this method outperforms ODIN and VIB on image datasets and also achieves comparable performance on Kaggle Credit Fraud dataset.

The paper is overall well-written and easy to follow. I only have a few comments about the work.

I think the authors should address the following points in the paper.
- What is the size of the ensemble for the experiments?
- How does the size of the ensemble influence the measured performance?
- It is Fast Gradient Sign Method (FGSM), not FSGM. See [1]. Citing [1] for FGSM would also be appropriate.

Quality. The submission is technically sound. The empirical results support the claims, and the authors discuss the failure cases. 
Clarity. The paper is well-written and easy to follow while providing useful insight and connecting previous work to the subject of study.
Originality. To the best my knowledge, the proposed approach is a novel combination of well-known techniques.
Significance. The presented idea improves over the state-of-the-art.


References
[1] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” in ICLR, 2015.
-------------------
Revision. The rating revised to 6 after the discussion and rebuttal.
 ","The review starts with a positive sentiment, highlighting the novelty and clarity of the paper. The reviewer finds the paper well-written and technically sound, with empirical results supporting the claims. While the reviewer provides constructive feedback and requests clarifications, these are presented politely and aim to improve the paper. The overall tone suggests a positive disposition towards the paper.",75.0,90.0
"The paper presents an unsupervised sentence encoding method based on automatically generating inconsistent sentences by applying various transformations either to a single sentence or a pair and then training a model to classify the original sentences from the transformed ones.

Overall, I like the paper as it presents a simple method for training unsupervised sentence models which then can be used as part of further NLP tasks.

A few comments on the method and results:

- The results on Table 2 shows that supervised methods outperform unsupervised methods as well as the consistency based models with MultiTask having the largest margin. It would've been interesting to experiment with training multi-task layers on top of the sentence encoder and see how it would've performed.
- The detail of the architecture is slightly missing in a sense that it's not directly clear from the text if the output of the BiLSTMs is the final sentence encoding or the final layer before softmax?
- Also I would've thought that the output of LSTMs passed through nonlinear dense layers but the text refers to two linear layers.
- When I first read the paper, my eyes were looking for the result when you combine all of the transformations and train a single model :) - any reason why you didn't try this experiment?
- The paper is missing comparison and reference to recent works on universal language models (e.g. Radford et al 2018, Peters et al 2018, Howard et al 2018) as they rely on more elaborate model architectures and training compared to this paper but ultimately you can use them as sentence encoders. 
- One final note, which could be a subsequent paper is to treat these transformations as part of an adversarial setup to further increase the robustness of a language model such as those mentioned previously.","The reviewer explicitly states ""Overall, I like the paper..."" which indicates a positive sentiment. They provide constructive criticism and suggestions for improvement, but the overall tone is encouraging and suggests they find merit in the work. The language used is formal, respectful, and suggestive rather than demanding, indicating politeness.",75.0,80.0
"Title: DADAM: A consensus-based distributed adaptive gradient method for online optimization

Summary: 

The paper presented DADAM, a new consensus-based distributed adaptive moment estimation method, for online optimization. The author(s) also provide the convergence analysis and dynamic regret bound. The experiments show good performance of DADAM comparing to other methods. 

Comments: 

1) The theoretical results are nice and indeed non-trivial. However, could you please explain the implication to equation (7a)? Does it have absolute value on the LHS? 

2) Can you explain more clearly about the section 3.2.1? It is not clear to me why DADAM outperform ADAM here. 

3) Did you perform algorithms on many runs and take the average? Also, did you tune the learning rate for all other algorithms to be the best performance? I am not sure how you choose the parameter \alpha here. What if \alpha changes and do not base on that in Yuan et al. 2016? 

4) The deep learning experiments are quite simple. In order to validate the performance of the algorithm, it needs to be run on more datasets and networks architectures. MNIST and CIFAR-10 and these simple network architectures are quite standard. I would suggest to provide more if the author(s) have time. 

In general, I like this paper. I would love to have discussions with the author(s) during the rebuttal period. 
","The reviewer provides constructive criticism and acknowledges the strengths of the paper (e.g., ""The theoretical results are nice and indeed non-trivial."", ""In general, I like this paper.""). They also express a desire for further discussion, indicating a positive engagement. However, the review also points out areas for improvement, suggesting the paper is not perfect. Therefore, the sentiment is positive but not overly enthusiastic. The language used is polite and professional throughout, employing constructive phrasing and avoiding harsh criticism.",60.0,80.0
"This paper aims to distinguish between networks which memorize and those with generalize by introducing a new detection method based on NMF. They evaluate this method across a number of datasets and provide comparisons to both PCA and random ablations (as in Morcos et al., 2018), finding that NMF outperforms both. Finally, they show that NMF is well-correlated with generalization error and can be used for early stopping. 

This is an overall excellent paper. The writing is clear and and focused, and the experiments are careful and rigorous. The discussion of prior work is thorough. The question of how to detect memorization in DNNs is one of great interest, and this makes nice steps towards this goal. As such, it will likely have significant impact.  

Major comments:

1) The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.

Minor comments: 

1) While the comparisons to random ablations are mostly fair, it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA. 

2) The error bars on the plots are often hard to see. Increasing the transparency somewhat would be helpful.

Typos: 

1) Section 1, third paragraph: “We show that networks that networks that generalize…” should be “We show that networks that generalize...”

2) Section 3.1, third paragraph: “Because threshold is the…” should be “Because thresholding is the…”

3) Section 3.2, third paragraph: “In the most non-linear case we would…” should be “In the most non-linear case, we would…”

4) Figure 2 caption: “...with increasing level of…” should be “...with increasing levels of…”

5) Section 4.1.1, second to last line of last paragraph: missing space before final sentence

6) Figure 4a label: “Fahsion-MNIST” should be “Fashion-MNIST”
","The reviewer explicitly states that this is an ""overall excellent paper"", ""clear and focused"", ""careful and rigorous"" research with ""significant impact"". This points towards a very positive sentiment. The reviewer provides constructive criticism and suggestions for improvement, all of which are framed politely and professionally.",90.0,100.0
"###### Post-Revision ########################
Thank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. 

Unfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. 
###### Post-Revision ########################

This paper considers arranging the examples into mini-batches so as to accelerate the training of metric embeddings. The
- The paper doesn't have sufficient experimental evidence to convince me that the proposed method is useful. There is no comparison against baselines. The paper is not clearly written or well organized. Detailed comments below:
- For example, when introducing focus and context entities, it would be helpful to give examples of this to make it clearer. 
- In section 3, please clarify that after drawing both positive and negative examples, what is the size of the minibatch for which the gradient is calculated? 
- How do you choose the size of the microbatches? If the microbatch size is too small, then the effect of associating examples is small. 
- In the line, ""Instead, we use LSH modules that are available at the start of training and are only a coarse proxy of the target similarity"" Why are you not iteratively refining the LSH modules as the training progresses? Won't this lead to an improvement in the performance? 
- In the line ""The coarse embedding can come from a weaker (and cheaper to train) model or from a partially-trained model. In our experiments we use a lower dimension SGNS model."" Could you please clarify what is the additioanal computational complexity of the method? This involves additional computational cost? It doesn't seem to me that the results justify this increased computation. Please justify this. 
- In Lemma 3.2, the term s_i is undefined
- ""In early training, basic coordinated microbatches with few or no LSH refinements may be most effective. As training progresses we may need to apply more LSH refinements. Eventually, we may hit a regime where IND arrangements dominate."" This explanation is vague and has no theoretical or empirical evidence supporting it. Please clarify this. 
- Please fix the size of the axes and the legend in all the figures. 
- For figure 1, how is the step-size chosen? What is the dimensionality of the examples?
- From figure 3, it is not clear that the proposed methods lead to significant gains over the independently sampling the examples? Are there any savings in the wall clock time for the proposed methods? Why is there no comparison against other methods that have proposed non-uniform sampling of examples for SGD (like Zhang, 2017)? Are the hyper-parameters chosen in a principled way for these experiments? ","The reviewer outlines several shortcomings in the paper, particularly insufficient experimental evidence and lack of clarity. While they acknowledge improvements after revision, their primary concern regarding experimental validation remains. The tone, while direct and critical, maintains a professional and constructive approach by providing specific recommendations for improvement. ",-20.0,60.0
"This paper proposed a new hashing algorithm with a new loss function. A multi-indexing scheme is adopted for search.  There is one key issue: in general hashing is not good at multi-indexing search for vector-based search in the Euclidean distance or Cosine similarity. The advantage of hashing is reducing the code size and thus memory cost, but it is still not as good as quantization=based approach. 

Here are comments about the experiments.
(1) Table 1: do other algorithms also use multi-indexing or simply linear scan?
(2)  Figure 4: HDT-E is better than PQ. It is not understandable. Something important is missing. How is the search conducted for PQ? Is multi-indexing used? It is also strange to compare the recall in terms of #(distance comparisons). 

","The review starts by pointing out a key issue that makes the paper's proposal questionable. While it does acknowledge the proposed algorithm and its components, the following sentences highlight a fundamental flaw. The reviewer also poses critical questions, indicating skepticism and a need for major clarifications. The language, however, remains technical and professional throughout.",-50.0,50.0
"In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. 

 
(1) a grammar error at ""provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.""

(2) a grammar error at ""This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk""

(3) The sentence ""For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss"" is a little confusing to me. ""h(1)_k, ..., h(N)_k"" is only for one hidden layer, rather than ""each hidden layer"". Right?","The reviewer finds the paper's extension sensible and justified, praising its readability and acknowledging the open question as potentially requiring further research. While pointing out minor areas for improvement like grammar errors and clarifying confusing sentences, the tone remains constructive and overall positive.",75.0,80.0
"The idea proposed in this paper is to aid in understanding networks by showing why a network chose class A over class B.  To do so, the goal is to find an example that is close to the original sample, but belongs to the other class. As is mentioned in the paper, it is crucial to stay on the data manifold for this to be meaningful. In the paper, an approach using a GAN to traverse the manifold is proposed and the experimental evaluation is done on MNIST.

If my understanding is correct the proposed approach requires:
Finding a noise code z_0 such that the GAN generates an image G(z_0) close to the original input x. As a metric L2 distance is proposed.
Find a point close to z_b that is close z_0  s.t. Class B is the most likely class and class A is the second most likely prediction. Specifically it is required that
The log likelihood of but classified as class B with the same log likelihood of class B for G(z_b) is the same as the log likelihood of class A for the input x.
Such that all other classes have a log likelihood that is at least epsilon lower than both the one of class A and class B.

The proposed approach is compared to a set of other interpretability methods, which were 
Grad-Cam, lime, PDA, xGEM on MNIST AND Fashion MNIST data. The proposed evaluation is all qualitative, i.e. subjective. It must also be noted that in the methods used for comparison are not used as originally intended.


Currently, I do not recommend this paper to be accepted for the following reasons.
The idea of using a GAN is to generate images in input space is not novel by itself. Although the application for interpretability by counterfactuals is. It is unclear to me how much of the appealing results come from the GAN model and how much come from truly interpreting the network. I have detailed this below by proposing a very simplistic baseline which could get similar results.
The experimental approach is subjective and I am not convinced by the experimental setup.
On the other hand, I do really appreciate the ideas of traversing the manifold. 

Remarks 
Related work and limitations of existing interpretability methods are discussed properly. Of course, the list of discussed methods is not exhaustive. The work on the PPGAN and the “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks” is not mentioned although it seems very related to the proposed approach to traverse the manifold. What that work sets apart from the proposed approach is that is could be applied to imaganet and not just MNIST. 

Traversing the manifold to generate explanations is certainly a good idea and one that I completely support. One limitation of the proposed approach is that it is unclear to me whether a point on the decision boundary is desirable or that a point that is equally likely is desirable. My reasoning is that the point on the decision boundary is the minimal change and therefore the best explanation. In such a setup, the GAN is still crucial to make sure the sample remains on the data manifold and is not caused by adverarial effects.

The exact GAN structure and training approach should be detailed in this paper. Now only a reference is provided. 

Can you clarify how the constraints are encoded in the optimization problem?

The grad cam reference has the wrong citation

I do not understand the second paragraph of section 4.1. As mentioned in the paper, these other methods were not designed to generate this type of application. Therefore the comparison could be considered unfair. 

I would propose the following baseline. For image x from class A, find image y from class B such that x-y has minimal L2 norm and is correctly classified. Use y instead of the GAN generated image. Is the result much less compelling? Is it actually less efficient that the entire GAN optimization procedure on these relatively small datasets? 


I do have to say that I like the experiment with the square in the upper corner. It does show that the procedure does not necesarrily exploits adversarial effects. However, the baseline proposed above would also highlight that specific square?


Figure 5 shows that multiple descision boundaries are crossed. Is this behaviour desired? It seems very likely to me that it should be possible to move from 9 to 8 while staying on the manifold without passing through 5? Since the method takes a detour through 5’s is this common behaviour?


FINAL UPDATE
--------------------
Unfortunately, I am not entirely convinced by the additional experiments that we are truly looking into the classifier instead of analyzing the generative model. 
I believe this to be currently the key issue that, even after the revision, needs to be addressed more thoroughly before it can be accepted for publication. ","The reviewer expresses significant concerns and does not recommend the paper for acceptance in its current form. While they find the idea of traversing the manifold interesting, they are unconvinced by the experimental setup and results. The reviewer suggests a simpler baseline and questions the novelty of the approach. The language used is professional and not overtly negative, but the criticism is clear and direct.",-50.0,50.0
"The paper addresses the problem of training an object detection network that can achieve good performance on both clean and noisy images.   
The proposed approach is based on a gating network that decides whether
the image is clean or  noisy. in case of  noisy image a denoising  method is applied.  The network components form a mixture of experts architecture and are  jointly trained after a component-level pretraining.
How good is the gate performance? what happen if you use only one of the trained experts for all the clean/noisy  test data? It is not clear how you combined the results of the two experts. Are you computing a weighted average of the original and the enhanced images? Did you try to use a hard decision gating at test time? 
  ","The review starts with a neutral summary of the paper's topic and proposed approach. However, it raises several questions and requests clarifications, indicating potential concerns about the methodology and results. The phrasing of the questions, while direct, maintains a professional and respectful tone. Therefore, the sentiment leans slightly negative due to the uncertainties raised, and the politeness remains neutral.",-20.0,0.0
"Thank you for an interesting read.

The paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial ""reconstruction"" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. 

Experiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed ""reconstruction"" loss. Improvements has been shown on reconstructing input images with significant numbers.

Overall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.

There is one theoretical issue for the defined ""reconstruction"" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). 

This is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].

I think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). 

1. Indeed matching p_{\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.

2. The authors mentioned that picking r(y|x) = \delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section ""...we used a combination of reflecting 10% pad and the random crop to the same image size..."". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. 

In summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).

[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.
[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.","The reviewer finds the paper interesting, the idea novel, and the writing clear. However, they also point out theoretical issues and a lack of discussion on a crucial aspect of the proposed method. While they acknowledge the experimental results, they encourage further investigation and clarification. This suggests a generally positive view with reservations, hence a slightly positive sentiment. The language used is constructive and professional, indicating politeness.",60.0,80.0
"The contribution of the paper is to show that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While the WGAN formulation minimizes the Wasserstein distance of the transformed latent distribution and the empirical distribution which is already a nice measure of ""progress"", having a bound on the likelihood can be interesting.

Pros:
+ I like the entropic GAN formulation and believe it is very interesting as it gives access to the joint distribution of latent and observed variables. 
+ While there are some doubtful statements, overall the paper is well written and easy to read.

Cons:
- The assumption of injectivity of the generator could be problematic, as it might not be fulfilled due to mode collapse.
- I feel the theory is not very deep. Since one has a closed form of the transportation map (Eq. 3.7), the likelihood of the data is obtained by marginalizing out the latent space. However, this assumes that the inner dual maximization problem is solved to stationarity so that Eq 3.7 holds, which is not the case in practice (5 discriminator updates).
- Thus in Sec. 4.1 for the likelihood at various points in training it is not clear what is actually happening.
- Sec 4.3 for unregularized GANs might be problematic. In general, the transportation plan is not a density function, so I'm not certain whether Theorem 1 / Corollary 2 still hold. Furthermore, the heuristic for ""inverting"" G^* is very crude. 

- There are also some minor problematic statements in the paper. While they can be easily fixed, they give me doubts:
  * The original VAE paper is not cited in the introduction for VAEs
  * The 2013 paper by Cuturi cited on page 2 has nothing to do with ""computational aspects of GANs"". It is about fast computation of approximate OT between two discrete prob. measures. 
  * First-order / second-order Wasserstein distance is I think a bit unusual name for W_1, W_2
  * On pg. 4, the point of the entropy term is to make the objective strongly convex. Strict convexity has no computational benefits.
","The reviewer provides both positive and negative points about the paper. They find the topic interesting and the paper well-written. However, they also raise several concerns about the assumptions, theoretical depth, and some inaccuracies. The tone is critical but professional and not disrespectful. Therefore, the sentiment is slightly positive, and the politeness is neutral to slightly positive.",20.0,20.0
"
Brief summary: This work proposes a way to perform imitation learning from raw videos of behaviors, without the need for any special time-alignment or actions present. They are able to do this by using a recurrent siamese network architecture to learn a distance function, which can be used to provide rewards for learning behaviors, without the need for any explicit pose estimation. They demonstrate effectiveness on 2 different locomotion domains. 

Overall impression:
Overall, my impression from this paper is that the idea is to use a recurrent siamese network to learn distances which make sense in latent space and provide rewards for RL. This is able to learn interesting behaviors for 2 tasks. But I think the writing needs significant work for clarity and completeness, and there needs to be many more baseline comparisons. 

Abstract comments:
trail and error -> trial and error

Introduction comments:

Alternative reasons why pose estimation won’t work is because for any manipulation tasks, you can’t just detect pose of the agent, you also have to detect pose of the objects which may be novel/different

Few use image based inputs and none consider the importance of learning a distance function in time as well as space -> missed a few citations (eg imitation from observation (Liu, Gupta, et al))

Therefore we learned an RNN-based distance function that can give reward for out of sync but similar behaviour -> could be good to emphasize difference from imitation from observation (Liu, Gupta, et al) and TCN (Semanet et al), since they both assume some sort of time alignment

Missing related work section. There is a lot of related work at this point and it is crucial to add this in. Some things that come to mind beyond those already covered are:
1. Model-based Imitation Learning from State Trajectories
2. Reward Estimation via State Prediction
3. infoGAIL
4. Imitation from observation 
5. SFV: Reinforcement Learning of Physical Skills from Videos
6. Universal planning networks
7. https://arxiv.org/abs/1808.00928
8. This might also be related to VICE (Fu, Singh et al), in that they also hope to learn distances but for goal images only.
It seems like there is some discussion of this in Section 3.1, but it should be it’s own separate section.

Section 3 comments:
a new model can be learned to match this trajectory using some distance metric between the expert trajectories and trajectories produced by the policy π -> what does this mean. Can this be clarified?
 The first part of Section 3 belongs in preliminaries. It is not a part of the approach. 

Section 3.2
Equations 9 and 10 are a bit unnecessary, take away from the main point

What does distance from desired behaviour mean? This is not common terminology and should be clarified explicitly.

Equation 11 is very confusing. The loss function is double defined.  what exactly Is the margin \rho (is it learned?) The exact rationale behind this objective, the relationship to standard siamese networks/triplet losses like TCN should be discussed carefully. This is potentially the most important part of the paper, it should be discussed in detail.Also is there a typo, should it be || f(si) - f(sn)|| if we want it to be distances? Also the role of trajectories is completely not discussed in equation 11.

Section 3.3 
The recurrent siamese architecture makes sense, but what the positive and negative examples are, what exactly the loss function is, needs to be defined clearly. Also if there are multiple demonstrations of a task, which distance do we use then?

The RL simulation environment is it made in-house, based on bullet or something else?

Data augmentation - how necessary is this for method success? Can an ablation be done to show the necessity of this?

Algorithm 1 has some typos 
- > is missing in line 3
- Describe where reward r is coming from in line 10

Section 4.1
Walking gate -> walking gait

There are no comparisons with any of the prior methods for performing this kind of thing. For example, using the pose estimation baseline etc. Using the non-recurrent version. Using TCN type of things. It’s not hard to run these and might help a lot, because right now there are no baseline comparisons
","The reviewer provides constructive criticism, suggesting improvements in writing clarity, completeness, and baseline comparisons. While they acknowledge the paper's interesting idea and effectiveness in two tasks, the numerous recommendations for improvement indicate a sentiment leaning towards the neutral side of positive. The language used is professional and polite, focusing on areas for improvement without resorting to negativity.",60.0,80.0
"In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to “denoise” a recurrent neural network.  The idea is that for every normal step of an RNN, one induces an additional ""dimension"" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.

MAJOR

The point is taken that RNNs are susceptible to noise due to iterated application of the function. In my experience, countering noise (in the sense of gaussian noise added) isn’t a huge problem in practice because there are many regularization methodologies to handle it. This leads me to the point that I think the experiments need to compare across a number of regularization techniques.  The paper is motivated by discussion of noise, “noise robustness is a highly desirable property in neural networks”, and the experiments show improved performance on smaller datasets, all of which speak to regularization. So I believe comparisons with regularization techniques are pretty important here. 

MODERATE

There is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.

“Training is in complete batches to avoid the noise of mini-batch training.”  Please explain, I guess this is not a type of noise that the method handles? 

What about problems that require graded responses, which is likely anything requiring integration? For example,  what happens in the majority task if the inputs were switched to a non-discrete version, where one must hold analog numbers?


MINOR

Any discussion about the (presumably dramatic) increase in training time due to the attractor dynamics unrolling + additional batching due to noise vectors (if I understood correctly)?

What are your confidence intervals over?  Presumably, we’d like to get confidence over multiple network instantiations.

Pg 1. Articulated neural network? 


QUESTIONS

Does using a the ‘c’ variable as a bias instead of an initial condition really matter? 

How does supervised training via eqn (4) relate to the classic training of Hopfield nets? I assume not at all, but it would be useful to clarify?

What RNN architecture did you use in the Figure 5 simulations (tanh vanilla RNN or GRU?)
","The review is generally positive. The reviewer finds the idea novel and the paper well-written. However, they express concerns about the experiments, mainly the lack of comparison with other regularization techniques. The language used is polite and professional, offering constructive criticism and suggestions rather than harsh judgments.",60.0,80.0
"The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. 

Some of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:
1.	The main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record – why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork & Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?
2.	Another issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals. Therefore, are the sizes used in the experiments reasonable?
3.	In the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016
4.	The theoretical analysis of the algorithm is only implied and not stated clearly
5.	In reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?
6.	In describing the experiment it says that “For K\in\{1000,10000} data points are repeated.” This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?
7.	Since grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds
8.	The results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.

Additional comments
9.	The introduction is confusing since it uses the term “federated learning” as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.
10.	In the abstract the term “differential attacks” is used – what does it mean?
11.	“An independent study McMahan et al. (2018), published at the same time”- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.
12.	In the section “Choosing $\sigma$ and $m$” it is stated that the higher \sigma and the lower m, the higher the privacy loss. Isn’t the privacy loss reduced when \sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. 
13.	At the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.
14.	In the experiments (Table 1), what does it mean for \delta^\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?
15.	The methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?
16.	The citations are used incorrectly, for example “sometimes referred to as collaborative Shokri & Shmatikov (2015)” should be “sometimes referred to as collaborative (Shokri & Shmatikov, 2015)”. This can be achieved by using \citep in latex. This problem appears in many places in the paper, including, for example, “we make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).” Which should be “we make use of the moments accountant as proposed by Abadi et al. (2016).” In which case you should use only \cite and not quote the name in the .tex file.
17.	“We use the same deﬁnition for differential privacy in randomized mechanisms as Abadi et al. (2016):” – the definition of differential privacy is due to Dwork, McSherry, Nissim & Smith, 2006
18.	Notation is followed loosely which makes it harder to follow at parts. For example, you use “m_t” for the number of participants in time t but in some cases,  you use only m as in “Choosing $\sigma$ and $m$”.
19.	In algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. 
20.	Figure 2: I think it would be easier to see the results if you use log-log plot
21.	Discussion: “For K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.” – it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.
22.	In the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan


It is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. 

","The review is highly critical of the paper, pointing out numerous flaws in clarity, methodology, and presentation. The reviewer acknowledges potential in the work but finds it obscured by the issues presented. The language is direct and technical, focusing on the shortcomings rather than praising positive aspects. While firm, the language avoids personal attacks and maintains a professional tone.",-50.0,50.0
"In this work the authors propose an end to end approach for model based reinforcement learning from images, where the main building blocks are locally-linear dynamical systems and variational auto-encoders (VAE). Specifically, it is assumed that the input features (i.e., the images) are generated from a low dimensional latent representation mapped through parametric random functions; the latter are modeled via neural networks. A recognition model based on convolutional neural networks operates on the reverse way and is responsible for projecting the input features to the latent space, in order to proceed with the reinforcement learning task. The variational framework is employed in order to jointly learn the VAE and the linear dynamics on the latent state. As a final step, once the model is fitted a linear quadratic system (LQS) is solved in order to learn the cost function and the optimal policy. 

* The paper is well motivated and tries to solve an interesting problem, that of data-efficient reinforcement learning. The experiments are well picked and demonstrate the advantages of the proposed approach towards solving the task, however, the method is only evaluated on few environments and compared against only a couple of other methods. I would expect a broader evaluation and/or comparison against more methods. Since the model is able to reach TRPO’s performance in much less steps it would be nice to see how it performs against PPO from [Schulman et al. 2017] (at least on the simulated environments). Also, would it make sense to compare against [Levine et al. 2016] that has been evaluated on similar tasks?

[Schulman et al 2017] “Proximal Policy Optimization Algorithms”.
[Levine et al. 2016] “End-to-End Training of Deep Visuomotor Policies”.

* Methodologically, the paper is sound. The model part (as the authors point out) is based on [Johnson et al. 2016] and is well explained. On the other hand, the policy part, and in particular the policy update in Section 4.2 has some issues regarding readability. There is a strong interplay between Section 4.2, Section 2.1 and Appendix D and the authors did not manage to nicely explain what exactly is happening during the update phase. In the beginning the reader has the impression that we are finding the optimal policy via the closed-form LQS. Later on we switch to constrained optimisation for the cost by accounting for the KL divergence between the policy on two episodes. Finally, in the appendix we are back to the original quadratic cost. The authors need to clarify all the above. Also, they need to explicitly mention why they opt for stochastic optimisation (is it because of minibatching?)

* To continue with the policy, in Section 4.2 the authors argue that although the optimal policy can be found in closed form this is not desirable because the policy will overfit the model and will not generalise well in the real environment. I disagree with this statement. If this happens it effectively means that the learned model or the assumption/learning of the linear dynamics is not right. The authors seem to also agree with this since they clearly state in the the experimental section that “... our method does not heavily rely on an accurate model...”. To my understanding, this means that we need to refine the modelling strategy and not learn a sub-optimal policy. I am really interested in the authors opinion on that.

* The above argument is also directly related to the recognition model and learning of the policy in the latent state (I completely agree with that). The recognition network, which in this case is a convolutional neural network, is used as an inference mechanism to project the observations to the latent space. We learn the (variational) parameters of the recognition model by optimising the likelihood’s lower bound. This means that we are “allowed” to overfit the variational parameters as long as the bound gets tighter. This can possibly result in degraded performance during the policy update. Furthermore, the variational distribution of the latent state, i.e., q(z_t | s_t) is assumed to be mean field across time (independent z’s), while clearly this is not the case in the posterior. You somehow mitigate that by augmenting the observed state (feeding consecutive frames to the network), but still this is not ideal. Finally, is there a reason why we only use the mean of the recognition model to fit the cost on the projected latent states? Why are we throwing away the uncertainty? Especially since you do not use an exact solver and follow a stochastic gradient.

* In the end of Section 2.1, the authors argue regarding the fact that the prior work assumes access to a compact low-dimensional representation which does not allow them to perform well on images. Reference is needed.

* In the related work the authors mention modelling bias as a downside of prior work. Can you please elaborate on that? Where does the bias come from and, more importantly, how does your approach overcome this issue?

* In the experiment and specifically in Figure 4 am I right in assuming that the distance to target is measured in actual pixels? Furthermore, why the relevant plot for the reacher task is depicting rewards instead of the distance to target. To me this suggests that the task is not solved. In general what I find very upsetting in the field are plots that only depict accumulated reward for a specific task. There are many situations where the agent learns a weird behaviour that happens to give good rewards (e.g., spinning around the cart-pole), and unfortunately such behaviours are not spotted on the reward plots.

Overall, the paper is nicely presented and definitely an interesting work. However, given the fact that methodologically we have not learned anything new from this paper and in combination with the not satisfying experimental evaluation I warrant for rejection.","The reviewer acknowledges the paper tackles an interesting problem and presents a well-motivated approach. However, they express concerns about the limited experimental evaluation and methodological novelty. The reviewer finds the comparison against other methods insufficient and criticizes the clarity of the policy update explanation. While acknowledging the paper's merits, the reviewer leans towards rejection due to the lack of significant contribution.",-20.0,50.0
"Review for CO-MANIFOLD LEARNING WITH MISSING DATA
Summary:
This paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction.
The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. 
Novelty/Significance:
The overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn’t have a well established solution.
Questions/Clarity:
Smooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn’t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? 
 “Replacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties” – The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function’s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2.
Proposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1.
The authors write: “Missing values can sabotage efforts to learn the low dimensional manifold underlying the data. … As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.” However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue.
Why do the author’s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound.
Equation 5 is not clear that it is the first order taylor approximation. Omega’ is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ?
 “first-order Taylor approximation of a differentiable concave function provides a tight bound on the function” – Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won’t be anything close to tight.
The authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn’t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. 
It is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is.
Figure 4 appears before it is mentioned and is displayed as part of the previous section.
For the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise.
How big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)?
","The review is mixed, leaning slightly towards the negative. While it acknowledges the novelty and significance of the proposed method, it raises several concerns about the clarity, logic, and experimental results. The reviewer points out potential flaws in the algorithm and questions the interpretation of the results, particularly for the lung dataset. The language used is generally professional and polite, but the criticism is direct and pointed. Overall, the review suggests that the paper needs significant revisions before it can be considered for publication.",-20.0,60.0
"# Summary of the paper

Inspired by the success of deep filter banks, this paper presents a designed deep filter bank for graphs that is based on random walks.  More precisely, the technique uses lazy random walks, expressed in terms of the graph Laplacian, and re-frames this in terms of graph signal processing. Similarly to wavelets, graph node features are calculated at different scales and subsequently summed in order to remain invariant under permutations. Several experiments on graph data sets demonstrate the performance of the new technique.

# Review

This paper is written very well and explains its method with high clarity. The principal issues I see are as follows:

- The originality of the contributions is not clear
- Missing theoretical discussion
- The experimental setup is terse and slightly confusing

Concerning the originality of the paper, the differences to Gama et al., 'Diffusion Scattering Transforms on Graphs' are not made clear. Cursory reading of this publication shows a large degree of similarity. Both of the papers make use of diffusion geometry, but Gama et al. _also_ define a multi-scale filter bank, similar to Eq. 4 and 5. The paper needs to position itself more clearly vis-à-vis this other publication. Is the present approach to be seen more as an application of the theory that was developed in the paper by Gama et al.? What are the key similarities and differences? In terms of space, this could be added to Section 3.2, which could be rephrased as a generic 'Differences to other methods' section and has to be slightly condensed in any case (see my suggestions below). Another publication by Zou & Lerman, 'Graph Convolutional Neural Networks via Scattering', is also cited as an inspiration, but here the differences are larger in my understanding and do not necessitate further justification. Last, the publication 'Graph Capsule Convolutional Neural Networks' by Verma & Zhang is also cited for the definition of 'scattering capsules'. Again, cursory reading of the publication shows that this approach is similar to the presented one; the only difference being which features are used for the definition of capsules. I recommend referring to the invariants as 'capsules' and link it back to Verma & Zhang so that the provenance of the terminology is clear.

Concerning the theoretical part of the paper, I miss a discussion of the complexity of the approach. Such a discussion does not have to be long, but in particular since the paper mentions that the applicability of scattering transforms for transfer learning (and also remarks about the universality of them in Section 4), some space should be devoted to theoretical considerations (memory complexity, runtime complexity). This would strengthen the paper a lot, in particular in light of the complexity of other approaches! Furthermore, an additional experiment about the stability of scattering transforms appears warranted. While I applaud the experimental description in the paper (number of scales, how the maximum scale is chosen, ...), an additional proof or experiment in the appendix should deal with the stability. Let's assume that for extremely large graphs, I am content with 'almost-but-not-quite-as-good' classification performance. Is it possible to achieve this by limiting the number of scales? How much to the results depend on the 'right' choice here?

Concerning the experimental setup, I think that the way (average) accuracies are reported at present is slightly misleading. The paper even remarks about this in footnote 2. While I understand the need of demonstrating the universality of these features, I think that the current setup is not optimal for this. I would recommend (in addition to reporting accuracies) a transfer learning setup rather in which the beneficial properties of the new method can be better explored. More precisely, the claim from Section 4, 4th paragraph ('Since the scattering transform...') needs to be further explored. This appears to be a unique feature of the new method. The current experimental setup does not exploit it. As a side-note, I realize that this might sound like a standard request for 'show more experiments', but I think the paper would be more impactful if it contained one scenario in which its benefits over other approaches are clear.

# Suggestions for improvement

The paper flows extremely well and it is clear that care has been taken to ensure that everything can be understood. I liked the discussion of invariance properties in particular. There are only a few minor things that can be improved:

- 'covariant' and 'equivariant', while common in (graph) signal processing, could be briefly explained to increase accessibility and impact
- 'order' and 'layer' are not used consistently: in the caption of Figure 2a, the term 'order' is used, but for Eq. 4 and 5, for example, the term 'layer' is employed. Since 'layer' is more reminiscent of a DNN, I would suggest to use 'order' throughout the paper, because it meshes better with the way the scattering invariants are defined.
- the notation $Sx$ is slightly overloaded; in Figure 2a, for example, it is not clear at first that the individual cascades are supposed to form a *set*; this is only explained at the end of Section 3.1; to make matters more consistent, the figure should be updated and the combination of individual cascades should be made clear
- In Eq. 5, the bars of the absolute value are not set correctly; the absolute value should cover $\psi_j x(v_i)$ and not $(v_i)$ itself.
- minor 'gripe': $\psi^{(J)}$ is defined as a set in Eq. 2, but it is treated as a matrix or an operator (and also referred to as such); this should be more consistent
- The discussion of the aggregation of multiple statistics in Section 3.2 appears to be somewhat redundant in light of the discussion for Eq. 4 and Eq. 5 in the preceding section
- in the appendix, more details about the training of the FCN should be added; all other parts of the experiments are described in sufficient detail, but the training process requires additional information about learning rates etc.","The review is largely positive, highlighting the paper's clarity and well-explained methodology. However, it raises significant concerns about originality and theoretical depth, requesting clearer differentiation from existing work and a more robust theoretical analysis. While acknowledging the experimental descriptions, the reviewer suggests improvements to the setup, particularly advocating for a transfer learning scenario to better demonstrate the method's advantages. The reviewer also provides constructive feedback on minor areas like terminology consistency and notation clarity. Overall, the tone is critical but constructive, aiming to guide the authors towards a stronger publication.",40.0,80.0
"This paper presents a way use using FSA-augmented MDPs to perform AND and OR of learned policies. This idea is motivated by the desirability of compositional policies. I find the idea compelling, but I am not sure the proposed method is a useful solution. Overall, the description of the method is difficult to follow. With more explanations (perhaps an algorithm box?), I would consider increasing my score.

The experiments demonstrate that this method can outperform SQL at skill composition. However, it is unclear how much prior knowledge is used to define the automaton. If prior knowledge is used to construct the FSA, then a missing comparison would be to first find the optimal path through the FSA and then optimize a controller to accomplish it. As the paper is not very clear, that might be the method in the paper. 

Questions:
- How do you obtain the number of automaton states? 
- In Figure 1, are the state transitions learned or handcoded? Are they part of the policy's action space?
- In section 3.2, you state  s_{t:t+k} |= f(s)<c ⇔ f(s_t)<c    What does s without a timestep subscript refer to? Why does this statement hold?

Can you specify more clearly what you assume known in the experiments? What is learned in the automata? In Figure 5, does SQL have access to the same information as Automata Guided Composition?","The reviewer finds the idea compelling, but has concerns about the clarity and potential limitations of the method. They are not fully convinced, leading to a slightly negative sentiment. The language used is professional and polite, asking for clarifications and suggesting improvements.",-20.0,70.0
"In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. 

The proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. 

The downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well?



Minor:

The operator * in (1) is undefined.

What does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one.
","The review starts with a factual summary of the paper's contributions, which is a neutral starting point. While acknowledging the merits of the approach (simplicity, good performance with few samples), the reviewer raises a significant concern about the lack of explanation regarding *why* the method works. They even prompt the authors to elaborate on this point. This suggests the reviewer finds the paper's contributions potentially valuable but lacking in theoretical grounding. The tone remains professional and constructive throughout, even when pointing out the undefined operator and unclear table formatting. Therefore, the sentiment leans slightly towards the positive side due to the acknowledgment of merits, but not overly positive due to the lack of theoretical justification. The politeness remains firmly in the positive range.",30.0,70.0
"This paper introduces LIT, a network compression framework, which uses multiple intermediate representations from a teacher network to guide the training of a student network. Experiments are designed such that student networks are shallower than teacher networks, while maintaining their width. The method is validated on CIFAR-10 and 100 as well as on Amazon Reviews.

The paper is clearly written and easy to follow. The main novelty of the paper is essentially using the teacher intermediate representations as input to the student network to stabilize the training, and applying the strategy to recent networks and tasks.

The authors claim that they are only concerned with knowledge transfer between layers of the same width, that is teacher and student network been designed (by model construction) to have the same number of downsampling operations, while maintaining the same number of stages (referred to as sections in the paper). However, resnet-based architectures have been shown to perform iterative refinement of their features between downsampling operations (see e.g. https://arxiv.org/pdf/1612.07771.pdf and https://arxiv.org/pdf/1710.04773.pdf ). Moreover, these models were also shown to be good regularizers, since they can reduce their model capacity as needed (see https://arxiv.org/pdf/1804.11332.pdf).  Therefore, having experiments skipping stages would be interesting, and may allow to further compress the networks (by skipping layers or stages which do not incorporate much transformation). Following https://arxiv.org/pdf/1804.11332.pdf, for the sake of completeness, it might also be interesting to compare LIT results to the ones obtained by just removing layers in the teacher network which have small weight norms.

In method, the last sentence before ""knowledge distillation loss"" suggests the training of student networks might not be done end-to-end. Could the authors clarify this?
It seems there might be a typo in the KD loss of ""knowledge distillation loss"", equation (2). Shouldn't the second term of the equation be a function of p^T and q^T (with temperature)?

I would suggest changing ""sections"" to stages, as previously introduced in https://arxiv.org/pdf/1612.07771.pdf .

As for the experiments, it would be more interesting to see this kind of analysis on ImageNet (pretained resnet models are readily available).
Figure 3, why not add hint training as well?
Figure 4, what's the dataset used here?

In Section 4.2, it seems that the choice of the IR layer in the analysis could have a significant impact. How was the layer chosen for the ablation study experiments?

There are a few overstatements in the paper:
- page 5, paragraph 2: FitNets proposes a general framework to transfer knowledge from a teacher network to a student network through intermediate layers. Thus, the framework itself does not require the student networks to be deeper and thinner than the teacher network.
- page 6, ""LIT can compress GANs"": authors claim to overcome limitations of KD when it comes to applying knowledge transfer to pixel-wise architecture that do not output distributions. It seems that changing the loss and using a l2 loss instead is a rather minor change, especially since performing knowledge transfer by means of l2 (although at intermediate layers) has already been explored in FitNets.

Please add references for inception and FID scores.
Please fix references format in page 10.","The review is mostly positive. The reviewer acknowledges the clarity and novelty of the paper. While the reviewer suggests several interesting extensions and points out minor limitations and a few overstatements, these are constructive criticisms aimed at improving the paper. The language used is polite and professional throughout, employing a typical academic tone.",60.0,80.0
"Summary of the paper:
This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. ""structured"" means that instead of just minimizing the L2 norm of the gradients, a ""mahalanobis norm"" is minimized. The covariance matrix is updated continuously to track the ""structure"" of gradients/perturbations. Whitebox attack and blackbox attack 

The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting.

However, I believe the paper has major flaws in several aspects.

The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challenge
I noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST.

In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because ""intrinsic"" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated.
So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.

There're also a few problems in the motivation / analysis. 
""""""A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations.""""""
The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss.

""""""Thus, under the assumption that \phi \approx \phi^* and of small perturbations (such that we can ignore higher order terms.""""""
The Bayes optimal assumption seems to be arbitrary to me. If \phi is nearly Bayes-optimal, why would we worry about adversarial examples?



Other relatively minor problems

In the caption of Figure 1, """"""Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure.""""""
PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse?

In Section 3.1, the paper talks about both centered and uncentered adversarial examples.
I assumed that the authors mean that the distribution of perturbations are centered?
First, I think this the authors should make this more explicit.
Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered?

Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper.

I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved?

Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.","The reviewer acknowledges the paper is well-written with clear explanations of theory and experiments. However, they express significant concerns about the paper's core arguments and experimental methodology. They find flaws in the robustness evaluation, particularly the reliance on a weak white-box attack and the averaging of results across epsilons. The reviewer also questions the motivation and analysis, finding fault with the focus on black-box robustness and the assumptions made about the adversarial robustness problem. While the reviewer lists several minor issues, their overall tone and the severity of the major concerns point towards a negative sentiment. Despite the criticism, the language remains professional and avoids personal attacks, indicating a neutral to polite approach.",-50.0,50.0
"Summary: Given two sets of data, where one is unlabelled and the other is a reference data set with a particular factor of variation that is fixed, the approach disentangles this factor of variation from the others. The approach uses a VAE whose latents are split into e that represents the factor of variation and z that represents the remaining factors. A symmetric KL loss that is approximated using the density-ratio trick is optimised for the learning, and the method is applied to MNIST digit style disentangling and AffectNet facial expression disentangling.

Pros:
- Clearly written
- Results look promising, both quantitative and qualitative.

Cons:
- Mathieu et al disentangle a specific factor from others without explicit labels but by drawing two images with the same value of the specified factor (i.e. drawing from the reference set) and also drawing a third image with a any value of the specified factor (i.e. drawing from the unlabelled set). Hence their approach is directly applicable to the problem at hand in the paper. Although Mathieu et al use digit/face identity as the shared factor, their method is directly applicable to the case where the shared factor is digit style/facial expression. Hence it appears to me that it should be compared against.
- missing reference - Bouchacourt - explicit labels aren’t given and data is grouped where each group shares a factor of var. But here the data is assumed to be partitioned into groups, so there is no equivalent to the unlablled set, hence difficult to compare against for the outlined tasks.
- Regarding comparison against unsupervised disentangling methods, there have been more recent approaches since betaVAE and DIP-VAE (e.g. FactorVAE (Kim et al) TCVAE (Chen et al)). It would be nice to compare against these methods, not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers.

Other Qs/comments
- the KL terms in (5) are intractable due to the densities p^u(x) and p^r(x), hence two separate discriminators need to be used to approximate two separate density ratios, making the model rather large and complicated with many moving parts. What would happen if these KL terms in (5) are dropped and one simply uses SGVB to optimise the resulting loss without the need for discriminators? Usually discriminators tend to heavily underestimate density ratios (See e.g. Rosca et al), especially densities defined on high dimensions, so it might be best to avoid them whenever possible. The requirement of adding reconstruction terms to the loss in (10) is perhaps evidence of this, because these reconstruction terms are already present in the loss (3) & (5) that the discriminator should be approximating. So the necessity of extra regularisation of these reconstruction terms suggests that the discriminator is giving poor estimates of them. The reconstruction terms for z,e in (5) appear sufficient to force the model to use e (which is the motivation given in the paper for using the symmetric KL), akin to how InfoGAN forces the model to use the latents, so the necessity of the KL terms in (5) is questionable and appears to need further justification and/or ablation studies.
- (minor) why not learn the likelihood variance lambda?

************* Revision *************
I am convinced by the rebuttal of the authors, hence have modified my score accordingly.","The review is mostly positive, highlighting the paper's clarity and promising results. However, it raises valid concerns about missing comparisons with relevant methods and questions the necessity of certain components in the proposed approach. The reviewer's tone is critical but professional and seeks clarification and improvements. The revision part suggests a positive shift in sentiment after the authors' rebuttal, indicating a constructive dialogue.",50.0,70.0
"The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. 

The paper has, however, three major problems:

The motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. 

I’m not sure about the correctness of some of the theorems. For instance, Theorem 2 states 
“For any fixed k > 0, there exists a function f(·) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.”  I’m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven’t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. 

The experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets?
If you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. ","The review acknowledges the relevance of the problem tackled in the paper, which is a positive point. However, it then goes on to point out three major problems: lack of motivation, potential incorrectness of theorems, and insufficient experiments. The reviewer is quite critical, using phrases like ""somewhat lacking,"" ""I'm not sure about the correctness,"" and ""The experiments are insufficient."" This suggests a negative sentiment overall. However, the reviewer does offer constructive criticism and suggestions for improvement, maintaining a professional and polite tone throughout.",-30.0,50.0
"This paper builds on the (Alemi et al 2018) ICML paper and presents a formal framework for representation learning. The authors use a graphical model for their representation learning task and use basic information theoretic inequalities to upper-bound their measure of performance which is a KL divergence. The authors then define the optimal frontier which corresponds to the lowest possible upper-bound and write it as an optimization problem. Written with Lagrange multipliers, they obtain several known cost functions for different particular choices of these parameters.
Then the authors make a parallel with thermodynamics and this part is rather unclear to me. As it is written, this section is not very convincing:
- section 4.1 after equation (27) which function is 'smooth and convex'? please explain why.
- section 4.1 '...the actual content of the law is fairly vacuous...'
- section 4.2 the explanation of equation (30) is completely unclear to me. Please explain better than 'As different as these scenarios appear (why?)...'
- section 4.2 'Just as in thermodynamics, these susceptibilities may offer useful ways to characterize...'
- section 4.2 'We expect...'
- section 4.3 ends with some unexplained equations.
As illustrated by the examples above, the reader is left contemplating this formal analogy with thermodynamics and no hint is provided on how to proceed from here. 

","The review starts with a neutral summary of the paper's content. However, it quickly points out several issues with clarity, particularly concerning the thermodynamics analogy. The reviewer finds this section unconvincing and uses phrases like 'rather unclear,' 'not very convincing,' and 'completely unclear.' The repeated use of such phrases and direct questions directed at the authors indicate a negative sentiment. While the reviewer points out specific areas for improvement, the tone remains professional and avoids harsh language, suggesting politeness.",-40.0,60.0
"This paper seems to be an exposition on the primary performance affecting aspects of generative adversarial networks (GANs).  This can possibly affect our understanding of GANs, helping practitioners get the most in their applications, and perhaps leading to innovations that positively affect GAN performance.

Normally, expositions such as this I find difficult to recommend for publication. In these times, one can find ""best practices"" with a reasonable amount of rigor on data science blogs and such. An exposition that I would recommend for publication, would need to exhibit a high sense of depth and rigor for me to deem it publication worthy. This paper, for me, achieves this level of quality.

The authors start off by giving a precise, constrained list of hyperparameters and architectural components that they would explore. This is listed in the title and explained in detail in the beginning of the paper. The authors are right in explaining that they could not cover all hyperparameters and chose what I feel are quite salient ones. My one ask would have been a survey of how activations might affect performance. I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.

The authors then explain the metrics for evaluation and datasets. The datasets offered a healthy variety for typical image recognition tasks. It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).

The  authors explain, with graphs, the results of the loss, normalization, and architectures. I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied. Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader. The only observation I gained as far as this is that non-saturating loss would possibly be stable across various datasets.

Regularization and normalization are discussed in much more detail, and I think the authors made helpful and interesting observations, such as the benefits of spectral normalization and the fact that batch normalization in the discriminator might be a harmful thing. These are good takeaways that could be useful to a vast number of GANs researchers.

For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail. I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance. Unless I am misunderstanding something, it seems that the authors simply tested one more architecture, for the express purpose of testing whether their observations about normalization would hold.

As a bonus, the authors bring up some problems they had in making comparisons and reproducing results. I think this is an extremely important discussion to have, and I am glad that the authors detailed the obstacles in their journey. Hopefully this will inspire other researchers to avoid adding to the complications in this field.

The graphs were difficult to parse. I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion. In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs. If this can be changed before publication, I would strongly suggest it.

I appreciate that the authors provided source code via GitHub. However, in the future, the authors should be careful to provide an anonymous repository for review purposes. I had to be careful not to allow myself to focus on the author names which are prominent in the repository readme, and one of whom has his/her name in the GitHub URL itself. I didn't immediately recognize the names and thus it was easy for me not to retain them or focus on them. However, if it had been otherwise, it might have risked biasing the review.

In all, I think this is a good and useful paper from which I have learned and to which I will refer in the future as I continue my research into GANs and VAEs. I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures). But altogether, I believe this is a paper worth publishing at ICLR.","The reviewer explicitly states this is a ""good and useful paper"" and that they ""learned"" from it and will ""refer to it in the future."" They recommend the paper for publication but suggest some improvements. This points to a positive sentiment overall. The language used is constructive, professional, and polite throughout the review.",75.0,90.0
"-> Summary

The authors propose to extend the analysis of Shwartz-Ziv & Tishby on the information bottleneck principle in artificial neural network training to realistic large-scale settings. They do so by replacing otherwise intractable quantities with tractable bounds in forms of classifiers for I(y;h) and Pixel CNNs for I(x;h). In conclusion, they observe two phases during training, one that maximizes mutual information between input and hidden representation and a second one that compresses the representation at the end of training, in line with the predictions from toy tasks of Shwartz-Ziv & Tishby.

-> Quality

The paper is very well written, all concepts are well-motivated and explained.

-> Significance

The main novelty is to replace intractable quantities in the analysis of the information bottleneck with tractable bounds in form of auxiliary models. The idea is neat and makes a lot of sense. On the other hand, some of the results and the bounds themselves are well-known and can thus not be considered novel. The main contribution is thus the empirical analysis itself and given some overly confident claims on qualitative results and missing ablation on the quantitative side, I am not convinced that the overall results are very conclusive.

-> Main Concerns

The authors make a lot of claims about the qualitative diversity of samples from deeper layers h4 of the network as compared to h1 and h2. However, I do not agree with this. When I look at the samples I see a lot of variations early in training and also in layers h1 and h2. The difference to h4 seems marginal at best and not as clear cut as the authors present it. Thus, these claims should be softened.

In figure 1 I tend to say that samples at epoch 1 are more varied than at epoch 200. In figure 5 (b) seems pretty color invariant and not only (f) as claimed. In fact (f) seems pretty stable and consistent to me.

The bound in equation (2) might be quite loose, depending on the quality of the classifier or pixel CNN. Even though there is no way to test this, it should be discussed.

What is the effect of weight decay here? I suspect that weight decay plays a crucial role in the final compression phase observed in e.g. figure 3 (c), but might not be a necessary condition to make the network generalize. An ablation experiment verifying or falsifying this statement would be important to conduct and without it I am not convinced that the shown curves are conclusive.

-> Minor

- You seem to use a weird math font, is this on purpose? It does not seem to be the ICLR standard.
- The bound in equation (2) is a standard variational bound and has been used many times, the authors make it sound like it is their contribution. You should maybe cite basic work and recent work on variational information bottleneck here.","The reviewer provides both positive and negative points. They praise the paper's clarity, the neat idea, and well-motivated concepts. However, they also express concerns about the novelty of some results, find some claims overly confident, and point out missing ablation studies. Overall, the feedback is mixed but leans slightly towards the positive due to the appreciation of the paper's core idea and presentation.",20.0,70.0
"First off, the paper presents a relatively straight-forward extension to video from the work done in image compression. The work uses 3D volumes instead of 2D images, and exploits this structure by adding a secondary network to both the encoder/decoder.

The work is therefore *marginally* novel, but it is one of the first to propose neural methods for compressing video.

My biggest complaint about this paper, however, is about evaluation. I don't think it's possible to take this paper seriously as is, due to the fact that the metrics use in the evaluation are absolutely skipped.

Given that this is such a crucial detail, I don't think we can accept this paper as is. The metrics need to be described in detail, and they should follow some previously used protocols (see below). 

For example, in libvpx and libaom (which is the current best performing method for video compression - AV1), there are two versions of PSNR: Global and Average PSNR respectively, and this is what gets reported in publications/standards meetings.

Global PSNR: Compute MSE for the entire sequence combining Y, Cb, Cr components, and then compute PSNR based on the combined MSE.
Average PSNR: Compute MSE for each frame combining Y, Cb, Cr, components; then compute PSNR for the frame based on the combined MSE and cap it to a max of 100. Then average the PSNR over all the frames.

MPEG uses something like computing Average PSNR for each component (similar to what I mentioned above, but for each component) and then combine the Y-, Cb- and Cr- PSNRs using a weighted average. For 420 that will be equivalent to [4*MSE(y) + MSE(Cb) + MSE(Cr)/6. For 422 that will be equivalent to [2*MSE(y) + MSE(Cb) + MSE(Cr)/4. For 444 that will be equivalent to [MSE(y) + MSE(Cb) + MSE(Cr)/3.  Additionally, when using YCbCr, the authors also need to refer to which version of the color standard is employed, since there are multiple ITU recommendations, all of which differ in how to compute the color space transforms.

Please note that video codecs DO NOT OPTIMIZE FOR RGB reconstruction (humans are much more sensitive to brightness details than they are to subtle color changes), so comparing against them in that color space puts them at a distinct disadvantage. In the video compression literature NOBODY reports RGB reconstruction metrics.

Please note that I computed the PSNR (RGB) for H.264, on the resized MCL-V dataset (640x360) as the authors proposed and I observed that the metric has been ***MISREPRESENTED*** by up to 5dB. This is absolutely not OK because it makes the results presented not be trustworthy at all.

Here is the bpp/RGB PSNR that I obtained for H.264 (for completeness, this was computed as follows: used version 3.4.2 of ffmpeg, and the command line is ""ffmpeg -i /tmp/test.y4m -c:v h264 -crf 51 -preset veryslow"", tried many settings for crf  to be able to get roughly the same bpp per video, then compute RGB PSNR for each frame per video, aggregate over each video, then average cross videos):

BPP, Average PSNR RGB (again, not a metric I would like to see used, but for comparison's sake, I computed nonetheless -- also, note that these numbers should not be too far off from computing the average across all frames, since the video length is more or less the same)):
0.00719, 23.46
0.01321, 26.38
0.02033, 28.92
0.03285, 31.14
0.05455, 33.43

Similar comments go for MS-SSIM. 

Lastly, it is unfair to compare against H263/4/5 unless the authors specify what profiles were used an what kind of bitrate targeting methods were used. ","The review starts with a neutral tone, acknowledging the straightforward nature of the work. However, it quickly transitions into a highly critical assessment, using strong language like ""biggest complaint"", ""absolutely skipped"", ""not possible to take this paper seriously"", ""absolutely not OK"", ""MISREPRESENTED"", and ""unfair"". The reviewer's frustration and negative sentiment are palpable due to the perceived flaws in the evaluation methodology.",-50.0,-40.0
"The paper considers the problem of obtaining reliable predictive uncertainty estimates. The authors propose noise contrastive priors — the idea being to explicitly encourage high uncertainties for out of distribution (OOD) data through a loss in the data space.  OOD data is simulated by adding noise to existing data and the model is trained to maximize the likelihood wr.t. training data while being close in the KL sense to a (wide) conditional prior p(y | x) on the OOD responses (y).  The authors demonstrate that the procedure leads to improved uncertainty estimates on toy data and can better drive active learning on a large flight delay dataset.

The paper is well written and makes for a nice read. I like the idea of using “pseudo” OOD data for encouraging better behaved uncertainties away from the data. It is nice to see that even simple schemes for generating OOD data (adding iid noise) lead to improved uncertainty estimates. 

My main concern about this work stems from not knowing how sensitive the recovered uncertainties are to the OOD data generating mechanism and the parameters thereof. The paper provides little evidence to conclude one way or the other.  The detailed comments below further elaborate on this concern.

Detailed Comments: 
a) I like the sensitivity analysis presented in Figure 4, and it does show for the 1D sine wave the method is reasonably robust to the choice of \sigma_x. However, it is unclear how problem dependent the choice of sigma_x is. From the experiments, it seems that \sigma_x needs to be carefully chosen for different problems, \sigma^2_x < 0.3 seems to not work very well for BBB + NCP for the 1D sine data, but for the flight delay data \sigma^2_x is set to 0.1 and seems to work well. How was \sigma_x chosen for the different experiments?

b) It is also interesting that noise with a shared scale is used for all 8 dimensions of the flight dataset. Is this choice mainly governed by convenience — easier to select one hyper-parameter rather than eight? 

c) Presumably, the predictive uncertainties are also strongly affected by both the weighting parameter \gamma and the prior variance sigma^2_y . How sensitive are the uncertainties to these and how were these values chosen for the experiments presented in the paper? 

d) It would be really interesting to see how well the approach extends to data with more interesting correlations. For example, for image data would using standard data-augmentation techniques (affine transformations) for generating OOD data help over adding iid noise. In general, it would be good to have at least some empirical validation of the proposed approach on moderate-to-high dimensional data (such as images).

==============
Overall this is an interesting paper that could be significantly strengthened by addressing the comments above and a more careful discussion of how the procedure for generating OOD data affects the corresponding uncertainties.","The review starts with positive sentiment, praising the paper's clarity, novelty, and promising results on simple datasets. However, the main concern revolves around the sensitivity of the method to its hyperparameters and the lack of evidence for its effectiveness on more complex data. This concern, phrased as constructive questions and suggestions for further investigation, shifts the sentiment towards neutral. The reviewer acknowledges the potential but needs more convincing evidence for robustness and wider applicability. The language remains polite throughout, employing a professional and constructive tone.",50.0,80.0
"Analysis of Spectral Bias of ReLU networks

The paper uses Fourier analysis to study ReLU network utilizing its continuous piecewise linear structure.

Main finding is that these networks are biased towards learning low frequency which authors denote `spectral bias’.  This provides another theoretical perspective of neural networks preferring more smooth functions while being able to fit complicated function. Also shows that in terms of parameters networks representing lower frequency modes are more robust. 

Pro: 
- Nice introduction to Fourier analysis providing non-trivial insights of ReLU networks.
- Intuitive toy experiments to show spectral bias and its properties 
- Thorough theoretical analysis and empirical support

Con: 
- The analysis is clearly for ReLU networks although the title may provide a false impression that it corresponds to general networks with other non-linearities. It is an interesting question whether the behaviour characterized by the authors are universal. 
- At least for me, Section 4 was not as clearly presented as other section. It takes more effort to parse what experiments were conducted and why such experiments are provided.
- Although some experiments on real dataset are provided in the appendix, I personally could not read much intuition of theoretical findings to the networks used in practice. Does the spectral bias suggest better way of training or designing neural networks for example?

Comments/Questions:
- In Figure 1, two experiments show different layerwise behaviour, i.e. equal amplitude experiment (a) shows spectral norm evolution for all the layers are almost identical whereas in increasing amplitude experiment (b) shows higher layer change spectral norm more than the lower layer. Do you understand why and does Fourier spectrum provide insights into layerwise behaviour?
- Experiment 3 seems to perform binary classification using thresholding to the logits. But how do you find these results also hold for cross-entropy loss?
“The results confirm the behaviour observed in Experiment 2, but in the case of classification tasks with categorical cross-entropy loss.”


Nit: p3 ReLu -> ReLU / p5 k \in {50, 100, … 350, 400} (close bracket) / p5 in Experiment 2 and 3 descriptions the order of Figure appears flipped. Easier to read if the figure appears as the paper reads / p7 Equation 11 [0, 1]^m


********* updated review *************

Based on the issues raised from other reviewers and rebuttal from authors, I started to share some of the concerns on applicability of Thm 1 in obtaining information on low k Fourier coefficients. Although I empathize author's choice to mainly analyze synthetic data, I think it is critical to show the decays for moderately large k in realistic datasets. It will convince other reviewers of significance of main result of the paper.
","The review is mostly positive, highlighting the novelty and insights from the Fourier analysis of ReLU networks. The reviewer acknowledges the theoretical and empirical contributions. However, they express concerns about the generalizability of the findings beyond ReLU networks and find Section 4 less clear. The reviewer also seeks clarification on certain experimental results and their implications for practical applications. The updated review, while still positive, introduces a note of reservation. The reviewer, influenced by other reviewers, expresses concern about the applicability of the main theorem to realistic datasets, suggesting the need for further validation in this context.  The language remains polite throughout, employing constructive criticism and seeking clarification rather than stating outright flaws.",60.0,80.0
"This paper uses deep reinforcement learning (DRL) for malware detection. It can get better performance than LSTM or GRU based models.

Deep reinforcement learning (DRL) has already used for classification or detection. I am not sure about the main contribution of this work. The new application of DRL can not convince me.

As the dataset is not a public dataset, it is difficult to evaluate the performance. As for the comparing models, i think some CNN based methods should be included. If the task is a detection, i think some attention methods should also be investigated and compared. LSTM combined with attention should already be well investigated in other classification/detection tasks.","The reviewer acknowledges the technical improvement (better performance than LSTM or GRU) but expresses uncertainty about the paper's contribution. They find the application of DRL not convincing enough and point out the lack of comparison with other relevant methods (CNN, attention). The language, while direct, maintains a professional and neutral tone.",-20.0,50.0
"This paper empirically finds that the distribution of activations in quantized networks follow  Gaussian or Laplacian distribution, and proposes to determine the optimal clipping factor by minimizing the quantization error based on the distribution assumption.

The pros of the work are its simplicity, the proposed clipping and quantization does not need additional re-training. However, while the key of this paper is to determine a good clipping factor, the authors use uniform density function to represent the middle part of both Gaussian and Laplacian distributions where the majority of data points lie in, but exact computation for the tails of the distributions at both ends. Thus the computation of quantization error is not quite convincing. Moreover, the authors do not compare with the other recent works that also clip the activations, thus it is hard to validate the efficacy of the proposed method.

For the experiments, the authors mention that a look-up table can be pre-computed for fast retrieval of clipping factors given the mean and sigma of a distribution.  However, the mean and sigma are continuous numbers, how is the look-up table made?  Moreover, how is the mean and std estimated for each weight tensor and what is  the complexity?
","The review starts with a neutral summary of the paper's contributions. It then acknowledges the pros of the work, using terms like ""simplicity"" which indicates a somewhat positive view. However, the review then goes on to point out several significant weaknesses, using terms like ""not quite convincing"" and ""hard to validate"".  The questions at the end, while not phrased overly harshly, suggest skepticism and demand clarification. Overall, the tone is critical and points out serious concerns, suggesting the reviewer leans towards recommending revisions. Therefore, the sentiment is somewhat negative but not overly so.",-30.0,60.0
"The author proposed an extended version of MNIS where they introduced thickening/thinning/swelling/fracture. The operation is done using binary morphological operations.

* Providing benchmark data for tasks such disentanglement is important but I am not sure generating data is sufficient contribution for a paper. 
* I am not sure what conclusion I should draw from Fig 5 and Fig 6 about the data.
* Eventually this data can become a benchmark data when it is paired with a method. Then that method/data are a benchmark.

 ","The review starts with a neutral statement, summarizing the paper's proposal. However, the reviewer expresses doubt about the paper's contribution ('not sure generating data is sufficient'), questions the clarity of the results ('not sure what conclusion I should draw'), and suggests the work is incomplete ('Eventually this data can become a benchmark...'). These indicate a negative, but politely expressed, sentiment. The reviewer doesn't use harsh language and offers constructive criticism.",-50.0,70.0
"# Summary

This paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work.


# Strengths

Controlled toy experiments of Deep RL generalization issues:
The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper.

Investigating the impact of different GANs on the end task:
The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below).


# Weaknesses

Discrepancy between quantitative and qualitative results:
The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the ""translated"" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games).

Does not address the RL generalization issues:
Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here).

Experimental protocol:
The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment.

Data efficiency vs actual training efficiency:
The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware?



# First Recommendation

Using image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions.

I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results.


# Post-rebuttal Recommendation

Thanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I  also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result.

Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., ""de facto standard in RL""), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work.

As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of  just observing  / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience.  This might also require adding a third game to ensure more generalizable experimental insights.","The review starts with positive remarks, highlighting the strengths of the paper such as the controlled experiments and investigation of different GANs. However, it then delves into significant weaknesses, including the discrepancy between quantitative and qualitative results, and the argument that the paper doesn't truly address the RL generalization issue it claims to. The reviewer finds the results limited and preliminary. While acknowledging the authors' clarifications, the post-rebuttal still leans towards the negative side, suggesting more work is needed. The language used is critical but professional and within the bounds of academic discourse.",-20.0,60.0
"In this paper the authors propose a method called “All learning rates at once” (Alrao) which aims to save the time needed to tune learning rate for DNN models testing. The method sets individual learning rate to each feature in each layer of a network using the values sampled from truncated log-uniform distribution. The only cost of the method is the creation of several branches of the classifier layer. Each of the branches is trained with a predefined learning rate value, and the final predictions are obtained by model averaging. In the presented experiments Alrao demonstrates performance comparable to SGD with optimal learning rate and more stable results compared to Adam. The authors indicate limitations of Alrao caused by the overhead in the final layer which complicates the application of the method for models with large classifier layer.

Overall, the paper is written clearly and organized well. However, Equation (2) needs to be corrected. The denominator in the normalizing constant of log-uniform distribution should be \log\eta_{max} - \log\eta_{min}.

My main concern is related to the experimental evaluation of the method. I find the experimental evidence for the effectiveness of Alrao insufficient. As the authors propose to employ the method to quickly evaluate models and select best models to further training it would be beneficial to have more results in order to ensure that the method is reliable in this setting. Other demonstrations which would show possibly that the method enhances performance of architecture search methods may emphasize significance of the proposed method. Also, more experiments comparing Alrao against sampling learning rates per weight are needed. Given the current results, it is still unclear whether the proposed method performs better. Finally, I recommend to include comments explaining how much more time is needed in practice to train model with Alrao compared to SGD training.","The reviewer provides constructive criticism, acknowledges the paper's clarity and organization, and suggests specific improvements. While they express concerns about the experimental evaluation, the overall tone is professional and helpful, indicating a positive sentiment towards the paper's potential.",60.0,80.0
"Review: This paper proposed ""Dopamine"", a new framework for DeepRL.  While this framework seems to be useful and the paper seems like a useful guide for using the framework, I didn't think that the paper had enough scientific novelty to be an ICLR paper.  I think that papers on novel frameworks can be suitable, but they should demonstrate that they're able to do something or provide a novel capability which has not been demonstrated before.  

Strengths: 

-Having a standardized tool for keeping replay buffers seems useful.  

-The Dopamine framework is written in Python and has 12 files, which means that it should be reasonably easy for users to understand how it's functioning and change things or debug.  

-The paper has a little bit of analysis of how different settings effect results (such as how to terminate episodes) but I'm not sure that it does much to help us in understanding the framework.  I suppose it's useful to understand that the settings which are configurable in the framework affect results?  

-The result on how sticky actions affect results is nice but I'm not sure what it adds over the Machado (2018) discussion.  

Weaknesses: 

-Given that the paper is about documenting a new framework, it would have been nice to see more comprehensive baselines documented for different methods and settings.  

-I don't understand the point of 2.1, in that it seems somewhat trivial that research has been done on different architectures and algorithms.  

-In section 4.2, I wonder if the impact of training mode vs. evaluation mode would be larger if the model used a stochastic regularizer.  I suspect that in general changing to evaluation mode could have a significant impact.  
","The reviewer expresses significant reservations about the paper's novelty, stating its usefulness as a framework guide but lacking in significant scientific contribution. While they acknowledge some strengths like the framework's user-friendly design and analysis of setting impacts, these are presented as minor positives. The reviewer's criticisms, particularly the lack of comprehensive baselines and questioning the relevance of certain sections, point towards a negative sentiment. However, the language remains professional and avoids harsh or disrespectful tones, suggesting a neutral politeness. ",-40.0,0.0
"Quality (5/10): This paper proposes DppNet, which approximates determinantal point processes with deep networks by inhibitive attention mechanism. The authors provided a theoretical analysis under some condition that the DppNet is of log-submodularity.

Clarity (9/10): This paper is well written and provides a clear figure to demonstrate their network architecture.

Originality (6/10): This paper is mainly based on the work [Vaswani et al, Attention is all you need, 2017]. It computes the dissimilarities by subtracting attention in the original work from one, and then samples a subset by an unrolled recurrent neural network. 

Significance (5/10): This paper uses negative log-likelihood as the measurement to compare DppNet with other methods. Without further application, it is difficult to measure the improvement of this method over other methods.

Pros: 
(1) This paper is well written and provides a figure to clearly demonstrate their network architecture.

(2) This paper provides a deep learning way to sample a subset of data from the whole data set and reduce the computation complexity.

There are some comments.
(1) Figure 4 shows the sampled digits from Uniform distribution, DppNet (with Mode) and Dpp. How about the sampled digits from k-Medoids? Providing the sampled digits from k-Medoids can make the experiments more complete.

(2) The object of DppNet is to minimize the negative log-likelihood. The DPP and k-Medoids have other motivations, not directly optimizing the negative log-likelihood. This may be the reason why DppNet has a better performance on negative log-likelihood, even than DPP. Could the authors provide some other measures (like the visual comparison in figure 4) to compare these methods?

(3) Does GenDpp Mode in Table 2 mean the greedy mode in Algorithm 1? A clear denotation can make it more clear.","The reviewer acknowledges the clarity and contribution of the paper but raises concerns about its significance and suggests additional experiments for comparison. The tone is constructive and professional, suggesting areas of improvement rather than outright criticism.",50.0,70.0
"The paper considers adaptive regularization, which has been popular in neural network learning.  Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix.

When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?  As a matter of good implementation, one never takes the inverse of anything.  Instead, on solves a linear system, via other means.  Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up.

There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.  The latter may be important in practice, but it is orthogonal to the full matrix theory.

There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.  Instead, it is a low-rank approximation to the full matrix.  If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.

The discussion of convergence to first order critical points is straightforward.

Adaptivity ratio is mentioned in the intro but not defined there.  Why mention it here, if it's not being defined.

You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.  It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..

It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.

The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.  If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.  More papers should present this, and those that do should do it more systematically. 

You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)","The review is critical of the paper, pointing out several areas of concern regarding clarity, methodology, and interpretation of results. The reviewer questions the novelty and contribution of the proposed method, suggesting that the theoretical analysis is not well-supported and the empirical results are confounded by implementation tweaks. While the reviewer acknowledges the potential value of the spectral analysis presented in Figure 4, the overall tone suggests that the paper requires significant revisions to address the raised concerns. The language used is direct and critical, but maintains a professional and academic tone. There's no personal attack or disrespectful language used.",-30.0,60.0
"Summary
------------------

The authors propose a new method to sparsify DNNs based on a dropout induced by a Beta-Bernoulli prior. They further propose a data-dependent dropout by linking the Beta-Bernoulli prevalence to the inputs, achieving a higher sparsification rate. In the experimental section they show that the proposed method achieves better compression rates than other methods in the literature. However, experiments against some recent methods are missing. Also, some additional experiments using data-dependent dropouts not based on the Beta-Bernoulli prior would help to better disentangle the effects of the two contributions of the paper. Overall, the paper is well-written but the mentioning of the IBP is confusing. The authors devote quite a bit of space to the IBP when it is actually not used at all.

 Detailed comments
-------------------------

1)	Introduction

The paper is well motivated and the introduction of the paper clearly states the two main contributions of the paper: a Beta-Bernoulli dropout prior and a dependent Beta Bernoulli dropout prior. 

2)	Background

Section 3.1 is a nice summary of variational inference for BNNs. On the other hand, Section 3.2 is misleading. The authors use this section to introduce the IBP process (a generative sequential process to generate samples from a random measure called the Beta-Bernoulli process). However, this is not used in the paper at all. Then they introduce the Beta-Bernoulli prior as a finite Beta-Bernoulli process. I find this quite convoluted. I would suggest to introduce the Beta-Bernoulli distribution as a prior directly, and state that for alpha/K this is a sparse-inducing prior (where the average number of features is given by \frac{\alpha}{1 + \frac{\alpha}{K} ). No need to mention the IBP or the Beta Bernoulli process. 

3)	Main Contribution

I think the design of a link function that allows to implement a data-dependent Beta-Bernoulli dropout is one of the keys of the paper and I would suggest that the author clearly state this contribution at the beginning of the paper. I would also like to see the application of this link-function to other sparsity inducing priors different than the Beta-Bernoulli. This would allow to further understand the data-dependent contribution to the final performance and how transferable this is to other settings. Also, Have the authors try to train the data-dependent Beta-Bernoulli from scratch, i.e. without the two steps approach? I am assuming the performance is worse, but I would publish the results for completeness.

4)	Experiments

The main issues with the experimental section are:
a)	I am missing some recent methods (some of them even cited in the related work section): e.g. Louizos et al. (2017). I would be interested in comparisons against the horshoe-prior and a data-dependent version of it. Also, a recent paper based on the variational information bottleneck have been recently published outperforming the state of the art in the field (http://proceedings.mlr.press/v80/dai18d.html).
b)	Table 1 should report the variance or other uncertainty measure: Given that they run the experiments 5 times, I do not understand why they only report the median. I would encourage the authors to publish the mean and the variance (at least).
In addition, one of my main question about the method is, once the network has been sparsified, how does this translate into a real performance improvement (in terms of memory and speed). In term of memory, you can always apply a standard compression algorithm. If the sparsity is about a certain threshold, you can resort to sparse-matrix implementations. However, regarding the speed only when you reach a certain sparsity level you would get a tangible improvement if your DL framework support sparse matrices. However, if you get an sparsity level below this threshold, e.g. 20%, you cannot resort to sparse matrices and therefore you would not get a speed improvement, unless you enforce structure sparsity or you optimize to low-level matrix multiplication routines. Are the Speedup/Memory results reported in Table 1 real or theoretical?

","The review is quite positive overall. The reviewer praises the paper's clarity, the novelty of the data-dependent dropout, and the experimental results. While the reviewer points out areas for improvement, such as missing comparisons and the need for additional experiments, these are framed as constructive suggestions rather than harsh criticisms. The language used is formal and professional throughout.",60.0,80.0
"In this paper, the authors introduce a sampling strategy that aims to combine the benefits of with- and without-replacement SGD. With-replacement strategies add more randomness to the process, which the authors claim helps convergence, while the without-replacement strategies ensure equal usage of all datapoints. The authors present numerical results showing better convergence and improved final accuracy. While I found the idea appealing, I felt that the paper needs more work before it can be published. I detail some of my primary concerns below:

- The entire motivation of the paper is predicated on the hypothesis that more randomness is better for training. This is not generally true. Past work has shown that specific kinds of random noise aid convergence through exploration/saddle point avoidance/escaping spurious minima while others either make no change, or hurt. Noise from sampling tends to be a structured noise that aids exploration/convergence over batch gradient descent, but it is not immediately clear to me why the choice between with- and without-replacement should imply exploration. 

- Maybe it's obvious but I'm not grasping why, mathematically, the number of accessible configurations for SRS is the same as original replacement sampling (4th paragraph on page 3).

- Given that the central motivation of the work was to enable with-replacement strategies while still ensuring equal usage, I recommend that the authors include a histogram of datapoint usage for three strategies (with, without, hybrid). This should help convince the reader that the SRS indeed improves upon the usage statistics of with replacement. 

- If one were to create a hybrid sampling strategy, one that is natural is doing 50-50 sampling with and without replacement. In other words, for a batch size of 64, say, 32 are sampled with replacement and 32 without. By changing the ratio, you can also control what end of the sampling spectrum you want to be on. Did you try such a strategy? 

- For the numerical experiments, as I see it, there are 3 differences between the SRS setup and the baseline: location of batch normalization, learning rate, and batch size. The authors show (at the bottom of Page 6) that the performance boost does not come from learning rate or mini-batch size, but what about the placement of the BN layer? Seems like that still remains as a confounding factor?

- ""SRS leads to much more fluctuations, and hence significantly more covariate shift"". How do the authors define covariate shift? Can the authors substantiate this claim theoretically/empirically?

- The authors claim that the method works better when the dataset size is low compared to number of classes. Again, can the authors substantiate this claim theoretically/empirically? Maybe you can try running a sub-sampled version of CIFAR-10/100 with the baselines?

- The writing in the paper needs improving. A few sample phrases that need editing: ""smaller mini-batch means a larger approximation"", ""more accessible configurations of mini-batches"", ""hence more exploration-induction"", ""less optimal local minimum""

- Minor comment: why is the queue filled with repeated samples? In Figure 1, why not have the system initialized with 1 2 3 in the pool and 4 5 in the queue? Seems like by repeating, there is an unnecessary bias towards those datapoints.  ","The reviewer acknowledges the appeal of the idea but raises several significant concerns regarding the paper's claims, methodology, and clarity. They find the core hypothesis insufficiently supported and point out potential flaws in the experimental setup and analysis. The reviewer also suggests additional experiments and calls for substantial improvements to the writing. Overall, the feedback indicates significant reservations about the paper in its current state, suggesting a need for major revisions.",-30.0,70.0
"The paper introduces a new approach to combine small RBMs that are pretrained in order to obtain a large RBM with good performance. This will bypass the need of training large RBMs and suggests to break them into smaller ones. The paper then provides experimental evidence by applying the method on ""invertible boolean logic"". MCMC is used to find the the solution to large RBM and compare it against the combined solutions of smaller RBMs.


The paper motivates the problem well, however, it is not well-written and at times it is hard to follow. The details of the approach is not entirely clear and no theoritcal results are provided to support the approach. For instance, in the introduced approach, only an example of combination is provided in Figure 1. It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model. From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach. Additionally, the details of the settings of the experiments are not fully discussed. For example, what are the atomic/smaller problems and associated RBMs? what is the larger problem and how is the corresponding RBM obtained? Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.

Remark: 
The term ""Combinatorial optimization"", which is used in the title and throughout the body of paper, sounds a bit confusing to the reviwer. This term is typically used in other contexts.

Typos:
** Page 2 -- Paragraph 2: ""Therefore, methods than can exploit...""
** Page 3 -- 2nd line of math: Super-scripts are missing for some entries of the matrices W^A and W^{A+B}
** Page 5 -- Last paragraph: ""...merged logical units is more likly to get get stuck in a ...""
** Page 5 -- Last paragraph: ""...and combining their distributions using the mulistart heuristic...""
","The reviewer acknowledges the paper's interesting idea but raises significant concerns about clarity, theoretical grounding, and experimental validation. They find the core approach poorly explained and the experimental evidence unconvincing. The use of ""Combinatorial optimization"" in the title is also questioned. While the reviewer provides constructive criticism and points out specific areas for improvement, the overall tone suggests significant revisions are needed. The presence of typos further detracts from the paper's quality. Therefore, the sentiment leans towards the negative side, although not extremely negative due to the constructive feedback provided.",-50.0,50.0
"The paper presents an intuitive architecture for learning cross-lingual sentence representations. I see weaknesses and strengths: 

(i) The approach is not very novel. Using parallel data and similarity training (siamese, adversarial, etc.) to facilitate transfer has been done before; see [0] and references therein. Sharing encoder parameters across very different tasks is also pretty standard by now, going back to [1] or so. 
(ii) The evaluation is strong, with a nice combination of standard benchmark evaluation, downstream evaluation, and analysis. 
(iii) While the paper is on cross-lingual transfer, the authors only experiment with a small set of high-resource languages, where transfer is relatively easy. 
(iv) I think the datasets used for evaluation are somewhat suboptimal, e.g.: 
a) Cross-lingual retrieval and multi-lingual STS are very similar tasks. Other tasks using sentence representations and for which multilingual corpora are available, include discourse parsing, support identification for QA, extractive summarization, stance detection, etc. 
b) Instead of relying on Agic and Schluter (2017), why don’t the authors use the XNLI corpus [2]?
c) Translating the English STS data using Google NMT to evaluate an architecture that looks a lot like Google NMT sounds a suspicious. 
(v) While I found the experiment with eigen-similarity a nice contribution, there is a lot of alternatives: seeing whether there is a linear transformation from one language to another (using Procrustes, for example), seeing whether the sentence graphs can be aligned using GANs based only on JSD divergence, looking at the geometry of these representations, etc. Did you think about doing the same analysis on the representations learned without the translation task, but using target language training data for the tasks instead? The question would be whether there exists a linear transformation from the sentence graph learned for English while doing NLI, to the sentence graph learned for German while doing NLI. 

Minor comments: 
- “Table 3” on page 5 should be Table 2. 
- Table 2 seems unnecessary. Since the results are not interesting on their own, but simply a premise in the motivating argument, I would present these results in-text. 

[0] http://aclweb.org/anthology/W18-3023","The reviewer acknowledges both strengths (strong evaluation, interesting eigen-similarity experiment) and weaknesses (lack of novelty, limited language selection, potentially suboptimal dataset choices). They offer constructive criticism and suggestions for improvement. Overall, the tone is balanced, suggesting a review that leans slightly towards the positive side but with clear reservations.",20.0,70.0
"
Summary:
The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.

Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce 
blurry results when making uncertain predictions. GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes. The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).

Strengths:
[+] GANs are notoriously unstable to train, especially for video. The authors formulate a VAE-GAN model and successfully implement it.

Weaknesses:
[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).

[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs). Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.

While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality. This work proposes to combine VAEs and GANs in a single model to get the benefits of both models. However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results. While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.

In order to better assess this model and compare it to its individual parts and other VAE models, could the authors:

1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?
2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects – implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?","The review is critical of the paper's contribution. While it acknowledges the technical achievement of combining VAEs and GANs for video prediction, it points out limitations and questions the significance of the results. The reviewer finds the improvements marginal compared to existing VAE models and raises concerns about the plausibility of generated samples. The language, while direct, maintains a professional and respectful tone, focusing on constructive criticism and suggestions for improvement.",-20.0,60.0
"======== Summary ============

The authors consider a setup where there is a set of trajectories (s_t, a_t, r_t) where r_t is a *vector* of rewards. They assume that each agent is trying to maximize \sum_t \gamma^t (\phi . r_t) where \phi is a preference vector that lives on the simplex. Their goal is to calculate \phi (and maybe also an optimal policy under \phi?). The 

The authors first prove that this problem can be decomposed into finding Q functions for optimal policies for each component of r_t individually, and then solving for \phi that rationalizes the trajectory of actions in terms of these Q functions. Given the entire collection of trajectories, they perform off-policy Q-learning on each component of r_t in order to learn the Q function for that component, and then use linear programming to solve for \phi based on these Q function.

========== Comments =============

I think it's a worthwhile direction to combine IRL with modeling a diversity of preferences among agents. I can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides ""to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors"". Is the goal to do better policy prediction? To do better policy prediction conditional on \phi? To infer \phi to understand people's preferences from a social science perspective? These all seems reasonable but not sufficiently teased out in the work. (For comparison, IRL is typically - although not always - interested in learning the reward function in order to construct robust policies that maximize it). The authors also don't seem to solve a particular task of importance on the WoW dataset.

The theoretical approach seems sound, and I liked the way their algorithm was motivated and the way the problem was decomposed into off-policy Q-learning and then solving for \phi.

However, I found myself quite confused in the experimental section (4.3). The authors evaluate their approach by action prediction. Given the trajectories, is \phi computed for each player and then compute actions based on that value of \phi? Is \phi computed on the same trajectory data used for evaluation or a different subset? Or is action prediction performed in aggregate across the entire population? The experimental setup was never clarified for this (main) experiment.

I was also confused about the motivation for Figure 2 and Appendix D. The authors are showing that their predictions about which reward is motivating the players is consistent with external factors. But wouldn't you see the same thing if you just plotted the observed *rewards* themselves? E.g. players in a guild will achieve more Relationship reward. 
The proposed approach takes the vector of reward, learns which actions are consistent with achieving each reward, then infers from the actions which reward is trying to be achieved. What advantages does this have vs. just looking at the empirical trajectory of rewards for each player/group?
I can certainly imagine that the IRL approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.

The writing could also use some improvement for a future iteration, I've listed a few points below:

pg.1, Neither Brown & Sandholm nor Moravcik et al use ""RL algorithms""
pg.1, Finn et al unmatched )
pg.1, ""a scalar reward despite observed or not"" -> ""a scalar reward whether observed or not""
pg.2, ""Either the range of"" -> ""Both the range of"" (and this sentence needs further cleanup)
pg.2, ""which records the pathing of players"" ??
Theorem 3: ""each of the set e_i has an unique element..."" This isn't clear. I think you mean ""For each e_i there is a unique vector v^\pi(s) for all \pi \in \Pi_{e_i} . The equality holds if these vectors are distinct for each e_i"".
pg. 5 ""If otherwise all elements in \phi are generative"" how can they be negative if they are on the simplex?
pg.5 ""we do not perform any scalarization on the reward...the model assumption is easier to be satisfied"" I think this is a strange comparison to IRL because in IRL you're trying to find a (possibly parametric) function (s,a) -> R, whereas here you're *given* the vector R and are trying to find \phi. So while you have more degrees of freedom by adding \phi, you lose the original degrees of freedom in the reward function.

","The reviewer provides both positive and negative points, indicating a mixed sentiment tending towards the positive side. While they see value in the direction and theoretical approach, they express concerns about clarity, motivation, and experimental setup. The language used is generally polite, offering constructive criticism and suggestions for improvement.",40.0,70.0
"Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove).

Clarity:
The paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither ""online"" nor ""offline"". The mathematical derivations are detailed, which is nice.

Originality:
The work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity.

Significance:
I should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm.
I like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to ""cosine similarity [...] is not backed by a solid theoretical foundation"".
I am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting.
","The reviewer acknowledges the paper's clarity and originality, suggesting a positive sentiment. However, they express reservations about the significance and practicality, mentioning a lack of familiarity with the specific dataset and not being fully convinced by the algorithm's improvement and efficiency. The reviewer's polite yet critical tone points towards a neutral-to-positive sentiment overall.",50.0,70.0
"The authors introduce Morph-Net, a single layer neural network where
the mapping is performed using morphological dilation and erosion.
I was expecting something applied to convolutional networks as such operators
are very popular in image processing, so the naming is a bit misleading.

It is shown that the proposed network can approximate any smooth function, 
assuming a sufficiently large number of hidden neurons, that is a nice result.

Clarity should be improved, for example it is mentioned that the structuring
element is learned but never clearly explained how and what difficulties it poses.
In the main text it is written that alpha is {-1, 1}, which would result in a
combinatorial search, but never explained how it is learned in practice.
This is shown only in the appendix but it is not clear to me that using a binarization
with the weights is not prone to degenerate solutions and/or to learn at all
if proper initialization is not used.
Did the authors experiment with smooth versions or other form of binarization with
straight-through estimator or sampling?

In the proof for theorem 1 it is not clear if the convergence of the proposed
network is faster or slower than that of a classic single layer network.

The main result of the paper is that the structuring element can be learned,
but there is no discussion on what it is learned. Also, there is no comparison
on related approaches that try to learn the structuring element in an end-to-end
fashion such as [1].

Experiments lack a more thorough comparison with state-of-the-art and at least
an ablation study to show that the proposed approach is effective and has merit.
For example, what is the relative contribution of using dilation and erosion
jointly versus either one of them.
What is the comparison with a winner-take-all unit over groups of neurons
such as max-pooling?

It seems that extending the work to multiple layers should be trivial but it is
not reported and is left to future investigations. This hints at issues with
the optimization and should be discussed, is it related to the binarization
mentioned above?

Overall the idea is interesting but the way the structuring element is learned
should be discussed in more details and exemplified visually. Experiments need
to be improved and overall applicability is uncertain at this stage.

=======
[1] Masci et al., A Learning Framework for Morphological Operators Using Counter--Harmonic Mean.
","The review acknowledges the interesting idea and the nice theoretical result (approximation of smooth functions). However, it raises several concerns about clarity, methodology, experimental validation, and comparison with related work. The reviewer suggests improvements and further investigations, indicating a lack of enthusiasm and potential issues with the paper.",20.0,50.0
"ML models are trained on a predefined dataset formed by a set of classes. Those classes use to be the same ones for training and testing. However, what happen when during testing time images with classes unseen during training are shown to the model? This article focus in this problem which is not currently taking much attention by the mainstream research community and is of great importance for the real world applications.

This article tries to detect areas of the image where those out-of-distribution situations appear in semantic segmentation applications. The approach used is by training a classifier that detects which pixels are out of distribution. For training two datasets are used: the dataset of interest and another different one. The classifier learns to detect if a pixel is from the dataset of interest or from another distribution.

The main problem I found with this article is that I couldn't fully understand it. Maybe because the text needs a bit more of review and improvement or maybe because Im not very familiar with the topic. Moreover the article is 10 pages while it is encouraged to be 8. I find that the method of the paper is quite simple and can be explained more straight forward and in less pages. The related work section overlaps a lot with the intro, I suggest to combine both. First two paragraphs of the method seam that should be in the intro. Model details from the experiments I consider that should be explained in the method. I miss a figure explaining the architecture of the model. Why using the semantic segmentation model proposed and no something standard? For instance Tiramisu (That is also based on dense layers). Note that the method used for semantic segmentation is 10 points lower than the SOTA in Cityscapes. Figure 1 is impossible to read as the captions are too small. The representations of figures 2-5 are difficult to interpret. There is no comparison to SOTA

","The review starts with a neutral summary of the paper's topic. However, the reviewer expresses significant concerns about the clarity and length of the paper, stating they couldn't fully understand it. They find the method simple and suggest it could be explained more concisely. The reviewer also lists several specific issues, including overlapping sections, unclear figure captions, and lack of comparison with state-of-the-art methods. The numerous criticisms and suggestions for major revisions indicate a negative sentiment. While the reviewer points out the flaws directly, the language remains professional and avoids personal attacks, suggesting a neutral politeness level.",-50.0,0.0
"A Stackelberg competition is a nonzero-sum game where 1) each player has their own objective, which do not sum up to a constant, and 2) there is an order at which the players interact. The proposed formulation only assumes that parameters of one player (data generator) partition in I tuples \gamma_i of parameters, where each tuple parameterizes a different data generator component (e.g., a separate neural network). Further, each of those components is assumed to contribute a term to the game's objective that only depends on the corresponding parameter tuple \gamma_i, and the other player's parameters \theta (e.g., weights of the discriminator). From a game theoretic perspective, this still yields a 2-player zero-sum game where the action space of the data generator is the product space of the I tuple spaces. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.

The gap between the a maximin and minimax solution is determined by the shape of the objective \phi(\gamma,\theta) and is zero, for example, if \phi is (quasi) convex in \gamma=[\gamma_1, ..., \gamma_I], and (quasi) concave in \theta. The authors bound the violation of this property w.r.t. the data generator components' parameters \gamma_i, and argue that this degree of violation is the same for the whole data generator parametrized by \gamma=[\gamma_1, ..., \gamma_I] if the data generator components are from the same family of mappings (e.g., having the same network architecture). While this conclusion is true under worst cast assumption, e.g., the globally maximal possible gap, this would also imply that all data generator components find the same global best solution, that is, yield the same mapping, in which case the gap would be identical to just using one of those components.

Intuitively, the only reason to have multiple data generator components is to learn different mappings such that the joint data generator -- mixing the outputs of the different components -- is more expressiv than just a single mapping. If the different mappings only result from the inability of finding the global best solution, a worst case argument is not very insightful; in this case, one should study the duality gap in the neighborhood of the starting solutions. On the other hand, if we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.

So why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments? Solving a maximin/minimax problem for highly non-convex-concave functions is challenging; there is an infinity of saddle point solutions which yield different ""performances"". The multi-branch GAN can be seen as a model averaging approach giving more stable results, whereas the proposed GAN seems more of an ensemble approach to stabilize the result. Though, this is speculative and I would encourage the authors to study this in-depth; the reasoning in Remark 1 is not convincing to me.

UPDATE:

I read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong.","The reviewer raises several significant concerns about the paper's core argument, questioning the validity of the findings and suggesting alternative explanations for the observed results. They find the authors' reasoning unconvincing in several places, particularly regarding the justification for a Stackelberg setting and the interpretation of the duality gap. While they acknowledge the potential merits of the proposed approach (e.g., stability), they ultimately remain unconvinced by the paper's central claims. This suggests a negative sentiment leaning towards neutrality due to the constructive criticism offered. The language used is formal, academic, and devoid of any personal attacks, indicating a neutral to polite tone.",-25.0,50.0
"This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings.  The authors evaluate their approach on two games against optimal solutions given by the Shapley value.

The work builds upon a substantial and growing literature on reinforcement learning for multiagent competitive and cooperative games. The most novel component of the work is a focus on the process of negotiation within cooperative coalition games. The two game environments studied examine a ""propose-accept"" negotiation process and a spatial negotiation process.

The main contribution of the work is the introduction of a reinforcement learning approach for negotiation that can be used in cases where unlimited training simulations are available.  This approach is a fairly straightforward application of RL to coalition games, but could be of interest to researchers studying negotiation or multiagent reinforcement learning, and the authors demonstrate the success of RL compared to a normative standard.

My primary concerns are:
- The authors advertise the work as requiring no assumptions about the specific negotiation protocol, but the learning algorithms used are different in the two cases studied, so the approach does require fine-tuning to particular cases.
- Maybe I missed it, but how many training games are required?
- In what real applications do we expect this learning algorithm to be useful?  
- The experiments where the RL agents are matched against bots include training against those specific bot types. How does the trained algorithm perform when matched against agents using rules outside its training set?  
- Since the Shapley value is easily computable in both cases studied.  If the bots are all being trained together, why wouldn't the bots just use that to achieve the optimal solution?
- Why are only 20 game boards used, with the same boards used for training and testing?  How do the algorithms perform on boards outside the training set?

Overall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal.
","The review acknowledges the paper's novelty and technical soundness but raises several concerns, suggesting the contribution is ""marginal."" The reviewer uses phrases like ""My primary concerns are:"" and questions like ""Why are only 20 game boards used...?"" indicating a critical but not disrespectful tone. ",20.0,60.0
"The paper proposes Partial VAE to handle missing data and a variable-wise active learning method. The model combines Partial VAE with the acquisition function to design an intelligent information acquisition system. The paper nicely combines the missing value problem with an active learning strategy to in an acquisition pipeline and demonstrate the effectiveness on several datasets.

I have following comments/questions:

1.  Does p(x_i | z) include parameters? How do these parameters be trained?

2. Does sample from p(x_i | x_o) follow by sampling z from q(z|x_o) then sample x_i from p(x_i | z)? How to sample from p(x_\phi | x_i, x_o) in Eq (7)?

3. In Eq (9), it uses q(z_i|x_o), q(z_i | x_i, x_o),  q(z_i | x_i, x_o, x_\phi) while in Eq (4) it only shows how to learn q(z|x_o). Does it need to learn multiple partial inference networks for all combination of i and \phi ?

4. The comparison with similar algorithms seems to be weak in the experiment section. RAND is random feature selection, and SING is global feature selection by using the proposed method. These comparison methods cannot provide enough information on how well the proposed methods performs. There are plenty of works in the area of “active feature acquisition” and also many works in feature selection dated back to Lasso which should be considered as comparison targets.

5. In the “personalized” implementation of EDDI on each data instances, is the model trained independently for each data point or share some parameters across different data? If so, what are the shared parameters?","The review starts with a positive sentiment, acknowledging the novelty and contribution of the paper. However, it then raises several technical questions and points out a significant weakness in the experimental comparison. The questions suggest a need for clarification and further details, indicating that the reviewer might not be fully convinced about the technical soundness of the approach. The criticism regarding the weak comparison to existing methods is substantial, implying that the paper's claim of effectiveness might not be fully supported. Overall, the sentiment leans towards the positive side due to the initial appreciation, but the technical concerns and the experimental limitations bring the score down. The language used is polite and professional throughout the review. The reviewer asks questions and raises concerns in a constructive manner, aiming to improve the paper.",50.0,80.0
"The paper proposes to include within regular programs, learned parameters that are then tuned in an online manner whenever the program is invoked. Thus learning is continuous, integration with the ML backend seamless. The idea is very interesting however, it seems to me that while we can replace native variables with learned parameters, the hyperparameters involved in the learning become new native variables (e.g. the value of feedback). Perhaps with some effort we can replace the  hyperparameters with predicted variables too. Other concerns of mine stem from the programmer in me. I think of a program as something deterministic and predictable. With continuous, online, self-tuning, these properties are gone. How do the authors propose to assuage folks with my kind of mindset? Is debugging programs with predicted variables an issue? Consider a situation where the program showed some behavior with a certain setting of q which has since been tuned to another value and thus the same behavior doesn't show up. I find these to be very interesting questions but don't see much of a discussion in the current draft. Also, how does this work relate to probabilistic programming?","The reviewer finds the idea interesting, using terms like ""very interesting"". However, they also raise several concerns and questions, indicating that the paper needs more work. The concerns are presented in a neutral manner, not overly negative or dismissive. Therefore, the sentiment is mildly positive, and the politeness is neutral.",50.0,0.0
"I found the paper difficult to follow. The method proposed is not well motivated, and  the literature review explains well the novelty. Here are some questions/points for discussion:

- the token-level MLE training is not what causes the exposure bias: one can train with MLE and still avoid it by generating appropriate sequences using the RNN, as in scheduled sampling. The problem with MLE (or cross entropy) is that the labels to be predicted might not be the correct ones. See the paper by Ranzato et al. (ICLR2016) for a good discussion of the issue: https://arxiv.org/pdf/1511.06732.pdf

- The criticism against previous works for not comparing agains CRFs seems odd: CRFs are given the number of labels, words, etc. to predict, typically the same as the number of words to be tagged. If one  has this, as well as binary rewards for each decision, then there is little benefit for RL/IL based approaches to be used. The point for them is the use of non-decomposable loss functions such as BLEU, which are not common in tagging, but in tasks like MT, where CRFs can't be used. In fact, for the transliteration experiments in the paper, the CRF approach is padded to perform the task, which highlights that it is not the right comparison. 

- the approach proposed seems very similar to MIXER, which also learns a regressor to predict the reward for each action. A direct comparison both in terms of how the approaches operate and empirically is needed.

- why is it a problem that previous works by Ranzato, Bahdanau and Paulus combine MLE and RL? You are using the same supervision, ie. the labeled corpus.

- the adjusted training seems to essentially not reward correct predictions (top branch in the equation). Why is this a good idea?

- In figure 1 it is not clear at all that the proposed approach works; depending on the epoch the ranking among the three variants differs


- what does it mean for one method to surpass the other in flexibilty? If anything the requirement for immediate rewards after every action restricts flexibility, as one can't use non-decomposable loss functions such as BLEU which are prety common in NLP.

- How is the training efficiency measured in the paper?

- Why not compare against MIXER, as well as more recent work by Leblonde et al. (2018): https://arxiv.org/abs/1706.04499 ? I don't see why the Rennie et al. 2017 method is picked for comparison.

- It is not true that in IL one needs a gold standard policy, one can learn with sub-optimal policies, see Sun et al. (2018): https://arxiv.org/pdf/1703.01030.pdf

- It is odd to say that an approach proposed earlier (Dagger) reduces to a variant of a later proposed one (Scheduled sampling), the reduction should be the other way around

- are the randomly initialized character embeddings for transliteration tuned during training?

- How were the alignments for training the CRF obtained?
","The review starts with a negative statement about the paper being difficult to follow and lacking clarity in novelty. Throughout the review, the reviewer raises multiple concerns and questions, indicating skepticism and disagreement with the authors' approach and claims. The language, while direct and critical, maintains a professional and academic tone.",-50.0,60.0
"Overall Score: 7/10.
Confidence Score: 7/10.

Detailed Comments: This paper introduces various Deep Recurrent Gaussian Process (DRGP) models based on the Sparse Spectrum Gaussian Process (SSGP) models and the Variational Sparse Spectrum Gaussian Process (VSSGP) models. This is a good paper and proposed models are very sound so I recommend for acceptance although as main weakness I can say that is very technical so it can be difficult to follow. Adding more intuitive ideas, motivation and maybe a figure for each step would be a solution. Apart from that it is a really good paper, congratulations.

Related to: RNN models and Sparse Nystrom approximation.

Strengths: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done.

Weaknesses: It is too difficult to follow and it is written in an extreme technical way. More intuitions and a proper motivation both in the abstract and introduction may be put in order to make the paper easier to read and, hence, more used by researchers and data scientists.

Does this submission add value to the ICLR community? : Yes it does, the experiments show the efficiency of the proposed methods in some scenarios and are valid methodologies.

Quality:
Is this submission technically sound?: Yes it is.
Are claims well supported by theoretical analysis or experimental results?: Experimental results prove empirically the methods and appendixes show the analysis performed in a clear and elegant way.
Is this a complete piece of work or work in progress?: Complete piece of work.
Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, and I would enfatize that I have liked that some experiments are won by other methods such as GP-LSTM, they are very honest.

Clarity:
Is the submission clearly written?: Yes, but it is difficult for newcomers due to the reasons that I have stated before.
Is it well organized?: Yes it is.
Does it adequately inform the reader?: Yes it is.

Originality:
Are the tasks or methods new?: Yes, they are sound.
Is the work a novel combination of well-known techniques?: Yes it is.
Is it clear how this work differs from previous contributions?: Yes.
Is related work adequately cited?: Yes, being a strength of the paper.

Significance:
Are the results important?: I would argue that they are and are a clear alternative to consider in order to solve these problems.
Are others likely to use the ideas or build on them?: If the paper is written in a more friendly way, yes.
Does the submission address a difficult task in a better way than previous work?: Yes I think.
Does it advance the state of the art in a demonstrable way?: Yes, empirically.

Arguments for acceptance: Models are very sound, solutions are solid, the proposed methodology is correct and the empirical results and experiments are valid and properly done

Arguments against acceptance: Clarity of the paper.

Minor issues and typos:
-> (V)SS not defined before being used.
-> Abstract should be rewritten adding a motivation and focusing more on the problems being solved and less in the details of the solutions.
-> Recurrent indexes that go backwards (i) of Eq. 1. should be explained why are going backwards before being used like that. Newcomers may be confused.
-> Section 2 writing style lacks a bit of cohesion, relating the paragraphs may be a solution.
-> Q is not defined in section 3.1 paragraph 1.
-> A valid covariance function must produce a PSD matrix, put that in section 3.1. 
-> I do not see how U marginalizes in Eq. 7, kind of confused about that, I think that it should be p(y|X,U).
-> Section 3.4 statistics should be explained.

Reading thread and authors response rebuttal decision:
=================================================

I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication.","The reviewer gives an overall positive sentiment to the paper, recommending its acceptance. They praise the soundness of the models, the solutions, the methodology, and the empirical results. The main criticism lies in the clarity of the paper, which the reviewer finds too technical and difficult to follow for newcomers. The reviewer acknowledges the authors' efforts in addressing the clarity issue in the rebuttal and upgrades the score, ultimately recommending the paper for publication. Based on this, the sentiment is positive, but with a significant area for improvement, hence not the highest score. The language is polite and professional throughout, with constructive criticism and suggestions for improvement.",60.0,100.0
"The paper proposes node embedding methods for applications where nodes are sequentially related. An example application is the ""Wikispeedia"" dataset, in which nodes are connected in a graph, but a datapoint (a wikispeedia ""game"") consists of a sequence of nodes that are visited. Each node is further attributed with textual information.

The methods proposed are most closely related to skipgrams, whereby the sequence of nodes are treated like words in a sentence. Then, node attributes (i.e., text) and node representations must be capable of predicting neighboring nodes/words. (Fig.s 1/2 are a pretty concise overview of the proposed architecture).

Positively, this is a quite sensible extension and modification of existing ideas in order to support a new (or different) problem setting.

Negatively, I'd say the applications for this technique are fairly niche, which may limit the paper's readership. The method is mostly fairly straightforward and not methodologically groundbreaking (probably borderline in terms of expected methodological contribution for ICLR). I also didn't understand whether the theoretical claims were significant.

The wikispedia/physics experiments feel a bit more like proofs-of-concept rather than demonstrating that the technique has compelling real-world uses. The experiments are quite well fleshed-out and detailed though.
","The reviewer provides a balanced perspective, acknowledging both the strengths and weaknesses of the paper. They highlight the positive aspects (sensible extension of existing ideas) but also point out limitations (niche application, limited methodological novelty). The reviewer's tone is critical but professional and ultimately leans slightly towards the negative side due to concerns about impact and novelty. ",-10.0,50.0
"The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert’s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. 

Overall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues:
- the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.
- Section 4.2.: “As opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.” - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. 
- I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce “not too hard” training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \delta > 0.
- Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy’s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?

I do have concerns regarding the experimental evaluation:
- the “Demos” baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform “Demos”, which it does pretty often.
- it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?
- the results of “HandReach” experiments, where the proposed method works much worse than “Demos” are not discussed in the text at all
- overall, there is no example of the proposed method making a difference between a “working” and “non-working” system, compared to “Curiosity” and “Random”. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. ""Demos"" baseline doesn't perform much better, but what would happen with 10000 demonstrations?
- there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like ""Demos"" does?

I think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed.
","The reviewer finds the idea appealing and acknowledges the clarity of the writing. However, they express concerns about the experimental evaluation and the overall low performance of the proposed algorithm. The language used is constructive and professional, suggesting areas for improvement without resorting to harsh criticism.",40.0,80.0
"Summary:
Eslami et al. (2018) proposed a deep neuronal framework for a scene representation and renderer (the Generative Query Networks: GQN), which generate an image from a scene representation and a query camera pose. In this work, the authors use the GQN to estimate the camera pose from a target image. Existing learning approaches are discriminative, meaning that they are trained to output the camera pose in an end-to-end fashion, while this paper proposes a generative method more in the line of hand-crafted methods which still largely outperform learning approaches. Using the GQN with the proposed attention mechanism, the method captures an implicit mapping of the environment at a more abstract level. This implicit representation is then used to optimize the likelihood of the target pose in a probabilistic graphical model framing. They compare their solution to a discriminative baseline, based on a reversed GQN.

Pros:
- As shown in Figure 7, the generative approach seems to capture better the implicit representation associated to the mapping from the scene geometry and the image. 
- The proposed generative solution seems to be more accurate than the discriminative baseline. 
- As shown in Table 1, the proposed attention mechanism allows to focus on relevant parts of the context images, giving flexibility for more complex scenes.
- Unlike classical discriminative methods, the proposed solution can be easily used in new scenes (different from the one used for the learning) thanks to the representation network. 

Cons:
- The contribution seems incremental with respect to Eslami et al. (2018). 
- Lack of comparisons to state of the art, in particular a comparison with PoseNet is necessary.
- The results are shown only on simple datasets of small images (32x32 pixels). 
- Tradeoff between precision and time computing is necessary to handle large environments because of space discretization. Then, the method seems to be far to be exploited in a real life SLAM application (e.g. autonomous vehicle). 
","The review acknowledges the strengths of the paper, such as the effectiveness of the generative approach and the attention mechanism. However, it also points out significant limitations, including the incremental nature of the work, the lack of comparison with state-of-the-art methods, and the limited applicability to real-world scenarios. The tone is critical but professional and not disrespectful. Overall, the review leans slightly towards the negative side due to the mentioned limitations.",-20.0,50.0
"This paper introduces a data dependent strategy to mask parts of the partial derivatives in the chain rule computation. 

Typically with papers proposing modifications of the training regime of the neural network one would expect one of three outcomes:
 - a well justified, mathematically sound method, well tested in simple cases and with some proof of concept results on proper tasks
 - a more heuristic, empirical driven research, where strong results on proper tasks
 - method, however justified, allows us to do something previously impossible, removing some limitations/constraints (like biologically plausible learning etc.)

In its current form paper seems to lack any of these characteristics. On one hand method lacks any guarantees and on the other paper does not present significant improvements under any approved metrics, nor it introduces new which can be properly quantified. In fact, authors explicitly claim that empirical section ""Note that in these experiments, the purpose is not to achieve state of the art performance, but to exemplify how backdrop can be used and what measure of performance gains one can expect."".  

With methods like this it is almost obvious that resulting update is not an unbiased gradient estimator of any function. Consequently convergence/learning guarantees that we have for GD or SGD no longer apply. Do authors have any thoughts on how bad can it get? As noted in the text, other methods of ""dropping"" data (such as dropout) don't have this issue as they still estimate proper gradients. Here, since dropping is done inside the network only on backwards pass, resulting estimates could, in principle, lead to oscilations, divergence and other issues. If these are not encountered in practice it might be interesting to understand why. 

If authors prefer to go through more empirical path, one would expect at least to see some baselines for tasks proposed, rather than comparing Backdrop to SGD. There are many methods that could be applied in scenarios like this, including dozens forms of dropout (which, as authors note, is not aimed at the same goals, but this does not mean that it will not shine under the metrics introduced, as they are non-standard and so - noone tested them in this exact regime).

I am happy to revisit my rating given authors restructure paper towards one of these paths (or other one which is not listed here).","The reviewer outlines three common expectations for papers of this type, finding the paper lacking in all three. They find the method lacks guarantees and the paper doesn't present significant improvements or introduce properly quantified new metrics. While the reviewer acknowledges the authors' statement about the purpose of the empirical section, they still find it insufficient. The reviewer suggests potential issues with the method and compares it unfavorably to other techniques like dropout. They suggest either strengthening the theoretical grounding or providing more comprehensive empirical results, including comparisons with relevant baselines. The language is critical but professional and offers concrete suggestions for improvement. Therefore, the sentiment is negative but not overly harsh, and the politeness remains within an acceptable academic discourse.",-50.0,50.0
"The paper provides a dynamic sparse reparameterization method allowing small networks to be trained at a comparable accuracy as pruned network with (initially) large parameter spaces. Improper initialization along with a fewer number of parameters requires a large parameter model, to begin with (Frankle and Carbin, 2018).  The proposed method which is basically a global pooling followed by a tensorwise growth allocates free parameters using an efficient weight re-distribution scheme, uses an approximate thresholding method and provides automatic parameter re-allocation to achieve its goals efficiently. The authors empirically demonstrate their results on MNIST, CIFAR-10, and Imagenet and show that dynamic sparse provides higher accuracy than compressed sparse (and other) networks.

The paper is addressing an important problem where instead of training and pruning, directly training smaller networks is considered. In that respect, the paper does provide some useful tricks to reparameterize and pick the top filters to prune. I especially enjoyed reading the discussion section.

However, the hyperbole in claims such as ""first dynamic reparameterization method for training convolutional network"" makes it hard to judge the paper favorably given previous methods that have already proposed dynamic reparameterization and explored. This language is consistent throughout the paper and the paper needs a revision that positions this paper appropriately before it is accepted.

The proposed technique provides limited but useful contributions over existing work as in SET and DeepR. However, an empirical comparison against them in your evaluation section can make the paper stronger especially if you claim your methods are superior.

How does your training times compare with the other methods? Re-training times are a big drawback of pruning methods and showing those numbers will be useful.","The reviewer acknowledges the importance of the problem tackled and the usefulness of the proposed method. They find the discussion section insightful. However, they express concerns about the paper's overclaiming its novelty and lacking comparison with closely related works. The reviewer's tone is critical but professional and suggests concrete improvements, indicating a willingness to reconsider the paper after revision.",40.0,60.0
"i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.

The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between ""zero-confidence attacks"" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. 

The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.

Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty.

","The reviewer's sentiment is mixed. They acknowledge the authors' efforts and thoroughness in addressing the initial review, leading to an increased rating. However, they express ongoing concerns about the paper's core concepts and lack of practical validation. The language used is critical but professional, focusing on the scientific merit rather than personal attacks.",20.0,60.0
"The paper proposes a compositional generative model for GANs. Basically, assuming existence of K objects in the image, the paper creates a latent representation for each object as well as a latent representation for the background. To model the relation between objects, the paper utilizes the multi-head dot-product attention (MHDPA) due to Vaswani et a. 2017. Applying MHDPA to the K latent representations results in K new latent representations. The K new representations are then fed into a generator to synthesize an image containing one object. The K images together with the synthesized background image are then combined together to form the final image. The paper compares to the proposed approach to the standard GAN approach. The reported superior performance suggest the inductive bias of compositionality of scene leads to improved performance.

The method presented in the paper is a sensible approach and is overall interesting. The experiment results clearly shows the advantage of the proposed method. However, the paper does have several weak points. Firs of all, it misses an investigation of alternative network design for achieving the same compositionality. For example, what would be the performance difference if one replace the MHDPA with LSTM. Another weak point is that it is unclear if the proposed method can be generalize to handle more complicated scene such as COCO images as the experiments are all conducted using very toy-like image datasets. ","The review starts with a positive note, acknowledging the proposed method as sensible and interesting with clearly shown advantages. However, it then points out weaknesses, such as the lack of investigation into alternative network designs and limited generalizability testing on more complex datasets. The criticism, while direct, is presented in a constructive and neutral tone, focusing on areas for improvement rather than outright negativity.",50.0,50.0
"The paper studies the benefit of an anisotropic gradient covariance matrix in SGD optimization for training deep network in terms of escaping sharp minima (which has been discussed to correlate with poor generalization in recent literature). 

In order to do so, SGD is studied as a discrete approximation of stochastic differential equation (SDE). To analyze the benefits of anisotropic nature and remove the confounding effect from scale of noise, the scale of noise in the SDE is considered fixed during the analysis. The authors identify the expected loss around a minimum as the efficient of escaping the minimum and show its relation with the hessian and gradient covariance at the minimum. It is then shown that when all the positive eigenvalues of the covariance matrix concentrate along the top eigenvector and this eigenvector is aligned with the top eigenvector of the Hessian of the loss w.r.t. the parameters, SGD is most efficient at escaping sharp minima. These characteristics are analytically shown to hold true for a 1 hidden layer network and experiments are conducted on toy and real datasets to verify the theoretical predictions.

Comments:

I find the main claim of the paper intuitive-- at any particular minimum, if noise in SGD is more aligned with the direction along which loss surface has a large curvature (thus the minimum is sharp along this direction), SGD will escape this minimum more efficiently. On the other hand, isotropic noise will be wasteful because a sample from isotropic noise distribution may point along flat directions of the loss even though there may exist other directions along which the loss curvature is large. However, I have several concerns which I find difficult to point out because *many equations are not numbered*. 

1. In proposition 2, it is assumed under the argument of no loss of generality that both the loss at the minimum L_0=0 and the corresponding theta_0 =0. Can the authors clarify how both can be simultaneously true without any loss of generality?
2. A number of steps in proposition 2 are missing which makes it difficult to verify. When applying Ito's lemma and taking the integral from 0 to t, it is not mentioned that both sides are also multiplied with the inverse of exp(Ht).
3. In proposition 2, when computing E[L(theta_t)] on page 12, the equalities after line 3 are not clear how they are derived. Please clarify or update the proof with sufficient details.
4. It is mentioned below proposition 2 that the maximum of Tr(H. Sigma) under constraint (6) is achieved when Sigma* = Tr(Sigma). lambda_1 u1.u1^T, where lambda_1 is the top eigenvalue of H. How is lambda_1 a factor in Sigma*? I think Sigma* should be Tr(Sigma). u1.u1^T because this way the sum of eigenvalues of Sigma remains unchanged which is what constraint (6) states.
5. The proof of proposition 5 is highly unclear.Where did the inequality ||g_0(theta)||^2 <= delta.u^TFu + o(|delta|) come from? Also, the inequality right below it involves the assumption that u^Tg_0 g_0u <= ||g_0||^2 and no justification has been provided behind this assumption.


Regarding experiments, the toy experiment in section 5.1 is interesting, but it is not mentioned what network architecture is used in this experiment. I found the experiments in section 5.3 and specifically Fig 4 and Fig 7 insightful. I do have a concern regarding this experiment though. In the experiment on FashionMNIST in Fig 4, it can be seen that both SGD and GLD 1st eigvec escapes sharp minimum, and this is coherrent with the theory. However, for the experiment on CIFAR-10 in Fig 7, experiment with GLD 1st eigvec is missing. Can the authors show the result for GLD 1st eigvec on CIFAR-10? I think it is an important verification of the theory and CIFAR-10 is a more realistic dataset compared with FashionMNIST.

A few minor points:

1. In the last paragraph of page 3, it is mentioned that the probability of escaping can be controlled by the expected loss around minimum due to Markov's inequality. This statement is inaccurate. A large expected loss upper bounds the escaping probability, it does not control it.
2. Section 4 is titled ""The anisotropic noise of SGD in deep networks"", but the sections analyses a 1 hidden layes network. This seems inappropriate.
3. In the conclusion section, it is mentioned that the theory in the paper unifies various existing optimization mentods. Please clarify.

Overall, I found the argument of the paper somewhat interesting but I am not fully convinced because of the concerns mentioned above.","The reviewer acknowledges the intuitiveness of the paper's main claim and finds some of the experiments insightful. However, they also raise several concerns regarding the clarity and completeness of the mathematical proofs, missing experimental results, and some inaccuracies in the statements. The tone of the review is critical but professional and polite.",20.0,60.0
"This paper proposes to use 8/4-bit approximation of activations to save the memory cost during gradient computation.  The proposed technique is simple and straightforward. On the other hand, the proposed method only saves up to a constant cost of the storage. With the constant factor (4x, 8x) depending on whether fp16 or fp32 is used during computation. Notably, there is a small but noticeable accuracy drop in the final trained model using this mechanism.

The alternative method, gradient checkpointing, can bring sublinear memory improvement, with at most 25%  compute overhead, with no loss of accuracy drop.

As a result, the proposed method has a limited use case. The author did mention, during the response that the method could be combined further with the sublinear checkpointing. However, since sublinear checkpointing already brings in significant savings, it is unclear whether low bit compression is necessary.

Given the limited technical novelty(can be described as oneliner ""store forward pass in 4/8 bit fixed point""),  limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), I think this is a boarder-line paper

On the positive side, the empirical result could still be interesting to some readers in the ICLR community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum.
","The reviewer acknowledges the simplicity of the proposed method but raises significant concerns about its limitations, particularly compared to existing techniques like gradient checkpointing. Phrases like ""limited use case,"" ""limited technical novelty,"" ""limited applicable scenarios,"" and ""limited improvement"" clearly indicate a negative sentiment. However, the reviewer also offers constructive feedback and suggests avenues for improvement, which balances the negativity somewhat. The language remains professional and avoids harsh or disrespectful tones, indicating a neutral-to-polite approach.",-30.0,50.0
"Revision:

The authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.

I believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work.

Original Review below:

Summary: They propose a biologically motivated short term attentive working memory (STAWM) generative model for images. The architecture is based on Hebbian Learning (i.e. associative memories are represented in the weight matrices that are dynamically updated during inference by a modified version of Hebbian learning rule). These memories are sampled from glimpses on an input image (using attention on contextual states, similar to [1]), in addition to a latent, query state. This model learns a representation of images that can be used for sequential reconstruction (via a sequence of updates, like a sketchpad, like DRAW [1], trained in an unsupervised manner). These memories produced by drawing can also be used for semi-supervised classification (achieves very respectable and competitive results for MNIST and CIFAR-10).

This paper is beautifully written, and the biological inspiration, motivation behind this work, and links to neuroscience literature as well as relation to existing ML work (even recent papers) is well stated. The main strength of this paper is that the author went from a biologically inspired idea to a complete realization of the idea in algorithmic form. The semi-supervised classification results are competitive to SOTA, and although the CIFAR-10 reconstruction results are not great (especially compared to generative adversarial models or recent variation models [2]), I think the approach is coming from a very different angle that is different enough compared to the literature to warrant some attention, or at least a glimpse, so to speak, from the broader community. The method may offer new ways to interpret ML models that is current models lack, which in itself is an important contribution. That being said, the fact that most adversarial generative models achieved a far better performance raises concern on the generalization ability of these memory-inspired learned representations, and I look forward to seeing future work investigate this area in more detail.

The authors also took great care in writing details for important parts of the experiments in the Appendix section, and open sourced the implementation to reproduce all their experiments. Given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks. Even without the code, I felt I might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.

I highly recommend this paper for acceptance, with a score of 8 (edit: revised to 9 after rebuttal period). The paper might warrant a score of 9 if they had also achieved higher quality results for image generation, on Celeb-A or demonstrated results on ImageNet, and provided more detailed analysis about drawbacks of their approach vs conventional generative models.

[1] https://arxiv.org/abs/1502.04623
[2] https://arxiv.org/abs/1807.03039

","The reviewer explicitly states their positive sentiment and excitement towards the paper multiple times,  commending the authors for their work, clarity, and code. They revised their score to a 9/10, which is very positive. The language used is very positive and encouraging throughout, without resorting to negativity. ",90.0,100.0
"
The authors provide new generalization bounds for recurrent neural networks.
Their main result is a new bound for vanilla RNNs, but they also have
bounds for gated RNNs.

They claim that their vanilla bound improves on an earlier
bound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.
The main result of the submission is incomparable in strength with the earlier result,
because this submission assumes that the activation functions in the hidden
layers are bounded, where the earlier paper did not.  Part of the difference in the results
(roughly speaking, the ""min"" in the bound) can be traced to this difference in the assumptions. 
 (This paper uses this assumption in the second-to-last line of the proof of Lemma 6.)

I think that the root cause of the remaining difference is that this paper,
at its core, adapts the more traditional analysis, used in Haussler's
1992 InfComp paper.  New analyses, like from the Bartlett, et al
NIPS'17 paper, strove for a weak dependence in the number of parameters,
but this proof technique appears to lead to a worse dependence on the
depth.  I think that, if you unwind the network, to view the function
from the first t positions of the input to output number t as a
depth t network, and apply Haussler's bound, you will get a qualitatively
similar result (in particular with bounds that scale polynomially with
d and t).  I think that Haussler's proof technique can be adapted to
take advantage of the weight sharing between layers in the unrolled
network.  

It is somewhat interesting to note that the traditional bounds have
a better dependence on depth, with correspondingly better dependence
on the length of the output sequence of the RNN.

I also do not see that substantial new insight is gained through the
analysis that incorporates gating.

I do not see much technical novelty in this paper.



","The reviewer acknowledges the authors' work on new generalization bounds for RNNs but doesn't find significant novelty or improvement. They point out that the main result relies on bounded activation functions, unlike previous work, and argue that the core proof technique is an adaptation of older methods, leading to worse dependence on depth. The reviewer also finds the analysis of gated RNNs to offer little new insight. Overall, the tone suggests a lukewarm reception with a focus on limitations rather than strengths.",-20.0,50.0
"The paper presents a novel hierarchical clustering method over an embedding space. In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt. The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods. 

The paper address a relevant problem, which is of great interest for extracting knowledge from data. In general, the quality of the paper is high. The presented approach is based on a sound formalization of hierarchical clustering and deep generative models. The paper is easy to follow in spite of the technical difficulty. The experimental evaluation is really extensive. It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view. 

The only issue with this paper is its degree of novelty, which is narrow. The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model. The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.","The reviewer explicitly states the paper ""address a relevant problem"", is ""of high quality"", ""easy to follow"", has an ""extensive"" experimental evaluation, and presents ""promising"" results. This points to a positive sentiment. However, the reviewer also mentions a ""narrow degree of novelty"", which slightly lowers the positivity. The language used throughout the review is formal and respectful, indicating a polite tone.",75.0,75.0
"Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds. The drawback of a vanilla GAN with a DeepSet classifier is analyzed. The rationality that decoupling the point generator with the object generator is also discussed. 
A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance. 
Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.

Comments:
1. Authors calculate W_U in a primal form via solving an assignment programming problem. Have authors ever tried Sinkhorn iteration? To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively. It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U. 

2. Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function. Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1). Does it mean that the W_s is not as stable as W_L or W_U during training?  Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05. In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U? Authors should analyze the stability of their method in details.

Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability. In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants. 

Typos:
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}
- The 3rd line of lemma 1: epsilon1 -> epsilon_1
- Page 14, Eq(14), \lambda should be s
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.
","The review starts with a summary of the paper's contributions, which is a neutral tone. However, the comments highlight potential weaknesses in the proposed method, particularly concerning its stability and the significance of its improvements. Phrases like ""it would be nice"" maintain a polite tone, but the overall critical analysis of the paper's core ideas suggests a slightly negative sentiment.",-20.0,60.0
"This paper focuses on the alignment of different Knowledge Graphs (KGs) obtained from multiple sources and languages - this task is similar to the Link Prediction setting, but the objective is learning a mapping from the entities (or triples) in one Knowledge Graph to another. In particular, the paper focuses on the setting where the number of available training alignments is small.

The model and training processes are slightly convoluted:
- Given a triple in the KG A, the model samples a candidate aligned triple from a KG B, from a (learned) alignment distribution.
- The objective is a GAN loss where the discriminator needs to distinguish between real and generated alignments.
The objective is non-differentiable (due to the sampling step), and it's thus trained via policy gradients.

Question: to me it looks like the whole process could be significantly simpler, and end-to-end differentiable. For instance, the loss may be the discrepancy between the alignment distribution and the training alignments. As a consequence, the whole procedure would be significantly more stable; there would be no need of sampling; or tricks for reducing the variance of the gradient estimates. What would happen with such a model? Would it be on par with the proposed one?

The final model seems to be better than the considered baselines.
","The review starts with a neutral summary of the paper's topic and approach. While it points out that the model and training process seem ""slightly convoluted,"" it phrases this as an observation rather than a harsh criticism. The reviewer also poses a question that suggests an alternative approach and encourages the authors to consider it. Finally, the review acknowledges that the model outperforms baselines. Overall, the tone is constructive and suggestive of potential improvements rather than dismissive.",40.0,70.0
"The paper proposes the inclusive neural random field model. Compared the existing work, the model is different because of the use of the inclusive-divergence minimization for the generative model and the use of stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo  (SGHMC) for sampling. Experimental results are reported for unsupervised, semi-supervised, and supervised learning problems on both synthetic and real-world datasets. Specific comments follow:

1. A major concern of the reviewer is that, given the related work mentioned in Section 3, whether the proposed method exerts substantial enough contribution to be published at ICLR. The proposed method seems like an incremental extension of existing works.

2. A major claim by the authors is that the proposed techniques can help explore various modes in the distribution. However, this claim can only seem easily substantiated by experiments on synthetic data. It is unclear whether this claim is true in principle or in reality.

Other points:
3. the experimental results of the proposed method seems marginally better or comparable to existing methods, which call in question the necessity of the proposed method.

4. more introduction to the formulation of the inclusive-divergence minimization problem could be helpful. The presentation should be self-contained.

5. what makes some of the statistics in the tables unobtainable or unreported?


============= After Reading Response from Authors ====================

The reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors’ argument. 

“The target NRF model, the generator and the sampler are all different.”
It is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.

As for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2’s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.

The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.

In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.

A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough.

","The reviewer expresses several concerns, viewing the paper's contributions as incremental and unconvinced by the claims of improved mode exploration. The experimental results are deemed marginally better or comparable to existing methods. While acknowledging the use of SGLD/SGHMC as a contribution, the reviewer doesn't find it substantial enough. The overall tone is critical, questioning the necessity and significance of the work. Therefore, the sentiment leans towards the negative side. The language used is professional and avoids personal attacks, maintaining a level of politeness.",-60.0,60.0
"This paper poses and addresses the problem of language drift in multi-agent communication paradigms. When two pretrained natural-language agents are jointly optimized to communicate and solve some external non-linguistic objective, their internal communication often diverges to a code-like, unnatural communication system. This paper solves this “language drift” problem by requiring that the messages between agents be usable as inputs to an image caption retrieval system. They demonstrate that the jointly optimized agents perform best when regularized in this manner to prevent language drift.

1. Framing: I’m uncertain about the framing of this paper. The authors pose the problem of “language drift,” which is indeed a frequent problem in multi-agent communication tasks where the principle supervision involves non-linguistic inputs and outputs. They then design a three-language MT task as a test case, where the inputs and outputs are both linguistic. Why attack this particular task and grounding solution? I can imagine some potential goals of the paper, but also see more direct ways to address each of the potential goals than what the authors have chosen:
1a. Study how to counter language drift in general — why not choose a more intuitive two-agent communication task, e.g. navigation, game playing, etc.?
1b. Study how to counter language drift in the MT task — aren’t there simpler solutions to prevent language drift in this particular task? e.g. require “cycle-consistency” – that it be possible to reconstruct the French input using the French output? Why pick multimodal grounding, given that it imposes substantial additional data requirements?
1c. Build a better/more data-efficient machine translation system — this could be an interesting goal and suitable for the paper, but this doesn’t seem to be the framing that the authors intend.

2. Interpretation of first results:
2a. Thanks for including standard deviation estimates! I think it’s also important that you do some sort of significance testing on the comparison between PG+LM+G and PG+LM performance for Fr->En->De — these numbers look pretty close to me. You could run e.g. a simple sign test on examples within each corpus between the two conditions.
2b. It would also be good to know how robust your results are to hyperparameter settings (especially the entropy regularization hyperparameter).

3. Token frequency results: These are intriguing but quite confusing to me!
3a. How sensitive are these results to your entropy regularization setup? How does PG behave without entropy regularization?
3b. Table 6 shows that the PG model has very different drift for different POS categories. Does this explain away the change in the token frequency distribution? What do the token frequency effects look like for PG within the open-class / content word categories (i.e., controlling for the huge difference in closed-class behavior)?

4. Minor comments:
4a. There’s a related problem in unsupervised representation learning for language. Work on VAEs for language, for example, has shown that the encoder often collapses meaning differences in the latent representation, and leans on an overly powerful decoder in order to pick up all of the lost information. It would be good to reference this work in your framing (see e.g. Bowman et al. (2015)).
4b. In sec. 3.1 you overload notation for R. Can you subscript these so that it’s especially clear in your results which systems are following which reward function?
4c. Great to show some qualitative examples in Table 7 — can you explicitly state where these are from (dev set vs. test set?) and whether they are randomly sampled?

References:
Bowman et al. (2015). Generating sentences from a continuous space. https://arxiv.org/abs/1511.06349
","The review is quite critical of the paper's framing and methodology, suggesting alternative approaches and highlighting potential flaws. While it acknowledges the paper's contributions, the numerous concerns and suggestions for improvement indicate a lukewarm reception overall. The language, however, remains polite and professional throughout, employing constructive criticism and respectful suggestions.",-20.0,80.0
"This paper proposed a progressive weight pruning approach to compress the learned weights in DNN. My major concerns about the paper are as follows:

1. Novelty: The proposed approach heavily relies on the one in (Zhang et. al. 2018b) as shown in Sec. 2.2 for 1 page, making the paper as being an incremental work, like finding better initialization for (Zhang et. al. 2018b).

2. Faster convergence: First of all, from Fig. 3 I do not believe that both methods converged, as both performances vary a lot with significant gaps. In terms of being faster, I do not think that it makes sense by comparing numbers of epochs in training with only one approach. There is no theoretical or empirical evidence (e.g. running time) to support this claim.

3. I do not understand how the proposed approach is motivated by DP. To me it is more like a greedy search algorithm, while DP has the ability to locate global maximum. Does the proposed approach guarantee to find the maximum accuracy? Also, in Fig. 2 why was the best partial model replaced with the new one, rather than the worse one? There is no explanation to this at all. Besides, I do think this approach is very heuristic, same as some other approaches in the related work.

4. Experiments: Since the performance varies a lot as shown in Fig. 3, how are the numbers calculated? Average? Best one? With/without cross-validation to tune parameters? How much gain in terms of running time in testing can you get with more compact models in practice? A training/testing behavior analysis is highly appreciated.
","The reviewer raises several serious concerns about the paper, questioning its novelty, the validity of its claims (faster convergence), the understanding and application of DP, and the experimental methodology. The tone is critical and points out significant weaknesses. However, the language remains professional and avoids personal attacks or disrespectful phrasing.",-50.0,50.0
"This paper proposed a query-synthesis-based active learning algorithm that uses GAN to generate high entropy sample; instead of annotating the synthesized sample, the paper proposed to find the most similar unlabeled data from the pool via nearest neighbor search, with the latter is the main contribution of the paper.

Pros: 
(1)	the paper is well written and easy to follow;
(2)	evaluations look reasonable and fair

Cons:
(1)	The idea of using GAN for active query synthesis isn’t new. As the authors pointed out, this idea is mainly from GAAL (Zhu & Bento 2017). The main difference is sample matching that searches the nearest neighbor from pool and add the real unlabeled data for AL. So the novelty of the paper isn’t significant.
(2)	In terms of accuracy comparison, on Cifar-10-ten classes experiments, all ASAL variants have similar accuracies as random sampling, while traditional pool-based max-entropy clearly works much better. Although the former is much faster (O(1) vs. O(N)), this benefit is mainly due to GAAL (Zhu & Bento 2017).

The paper provides additional evidence showing that GAN-based active learning might be an interesting research direction for active query synthesis. However, given the reasons above, particularly novelty, I think the authors might need to additional work to improve the method.
","The reviewer acknowledges the paper's clarity and fair evaluation (positive aspects). However, they find the novelty limited and question the method's effectiveness compared to existing techniques. The suggestion for ""additional work"" implies the paper is not ready for publication in its current state, indicating a negative sentiment overall.",-20.0,50.0
"In this paper, the authors present how to integrate replay buffer and on-policy trust region policy optimization (TRPO) by generalizing Q/V/advantage function and then empirically show the proposed method outperforms TRPO/DDPG.

The generalization of advantage function is quite interesting and is well written. One minor issue is that d^{\pi_n} (s) is confusing since it appears after ds. 

The theory in Section 3.1 makes sense. However, due to the limitation in Theorem 1 that $\theta$ is the joint parameters, applying Theorem 1 can be difficult. In Eq (25), what is the $\theta$ here? And what does $\nabla_\theta \pi_n$ mean? Does $\pi_n$ uses $\theta$ for computation? One of the problems of using replay buffers in on-policy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue. Further explanation on Eq (25) might help. If the distributions of states are assumed to match, then the joint distribution of states and actions may mismatch so additional importance sampling might help, as suggested in [1] Eq (3). 

Another problem is on the barrier function. In Eq (26), if we only evaluate $\rho_b(\theta)$ (or its gradient w.r.t. $\theta$) at the point $\theta_old$, it doesn't differ with or without the barrier function. So in order to show the barrier function helps, we must evaluate $\rho_b(\theta)$ (or its gradient) at a point $\theta \neq \theta_old$. As far as I know, the underlying optimizer, K-FAC, just evaluates the objective's (i.e., $\rho_b$) gradients at $\theta_old$. Both Conjugate Gradient (CG), which TRPO uses, and K-FAC are trying to solve $F^{-1} g$ where $g$ is the gradient of the objective at the current point. 

The experiments show significant improvement over TRPO/DDPG. However, some experiments are also expected.
1. How is the proposed algorithm compared to PPO or Trust PCL? 
2. How does the barrier function help? More importantly, what's the comparison of the barrier function to [1] Eq (5)? 

The proposed algorithm seems more like a variant of ACKTR instead of TRPO since line search is missing in the proposed algorithm and the underlying optimizer is K-FAC instead of CG.

Ref:
[1]: Proximal Policy Optimization Algorithms, by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 
","The review acknowledges the paper's merits, particularly the interesting generalization of the advantage function. However, it raises several concerns and questions, indicating a critical stance. While the language is professional and not disrespectful, the numerous concerns suggest a somewhat negative sentiment. The reviewer points out potential flaws in the theoretical grounding, questions the clarity of explanations, and suggests alternative approaches. The request for additional experiments further emphasizes the need for improvements.",-20.0,60.0
"Summary: The authors study building models for edits in source code. The application is obvious: a system to accurately predict what the next edit should be would be very valuable for developers. Here, edits are modeled by two types of sequences: one that tracks the state of all edits at each time step (and is thus very long), and one that contains the initial step and a changelist that contains the minimal information required to derive the state at any time. The authors train models on top of both of these representations, with the idea being to match the performance of the explicit (heavy) model with the implicit model. This is shown to be challenging, but a clever model is introduced that achieves this, and is thus the best of both worlds. There are synthetic and real-world code (text) edit experiments.

Strengths: The problem is well-posed and well-motivated. There's a nice application of powerful existing models, combined and tailored to the current work. The writing is generally quite clear. The number of experiments is quite solid. 

Weaknesses: The main flaw is that nothing here is really specifically for souce code; the authors are really just modeling edits in text sequences. There's not an obvious way to integrate the kinds of constraints that source code typically satisfies either. There's some confusion (for me) about the implicit/explicit representations. More questions below.

Verdict: This is a pretty solid paper. It doesn't quite match up to its title, but it sets out a clearly defined problem, achieves its aims, and introduces some nice tricks. Although it doesn't produce anything genuinely groundbreaking, it seems like a nice step forward.

Comments and Questions:

- The problem is written in the context of source code, but it's really setup just for text sequences, which is a broader problem. Is there a way the authors take can advantage of the structural requirements for source code? I don't see an obvious way, but I'm curious what the authors think.

- What's the benefit of using the implicit representation for the positions? The explicit/implicit  position forms are basically just using the permutation or the inverse permutation form, which are equivalent. I don't see directly what's saved here, the alphabet size and the number of integers to store is the same.

- Similar question. The implicit likelihood is s^0, e^(1),...,e^(t-1), with the e^(i)'s being based on the implicit representations of the positions. Seems like you could do this with the *explicit* positions just fine, they carry enough information to derive s^(i) from s^(i-1). That is, the explicit/implicit problems are not really related to the explicit/implicit position representations.

- Just wanted to point out that this type of approach to sequences and edits has been studied pretty often in the information/coding theory communities, especially in the area of synchronization. There, the idea is to create the minimal ""changelist"" of insertions/deletions from two versions of a file. This could come in handy when building the datasets. See, for example, Sala et al ""Synchronizing files from a large number of insertions and deletions"".

- The problem statement should be stated a bit more rigorously. We'd like to say that the initial state is drawn from some distribution and that the state at each time forms a stochastic process with some transition law. As it stands the problem isn't well-defined, since with no probability distribution, there's nothing to predict and no likelihood.

- The ""analogical decoder"" idea is really nice.

- For the synthetic dataset, why are you selecting a random initial string, rather than using some existing generative text or source code model, which would get you synthetic data that more closely resembles code?

- I really liked the idea of using an oracle that gives the position as upper bound. Would it make sense to also have the opposite oracle that gives the edit symbol, but doesn't tell the location? I'm really curious which is the ""harder"" task, predicting the next symbol or the next location. In the information-theory setting, these two are actually equally hard, but the real-world setting might be pretty different. It would also be interesting to train models on top of the POMP. That would produce genuine upper bounds to the model performances. 

- The explicit baseline model performs very well on all the edit types in Table 1. Are there cases where even this explicit case works poorly? Is the improved implicit model *always* upper bounded by the explicit model (to me it seems like the answer should always be yes, but it would be interesting to check it out for cases where explicit is not very high accuracy). ","The review is generally positive, highlighting the paper's strengths such as a well-defined problem, clear writing, and a solid number of experiments. The reviewer acknowledges the paper's contribution as a 'nice step forward'. While the reviewer points out some weaknesses and areas for improvement, these are presented constructively as questions and suggestions rather than harsh criticisms. The language used is formal and objective, typical of academic peer reviews.",60.0,80.0
"Summary:
The authors present a straightforward method to improve generative quality of GANs that can allow for fewer parameters by separating the task into a basic-generation followed by a chain of multiple edits to the base generation with different editor networks. The fact that separating the task allows for smaller networks is key to the reduction in parameters.  Each editor is trained separately in alternance, with its associated critic. Authors test their approach mostly on CIFAR10, as well as CelebA and MNIST.

Pros:
- The proposed method is simple and makes intuitive sense. Having editor generators acting like highly-conditioned GANs should make their job easier to produce better samples.

- Empirical results show that when removing editors for evaluation, some well-known architecture (DCGAN - WGAN+GP) can be outperformed with less parameters when comparing IS scores.

- Interesting leads and negative results are discussed in Section 5.

Cons : 
- Comparisons with end-to-end training seems inadequate. The authors invalidate the end-to-end approach with two arguments; (1) that it doesn’t allow for the removal of superfluous editors once the training is done (Section 3.3), and (2) that IS scores are significantly lower (Section 4.2, Table1). It seems that both these statements are true simply because end-to-end learning is performed with a single score outputted by a single critic at the end of the chain, while it would be entirely possible and simple to keep all discriminators and associated losses and train end-to-end. This would still push all editors to produce good samples, thus allowing removal of editors at test-time, and would probably yield better results than those reported in Table 1. It could also invalidate the results shown in Figure 4 (left). For me this is an important missing comparison, as it might even yield better results than the proposed approach and invalidates one of the proposed advantage of the method.

- I think this idea of sequential generation has been explored before, (e.g. StackGAN [1, 2] or LAPGAN [3] and others?), in which unconditional image generation is performed on relatively complicated datasets with a somewhat more principled way of actually simplifying the task of the base generator. Therefore, I think important citations and valid comparisons are missing.

- The only reported metric is the Inception Score (IS), while most of the recent literature agrees that the Fréchet Inception Distance is a better metric. I think FID should be presented as well to be better aligned with recent literature, even in cases where comparison with previously reported performance is impossible (if previous works only presented IS). If you want to be compared to in future work, I think this is necessary.

- It would be a good addition to have FID/IS scores for each editor output, as we could see the quantitative increase in performance at each editing step.

- In Section 4.2, you specify that all experiments are done using the WGAN-GP training formulation. Looking at Table 1 this is unclear, as you specify this training scheme only for model 6 and model 9. If all models use the same training scheme, this information should be absent from the Table.

- CelebA results. Experiments are reported in the main text, without any results, which are only in the Appendix. These results don’t show any quantitative metrics and are visually disappointing. It is hard to see if the editors actually improve the generation.

- Some of the main results or discussions are based on Section 7, which is not the main article even though it is used as another Section instead of an Appendix. I think Section 7 should be separated into Appendices A, B, etc. Maybe some important aspects of the research presented could fit into the main text, given some removal of repetitions, and some compression of the intro to GANs, which should be vastly known by the ICLR community by now.

- Wrong citation format at the end of Section 2.1.

- Section 3.3 : “train the critic more than the generator in each iteration”. This could be clarified by stating exactly what you do (training the critic for k steps for each generator steps).

- It’s not always clear what the boldface represents throughout tables.

- The discussion in Section 5 about promising techniques explored is somewhat disappointing as efforts to investigate why training failed are not apparent.

- Every result shown from the proposed method is performed with ‘small’ or ‘tiny’ versions of existing architectures. This method could have additional value if it could boost performance on the same architecture, even if there are added editors and trainable parameters. The fact that such results are absent makes me suspicious of such a behavior.

Overall I think this paper presents a relevant and interesting idea. However I think this idea has been explored before with more convincing results and with a more principled approach. There are some important flaws in the comparisons made to assess the advantages of the method, and the overall results fail to convince of any important benefit. Based on the pros/cons stated above, I think this paper does not reach the required quality for acceptance in ICLR.

[1] StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al. 2017)
[2] StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al. 2017)
[3] Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (Denton et al. 2015)
","The review is structured as a summary of the paper's contributions, followed by a list of pros and cons. While the reviewer acknowledges the simplicity and intuitiveness of the proposed method, they raise several significant concerns, particularly regarding the fairness of comparisons and the novelty of the idea. The cons outweigh the pros, leading to a negative sentiment. The language used is professional and polite, focusing on the scientific aspects of the work without resorting to personal attacks or disrespectful tones.",-30.0,70.0
"The authors propose to include phrases (contiguous n-grams of wordpieces) in both the self-attention and encoder-decoder attention modules of the Transformer model (Vaswani et al., 2017). In standard multi-head attention, the logits of the attention distribution of each head is computed as the dot-product between query and key representations, which are position-specific. In the phrase-based attention proposed here, a convolution is first computed over the query, key, value sequences before logits are computed as before (a few variants of this scheme are explored). Results show an improvement of up to 1.3 BLEU points compared to the baseline Transformer model. However, the lack of a controlled experiment sheds substantial doubt on the efficiacy of the model (see below).

Contributions
-------------------
Proposes a simple way to incorporate n-grams in the Transformer model. The implementation is straightforward and should be fully replicable in an afternoon.

Having an inductive bias towards modeling of longer phrases seems intuitively useful, in particular when using subword representations, where subword units are often ambiguous. This is also motivated by the fact that prior work has shown that subword regularization, where sampling different subword segmentations during training can be useful.

Improvements in BLEU scores are quite strong.

Issues
---------
The experiments do not control for parameter count. The phrasal attention model adds significant number of parameters (e.g., ""interleaved attention"" corresponds to 3x the number of 1D convolution parameters in the attention layer). It is well established that more parameters correspond to increased BLEU scores (e.g., the 2x parameter count in the ""big"" Transformer setting from Vaswani et al. (2017) results in over 1 BLEU point improvement). This needs to be fixed!

The model is a very modest extension of the original Transformer model and so its value to the community beyond improved numbers is somewhat questionable.

While an explicit inductive bias for phrases seem plausible, it may be that this can already be fully captured by multi-head attention. With positional encodings, two heads can easily attend to adjacent positions which gives the model the same capacity as the convolutional phrase model. The result in the paper that trigrams do not add anything on top of bigrams signals to me that the model is already implicitly capturing phrase-level aspects in the multi-head attention. I would urge the authors to verify this by looking at gradient information (https://arxiv.org/abs/1312.6034).

There are several unsubstantiated claims: ""Without specific attention to phrases, a particular attention function has to depend entirely on the token-level softmax scores of a phrase for phrasal alignment, which is not robust and reliable, thus making it more difficult to learn the mappings."" - The attention is positional, but not necessarily token-based. The model has capacity to represent phrases in subsequent layers. WIth h heads , a position in the k-th layer can in principle represent h^k grams (each slot in layer 2 can represent a h-gram and so on).

The differences in training setup compared to Vaswani et al. (2017) needs to be explicit (""most of the training settings"" is too handwavy). Please list any differences.

The notation is somewhat cumbersome and could use some polishing. For example, the input and output symbols both range over indices in [1,n]. The multi-head attention formulas also do not match the ones from Vaswani et al. (2017) fully. Please ensure consistency and readability of symbols and formulas.

The model inspection would be much improved by variance analysis. For example, the numbers in table 3 would be more useful if accompanied by variance across training runs. The particular allocation could well be an effect of random initialization. I could also see other reasons for this particular allocation than phrases being more useful in intermediate layers (e.g., positional encodings in the first layer is a strong bias towards token-to-token attention, it could be that the magnitude of convolved vectors is larger than the batch-normalized unigram encodings, so that logits are larger.

Questions
--------------
In ""query-as-kernel convolution"", it is unclear whether you map Q[t, :] into n x d_q x d_k convolution kernel parameters, or if each element of the window around Q[t] of width n is mapped to a convolution kernel parameter. Also what is the exact form of the transformation. Do you transform the d_q dimensional vectors in Q to a d_q x d_k matrix? Is this done by mapping to a d_q * d_k dimensional vector which is then rearranged into the convolution kernel matrix?

Does the model tend to choose one particular n-gram type for a particular position, or will it select different n-gram types for the same position?

""The selection of which n-gram to assign to how many heads is arbitrary"" - How is this arbitrary? This seems a rather strong inductive bias?

""However, the homogeneity restriction may limit the model to learn interactions between different n-gram types"" - How is the case? It seems rather that the limitation is that the model cannot dynamically allocate heads to the most relevant n-gram type?

I do not understand equation 14. Do you mean I_dec = I_cross = (...)?

""Phrase-to-phrase mapping helps model local agreement, e.g., between an adjective and a noun (in terms of gender, number and case) or between subject and verb (in terms of person and number)."" Is this actually verified with experiments / model inspection?

""This is especially necessary when the target language is morphologically rich, like German, whose words are usually compounded with sub-words expressing different meanings and grammatical structures"" This claim should be verified, e.g. by comparing to English-French as well as model inspection.
","The review is quite critical of the paper, pointing out several methodological flaws and questioning the significance of the findings. While the reviewer acknowledges some positive aspects (e.g., simplicity, potential usefulness of the idea), the overall tone is skeptical and suggests substantial revisions are needed. The language is direct and critical, but not disrespectful. The reviewer provides concrete suggestions for improvement, indicating a desire to see the work strengthened rather than simply dismissed.",-30.0,60.0
"The paper proposes a new representation of Wasserstein AutoEncoder and provides the formal analysis of learning autoencoders with optimal transport theory. The proposed model, SAE, employs the constraints on the equality of prior and posterior latent spaces with a Sinkhorn distance. Moreover, the proposed model is also backed up with some theoretical guarantees.

The paper is well-written and easy to follow. The experimental results with different priors have demonstrated the effectiveness of the newly formulated model. However, it is not convinced that what is the advantages of the proposed model with WAE. Can the authors provide more insight and comparison with its counterpart, WAE?

In term of time-complexity, computing Sinkhorn distance in Alg 1 introduces computation overhead, especially with small \epsilon. In compared with WAE, what is the computation overhead of the proposed model? Can the authors provide some theoretical analysis of time-complexity and experimental results?

In Table 2, there are only FID values for WAE with MMD cost. Can the authors show the numbers with WAE-GAN on these datasets?

Conclusion: The theoretical and experimental contributions are significant to publish at the venue.","The review starts with positive statements, highlighting the well-written nature and theoretical contributions. However, it raises several questions and requests for further comparisons and analysis, indicating a need for improvement. Overall, the tone is constructive and professional, suggesting minor revisions are needed. Thus, the sentiment is mildly positive, and the politeness is highly positive.",60.0,80.0
"This paper uses a mixed strategy perspective for GANs. With this formulation the non-convex game formulation of GANs can be transformed into a infinite dimensional problem analog to a finite dimensional bilinear problem.  

I really like this approach, that tries to find methods that converge globally to (mixed) Nash equilibriums. However I have some concerns. 

- I'm concerned about the definition of a $O(T^{-1})-NE$. Actually, this merit function is not standard for game. It can be 0 even if $x_t,y_t$ is far from the equilibrium (for instance for the problem $\min_{x \in \Delta_d}\max_{y \in \Delta_d} x^\top y$ with $x_t = (1,0,\ldots,0)$ and $y_t= (F(x_{NE},y_{NE}),1-F(x_{NE},y_{NE}),0,\ldots,0)$ we have $F(x_t,y_t) = F(x_{NE},y_{NE})$ but $x_{NE} = y_{NE} =(1/d,\ldots,1/d)$). One merit function that could be considered is $\max_{y} F(x,y_t) - \min_{y} F(x_t,y)$. 

- There is a gap between the theory and the practical method that could be bridged. Actually Theorem 2 assume that the stochastic derivatives are unbiased but since your Langevin dynamics gives you an *approximate* of the next distribution an analysis taking into account this bias would provide much stronger results. More precisely, it would be interesting to have a result similar as Theorem 2 with conditions on $\epsilon_t$ and $K_t$. For instance, if the theoretical $K_t$ is too large it would reduce the interest of your algorithm. I think this analysis is key since it allows to claim that you can properly approximate the distributions of interest.

If you are able to ease these concerns I'm eager to increase my grade.


- ""(5) is exactly the infinite-dimensional analogue of (1):"" Actually it is not exactly the analogue since $<.,.>$ is not a scalar product anymore (particularly, $<g,\mu>$ is not defined) but the canonical pairing between a space and its dual (we are loosing something going to infinite dimension).
I think it should be clarified somewhere. 

Minor comments: 
- on the updates rules of $\theta$ and $\omega$ (Page 6) the Gaussian noises are missing. 
- On algorithm 3,4,5 and 6 the Gaussian noise is too wide and causes an Overfull.

=== After Authors response ===
The authors fixed some major issues. That is why I improved my grade. 
However I'm still concerned about the scalability of this algorithm
","The reviewer highlights several concerns with the paper, particularly regarding the merit function and the gap between theory and practice. However, they express a positive sentiment towards the approach and are 'eager to increase [their] grade' if the concerns are addressed. The reviewer also acknowledges that the authors have addressed some major issues in their response, leading to an improved grade. This suggests an overall positive view of the paper. The language used is formal, respectful, and constructive, indicating a high level of politeness.",60.0,80.0
"Brief Summary:
The authors present a novel adversarial attack on node embedding method based on random walks. They focus on perturbing the structure of the network. Because the bi-level optimization problem can be highly challenging, they refer to factorize a random walk matrix which is proved equivalent to DeepWalk. The experimental results show that their approach can effectively mislead the node embedding from multi-classification. 

Quality:
This paper is well-written except for some minor spelling mistakes

Clarity:
This paper is quite clear and easy to follow.

Originality:
This work follow the proposal Qiu et al.(WSDM'18)'s proof and present a novel approach to calculate the loss when the network changes by A'.

Pros:
1. Detailed proofs presented in appendix
2. They present 6 questions and answer them with effective experiments.
3. They present a new way to attack node embedding by factorizing a equivalent matrix.

Cons:
1. I have noticed that Zügner et al.(KDD'18) present an adversarial attack method on GCN for graph data. I think it is reachable by the day the authors submitted this paper. This is opposite to the first sentence ""Since this is the first work considering adversarial attacks on node embeddings there are no known
baselines"" said in Section 4.
2. The author present the time analysis of their approach but the efficiency result of their approach is not presented.
3. To enforce misclassification of the target node t, the author set the candidate flip edges as edges around t. Does this mean only the node's local edges can mislead the target node from downstream tasks? I think the authors should consider more candidate edges but this may lead to larger time complexity.
4. Figure. 4 tells that low-degree nodes are easier to mis-classify. If the baseline method B_{rnd} randomly select edges to flip among the local area of node t, I think the result should be similar to the proposed approach on low-degree nodes because the flipped edges should be the same. 

== I have read the rebuttal. Thanks for the response.","The review acknowledges the paper's clarity, originality, and detailed proofs. While it lists several pros, it also raises valid concerns about prior work, missing efficiency analysis, and potential limitations of the proposed approach. The reviewer maintains a neutral tone throughout, suggesting areas for improvement rather than outright criticism. The use of ""I think"" and ""I have noticed"" further softens the critique. The final sentence about reading the rebuttal and thanking for the response indicates a willingness to engage constructively with the authors.",50.0,75.0
"The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling. The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size. The idea is an interesting one, but this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.

More specific comments
--

The initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).

The main problem with the paper is the use of simple baseline setups as the only experimental configuration:

o feedforward rather than recurrent network;
o use of the Penn Treebank dataset only;
o use of a small n for the n-grams.

All or at least some of these decisions would need to be relaxed to make a convincing paper.

The reasons for the use of the energy-based formulation are not clear to me. Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?

Just before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries. But there are some minus ones in the random projection? 

The PPL expression at the bottom of p.5 doesn't look right. The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.

It looks like all the results are given on the test set. Did you not do any tuning on the validation data?

The plots in figure 4 are too small. It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.

The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission. For example, there are lots of typos such as ""instead of trying to probability of a target word"".
","The review acknowledges the interesting core idea (""The idea is an interesting one..."") but outlines significant shortcomings that make the paper not ready for publication in its current form. The reviewer suggests a workshop as a more suitable venue. The language is quite direct and critical, but remains within the bounds of professional academic feedback. There are no personal attacks or overly negative comments.",-20.0,60.0
"This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. This enables insightful new interpretations of standard issues in MLE training in terms of exploration for instance.
Based on this new perspective, a new algorithm is introduced. Its performance is analysed on a machine translation and a text summarisation task.

==> Quality and clarity
The paper is overall well-written, although it can be improved upon (see details below). The bibliography for instance does not reference the conference/journals where the articles were published and lists many (>10) published papers as arXiv preprints.

The ideas are clearly presented, which is crucial in a paper trying to unify different approaches, and the new perspective on exploration is well motivated.

==> Originality and significance
The unifying framework is interesting, and helps shed new light on some standard issues in sequence generation.
On the other hand, the new algorithm and its analysis seem like a slightly rushed attempt at leveraging the unifying framework. 
The experiments, in particular, present several issues.
- For instance, it's clear from Figure 3 that both MLE and RAML are overfitting and would benefit from more dropout (in the literature, 0.3 is commonly used for this type of encoder-decoder architecture). Having access to these experimental results is important, since it would enable the reader to understand whether the benefits of the new approach are subsumed by regularisation or not.
- Further, the performance of the competing methods seems a bit low. MLE reports 26.44 BLEU, which is a bit surprising considering that: 
   - with beam-search (beam of size 10, not 5, admittedly), Bahdanau et al (2016) get 27.56 BLEU, and this is without dropout.   
   - with dropout 0.3 (but without beam search), Leblond et al (2018) get 27.4 BLEU.
Making a strong case for the benefits of the new algorithm requires more thorough experiments.

Overall, the first half of the paper is interesting and insightful, while the second would benefit from more time. 

Pros
- clarity of the ideas that are presented
- interesting unifying perspective on sequence generation algorithms
- insightful new interpretations of existing algorithms in terms of exploration

Cons
- the example new algorithm is not very original
- the associated experiments are incomplete

==> Details
1. page 2, ""Dayan & Hinton (1997); Levine (2018); Abdolmaleki et al. (2018) study in a probabilistic inference perspective."" is an incomplete sentence.
2. at the beginning of section 3.1, policy optimisation is a family of algorithmS
3. page 7 in the setup of the experiments, ""We use the Adam optimizer for SGD training"" is incorrect since SGD is not a family but a specific algorithm, which is different from Adam.","The review is overall positive, highlighting the interesting perspective and clarity of the paper. However, it also points out significant weaknesses in the experimental validation and novelty of the proposed algorithm. The language is polite and professional, providing constructive criticism and suggestions for improvement.",50.0,75.0
"This paper introduces a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. The kernels of the new layer are controlled by a set of (temporal) Gaussian distribution parameters, which significantly reduce learnable parameters. The results are complete on four benchmarks and show consistent improvement. I just have minor comments. 

1. I am curious what the learned feature look like. As the author mentioned, ""The motivation is to make each temporal Gaussian distribution specify (temporally) ‘where to look’ with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features."" So does the paper achieve this goal? 

Another thing is, can the authors extract the features after TGM layer, and maybe perform action recognition on UCF101 to see if the feature really works? I just want to see some results or visualizations to have an idea of what TGM is learning. 

2. How long can the method actually handle? like hundreds of frames? Since the goal of the paper is to capture long term temporal information. 

3. It would be interesting to see an application to streaming video. For example, surveillance monitoring of human activities. 



","The reviewer starts with a positive note, acknowledging the contribution of the paper and mentioning the ""consistent improvement"" shown. They also state ""I just have minor comments"", further indicating a positive stance overall. However, the questions raised, while framed constructively, suggest potential areas of improvement, thus preventing a completely positive score. Therefore, the sentiment leans towards the positive side, but not overly so. The language used is polite and professional throughout, employing phrases like ""I am curious"" and ""It would be interesting to see"" to express their suggestions.",60.0,80.0
"This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets.

The paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it’s unclear what we can really learn from it. 

Approach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan’s group and Song-Chun Zhu’s group (see some examples below). Apart from that, this paper doesn’t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture.

The results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive.

In general, I’m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers.

A minor issue:
-	In fig 5. The object colors are not matched for GT and Factor3D and ours.

Related work
Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV’18.
Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV’18.
Characterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH’11.
Example-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia’12.
","The reviewer finds the paper well-written and the approach interesting. However, they do not find it groundbreaking or particularly inspiring. The reviewer acknowledges the good results but points out limitations in their applicability and significance. The overall tone is one of moderate skepticism, leaning towards rejection due to lack of significant novelty.",-10.0,60.0
"This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.

1. Stochastic Line Search

Determining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors’ attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.

First, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so I’m on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. 

How are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?

The theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?

The algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?

In deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\min(1, \xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?

The theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\hat{p}_k^T \hat{g}_k] = E[\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.

All the proofs also depend on a linear Taylor approximation that is not well-explained, and I’m wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\hat{f}_{z’} (x + \alpha \hat{p}_z) = \hat{f}_{z’} + \alpha \hat{p}_z’ \hat{g}_z(x + \bar{\alpha} \hat{p}_z)$, where $\bar{\alpha} \in [0, \alpha]$. How does this affect the proof?

Lastly, I would propose for the authors to change the name of their condition to the “Armijo condition” rather than using the term “1st Wolfe condition” since the Wolfe condition is typically associated with the curvature condition (p_k’ g_new >= c_2 p_k’ g_k), hence referring to a very different line search. 

2. Design of the Quasi-Newton Matrix

The authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because “it is not obvious that enforced symmetry necessarily produces a better search direction” and “treating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor”. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the “true” gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.

Additionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.

The matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.

Lastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?

All of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.

3. Numerical Experiments

As written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)

In addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and it’s not clear if either of these are done in the paper.

Typos/Grammatical Errors:
- Pg. 1: Commas are needed in some sentences, i.e. “Firstly, for large scale problems, it is…”; “…compute the cost function and its gradients, the result is…”
- Pg. 2: “Interestingly, most SG algorithms…”
- Pg 3: Remove “at least a” in second line
- Pg. 3: suboptimal, not sup-optimal
- Pg. 3: “Such a solution”, not “Such at solution”
- Pg. 3: Capitalize Lemma
- Pg. 4: fulfillment, not fulfilment
- Pg. 7: Capitalize Lemma
- Pg. 11: Before (42), Cov \hat{g} = \sigma_g^2 I
- Pg. 11: Capitalize Lemma

Summary:

In summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.

References:
[1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. ""A multi-batch l-bfgs method for machine learning."" Advances in Neural Information Processing Systems. 2016.
[2] Byrd, Richard H., et al. ""A stochastic quasi-Newton method for large-scale optimization."" SIAM Journal on Optimization 26.2 (2016): 1008-1031.
[3] Schraudolph, Nicol N., Jin Yu, and Simon Günter. ""A stochastic quasi-Newton method for online convex optimization."" Artificial Intelligence and Statistics. 2007.","The review is mostly negative, highlighting a lack of clarity and theoretical/empirical support for the proposed method. While acknowledging the novelty, the reviewer raises serious concerns about the line search, quasi-Newton matrix design, and experimental setup. The language is critical but professional, focusing on the paper's shortcomings rather than directly attacking the authors.",-50.0,50.0
"The paper proposes a second order method to represent images. More exactly, multiple (low-dimensional) projections of Kronecker products of low-dimensional representations are used to represent a limited set of dimensions of second-order representations. It is an extension of HPBP (Kim et al., ICLR 2017) but with codebook assigment. 

The main advantage of the method is that, if the number of projection dimensions is small enough, the number of learned parameters is small and the learning process is fast. The method can be easily used as last layers of a neural network. Although the derivations of the method are straightforward, I think the paper is of interest for the computer vision community. 

Nonetheless, I think that the experimental evaluation is weak. Indeed, the article only considers the specific problem of transfer learning and considers only one evaluation metric (recall@k). However, recent papers that evaluate their method for that task also use the Normalized Mutual Information (NMI) (e.g. [A,B]) or the F1-score [B] as evaluation metrics. 
The paper does not compare the same task and datasets as (Kim et al., ICLR 2017) either.
It is then difficult to evaluate whether the proposed representation is useful only for the considered task. Other tasks and evaluation metrics should be considered.
Moreover, only the case where D=32 and R=8 are evaluated. It would be useful to observe the behavior of the approaches for different values of R. 
In Section 3.2, it is mentioned that feature maps become rapidly intractable if the dimension of z is above 10. Other Factorizations are then proposed. How do these factorizations affect the second order nature of the representation of z? Is the proposed projection in Eq. (10) still a good approximation of the second order information induced by the features x?


The paper says that the method is efficient but does not mention training times. How does the method compare in terms of clockwork times compared to other approaches (on machines with similar architecture)?

In conclusion, the experimental evaluation of the method is currently too weak.


[A] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy: Deep Metric Learning via Facility Location. CVPR 2017
[B] Wang et al., Deep Metric Learning with Angular Loss, ICCV 2017

after rebuttal:
The authors still did not address my concern about testing on only one task with only one evaluation metric.","The reviewer finds the paper interesting and acknowledges its potential value to the computer vision community. However, they express concerns about the experimental evaluation, mainly its limitations to a single task and metric, making it difficult to assess the general usefulness of the proposed method. The reviewer also points out missing comparisons with related work and a lack of analysis regarding different parameter settings. While the reviewer appreciates the straightforward derivations, the lack of comprehensive evaluation leads to a somewhat reserved but not negative sentiment. The language used is polite and professional, focusing on constructive criticism and suggestions for improvement.",40.0,80.0
"This paper makes two different contributions in the field of adversarial training and robustness.
First the authors introduce a new type of attack that exploits second-order information while traditional attacks typically rely on first-order information.
Another contribution is a theorem that using the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels.

Overall, I find that the paper lacks clarity and does not properly contrast their work to existing results. They are also some issues with the evaluation results. I provide detailed feedback below.

1) Prior work
a) Connection between adversarial defense and robustness to random noise
This connection is established in Fawzi, A., Moosavi-Dezfooli, S. M., & Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems (pp. 1632-1640).
b) Connection between minimal perturbation required to confuse classifier and its confidence was discussed for the binary classification in Section 4 of
Fawzi, Alhussein, Omar Fawzi, and Pascal Frossard. ""Analysis of classifiers’ robustness to adversarial perturbations."" Machine Learning 107.3 (2018): 481-508.
c) The idea to compute the distribution of classifier outputs when the input is convolved with Gaussian noise was already “anticipated” in Section V of the following paper which relates the minimum perturbation needed to fool a model to it’s misclassification rate under Gaussian convolved input:
Lyu, Chunchuan, Kaizhu Huang, and Hai-Ning Liang. ""A unified gradient regularization family for adversarial examples."" Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015.

These papers should be discussed in the paper, please elaborate how you see your contribution regarding the results derived there.

2) Second-order attack introduced in the paper
I think they are a number of important details that are ignored in the presentation.
a) Regarding the assumption that the gradient vanishes in the difference of the loss, I think the authors should elaborate as to why this is a reasonable assumption to make. If we assume that the classifier has been trained to optimality then expanding the function at this (near-)optimum would perhaps indeed yield to a gradient term of small magnitude (assuming the function is smooth). However, nothing guarantees that the magnitude of the gradient term is negligible compared to the second-order information. The boundary of the classifier could very well be in a region of low-curvature.
b) The approximation of the second-order information is rather crude. However, the update derived is very similar to PGD with additional noise. In optimization, the use of noise is known to extract curvature, see e.g. (Xu & Yang, 2017) who showed that noisy gradient updates act as a noisy Power method that extracts negative curvature direction.
Xu, Y., & Yang, T. (2017). First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. arXiv preprint arXiv:1711.01944.

3) Issue of ""degenerate global minimum"": The authors argue that multistep attacks also suffer from this issue. However, the PGD attack of Madry is also initialized at a random point within the uncertainty ball around x, i.e. PGD attack first adds random noise to x before iteratively ascending the loss function. This PGD update + noise at first iteration seem rather similar to the update derived by the authors that uses random noise at every iteration. It could therefore be that the crude approximation of second-order information is not so different from previous work. This should be further investigated either theoretically or empirically.

4) Lack of details regarding some important aspects in the paper
a) “Note the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedure”
The authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which I think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.

b) “the noise is not necessarily added directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer, one can still calculate an upper bound using our analysis. We omit the details here for simplicity”
What exactly changes here? How do you estimate the Lipschitz constant in practice?


5) Main theorem needs to be contrasted to previous results
The main Theorem uses the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels. There are already many results in the field of robust optimization that already derive similar results, see e.g.
Namkoong, H., & Duchi, J. C. (2017). Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980).
Gao, R., & Kleywegt, A. J. (2016). Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint arXiv:1604.02199.
Can you elaborate on the difference between your bounds and these ones? You do mention some of them require strong assumptions such as smoothness but this actually seems like a mild assumption (although some activation functions used in neural nets are indeed not smooth).

6) Adversarial Training Overfit to the Choice of norms
The main theorem derived in the paper uses the l_2 norm. What can be said regarding other norms?

7) Experiments:
a) the authors only report accuracies for attacks whose l2-norm is smaller than a fixed constant 0.8. However, this makes the results difficult to interpret and the authors should instead state the signal to noise ratio, i.e. dividing the l2-norm of the perturbation by the l2-norm of the image. Otherwise, it is not clear how strong or weak such perturbations are. (In particular, the norm depends on the dimension of the image, so l2-norms of perturbations for MNIST and CIFAR10 are not comparable).
b) In Section 6.2, the authors state that an l_infty trained model is vulnerable against l_2 perturbations. Why not training the model under both l_infty and l_2 perturbations?
c) Figure 1
Based on the results predicted in Theorem 2, it seems it would be more interesting to evaluate the largest L for which the classifier predictions are the same. Why did you report a different results?

8) Other comments
section 2.1: “Note this distribution is different from the one generated from softmax”. Why/How is this different?
connection to EOT attack’: authors claim: E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d] = ∇_x E_{d∼N(0,σ2I)} [∇_x L(θ, x, y)|x+d]. There is a typo on the RHS where ∇_x is repeated twice. This is also the common reparametrization trick so could cite 
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.
","The review is quite critical of the paper, pointing out several areas of concern regarding clarity, methodology, and comparison with existing literature. The reviewer provides detailed feedback and suggestions for improvement, indicating a desire to see the paper strengthened rather than rejected outright. While the language is direct and pointed, it maintains a professional and objective tone without resorting to personal attacks or disrespectful language.",-30.0,60.0
"The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). 
The proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ","The review acknowledges the novelty of the proposed framework and its contribution to the field. It highlights the positive results, particularly the efficiency and effectiveness of the Gumbel attack. However, it also raises a significant concern about a core assumption (Eq. 5), questioning its validity and demanding further clarification. This critical analysis, combined with the positive points, suggests a review that is interested but not fully convinced, leading to a slightly positive sentiment. The language used is formal, objective, and devoid of any harsh or disrespectful terms, indicating a polite tone.",20.0,70.0
"The paper introduces hierarchical attention, where they propose to weighted combine all the intermediate layers of multi-level attention. The idea is simple and seems to be promising, however originality seems incremental.

In order to fully demonstrate the significance of the proposed algorithm, the authors should conduct more comparisons, for example, to multi-level attention. Just comparing with one-level attention seems unfair given the significant increase of computation. Another aspect of comparison may be to consider computation and performance improvements together and discuss the best trade-off. The authors should also include some standard benchmark datasets for comparisons. The current ones are good but it is not so clear what is the best state-of-the-arts results on them when compared with all other methods.

The analysis on the network's representation and convergence is nice but it does not bring much insights. The argument for decreasing global minimal of the loss function in terms of increasing parameter size can be made for nearly all models but it is of little practical use since there is no guarantee one can reach the global optimal of these models.

I recommend the authors to analyze/demonstrate how effective this weighted combination is. For example, the paper can benefit from some clear examples that show the learned weights across the layers and which ones are more important.

The presentation of the paper needs some polishing. For example, there are numerous typos, grammatical errors everywhere.","The reviewer acknowledges the paper's contribution as potentially promising but finds its originality limited. The reviewer's criticism towards the lack of comprehensive comparisons and limited analysis insights suggests a somewhat negative sentiment. However, the language remains constructive and professional, focusing on areas for improvement. Therefore, the sentiment leans slightly negative, and the politeness remains neutral to slightly positive.",-20.0,20.0
"The paper proposes PDDPG, a combination of prioritized experience replay, parameter noise exploration, and DDPG. Different combinations are then evaluated on MuJoCo domains, and the results are mixed. 

The novelty of the work is limited, and the results are hard to interpret: sometimes PDDPG performs better, sometimes worse, and the training curves are only obtained with a single random seed. Also presented results are substantially worse than current state of the art (e.g., TD3, SAC).
","The review starts with a neutral statement, outlining the topic of the paper. However, it quickly transitions into a negative sentiment, highlighting limited novelty, hard-to-interpret results, and a lack of comparison with state-of-the-art methods. The language used, while direct, maintains a professional and academic tone.",-60.0,50.0
"The paper studies the standard denoising problem under the assumption that the unknown n-dimensional signal can be written as the output of a known d-layer neural network G mapping k dimensions to n dimensions. The paper specifies an algorithm to perform this denoising and the algorithm is based on a variant of the usual gradient method. Then, under additional assumptions on the neural network G, the paper proves that their algorithm produces a denoised signal that achieves a mean squared accuracy of k/n. Because the input signal has ""effective"" dimensionality k (as it can be written as G(x) for some k-dimensional x), it is nice that it can be recovered at the accuracy k/n by Gradient Descent despite the complicated nature of G. In this respect, the result is quite interesting. However, the underlying assumptions are too strong in my opinion as described below: 

1. It is assumed that the Weights of the neural network G are all Gaussian (and also specific Gaussians with mean zero and variances determined by the layer dimensions). This of course is highly impractical. In practice, these network weights are pre-learned (say based on similar datasets) and there is hardly any reason to believe that they will satisfy the Gaussian assumption. 
2. It is assumed that the network is expansive in some sense with an expansivity constant \epsilon. This \epsilon then gets into the accuracy bound which basically means that \epsilon has to be set very small. Unfortunately, this leads to the expansivity condition being quite stringent which will further lead to k being very small (especially if d is large). It is unrealistic to believe that real-world signals will come from a neural network with small k. 

Given that there do not seem to be other such results for the accuracy of neural network denoising, the paper might still be considered interesting despite the above shortcomings. However, I believe that the theoretical result has near-zero relevance to a practical neural network denoiser.

Another concern is that the paper seems to borrow quite a lot of ideas from the paper ""Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk"" by Hand and Voroninski. It will be good if the authors can explain the essential differences between the present paper and this earlier paper. ","The reviewer finds the paper interesting and acknowledges its novelty in the field. They praise the idea and the achieved accuracy. However, they express strong concerns about the practicality of the assumptions made, significantly impacting the real-world applicability of the findings. While the language is critical, it maintains a professional and analytical tone without resorting to personal attacks or disrespectful language.",20.0,60.0
"This paper offers the argument that dropout works not due to preventing coadaptation, but because it gives more gradient, especially in the saturated region. However, previous works have already characterized how dropout modifies the activation function, and also the gradient in a more precise way than what is proposed in this paper. 

## Co-adaptation
co-adaptation does not seem to mean correlation among the unit activations. It is not too surprising units need more redundancy with dropout, since a highly useful feature might not always be present, but thus need to be replicated elsewhere.

Section 8 of this paper gives a definition of co-adaptation,
based on if the loss is reduced or increased based on a simultaneous change in units.
https://arxiv.org/abs/1412.4736
And this work, https://arxiv.org/abs/1602.04484, reached a conclusion similar to yours
that for some notion of coadaptation, dropout might increase it.

## Gradient acceleration
It does not seem reasonable to measure ""gradient information flow"" simply as the norm of the gradient, which is sensitive to scales, and it is not clear if the authors accounted for scaling factor of dropout in Table 2.

The proposed resolution, to add this discontinuous step function in (7) with floor is a very interesting idea backed by good experimental results. However, I think the main effect is in adding noise, since the gradient with respect to this function is not meaningful. The main effect is optimizing with respect to the base function, but adding noise when computing the outputs. Previous work have also looked at how dropout noise modifies the effective activation function (and thus its gradient). This work, http://proceedings.mlr.press/v28/wang13a.html, give a more precise characterization instead of treating the effect as adding a function with constant gradient multiplied by an envelop. In fact, the actual gradient with dropout does involve the envelope by chain rule, but the rest is not actually constant as in GAAF. 
","The review is critical of the paper's novelty and methodology. The reviewer points out that previous works have already explored similar ideas and that the paper's approach has limitations. The reviewer also questions the validity of some of the paper's claims. However, the reviewer does find the proposed resolution interesting and acknowledges the good experimental results. The language used is formal and academic, without resorting to personal attacks or overly negative phrasing.",-30.0,60.0
"This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations. By combining several results, the authors obtain a generalization upper bound which does not increase with the network size. Furthermore, they show that the maximum normalized margin is, up to a scaling factor, the l1 svm margin over the lifted feature space of an infinite-size network. Finally, in a setting of infinite-sized networks, it is proved that perturbed Wasserstein gradient flow finds a global minimum in polynomial time.

I think that the results are interesting and relevant to current efforts of understanding neural networks. The techniques and ideas seem promising and may be applied in more general settings. The paper is mostly clearly written, but there are some issues which I outline below.
1.	It is not clear what is the novelty in sections 2 and 3.1 except the combination of all the results to get a generalization bound which does not increase with network size (which on its own is non-trivial). Specifically, 
a.	What is the technical contribution in Theorem 2.1 beyond the results of the two papers of Rosset et al. (journal paper and the NIPS paper which was mentioned in the comment on missing prior work)?
b.	How does Theorem 3.1 compare with previous Rademacher bounds for neural networks which are based on the margin? In Neyshabur et al. (2018), it is shown that margin-based generalization bounds empirically increase with network size. Does this hold for the bound in Theorem 3.1?

2.	In the work of Soudry et al. (2018) section 4.3, they consider deep networks with an unregularized loss and show that gradient descent converges to an l2 max margin solution under various assumptions. What is the connection between this result and the l1 max margin result in section 3.3?

3.	What are the main proof ideas of Theorem 4.3? Why is the perturbation needed?

4.	What is the size of the network that was trained in Section 5 in the experiments of Figure 3? Only the size of the ground truth network is mentioned.


---------Revision------------

I have read the author's response and other reviews. I am not changing the current review.
I have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this?","The reviewer finds the paper's results interesting and relevant, indicating a positive sentiment. They also acknowledge the paper is well-written. However, the review mainly consists of questions about novelty, comparison with existing work, and clarification of technical details. This suggests the reviewer sees potential in the work but isn't fully convinced yet. Therefore, the sentiment is moderately positive. The language is polite and professional throughout, using phrases like ""It is not clear"" instead of accusatory wording. The reviewer also expresses appreciation and maintains a respectful tone even when posing critical questions.",60.0,80.0
"This paper presents an empirical evaluation of continual learning approaches for generative modelling. Noting that much of previous work focuses on supervised tasks, the paper evaluates various combinations of continual learning strategies (EWC, rehearsal/replay-based, or generative replay) and generative models (GANs or likelihood-based).
The experiments evaluate all combinations on MNIST and Fashion MNIST, and the resulting best-performing combination on CIFAR.
The paper is well-written and structured, and although there are no new proposed algorithms or measures, I think this has the potential to be a useful empirical study on the relatively unstudied topic of continual learning with generative models.

However, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate *why* certain combinations are more effective than others. For example:
- Is the reason GANs are better than likelihood models with generative replay purely because of sample quality? Or is it sufficient for the generator to learn some key characteristics for a class that lead to sufficient discriminability?
- Why is rehearsal better for likelihood models? (And how does this relate to the hypothesis of overfitting to a few real examples?)

The CIFAR-10 results also require more work - it is unclear why the existing approaches could not be made to work, and whether this is a fundamental deficiency in the existing approaches or other factors (hyperparameters, architecture choices, lack of time, etc). Presuming the sample quality is as good as in the WGAN-GP work (given the original implementation is used for experiments), why is this insufficient for generative replay? More detailed analysis / discussion, or another combinatorial study, would help for CIFAR too.

Some comments:
- The poor performance of EWC across the board is concerning. Was this implemented by computing the Fisher of the ELBO with respect to parameters? Was the empirical or true Fisher used? Why does the performance appear so poor compared to Seff et al (2017)? This suggests that either more thought is required on how to best protect parameters of generative models, or the baseline has not been properly implemented/tuned.
- Given this is an entirely empirical study, I would strongly encourage the authors to release code sooner than the acceptance deadline - this can be achieved using an anonymous repository.
- Figure 2 and 3 plots are a little difficult to parse without axis labels.","The reviewer finds the paper promising and well-written, highlighting its potential value in the field. This points towards a positive sentiment. However, they also raise significant concerns about the depth of analysis and call for more investigation into the ""why"" behind the results. This critical feedback pulls the sentiment back from being highly positive. The language used is constructive and professional, suggesting a polite and respectful tone. The reviewer provides specific suggestions for improvement without being demanding or demeaning.",60.0,80.0
"This paper identifies bias of commercial Face detection API (Microsoft, Google, Face++, IBM) by sending face images generated from AirSim, in which different face attributes (e.g., skin color, age, face orientation, lighting conditions, etc) can be controlled and explored. The paper shows that for darker skin color and old age, the classifier tends to have a higher false negative rate (miss the face more). This is in particular more apparent if Bayesian Optimization is used to explore the parameter space based on the previous detection results to find the failure cases. 

There are several concerns:

1. Bayesian Optimization might itself create a bias in the input data distribution, since it selectively pick some parameter configuration over the others.   

2.  Using simulator might create additional biases. Maybe the faces generated by the simulator using the parameters of skin color of the minority / old age are less realistic than other faces, which lead to higher misclassification rate. In the paper there is no analysis in that aspect. 

Overall I feel this is an interesting paper and it may identify important problems in the existing commercial AI system. However,  I am not an expert in this field so I am less confident about the thoroughness of experiments, as well as the fairness of approaches. 

Minor issue:

Fig. 4 “Age”, skin detection => age. 
","The reviewer starts by acknowledging the importance of the paper's topic and finds the findings interesting. They do, however, raise valid concerns about the methodology, which makes them less confident about the results. The language used is formal and respectful, suggesting a professional academic exchange.",50.0,70.0
"Main idea:
This paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   
To support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.

Clarity issues:
1. ""branching"" has been used many times, but AFAIK, this seems not a standard terminology. What do ""branching on the samples"", ""conditional branching"", ""branching paths"" mean?
2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence 
""the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x"". 
However, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. 

Questions:
1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?
2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  
3. In Fig 4, why WS doesn't achieve a better performance when K increasing?

Experiments:
1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. 
2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. 
","The review is generally positive, acknowledging the importance of the problem tackled and the novelty of the proposed solution. The reviewer finds the paper interesting and promising, particularly highlighting the ""zero-forcing failure mode"" discovery. However, they also raise several clarity issues and call for more comprehensive experiments. The language used is constructive and polite, typical of academic peer reviews. There's no harsh criticism or personal attacks, focusing on improvements and further investigation.",60.0,70.0
"summary--
The paper focuses on improving object localization, though the title highlights ""interpreting deep neural network"" which is another area. It analyzes the classifier weights for image classification, and compute the derivative of the feature maps from the network for a sensitivity map of the image. Then it learns a simple linear mapping over the sensitivity map for bounding box regression. Experiments report competitive performance.

However, there are several major concerns.

1) The paper appears misleading from multiple claims. For example, [abstract] ""common approaches to this problem involve the use of a sliding window,... time consuming"". However, current state-of-the-art methods accomplish detection in a fully convolutional manner using CNN, and real-time performance is achieved. the paper claims that ""computer vision can be characterized as presenting three main tasks... (1) image classification, (2) image localization and (3) image detection"". This appears quite misleading. There are way more topics, from low-level vision to mid-level to high-level, e.g., stereo, boundary detection, optical flow, tracking, grouping, etc. Moreover, just in ""localization"", this could be object localization, or camera localization. Such misleading claims do not help readers learn from the paper w.r.t related work in the community.


2) The approach ""is rooted in the assertion that any deep CNN for image classification must contain, implicit in its connection weights, knowledge about the location of recognized object"". This assertion does not appear obvious -- an reference should be cited if it is from other work. Otherwise, recent work shows that deep CNN can overfit random training data, in which case it is hard to imagine why the object location can be implicitly captured by the CNN [R1]. Similarly, the paper claims that ""once weights are found, the gradient... with regard to X would provide information about the sensitivity of the bounding box loss function with regard to the pixels in the images"". This is not obvoius either as recent work show that, rather than the whole object, a part of it may be more discriminative and captured by the network. So at this point, why the gradient can be used for object location without worrying that the model merely captures a part? 

[R1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning requires rethinking generalization, ICLR 2017.
[R2] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Learning deep features for discriminative localization, CVPR 2016.

3) The paper admits in Section 2.2 that ""we have not yet done a formal comparison of these two approaches to constructing the sensitivity map"". As the two approaches are suggested by the authors, why not comparing in this paper. It makes the paper less self-contained and not ready to publish. A formal comparison in the rebuttal may improve the rating of the paper.

4) In Equation 3, how to represent the bounding box coordinate? Are they any transforms? What does it mean by ""bias weights""? Are they different from Cartesian coordinates, or the one used in Equation (2)?


5) The experiments are not convincing by merely reporting the metric of IoU>0.5 without any in-depth analysis. Perhaps some visualization and ablation study improve the quality of experiments.

6) In Section 3.2, why using two different aggregation methods for producing the final sensitivity map -- max-pool along the channel for PACAL VOC 2017 dataset and avg-pool for ImageNet dataset, respectively? Are there some considerations?

7) In Table 1, it shows the proposed method outperforms the other methods significantly, achieving 41% better than the second best method. However, there is no in-depth analysis explaining why the proposed method performs so well for this task. Moreover, from Figure 1 and Figure 3, it is straightforward to ask how a saliency detection model performs in object detection given that the images have clean background and objects are mostly centered in the image.

8) What does it mean by ""CorLoc (mAP)"" in Table 2? As defined in Equation 4, CorLoc acounts the portion of detection whose IoU greater than 0.5 compared to the ground-truth. But mAP accumulates over a range of IoU threshold and precision across classes.

9) As the proposed method is closely related to the CAM method, how does CAM perform on these datasets? This misses an important comparison in the paper.


10) The readability of the paper should be improve. There are many typos, for example --
1. What does ""..."" mean above and below Equation (2)?
2. inconsistent notation, like $w_{ki}$ and ${\bf w}_{ki}$ in Equation (2).
3. conflicted notation, w used in Equation 2 and Equation 3.","The review is highly critical of the paper, pointing out major concerns about the research and presentation. The reviewer questions the validity of the authors' claims, finds the methodology lacking, and criticizes the experimental validation and overall readability. The language used, while direct and critical, maintains a professional and academic tone. There's no personal attack or disrespectful language used. ",-50.0,50.0
"The paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. In the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. The authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. Then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.

The paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.

- Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.
- Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix.
- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then.

- Minor comments
1. “2000 iteration” -> “2000 iterations”
2. The text says “Network ND trained on frog image” while the following next sentence says that “the network reconstructed the digit 3”. Please clarify.
3. “Network ND reconstructed the digit 3 with a training loss of 10^-4 and Network ND with loss 10^-2”. It seems that one of these should be “Network D”.
4. “(with downsamling)” ->  “(with downsampling)”","The reviewer finds the idea presented in the paper interesting, which points towards a positive sentiment. However, they also suggest significant improvements to the presentation and theoretical foundation. The use of phrases like ""Please elaborate"" and ""Please consider"" indicates politeness. Overall, the tone is constructive and helpful, suggesting a positive but not overly enthusiastic response. Therefore, the sentiment is moderately positive, and the politeness is high.",60.0,80.0
"The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.

The authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.

I find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  

More broadly, there's an additional axis to the optimization problem which is ""How much does the training scheme know about the particulars of the problem?"", ranging from ""Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)"" to ""knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)"" to ""knows a little bit about the problem structure, and uses hyperparameter tuned ADAM"" to ""knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD"".

Model completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.

An excellent contribution, and I'm excited to see follow-up work.



* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.","The reviewer explicitly states ""An excellent contribution, and I'm excited to see follow-up work."", which clearly indicates positive sentiment. While the reviewer raises some questions and suggests potential improvements, they are presented constructively and frame them as opportunities for further exploration rather than criticisms. The language used is formal, respectful, and encouraging throughout.",85.0,90.0
"Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores.

Pro:
- A simple idea to make the problem progressively more difficult.
- The writing is relatively easy to follow.
- Standardized experimental setup.

Con:
- Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach.
- Clarity can be improved: several pages of theory can really be summarized into “learning the joint distribution implies that the marginals are also correctly learned’ (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. 
- Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a ""lens"" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features.
- Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]?

[1] http://proceedings.mlr.press/v80/sajjadi18a.html
[2] https://arxiv.org/abs/1706.08500","The review acknowledges the proposed idea as simple and well-written (positive). However, it raises significant concerns about the lack of ablation studies and questions the core argument of the paper (negative). While the reviewer doesn't use harsh language, the criticism is quite direct and pointed, suggesting a neutral-to-negative politeness level.",-20.0,-10.0
"
The authors proposed a new model Adaptive Neural Trees(ANTs) by combining the representation learning and gradient optimization of neural networks with architecture learning of decision trees. The key advantage of the new model ANTs  over the existing methods(Random forest, Linear classifier, Neural decision forest, et al) is: it may achieve high accuracy(above $90\%$) with relatively much smaller number of parameters, as shown by the experiments on the datasets MNIST and CIFAR-10. Besides, the authors proposed single-path inference based on the greedily-selected leaf node to approximate the multi-path inferences with the full predictive distribution. The experiments show the single-path inference doesn't lose much accuracy but it saves memory and time. This paper is acceptable after minor modification.


Questions:
In the second line below equation (1), $n$ in $t_{e_{n(j)}}^{\psi}$ is not defined. Also, should $t_{e_{1}}^{\psi}$ be $t_{e_{n(1)}}^{\psi}$? ","The review is positive about the paper, highlighting the advantages of the proposed model and the significance of the experiments. The reviewer finds the paper 'acceptable after minor modification', which indicates a positive stance. The language used is formal and polite, typical of scientific peer reviews. The reviewer points out a specific question in the paper, but it is presented in a neutral and constructive manner.",60.0,80.0
"The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids.

In general, the paper is well written and the proposed model is theoretically grounded. Unfortunately, the proposed approach shines only on specifically designed benchmarks. It is not a surprise that a CF can be learned by an architecture very similar to DPDA (with addition of learnable parameters). There is a number of specifically designed tasks to test long-term memorization, such as copy/addition, etc. Furthermore, RNNs are mostly used for natural language processing tasks. This paper only conducts experiments on IMDB sentiment analysis ignoring better benchmarked tasks, such as language modelling.

It is not absolutely clear why authors claim that cell is playing the role of memory. It is always possible to rewrite LSTM formulas with h' which is concatenation of hidden state h and cell c. Results on ""peephole connection""-inspired SR-LSTM-p should be benchmarked against an LSTM with peephole connections.

The claim repeated several times that RNNs operate like DFAs, not DPDAs. This is an important point in the paper and should be verbalized more. Does it mean that it is easier to learn regular languages with RNNs?

While intuitive, theorems 3.1-3.2 are very vague to be theorems. Otherwise, they should be proven or provided a sketch of proof. For example, how do you formalize ""state dynamics""?

The quality of writing of the related work section is worse that the rest of the paper. Authors should explore more other hidden state regularization methods. And, perhaps, give less attention to stochastic RNNs since the final version of the proposed model is not stochastic.

To summarize, this paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown.

Minor points:
- Citation of Theano is missing
- Give a sentence explaining what is hidden state ""drifting""
- a-priori -> a priori","The reviewer acknowledges the good writing and theoretical grounding of the paper, indicating a positive starting point. However, the review then points out several limitations, such as the model's effectiveness only on specific benchmarks and the lack of strong experimental validation. The reviewer also raises concerns about the clarity and justification of certain claims. While the reviewer offers constructive criticism and suggestions for improvement, the overall tone suggests a lukewarm reception to the paper. The language used is formal and professional, without resorting to harsh or disrespectful language.",20.0,70.0
"This paper presents an extension of Capsule Networks, Siamese Capsule Networks (SCNs), that can be applied to the problem of face verification. Results are reported on the small AT&T dataset and the LFW dataset. 

I like the direction that this paper is taking. The original Capsules work has been looking at fairly simple and small scale datasets, and the natural next step for this approach is to start addressing harder datasets, LFW being one of them. Also face verification is a natural problem to look at with Capsules.

However, I think this paper currently falls short of what I would expect from an ICLR paper. First, the results are not particularly impressive. Indeed, SCN doesn't outperform AlexNet on LFW (the most interesting dataset in the experiments). Also, I'm personally not particularly compelled by the use of the contrastive loss as the measure of performance, as it is sensitive to the scaling of the particular representation f(x) used to compute distances. Looking at accuracy (as in other face verification papers, such as DeepFace) for instance would have been more appropriate, in my opinion. I'm also worried about how hyper-parameters were selected. There are A LOT of hyper-parameters involved (loss function hyper-parameters, architecture hyper-parameters, optimizer hyper-parameters) and not much is said about how these were chosen. It is mentioned that cross validation was used to select some margin hyper-parameters, but results in Table 1 are also cross-validation results, which makes me wonder whether hyper-parameters were tuned on the performance reported in Table 1 (which of course would be biased).

The paper is also pretty hard to read. I recognize that there is a lot of complicated literature to cover (e.g. prior work on Capsule Networks has introduced variations on various aspects which are each complicated to describe). But as it currently reads, I can honestly say that I'm not 100% sure what exactly was implemented, i.e. which components of previous Capsule Networks were actually used in the experiments and which weren't. For example, I wasn't able to figure out which routing mechanism was used in this paper. The paper would strongly benefit from more explicitly laying out the exact definition of SCN, perhaps at the expense of enumerating all the other variants of capsules and losses that previous work has used.

Finally, regardless of the clarify of the paper, the novelty in extending Capsule Networks to a siamese architecture is arguably pretty incremental. This wouldn't be too much of a problem if the experimental results were strong, but unfortunately it isn't the case.

In summary:

Pros
- New extension of Capsule Networks, tackling a more challenging problem than previous work

Cons
- Novelty is incremental
- Paper lacks clarity and is hard to read
- Results are underwhelming

For these reasons, I'm afraid I can't recommend this paper be accepted.

Finally, I've noted the following typos:
- hinton1985shape => use proper reference
- within in => within
- that represent => that represents
- a Iterated => an Iterated
- is got => is obtained
- followed two => followed by two
- enocded => encoded
- a a pair => a pair
- such that to => such as to
- there 1680 subjects => there are 1680 subjects
- of varied amount => of the varied amount
- are used many => are used in many
- across the paper: lots of in-text references should be in parenthesis

","The reviewer expresses a liking for the paper's direction and acknowledges its contribution, but highlights significant shortcomings. They find the results unimpressive, methodology questionable, and clarity lacking. The reviewer acknowledges the complexity of the topic but still finds the paper hard to follow. While the tone is critical, it maintains a professional and analytical approach, focusing on content rather than resorting to personal attacks.",-20.0,60.0
"This paper analyzes the relationship between ""adversarial vulnerability"" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.

The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.

The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:
- This analysis only seems to work for ""well-behaved"" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.
- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.


On the empirical results, the authors made a few interesting observations, for example the close correspondence between ""Adv Train"" and ""Grad Regu"" models. 
My concern is that the experiments were done on a narrow range of models, which only have ""weak"" adversarial training / defenses.
Adversarial robustness is hard to achieve. What matters the most is ""why the strongest model is still not robust?"" not ""why some weak models are not robust?"" 
It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\infty robustness benchmark.
Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.
For example, re: the last sentence in the conclusion: ""They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques."" The reasoning is not obvious to me given the current evidence shown in the paper.

[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083","The reviewer acknowledges the novelty and clarity of the paper but expresses significant concerns. They find the theoretical analysis limited in scope, particularly because it doesn't apply to robust models or SOTA defenses. The reviewer criticizes the experiments for focusing on ""weak"" models and not addressing the robustness of the ""strongest"" models.  The concluding statement is deemed unjustified based on the presented evidence. Overall, the review highlights substantial weaknesses, suggesting a need for major revisions.",-30.0,50.0
"I have to say that this paper is not well organized. It describes the advantage function and CMA-ES, but it does not describe PPO and PPO-CMA very well. I goes through the paper twice, but I couldn't really get how the policy variance is adapted. Though the title of section 4 is ""PPO-CMA"", only the first paragraph is devoted to describe it and the others parts are brief introduction to CMA.

The problem of variance adaptation is not only for PPO. E.g., (Sehnke et al., Neural Networks 2009) is motivated to address this issue. They end up using directly updating the policy parameter by an algorithm like evolution strategy. In this line, algorithm of (Miyamae et al. NIPS 2010)  is similar to CMA-ES. The authors might want to compare PPO-CMA with these algorithms as baselines.","The review is critical of the paper's organization and clarity, particularly regarding the explanation of key concepts and algorithms. The reviewer states they had to read the paper twice and still struggled to understand certain aspects. They also suggest additional comparisons to existing algorithms, implying the paper lacks comprehensiveness. While the reviewer provides constructive criticism, the tone is direct and points out significant shortcomings. Therefore, the sentiment leans negative and the politeness is neutral, as the reviewer maintains a professional tone but doesn't soften their criticism.",-40.0,0.0
"This paper focuses on the extraction of high-quality model-agnostic saliency maps. The authors argue that when an extracted saliency map is directly dependent on a model, then it might not be useful for a different classifier and thus not general enough. To overcome this problem, they consider all the possible classifiers weighted by their posterior probabilities. This problem cannot be solved explicitly, and the authors suggest a scheme to approximate the solution using two networks. That is, pretrain an initial classifier and then, following an adversarial training procedure, one network is trying to confuse the classifier and the other one to maximize its accuracy. Using this formulation, the authors report state-of-the-art results for salience map extraction.

SUMMARY/OVERALL COMMENTS
The authors present a simple and effective way to produce classifier-agnostic saliency maps. The argument for the approach is well justified and the results seem convincing on a first read. However, the novelty of the method is a concern given the previous work of Fan et al. (2017), and the manuscript is not upfront about the differences between the two works. The experiments are another cause for concern: Fan et al. should have been tested as a baseline with similar implementation (controlling for architecture and \lambda), and implementation differences in prior works of Table 1 make it difficult to draw conclusions. 


RELATED WORKS
* In the introduction, the authors mention related works but fail to mention the work of Fan et al. (2017) which is clearly the most relevant. The first mention of Fan et al is on page 4 in a very specific discussion the regularization coefficient. The problem formulation in Section 2 and the approach is Section 3 is largely borrowed from Fan et al but not acknowledged until the last page. This introduces bias and confusion to the reader in regards to the novelty of the approach. Please, mention the work of Fan et al. (2017) in the introduction and clearly delineate the differences in the works earlier in the text. (--)

* Du et al. (2018), “Towards Explanation of DNN-based Prediction with Guided Feature Inversion”, use the VGG models for saliency map extraction and achieve a LE of 38.2. Note that Du et al. (2018), suggest that this modification could lead to SOTA results. I would like to see a comparison with this method. (-)

* The work of Kindermans, et al. (2017), “Learning how to explain neural networks: PatternNet and Pattern Attribution”, although they do not aim for weakly supervised localization and thus, they do not present the LE, they produce saliency maps. I would like to see a LE comparison with that method. (minor -)

* In the introduction, p1 (last paragraph) other methods are briefly mentioned (Extracted saliency maps show all the evidence….superpixels), etc.) without references. Please add references when needed. (-)

* The framework presented in this paper was first proposed by Fan et al. (2017). The authors claim four main differences in their approach. In my eyes, not all of them are major or novel - probably the most impactful is removing superpixels as it simplifies the problem and implementation. (+)


APPROACH
* The authors aim for simplicity (strong +)

* The authors justify their approach and present their arguments clearly (strong ++)

* In the algorithm section, the authors first mention the sampling procedure and then their motivation. Please alter the ordering of these to be conceptually easier to understand your approach

* In the first sentence after the equation 6 I guess that “(cf. Alg. 1)” is a typo and should be modified to (Alg. 1)”

* After Equation 6 it is argued that the method resembles the training procedure of GANs (Godfellow et al., 2014) but not the work of Fan et al., 2017. (--)


EXPERIMENTS
* The authors define as their baseline the F thinning strategy (i.e. use only the first classifier) which is a model dependent salience map. While this is a useful comparison against the classifier dependent methods, given the similarity to the work of Fan et al. (2017), experiments comparing the proposed model to Fan et al. are necessary. It is important to control for network architecture (ResNet-50) and choice of \lambda to properly determine if the four changes outlined in Section 6 result in any real improvement over Fan et al. (strong --)

* The authors use the Table 1 (borrowed from Fong and Vedaldi (2017)) to compare their results against other methods. This comparison is problematic as different approaches are using different models as classifiers which may lead to increase or decrease of the LE. (--)

* In Table 2 the authors do not report how many times they run the same experiments to get these values. They also run less experiments with non-shared weights and they report only the LE. In my eyes it looks that the authors are trying to force their argument that the sharing weights helps (probably because it is one of their novelties). Please report the statistics of your experiments and fill the empty entries in the table. (--)

* In Table 3, what does the last row represent?

* Table 1 errors: (1) You write “Localization evaluation using OM, LE and F1 scores”. Please remove the F1 score as you do not report it. Also, correct the first sentence of the “Localization” subsection which states that you use three different metrics to “two different metrics”. (2) The LE from Fong and Vedaldi (2017) should be 43.2 and not 43.1.

* Regarding the unseen classes (section 5): (1) Please report in the appendix the classes that you are using in each subset. Are there classes correlated? (-)  (2) I see that there is a strong correlation between the LE on subset A and E. It looks like you are training on E and you generalize on A.


NOVELTY/IMPACT
* Novelty is a strong concern, given the work of Fan et al. (2017) (strong --). Nevertheless, the authors propose some changes that can be seen as more general, but the effectiveness of the changes is clearly established.

* This paper’s strongest point is the simplicity (conceptually and implementation-wise) of the method, an advantage over previous works (+)


OTHER COMMENTS
* Fan et al. (2017), use an adaptive λ that pushes the mask to 10% of the image whereas you are using a fixed one that pushes the mask to approximately 50% of the image. How can you make sure that this is not the reason that you are getting better results?

If the authors can clearly and fairly demonstrate that the changes they propose over Fan et al (2017) result in improved performance, and the manuscript is adjusted to be more upfront about this prior work, I would consider increasing my rating.
","The review is critical of the paper's novelty and experimental setup, but acknowledges the simplicity and clarity of the approach. The reviewer suggests improvements and conditions for a higher rating, indicating a mixed but leaning towards negative sentiment. The language is direct and professional, but the criticism, while constructive, is quite strong.",-25.0,50.0
"This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.

While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.

Pros:
- Novel defense technique against very challenging white-box attacks.
- Sound threat model drawn from traditional security.
- Clearly written.

Cons:
- Poor clean accuracy makes the technique very impractical.
- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.","The reviewer acknowledges the novelty and clarity of the paper but ultimately leans towards rejection due to the significant decrease in clean accuracy. The reviewer finds the approach interesting but impractical in its current form. The language used is professional and respectful, suggesting areas for improvement without being overly critical.",-20.0,80.0
"This paper proposed a retrieval model based on the residual network and evaluated the use of ELMo word embedding with/without IDF weight. The results showed that there are significant gain when adding the residual network on top of the word embedding. 

Pros:
* This work set a strong baseline for the retrieving target paragraph for question answering on the SQuAD dataset.
* The experiments were sounds and leverage interesting points -- the use of word embedding itself as the feature representation didn't have as much impact to retrieval performance as the distance function.
* The studied problem -- retrieval for answering question rather than getting the most relevant document worth more attention.

Cons:
* The motivation of using the word embedding and contextual word embedding over the TF-IDF feature wasn't clear. Results on using simple feature like TF-IDF maybe useful to give readers better judgement of the use of word embedding.
* The choice of dataset, SQuAD over more retrieval based QA like TrivialQA also wasn't strongly motivated. Also, it would be nice to see how the QA result would be improve with better retrieval model. 
* Another use of TF-IDF/IDF and embedding is to use TF-IDF/IDF to identify the related document and then use word embedding to resolve semantic ambiguity. Do you have theoretical/empirical reason why this shouldn’t be considered?

Comment on writing:
    - In Section 3.1: the dimension of the tensor should reflect the meaning (vocab size, embedding size or the number of documents) rather than numbers.
    - In Section 3.1: since the weighting for each document is not shared, it would be clearer to just use M and W for each document instead of M’, W'
    - In Section 3.1: Evaluation metrics, e.g., recall@k, ROC, AUC; technical details, for example, tensor dimension, optimizer hyperparameters should be moved to the experiment section","The review acknowledges the merits of the paper, highlighting its strong baseline and interesting experimental points. While it raises several constructive criticisms and suggestions for improvement, the overall tone is positive and encouraging. The reviewer provides specific recommendations to enhance the paper's clarity, motivation, and completeness. ",60.0,80.0
"This work introduces a framework for learning implicit models that is robust to mode collapse. It consists in learning an explicit model of the implicit model through maximum likelihood while the later is used to teach the explicit model to better match the data distribution. The resulting bi-level optimization is carried out with truncated unrolled stochastic gradient descent.

# Quality

The method combines an interesting set of ideas. It is validated on some reasonable experiments. 

However after reading the paper, I remain with too many unanswered questions:
- Why should the method avoid mode collapse? Experiments clearly show that it indeed is resilient to mode collapse, but I have would have been curious in seeing some more discussion regarding this point. What is the exact mechanism that solves the issue?
- What is the effect of K? Is mode collapse solved only because of the unrolled gradients?
- What is the effect of M? How does the method behave for M=1, as usually done in GANs?
- What if the explicit model has not enough capacity?
- The original Unrolled GAN paper presents better results for the ring problem. Why are results worse in the experiments?

More fundamentally what is the main benefit of this approach with respect to models that can be trained straight with maximum likelihood? (e.g., flow-based neural generative models; and as required for the explicit model) Is it only to produce generative models that are fast (because they are implicit)? Why not training only the explicit model directly on the data?

# Clarity

The paper is in general well-written, although some elements could be removed to actually help with the presentation.
- The development around influence functions could be removed, as the method ends up instead making use of truncated unrolled gradients.
- The theoretical analysis is straightforward and could be compressed in a single paragraph to motivate the method.

# Originality

The method makes use of several ideas that have been floating around and proposed in different papers. As far as I know, the combination proposed in this work is original.

# Significance

Results show clear resistance to mode collapse, which is an improvement for implicit models. However, other types of generative models generally do not suffer from this issue. Significance is therefore limited.
","The reviewer acknowledges the interesting ideas and reasonable experiments but raises several concerns and questions. They find the method's advantage over directly trained models unclear and point out limitations in significance compared to models not prone to mode collapse. The tone, while questioning, remains professional and constructive.",20.0,70.0
"The paper proposes an alternative to commonly used ReLU activated networks. The ""gating"" and ""amount"" effects of the weights are decoupled. The authors claim that such architectures are easier to theoretically understand. That might be the case indeed, but I fail to see much value in obtaining such understanding of very contrived objects that are not being used in practice. Unless such architectures can be proven to be interesting from a practical standpoint I do not think there is much of a point in studying them. The argument provided by the authors that they can - in a simple situation - have as much expressive power as a standard ReLU activated architecture is insucfficient, in my opinion, to justify researching them. Also, if a strong, deep theorem was proven using GaLU networks was proven, I would be inclined to recommend the paper to be accepted. As is - I do not find the paper to be a contribution significant enough for ICLR.","The reviewer finds the paper's premise interesting in theory but ultimately lacking in practical application. They see little value in the proposed architecture as it stands. While the language is direct and critical, it avoids harsh or disrespectful phrasing. The reviewer expresses their reservations clearly but maintains a professional tone.",-50.0,50.0
"The authors propose a multi-objective neural architecture search based on an evolutionary algorithm. The contradicting objective functions are optimized by ranking the candidates by Pareto-dominance, replace the bottom 50% with new candidates generated by the top 50% candidates through random mutations. The multi-objective function considers classification accuracy and an approximation of the inference speed. The method is compared to MobileNet and Mobile NASNet on ImageNet indicating an improvement with respect to search time.

The authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear.

The paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels.

I think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A].

The comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).
Comparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements.

You mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong.

[A] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter: Simple And Efficient Architecture Search for Convolutional Neural Networks. CoRR abs/1711.04528 (2017)","The review is critical of the paper, pointing out several weaknesses such as missing comparisons with highly relevant work, lack of clarity and completeness in the methodology description, and concerns about the fairness of the presented results. While the reviewer acknowledges the incremental nature of the work, the overall tone suggests that the paper needs significant improvements before it can be considered for publication. The language used is professional and not overtly aggressive, but the criticism is direct and pointed.",-30.0,60.0
"This paper proposes a way to speed up initial training a model.  The key idea
is to:

1. Train an autoencoder on the full dataset and select a subset of training 
examples.  The subset is the union of examples that maximally activate each of
the dimensions of the autoencoder's low-dimensional embedding.

2. Then a target classifier is trained on the subset,

3. followed by final fine-tuning on the full dataset.

The paper is understandably written, although some crucial experimental details
need a bit of guesswork.

Their proposal is evaluated on only one dataset, CIFAR10, using an autoencoder  
and classifier of roughly similar design from the initial convolutional layers. 

They mention a baseline [classifier training, I presume] classifier training 
over ~200 epochs in 736 s (~12 min) to get 83% accuracy.  This skips steps 1. 
and 2.  Since this is already fast, CIFAR10 is perhaps too small a dataset
to spur readers to use their proposed method (which does require them to
additionally train an autoencoder) when tackling more ambitious problems.

They do not report the time taken to train their autoencoder for 800 epochs
(step 1.).  For larger networks and images, it might also be important to
investigate whether an autoencoder considerably simpler than the classifier
model can suffice for subset selection; for example, if I want to train a
Resnet-152 classifier can I use a poorer quality autoencoder?  Since
using a randomly selected subset 20% of the original size works about
as well as step 1 for CIFAR10, I cannot judge whether the time taken to
set up and train an autoencoder makes it worthwhile to further reduce
the training subset from 20% to ~8% of the original size.

They do not consider alternative subset selection (1.) methods.  For example,
one might use a pretrained network to select examplar images by a clustering
method (ex. [2]), possibly providing representative images per class.  Other 
selection criteria are also possible -- for example, [1] evaluates subset 
selection based on ""representativeness"" vs ""diversity"" criteria.

They do not compare with many existing approaches to training set compression.
Instead, they dismiss (Sec. 3 ""Related Work"") most previous work on selecting a 
small subset of training examples.  However, googling will quickly find many
papers on subset selection (exactly what they do) as well as related dataset
optimization techniques (such loss-based revisiting of training examplars, or
training example weighting etc.).  For example, review-type article [3] 
provides a good introduction to existing subset selection techniques, as well
as references to earlier papers.

It is unclear whether the autoencoder training time is included in their
experiments that fix the total training time to 7 minutes and compare results
with different numbers of fast epochs (step 2.).

No guidelines are given for how to select the dimensionality of the autoencoder
embedding, and how the selection procedure should be done in cases with large
numbers of classes, although they mention the possibility of using combinations
of activations for subset selection.  I do not understand how in problems with
larger numbers of classes I can guarantee that the training subset will contain
at least one representative from each class.  Some alternative subset selection
methods can provide such guarantees, which might be important for training
datasets with class imbalance.

Given that they do not use a very large dataset, where their technique would
really be needed, and that they provide no comparison with other possibly
faster and better ways to select a subset of training examples, I cannot argue
for acceptance of this paper.


[1] ""Learning From Less Data: Diversified Subset Selection and
Active Learning in Image Classification Tasks"", Kaushal et al.
https://arxiv.org/abs/1805.11191

[2] Li, D., & Simske, S. (2011). Training set compression by incremental
clustering. Journal of pattern recognition research, 1, 56-64.

[3] Borovicka, T., Jirina Jr, M., Kordik, P., & Jirina, M. (2012). Selecting
representative data sets. In Advances in data mining knowledge discovery and
applications. InTech.
","The review is critical of the paper's methodology and lack of comparison with existing techniques. The reviewer finds the paper's contribution limited due to its focus on a small dataset and the absence of comparisons with alternative subset selection methods. The reviewer's language, while direct, maintains a professional and constructive tone.",-50.0,50.0
"Paper summary: The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks. The authors show the improvements of this technique against different attacks and show that the robustness of their 2-step approach is comparable to the iterative multi-step methods. 

The paper presents an interesting technique, is nicely written and easy to read. The fact that their low-cost 2-step method achieves is robust enough to iterative multi-step methods that are expensive is significant.  

Pros: 
1) The technique is low-cost as compared to other expensive techniques like PGD and IFGSM 
2) The technique tries to use the categorical distribution of the generated example in the first step to generate an example in the second step, such that the generated image is most different from the first. This is important and different from the most common technique of iteratively maximizing the loss between the generated samples. 
3) The authors show the effetiveness  and improvement of the approach to various attack methods as compared to existing defense techniques
4) The authors evaluate their technique on MNIST and SVHN datasets


Cons or shortcomings/things that need more explanation :
1) It would have been really good to the kind of adversarial examples generated by this technique look like as compared to the examples generated by the other strategies. 
2) In table 2, for the substitute models of FGSM trained on H and S labels (rows 2 and 5), it is unclear why the accuracies are so low when attacked on FGSM (hard) and FGSM(soft) models. 
 ","The reviewer clearly states that they find the paper interesting and significant. They highlight the strengths of the paper, such as the novelty and effectiveness of the proposed method. While they do point out areas for improvement, these are presented as constructive suggestions rather than harsh criticisms. The language used is objective and professional throughout.",75.0,80.0
"In the manuscript entitled ""Likelihood-based Permutation Invariant Loss Function for Probability Distributions"" the authors propose a loss function for training against instances in which ordering within the data vector is unimportant.  I do not find the proposed loss function to be well motivated, find a number of confusing points (errors?) in the manuscript, and do not easily follow what was done in the examples.

First, it should be noted that this is a very restricted consideration of what it means to compare two sets since only sets of equal size are under consideration; this is fundamentally different to the ambitions of e.g. the Hausdorff measure as used in analysis.  The logsumexp formulation of the proposed measure is unsatisfactory to me as it directly averages over each of the independent probabilities that a given element is a member of the target set, rather than integrating over the combinatorial set of probabilities for each set of complete possible matches.  Moreover, the loss function H() is not necessarily representative of a generative distribution.

The definition of the Hausdorff distance given is directional and is therefore not a metric, contrary to what is stated on page 2.

I find the description of the problem domain confusing on page 3: the space [0,1]^NxF is described as binary, but then values of log y_i and log (1-y_i) are computed with y in [0,1] so we must imagine these are in fact elements in the open set of reals: (0,1).

Clarity of the examples could be greatly improved, in particular by explaining precisely what is the objective of each task and what are the 'ingredients' we begin with.","The review starts with a negative statement about the core proposal of the paper, finding it ""not well motivated."" It continues by listing several issues, including ""confusing points (errors?)"" and difficulty following the examples. The reviewer also uses phrases like ""unsatisfactory"" and ""not necessarily representative."" All of this points towards a negative sentiment. However, the language, while direct and critical, maintains a professional and academic tone. There are no personal attacks or disrespectful language, suggesting a neutral-to-polite approach.",-60.0,20.0
"This paper proposed the use of uncertainty measure evaluated by the prior network framework in (Malinin and Gales 2018) to detect adversarial inputs. Empirically, the best detector against three L_infinity based attacks (FGSM, BIM and MIM), is a prior network that is adversarially trained with FGSM, in both white-box and black-box settings. The results also showed superior performance over a detector based on Monte Carlo Dropout methods (MCDP). Although the idea is interesting and the presented results seem promising, there are some key experiments lacking that may prevent this work from making its claims on robustness and detectability. The detailed comments are as follows.

1. Detection performance against high-confidence adversarial examples is lacking : In many of Carlini-Wagner papers, they showed that some detection methods become weak by simply increasing the confidence parameter (kappa) in the CW attack. The three attacks considered in this work, FGSM, BIM, and MIM are all L_infinity attacks, which are known to introduce unnecessary noises due to the definition of L_infinitiy norm. On the other hand, CW attack is a strong L2 attack and it also offers a way of tuning confidence of the adversarial example. In addition, a variant of CW L2 attack, called Elastic-Net attack https://arxiv.org/abs/1709.04114, is able to generate L1-norm based adversarial examples that can bypass many detection methods. Without the results of attack performance vs different confidence levels against strong L1 and L2 attacks, the detection performance is less convincing. 

2. Lack of comparison to existing works - there are several detection works that already used uncertainty in detection. A representative paper is MagNet https://arxiv.org/abs/1705.09064 . MagNet paper showed that detection against FGSM/BIM is easy (even without adversarial training), and shows some level of robustness against CW L2 attack when the attacker is unaware of the detection. Later on, MagNet has been bypassed if the detection is known to the adversary https://arxiv.org/abs/1711.08478. Since MagNet and this paper have similar detection methodology using uncertainty, and the detection performance seems similar, the authors are suggested to include MagNet for comparison.

3. The objective of adaptive adversarial attack is unclear - inspecting how MagNet's detection performance is degraded when the attacker knows the detection mechanism https://arxiv.org/abs/1711.08478, the authors should do an adaptive attack that directly includes eqn (8) as one of the attack loss term, rather than using the KL term. In addition, if there is randomness in calculating the MI term for adaptive attacks, then averaged gradients over randomness should be used in adaptive attacks. Lastly, CW L2/EAD L1 attacks with an additional loss term using (8) should be compared.

4. The white-box attacks in Fig. 2 (b) to (c) seem to be quite weak - not be able to reach 100% success rate (saturates around 90%) when using BIM and MIM on the undefended model (DNN) with large attack strength. This might suggest some potential programming errors or incorrect attack implementation. 

5. What black-box attack is implemented in this work? It's not clear what kind of black-box attack is implemented in this paper: is it transfer attack? score-based black-box attack? or decision-based black-box attack? Can the proposed method be robust to these three different settings?

6. This paper heavily relies on the work in  (Malinin and Gales 2018), and basically treats adversarial input detection as an out-of-distribution detection problem. Please emphasize the major differences and differentiate the contributions between these two works.

7. In Fig. 2, it seems that adversarial training with FGSM is actually the key factor that makes the detection work (by comparing PN vs PN-ADV in (b) and (c)). To justify the utility of the proposed metric in detection adversarial inputs, the authors are suggested to run MCDP on FGSM-trained model and compare the performance with PN-ADV.","The review acknowledges the paper's interesting idea and promising results, leaning towards the positive side. However, it raises several significant concerns and suggests substantial revisions and additional experiments. The language is formal, direct, and critical in its academic context but maintains a professional and respectful tone.",40.0,70.0
"In this paper, authors proposed an ensemble approach for query reformulation (QR).  The basic idea is that 1) train a bunch of models/sub-agents on subsets, e.g., randomly partitioned, of the training data; 2) and then train an additional meta model/meta-agent to aggregate the results from the step 1).  They conduct experiments on document retrieval and question answering tasks to show the effectiveness of the proposed model.

This paper is well written and easy to follow.  
However there are several my concerns. 

1. It is counter intuitive, e.g., why sub-agents trained on full training dataset obtain worse results than on its subset. Regarding diversity, one may use different random seeds or different dropout rates instead of sample a subset of training data. 

2. The baseline is much lower than the current SOTA systems. Such as the best result on SearchQA in this paper is 50.5 in terms of F1 score. However R3 and Re-Ranker obtains 55.3 and 60.6 respectively. Could the proposed approach be adapted on those models? Note that those SOTA systems are released.

3. The proposed system is quite similar to Nogueira& Cho 2017 and Buck et al. 2018. I'm not very sure the contribution of this work and its novelty.  

Questions:
1. Why the authors didn't use beam search during the sub-agent training? 
2. It seems that the proposed framework is a pipeline model: firstly it trains a bunch of sub-agents; and then trains meta-agent. Is it possible to fine-tune the model jointly?
3. What is Extra Budget in Table 1?    ","The reviewer starts with positive remarks, highlighting the paper's clarity. However, they raise several concerns, questioning the method's intuition and novelty while pointing out limitations in comparison to existing SOTA systems. The numerous concerns, particularly regarding the core idea's validity, suggest a leaning towards the negative side. While the reviewer maintains a professional tone, the phrasing, like ""counter-intuitive"" and questioning the contribution, indicates a slightly less than neutral politeness level.",-20.0,-10.0
"TITLE
A VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION

REVIEW SUMMARY

Well written, interesting new idea, modest technical contribution, limited demonstration.

PAPER SUMMARY

The paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. 

QUALITY

The work appears technically sound except for minor typos. 

CLARITY

Overall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.

ORIGINALITY

The method is (to my knowledge) novel. 

SIGNIFICANCE

I think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. 

FURTHER COMMENTS

page 3

The presentation of the VAE objective is a bit oblique. The statement ""they require a different objectiv function"" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)

""with \hat v  the reconstructed vector"" Not clear. I assume \hat v is reconstructed from a sample from q given v ?

There is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\lambda_2)^(k_2)

","The reviewer explicitly states ""I think this paper is a significant contribution"" and ""I believe there should be a wide general interest."" They find the paper well-written, the idea interesting, and are motivated to try the approach. While they have some constructive criticism and suggestions for improvement, this is typical for a peer review and does not negate the positive sentiment. The language used is formal and respectful throughout.",75.0,100.0
"The authors present two methods for learning a similarity score between pairs of graphs. They first is to use a shared GNN for each graph to produce independent graph embeddings on which a similarity score is computed. The authors improve this model using pairs of graphs as input and utilizing a cross-graph attention-mechanism in combination with graph convolution. The proposed approach is evaluated on synthetic and real world tasks. It is clearly shown that the proposed approach of cross-graph attention is useful for the given task (at the cost of extra computation).

A main contribution of the article is that ideas from graph matching are introduced to graph neural networks and it is clearly shown that this is beneficial. However, in my opinion the intuition, effect and limitations of the cross-graph attention mechanism should be described in more detail. I like the visualizations of the cross-graph attention, which gives the impression that the process converges to a bijection between the nodes. However, this is not the case for graphs with symmetries (automorphisms); consider, e.g., two star graphs. A discussion of such examples would be helpful and would make the concept of cross-graph attention clearer.

The experimental comparison is largely convincing. However, the proposed approach is motivated by graph matching and a connection to the graph edit distance is implied. However, in the experimental comparison graph kernels are used as baseline. I would like to suggest to also use a simple heuristics for the graph edit distance as a baseline (Riesen, Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision Computing, 27(7), 2009).


There are several other questions that have not been sufficiently addressed in the article.

* In Eq. 3, self-attention is used to compute graph level representations to ""only focus on important nodes in the graph"". How can this be reconciled with the idea of measuring similarities across the whole graph? Can you give more insights in how the attention coefficients vary for positive as well as negative examples? How much does the self-attention affects the performance of the model in contrast to mean or sum aggregation?
* Why do you chose the cross-graph similarity to be non-trainable? Might there be any benefits in doing so?
* The note on page 5 is misleading because two isomorphic graphs will lead to identical representations even if communication is not reduced to zero vectors (this happens neither theoretically nor in practice).
* Although theoretical complexity of the proposed approach is mentioned, how much slower is the proposed approach in practice? As similarity is computed for every pair of nodes across two graphs, the proposed approach, as you said, will not scale. In practice, how would one solve this problem given two very large graphs which do not fit into GPU memory? To what extent can sampling strategies be used (e.g., from GraphSAGE)? Some discussion on this would be very fruitful.


In summary, I think that this is an interesting article, which can be accepted for ICLR provided that the cross-graph attention mechanism is discussed in more detail.


Minor remarks:

* p3: The references provided for the graph edit distance in fact consider the (more specific) maximum common subgraph problem.","The review is overall positive, acknowledging the novelty and contribution of the paper. The reviewer finds the approach interesting and the experimental comparison largely convincing. However, they also raise several important questions and suggest improvements, particularly regarding a deeper explanation of the cross-graph attention mechanism and its limitations. The tone is constructive and polite, suggesting specific ways to enhance the paper. Therefore, the sentiment is positive, but with room for improvement, reflected in a score closer to 'positive' than 'neutral'. The politeness is consistently high, using professional and considerate language.",65.0,85.0
"The paper proposes deep learning extension of the classic paradigm of 'conformal prediction'. Conformal prediction is similar to multi-label classification, but with a statistical sound way of thresholding each (class-specific) classifier: if our confidence in the assignment of an x to a class y is smaller than \alpha, then we say 'do not know / cannot classify'). This is interesting when we expect out of distribution samples (e.g., adversarial ones).

I think this paper, which is very well written, would make for nice discussions at ICLR, because it is (to my knowledge) the first that presents a deep implementation of the conformal prediction paradigm.  However, there are a couple of issues, which is why I think it is definitely not a must have at ICLR. The concrete, deep implementation of the approach is rather straightforward and substandard for ICLR: Features are taken from an existing, trained SOTA DNN, then input KDE, based on which for each class the quantiles are computed (using a validation set). Thus, feature and hypothesis learning are not coupled, and the approach requires quite a lot of samples per class (however, oftentimes in multi-label prediction we observe a Zipf law, ie many classes have fewer than five examples). Furthermore, there is no coupling between the classes; each class is learned separately; very unlikely this will work better than a properly trained multi-class or (e.g., one-vs.-rest) multi-label classifier in practice. Since a validation set is used to compute the quantiles, substantial 'power' is lost (data not used very efficiently; although that could be improved at the expense of expensive CV procedures).
","The reviewer starts with positive remarks, acknowledging the novelty and good writing of the paper. However, they then express several concerns about the simplicity of the implementation and potential limitations. They find the approach ""straightforward and substandard"" and point out issues like the lack of coupling between feature and hypothesis learning and the inefficient use of data. The overall tone suggests the reviewer leans towards rejection. Therefore, the sentiment is somewhat negative, but not overly harsh. The language remains professional and polite throughout.",-30.0,60.0
"===========================
Since the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers. 
===========================

Contributions:

The main contribution of this paper is the study of the currently adopted evaluation metrics for textual GAN models. It was shown that BLEU and Self-BLEU scores used by previous work are insufficient to evaluate textual GAN models, and the authors propose that Frechet Distance and reverse Language Model scores can be a good complement to the above BLEU score evaluations. 

Detailed Comments:

(1) Novelty: It seems to me that this paper is timely, as developing GAN models for text generation gains more and more attention in the research community, and it is indeed much needed to provide good evaluation methods. The proposed new metrics seem proper, and the observation that most GAN models do not yield obviously better results than conventional LM is also insightful. 

(2) Presentation: This paper is generally well-written and easy to follow. However, when discussing related work in section 3.1, I think one literature [*] is missed. It uses annealed softmax to approximate argmax for textual GAN. 

[*] Adversarial Feature Matching for Text Generation, ICML 2017

(3) Evaluation: The experiments are generally well-executed, with some questions listed below.

Questions:

(1) I have some concerns in terms of human evaluation. Though human evaluation is the golden metric, it seems that presenting individual sentences to human raters does not account diversity into consideration. Therefore, systems that generate high quality samples but with less diversity will get a high score in terms of human evalution. Can the authors provide some discussion on this? And if this is the case, how will this change the conclusions in this paper?

(2) I understand why the authors use simplified GAN models for evaluation. However, if the models are not simplified, what the performance will be for LeakGAN and MaskGAN, for example? This seems to be relatively easy to evaluate since the code is open sourced. 

Minor issues:

(1) I think the citation format needs to be changed. For example, in many places, it is more natural to use ""(Hassan et al., 2018)"" than ""Hassan et al. (2018)"" for example. ","The reviewer lowers the score due to lack of response to previous comments, indicating a slightly negative sentiment. However, the reviewer still believes the paper has potential and provides constructive feedback. The language used is formal and professional, without any rudeness.",-10.0,50.0
"The paper intends to utilize natural gradient induced by Wasserstein-2 distance to train the generator in GAN. Starting from the dynamical formulation of optimal transport, the authors propose the Wasserstein proximal operator as a regularization, which is simple in form and fast to compute. The proximal operator is added to training the generator, unlike most other regularizations that focus on the discriminator. This is an interesting direction. 

The motivation is clear but by so many steps of approximation and relaxation, the authors didn’t address what is the final regularization actually corresponding to? Personally I am not convinced that theoretically the proposed training method is better than the standard SGD. The illustration example in the paper is not very helpful as it didn’t show how the proposed proximal operator works. The proximal operator serves as a regularization and it introduces some error, I would like to know how does this carry over to the whole training procedure. 

In GAN, the optimal discriminator depends on the current generator. Many approaches to GAN training (i.e. WGAN-GP) advocates to update the generator once in every “outer-iteration”. I am not sure how the proposed approach fit in those training schemes.

In the simulation, the difference is not very significant, especially in FID vs iteration number. This could be due to parameter tuning in standard WGAN-GP. I encourage more simulation studies and take more GAN structures into consideration. 

Lastly, the stability mentioned in the paper lacks a formal definition. Is it the variance of the curves? Is it how robust the model is against outer iterations?","The review starts with a positive sentiment, acknowledging the interesting direction and clear motivation. However, it then raises several concerns about the theoretical grounding, practical implementation, and limited experimental validation. The reviewer questions the connection between the proposed regularization and the actual optimization process, finds the illustrative example unhelpful, and points out the lack of clarity regarding integration with existing GAN training schemes. The reviewer also suggests more comprehensive simulations and a clearer definition of ""stability."" Overall, the review leans towards the critical side, but it maintains a professional and constructive tone.",-10.0,60.0
"The paper proposes a technique (well, two) to prune convolutional layers to reduce the required amount of computation when  the convolutions are done using the winograd algorithm. Winograd convolutions first transform the image and the filter, apply a multiplication in the transformed space, and then retransform the image back to the intended image space. The transformation of the filter, however, means that sparsity in the regular domain does not translate to sparsity in the winograd domain.

This paper presents two techniques to achieve sparsity in the winograd domain: approximating winograd sparsity based on sparsity in the regular domain (thereby pruning with a non uniform cost model) and pruning in winograd space directly. The actual implementation alternates the first pruning technique and retraining the network with fixed sparsity followed by alternating winograd-space pruning and retraining. The tricky part is retraining in winograd space, which seems to require fine tuned per coordinate learning rates.

My main concern is that the method feels fairly fragile and hyperparameter-heavy: tuning all the learning rates and sparsity rates for all these iterated levels of pruning doesn't seem easy. Similarly, it's unclear why the first stage of pruning is even needed if it's possible to prune and fine tune in winograd space directly. It's unclear from reading the paper how, given a computational budget, to decide the time spent in each phase of the process.

","The review is quite balanced. It explains the proposed technique and its advantages, but also points out potential weaknesses and concerns. The reviewer is skeptical about the complexity and the need for fine-tuning, suggesting the method might be ""fragile"" and ""hyperparameter-heavy."" This suggests a somewhat negative sentiment. However, the language remains objective and professional throughout, without resorting to harsh or disrespectful language.",-20.0,60.0
"This paper presents a technique for embedding words in hyperbolic space, which extends previous non-euclidean methods to non-structured data like free text. The authors provide a new gradient based method for creating the embeddings and then evaluate them on standard word embedding benchmarks. Overall the paper is very well written and well executed. They find that in the low dimensions the approach outperforms standard Euclidean space methods while in higher dimensions this advantage disappears.

The results do not try to claim state of the art on all benchmarks, which I find refreshing and I appreciate the authors candor in giving an honest presentation of their results. Overall, I enjoyed this paper and am eager to see how the authors develop the approach further. 

However, along these same lines it would be great to have the authors provide more discussion about the next steps and potential applications for this approach. Is the interest here purely methodological? Are there potential use cases where they believe this approach might be superior to Euclidean approaches? More detail in the discussion and intro about the trajectory of this work would help the reader understand the methodological and application-specific implications. 

Pros:
- Clearly written and results are presented in a straightforward manner. 
- Extension of analogy reasoning to non-euclidean spaces.

Cons:
- Lack of clear motivation and compelling use case. 
- It would be nice to have a visualization of the approach in 2-dimensions. While Figure 3 is instructive for how analogies work in this space, it would be great to visualize an entire dataset. I'm sure that the proposed embeddings would result in  a very different space than euclidean embeddings (as the Poincare embedding paper showed), so it would be great to have at least one visualization of an embedded dataset. Presumably this would play to the strengths of the approach as it excels in lower dimensions. 
-  The largest of embedding dimension tested was 100, and it is common to use much larger embeddings of 500-d. Do the trends they observe continue to larger dimensions, e.g. is the performance gap even larger in higher dimensions? ","The reviewer explicitly states that they ""enjoyed this paper"" and praises the authors for their honesty and straightforwardness. They also express interest in seeing how the work is developed further. While they have some constructive criticism and suggestions for improvement, these are presented politely and with the aim of enhancing the paper's impact. Therefore, the sentiment is positive, and the language is polite.",75.0,90.0
"This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian). The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).  
  
Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.  Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score). Some potential options include:

1) Faster training times. It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).  However, the authors only presented per-batch processing times as opposed to overall training time for these models.   

2) Stabler training. Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure. The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.

3) Usefulness of the CW distance outside of the autoencoder context.
Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE). Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?

Without demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.

Other Comments:

- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians. See for example: 

Chwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf

which like CW-distance is also a quadratic-time closed-form distance between samples and a target density.

Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives. 

- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?

After reading the revision: I have raised my score by 1 point and recommend acceptance.","The reviewer provides constructive criticism, suggesting improvements and further investigations. While they acknowledge the paper's quality and clarity, they don't see a significant practical advance yet. The slightly positive sentiment stems from the reviewer raising their score after the revision, indicating a move towards acceptance.",10.0,80.0
"This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.

Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.

Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.

PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.  ","The reviewer finds the paper's novelty limited, arguing its core idea is a known concept (proximal gradient descent) applied to a different context. They present a detailed explanation of how the paper's framework is essentially a stochastic version of this existing method. The reviewer also mentions a lack of clarity in the paper's motivation. This suggests a negative sentiment. The language, while critical, maintains a professional and analytical tone without resorting to personal attacks or disrespectful language. Therefore, the politeness score leans towards neutral/polite.",-50.0,20.0
"The paper proposes the use of Survival Continuous Ranked Probability score instead of maximum likelihood estimation for personalised probabilistic forecasts of time-to-event data, thus estimating a distribution over future time. The authors describe the evaluation their method using (1) proper scoring rule objectives; (2) evaluation of calibration using sharpness as a metric; (3) the survival precision recall curve. The authors then apply these techniques to predicting time-to-mortality using an RNN that takes EHR patient records to predict the probability of death at a given time point. It’s not clear how this is related to the Survival CRPS model or how this model is incorporated into the RNN.
Overall, this is an important framework for estimating personalised predictions of survival events for patients with interval-censored data. The authors present a well thought-out paper with clearly and realistically articulated modelling  assumptions. The authors also give an excellent critique of the underlying assumptions of current state-of-the-art survival methods. The authors are also to be commended for the mathematical elegance 
Although the paper is very well written and extremely well structured, I struggled with the lack of experiments available in the paper.
The text embedded in Figure 3 is too small. 
The results section is somewhat sparse. Although the mathematical formulation is well-motivated and structured, it’s not clear what the contribution of this work is. The difference between CRPS-INTVL and MLE-INTVL is incremental and it’s unclear what the significant benefits are of CRPS vs MLE. What would the interpretation of these differences in a real-world setting? 
","The reviewer explicitly states positive aspects like ""well thought-out"", ""clearly and realistically articulated"", ""excellent critique"", ""mathematical elegance"", ""very well written and extremely well structured"". This points towards a positive sentiment. However, the reviewer also mentions some concerns and areas of improvement, which moderate the positivity. Hence, the sentiment is not overly positive but leans towards constructive criticism. The language used is polite and professional throughout, without any harsh or disrespectful wording. The reviewer expresses their concerns and suggestions constructively.",60.0,80.0
"This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck.

These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident).

The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier.

This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks.


[1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf
[2] https://arxiv.org/abs/1312.6114
[3] https://arxiv.org/abs/1801.02774","The reviewer explicitly states ""This is a solid paper, in my opinion."" and praises the clarity, experimental setup, and insights of the work. They also express confidence in the reproducibility of the results. All of this points to a positive sentiment. The language used throughout is formal, respectful, and constructive, indicating a high level of politeness.",85.0,90.0
"Summary

The authors aim to do continual learning to solve dependent tasks using ""single-stage end-to-end learning"". The resulting ""Unicorn"" agent trains on all tasks simultaneously. The idea is to use multi-task ""off-policy learning"", which uses (old) trajectories (experience) from task A to help learning on a related task B. Authors further distinguish between goals (inputs to Q) and tasks (different reward functions). A goal might a color/shape of an object to pick up.

The core model is a UVFA that learns a goal-conditioned Q-function Q(s,a,g). 

Some technical aspects: 
- use n-step returns.
- when training Q on goal g_i, authors also trajectories that were generated using Q conditioned on another goal(s) g_j. They then truncate the returns for task i when an action taken is not optimal under Q(s,a,g_j) conditioned on goal j. The intuition (seems) to be that this 
- authors do not use experience replay or a target Q-function, since the parallelized implementation is reported to be stable enough.
- unicorn sees all train task reward functions during training (but not hold out task rewards).
- unicorn is tested on several 3d maze environments with key-lock etc semantics. The tasks / goals seem simple, and the dependency is defined by changing colors / shapes of objects to be picked up. Authors argue unicorn has to learn to relate task rewards to these goal features.
- unicorn is compared against baselines that 1) do single-task learning (expert) 2) learn on a sum of task rewards (glutton), 3) uniformly random baseline. 
- authors show that 1) unicorn performs better on train tasks 2) performs better on hold-out tasks. Also, authors show results for zero-shot transfer learning, with adding abstract tasks (extra reward for picking up any object) improving performance, 

Pro
- Simple approach (e.g., no experience replay etc), and uses only a limited set of techniques (e.g., reward truncation). 
- Reward performance suggests the model has more properly related goal features to different payoffs.
- Analysis of qualitative behavior is nice.

Con
- The writing is a bit dense in places, e.g., the discussion of baselines is a bit hard to read.
- Description of algorithm is wrapped in long text, a clear algorithm box would make the approach much clearer.
- Not clear what kind of hyperparameters are introduced / used / tuned for Unicorn. 
- Authors say ""deep dependency"", but this seems to just refer to different colors / shapes between objects in the env used in the paper. How is ""dependency"" between goals and tasks defined in general? 
- The experimental setting seems a bit limited, authors only show results on a single domain, and do not offer rigorous definitions. This makes the scope of the paper rather limited.  

Reproducibility: 
- It's not clear what the variance in the baseline performance is (variance only shown for unicorn).","The review acknowledges both positive aspects ('Pro') and negative aspects ('Con'), suggesting a balanced sentiment. While it lists several limitations, these are largely constructive and aimed at improving the paper. The language used is formal and focused on the technical aspects, without resorting to harsh or negative phrasing.",50.0,70.0
"Summary:
The paper proposes a system of semantic segmentation based on sequential processing of the image in a patch-wise manner with multiple ""actors"", sharing a common external memory. This approach stands in contrast to the more usual approach of single-shot prediction for the whole image, where encoder-decoder architectures or dilated convolutions are used to capture the global context. The authors then discuss three-variants of this method, out of which two use external memory (Bi-MANN, SHAMANN), and one uses memory shared between actors (SHAMANN). Results are presented on segmentation of lung X-ray data and on MNIST digit completion.

Comments:
The paper is easy to read. The authors cite the relevant literature on the baseline semantic segmentation methods, as well as neural networks with external memories. However, similar patch-wise and sequential methods have been presented in the literature (e.g. https://arxiv.org/abs/1506.07452), including ones with external storage (e.g. https://www.nature.com/articles/s41592-018-0049-4), but these are not discussed as prior work.

Overall, the proposed approach is interesting, but significantly more complex than both the baselines and prior work. As is, the experimental results are not compelling enough to justify this (lack of clear quantitative improvement over state of the art). My recommendation would be to conduct additional experiments on semantic segmentation benchmark datasets. The proposed method seems promising for volumetric data as the authors note, but this also needs to be demonstrated experimentally.

Some more specific & technical questions follow:
- In Table 1, how is the confidence interval for the Dice score computed?
- Have any experiments been done with more than 2 actors?
- How exactly is the patch sequence formed, i.e. what is the spatial order of the patches? How much to the results depend on this order, if at all?
- In the discussion on page 6, it seems to be implied that the reduced parameter count should allow more efficient application to volumetric data. This is a bit surprising, since with modern networks it is usually the input size that is limiting, not the number of network parameters.
- Have experiments with Bi-MANN and Bi-LSTM been done on the X-ray segmentation data? How do the results compare to SHAMANN?
- How does the inference and training time compare to the baseline methods?","The review is mostly positive in its language, using terms like ""interesting"" and ""promising."" However, it also highlights significant shortcomings in terms of experimental validation and comparison with existing methods. The reviewer provides constructive criticism and offers concrete suggestions for improvement. The tone is professional and polite throughout, without resorting to harsh or negative language.",40.0,80.0
"The paper is easy to read and the presentation is clear, and I really appreciate this.

The authors address the very important topic of feature extraction and state representation learning. New results in this area are always valuable and welcome. However, my feeling is that the paper falls short in terms of making sufficient new contributions for an ICLR paper. 

1. The authors propose to learn a state representation by either training using a combined loss function, or training several representations using multiple loss functions followed by stacking. These are standard and well-known techniques in machine learning. The key contribution one looks for is in terms of new insights on why and when each approach works. The paper fails to provide much insight in this regard. Take this simple scenario: Suppose my input image is actually generated by a linear map plus gaussian noise on the true states. Then I can simply use a PCA as my ""auto encoder"" and happily learn a high quality state representation close to the ground truth. We know why this works. In the real task, the image is a complex non-linear transformation of the true states. What insights do I gain from this work in terms of how I should tackle this?

2. Section 3 states some desirable characteristics in constructing a state representation. These are well-known and fundamental aspects of machine learning -- applicable to almost all models that we want to learn. In this sense, I do not find the section very informative.

3. The empirical results (say, Table 1) seem too noisy to interpret (other than that using the ground truth provides the best performance). It almost seems to suggest that one should simply use random features (as done in the ""extreme learning machine"" approach). Again, not much insight to draw from this.

4. Last comment. Suppose I have a new robotic goal-directed task and my inputs are camera images. Does this work tell me something that I don't already know in terms of learning new feature representation that is highly suitable for my task?



","The reviewer starts with a positive note, acknowledging the clarity and importance of the topic. However, the overall tone becomes increasingly negative as they point out the lack of novel contributions and insights. They use phrases like ""falls short,"" ""fails to provide,"" ""not much insight,"" and ""too noisy to interpret,"" which all indicate a negative sentiment. Despite the criticism, the reviewer maintains a professional and polite tone throughout, focusing on the scientific aspects rather than resorting to personal attacks.",-40.0,60.0
"The reviewer finds that the proposed method interesting. The model is very clean, and the implication in causal inference is significant. The writing is also clean and clear. The reviewer has several concerns:

1) the algorithm seems not very scalable. In the two subproblems, there is one solved by a large number of parallel SDRs. SDR is quite expensive, and for each column in the data matrix one has to solve an SDR in each iteration. This is too much for large scale recommender systems. In fact, in the experiment 1 on MovieLens, the algorithm was only tested on a not-so-large dataset and run 5 iterations. The reviewer feels that more scenarios should be tested (e.g., more iterations, various sizes of dataset, etc.). Fixing the number of iterations also sounds a bit funny since it is more intuitive to stop the algorithm using some validation set or when the algorithm converges under a certain criterion.

2) The algorithm works with *probability* of binary data. This is quite hard to estimate in practice. For example, people ``'likes'' a movie for only once. It is hard to tell what is the probability of generating this ````""like"". It seems that the experiment part of this paper did not clearly state how to obtain the probability that the algorithm needs.

3) The proposed method is a special nonnegative matrix factorization, which could be unidentifiable. How to circumvent such situation? Since identifiability of NMF affects interpretability a lot.
 ","The review starts with positive statements, indicating interest in the method, praising its clarity, and acknowledging the significance of the implications. This suggests a positive sentiment. However, the reviewer also raises several concerns, indicating areas for improvement. While the concerns are substantial, they are presented constructively with specific suggestions for improvement, suggesting a polite tone overall.",60.0,70.0
"The paper proposes a new method for anomaly detection using deep learning. It works as follows. 

The method is based on the recent Multiple-Hypotheses predictions (MHP) model, the impact of which is yet unclear/questionable. The idea in MHP is to represent the data using multiple models. Depending on the part of the space where an instance falls, a different model is active. In this paper this is realized using VAEs. The details are unclear (the paper is poorly written and lacks some detailed explainations), but I am assuming that for each hypothesis (ie region of the space) different en- and decoder parameters are learned (sharing the same variational prior??). The authors mention that below this final layer all hypothesis share the same network parameters. An adversarial loss is added to the model (how that is done is not described; the relevant equation (5) uses L_hyp which is not defined) to avoid the mode collapse.

What is interesting about the paper:
- First of all, pushing the the MHP framework towards AD could be relevant by its own right for a very small subcommunity that is interested in this method
- The idea of using the adv loss for avoiding mode collapse can be useful in other settings; this is def a that I learned from the paper
- The method might actually work rather well in practice

Votum. As outlined above, the paper makes some rather interesting points, but is not well written and lacks some details. I am not entirely convinced that AD and MHP is a killer combination, but the experimental results are ok, nothing to complain here (except the usual bla: make it larger, more, etc), but honestly they really fine (maybe compare also again against more related work, e.g., Ruff et al ICML 2018).","The reviewer acknowledges some interesting points and potential in the paper's contribution, particularly in the application of MHP to AD and the use of adversarial loss. They find the experimental results to be ""fine"" and even acknowledge learning something from the paper. However, they also express significant concerns about the clarity and completeness of the paper, stating that it is ""poorly written"" and lacks detail. The reviewer's tone is critical but not disrespectful.",20.0,40.0
"This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward.

I found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good.

I had some issues with the motivations of some of the loss functions. 
- The JSD-based objective makes sense, but I don't think it's correct to call it an ""approximation"" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals.
- It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings.
- For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)?

Other notes:
Page 2:
Figure 1 is awfully confusing. Could this be clarified a little bit? I’m not sure what the small dots or their colors are supposed to represent.

Could diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)?

Why were the vision experiments stopped at 500 x 100k (500 million) frames?  I can’t validate the SOTA claims, but it seems like the model is still improving: are there’s further experiments?

An ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).","The reviewer provides positive feedback at the beginning, praising the paper's clarity and novelty. They find the approach interesting and well-structured. However, they also raise several questions and suggest improvements, indicating a critical but constructive approach. The tone is professional and polite throughout, focusing on the scientific content and potential enhancements.",50.0,75.0
"The paper proposes a meta algorithm to train a network with noisy labels.
It is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. 

My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. 
Hence it is difficult to evaluate to performance of the proposed method.
At the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. 
    
","The review is critical of the paper, pointing out flaws in the methodology and results. It questions the validity of the results and suggests additional experiments. The language used, while direct, is professional and typical of academic peer reviews. There's no personal attack or disrespectful tone. Therefore, the sentiment leans negative, and the politeness remains neutral.",-30.0,0.0
"Summary:
This paper effectively learns a variant of a Deep Value Network (Gygli et al 2017), a model consisting of an energy network that assigns scores to input-output tuples that is trained to mimic a task-specific loss. The primary differences between the model presented in this work (titled LDRSP) and DVNs are twofold: first, the initial label prediction used at test time for inference is the output of a model rather than being initialized to all zeros. Second, a GAN-inspired loss is used to train both the scoring function and the initial prediction estimator. This new setup is compared against a variety of recent structured prediction methods on the tasks of multilabel classification, semantic segmentation, and 3-class face segmentation.

Comments:
I think the ideas presented in this paper are interesting, but I think their presentation could be a bit clearer. As mentioned in the summary, what you’re presenting is still more or less a deep-value network with some additions - however, you don’t refer to it as such in the body of the paper anywhere I saw. The first addition is the use of a learned model to produce the initial prediction; this is a natural extension to Deep Value Networks, and on its own is somewhat incremental in nature. I do not think you adequately explained why you chose to use a GAN-like loss to learn these models. Another baseline that would have helped justify its use would be to train your G model to predict structured outputs in the standard way (max-margin or cross-entropy loss) and then train your energy function in the DVN way. 

The experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine. You make appropriate comparisons against previous structured prediction models as well as against different types of GAN-like losses. But, as I mentioned before, I think you needed to have more comparisons against different ways of training these networks that do not follow a GAN-inspired framework. 

Overall, I like the new ideas in this paper but I think a few more experimental settings are required before they should be published.

=== after rebuttal ===

I appreciate the response, but I still think further analysis of the model is needed to understand where the gains in performance are coming from. The claim is that this is due to the adversarial loss used, but without further ablations I feel this is too strong a claim to be making given the current evidence.
","The reviewer finds the ideas interesting but suggests the presentation could be clearer and requires further experimental settings. While the reviewer acknowledges the novelty of the GAN-like loss function and appreciates the comparisons made, they express concerns about the lack of comparisons with non-GAN based training methods. The overall tone suggests the reviewer leans towards the positive side but has reservations due to the lack of certain comparisons and further analysis.",50.0,70.0
"Summary: Authors proposed a model for input method for mobile or desktop devices. The goal is to convert the input sequence (from one language to another) or predict the next word. Their model is based on an LSTM with modified softmax activation function that is adjustable for large vocabulary sizes. They showed experimental results on Japanese BCCWJ data set.

Clarity: Paper is well-written and well-organized. Notions and methods are clearly expressed. 

Originality: This paper builds on an LSTM model without enough work or idea to show novelty. 

Significance: It is below average. Using LSTM is a well-known method for these types of tasks in the literature. Incremental selective softmax is potentially a good approach, however, this work lacks showing significant improvement. The experiments are limited and are done only on one data set.

More detailed comments:

- My concerns about this work are both on modeling aspects and experiments. Authors mainly focus on highlighting the benefits comparing to n-gram models, and briefly discuss the ongoing developments in neural based models. For example sequential modelings using RNN's have shown promising results in capturing long-term dependencies [1]. Unfortunately authors did not include any discussion on how their approach would compare to that framework nor did they present any experimental comparisons to them.

- Although mentioned briefly in the introduction and related work sections, no analytical or experimental comparisons are made to machine translation approaches when their work is closely related to it. I strongly suggest that authors compare their experimental results to some of benchmarks in neural based machine translation discussed in the related works.

- In the incremental selection softmax, they use ""match"" to return all lexicon items matching the partial sequence. How is this done and what are the effects of it on the computational time of the algorithm? Also, It is not clear how authors correct old probabilities in IS softmax step. As mentioned, they add logits of missing vocabulary to the denominators, how do they keep the properties of softmax so that it sums up to 1? And later in the discussion authors mentioned that in practice they compute union of all missing vocabularies, it is not clear how this is done since the advantage of using IS softmax is expressed to be incremental increasing. 

[1] A.B. Dieng, C. Wang, J. Gao and J. Paisley. TopicRNN: A recurrent neural network with long-range semantic dependency, International Conference on Learning Representations (ICLR), 2017.","The review is negative due to several concerns raised: lack of novelty and significant improvement, limited experiments, missing comparisons with relevant models (RNNs, machine translation), and unclear explanations of the incremental selection softmax. However, the reviewer acknowledges the clarity of the paper and provides constructive criticism with specific recommendations. The language is formal and polite, without resorting to harsh or disrespectful tones.",-50.0,70.0
"This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.

Originality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. 

The experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.
http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
Therefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.

Minor typos:
In Equation 1, the letter i is overloaded.","The reviewer acknowledges the work's potential to improve efficiency but expresses concerns about its novelty and impact. They find the first contribution ""straightforward"" and the second, while interesting, not ""super novel."" The primary concern lies in the experimental results, which the reviewer believes are not compared against a strong enough baseline, making it difficult to assess the significance of the findings. The language, while direct, maintains a professional and respectful tone.",-10.0,60.0
"The privacy definition employed in this work is problematic. The authors claim that ""Privacy can be quantified by the difficulty of reconstructing raw data via a generative model"". This is not justified sufficiently. Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy. 

The proposed method is not appropriately compared with the other methods in experiments.  In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct. At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better ""privacy"". However, the Pareto front of the proposed method is concentrated on a specific point. For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does. In this sense, the proposed method is not comparable with ""noisy"". In my understanding, this concentration occurs because the range of \lambda is inappropriately set. This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range. 

--
Minor:
In Eq. 1, the utility is evaluated as the probability Yi=Yi'. What randomness is considered in this probability?
In Eq 2, privacy is defined as maxmin of |Ii - Ii'|. Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T. 

In page 4. ""The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder."" I could not find any justification for this setting. Why ""exactly reversed mode"" can be the most powerful adversary? What is an exactly reversed mode?

Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously. The resulting model would thus be highly affected by the setting of n and k.  How can you choose k and n?","The review is highly critical of the paper's core premise, methodology, and results. The reviewer questions the fundamental definition of privacy used and finds the experimental comparisons and justifications lacking.  The tone, while direct and critical, maintains a professional and academic demeanor. There are no personal attacks or inflammatory language.",-70.0,60.0
"In the paper, WGAN with a squared zero centered gradient penalty term w.r.t. to a general measure is studied. Under strong assumptions, local stability of a time-continuous gradient ascent/descent dynamical system near an equilibrium point are proven for the new GP term. Experiments show comparable results to the original WGAN-GP formulation w.r.t. FID and inception score.

Overall, I vote for rejecting the paper due to the following reasons:
- The proven convergence theorem is for a time-continuous ""full-batch"" dynamical system, which is very far from what happens in practice (stochastic + time discrete optimization with momentum etc). I don't believe that one can make any conclusions about what is actually happening for GANs from such an idealized setting. Overall, I don't understand why I should care about local stability of that dynamical system.
- Given the previous point I feel the authors draw too strong conclusions from their results. I don't think Theorem 1 gives too many insights about the success of gradient penalty terms.
- There are only marginal improvements in practice over WGAN-GP when using other penalty measures. 

Further remarks:
- In the introduction it is claimed that mode collapse is due to JS divergence and ""low-dimensionality of the data manifold"". This is just a conjecture and the statement should be made more weak.

- The preliminaries on measure theory are unnecessarily complicated (e.g. partly developed in general metric spaces). I suggest that the authors try to simplify the presentation for the considered case of R^n and avoid unnecessarily complicated (""mathy"") definitions as they distract from the actual results. 

==after rebuttal==
After reading the authors rebuttal I increased the my rating to 6 as they addressed some of my doubts. I still think that the studied setting is too idealized, but it is a first step towards an analysis.","The reviewer initially votes to reject the paper (-40) due to concerns about the practicality of the theoretical analysis and limited practical improvements. They find the theoretical setting too idealized and question its relevance to real-world GAN training. The reviewer also points out overly strong conclusions and suggests improvements in clarity and simplification of mathematical presentation. However, after the rebuttal, the reviewer's sentiment shifts towards a more positive outlook (40) as the authors addressed some concerns. They acknowledge the idealized setting while recognizing it as a step towards further analysis. The language used throughout is critical but professional and respectful, staying within the bounds of academic discourse.",40.0,60.0
"The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].
More precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical 
bounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and 
[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some 
conditions) on the change of the layers (layer and interlayer cushion) as well as the activation 
contraction. It is also worth noting that the paper is using a differrent loss function comparing 
to [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.
Although the results seem interesting, the analysis is not convincible for me.
A plus point is that the paper presents interesting numerical experiments showing the promising of the approach.

Major comments:
1) The statement of the Theorem 1 is not clear: 
is it just under the assumptions of the lemmas
or is it under all definitions and lemmas?
2) The proof of Theorem 1 is not clear:
 how do you get the inequality (5)?
how do you get an upper bound on the KL divergence?
 This is not trivial for me!
3) What is \rho in Theorem 1 and in Definition 2?
4) Your remark after Theorem 1 is not clear for me.
  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?
 a simple counter example would fit better the explanation here, I guest.

Minor comments:
1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]
without precisely citations. I wonder how do you obtain your Lemma 1?
2) page3, after formula (1), your loss will first DECREASING, not ""increasing"".
Check the sentence ""Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance....""
","The review starts with a neutral tone, acknowledging the paper's contributions and improvements. However, it quickly transitions into a negative sentiment, expressing concerns about the clarity and validity of the analysis. The reviewer uses phrases like ""not convincible,"" ""not clear,"" and ""This is not trivial for me!"" which indicate dissatisfaction and skepticism. While the reviewer acknowledges the ""interesting numerical experiments,"" the overall focus on the perceived flaws in the analysis suggests a negative sentiment. 

The language used is direct and critical, but not disrespectful. The reviewer points out specific issues and asks for clarification, which is typical in peer reviews. However, phrases like ""This is not trivial for me!"" and ""a simple counter example would fit better the explanation here, I guest"" could be perceived as slightly condescending, lowering the politeness score slightly. ",-30.0,50.0
"This paper studies the generalization properties of a two layer neural network for a nonlinear regression problem where the target function has a finite spectral norm. The generalization bound comprises of an approximation term (dependent on the width) and an estimation term (dependent on spectral norm of the target and scaling as 1/sqrt{n}). 

The key contribution is the derivation of the generalization bound where the estimation term depends on the properties of the target function rather than properties of the class of two layer neural networks. These bounds are instantiated for a class of regularized estimators (with path norm regularization).

1. Theoretical Novelty: While both Theorem 3 (approximation) and Theorem 4 (a posterior generalization) were mostly following known results, the key development seems to be the bound on the path norm of the regularized solution in terms of the spectral norm of the target function. Given that the estimator is a path-norm regularized estimator, this seemed to be an incremental contribution. What would be more interesting is to obtain such a bound for an unregularized estimator: either saying something about the optimization procedure or relating this kind of regularization to properties of the dataset over which it is trained.

2. Regression vs Classification: While the focus of the paper is on a regression problem, the experiments and problem motivation seems to arise from a classification setting. This creates a mismatch between the what the paper is about and the problem that has been motivated. Would it be possible to extend these results to loss functions (other than squared loss ) like cross-entropy loss or hinge loss which indeed work in the classification setting?

3. Comparison with Klusowski & Barron (2016): In the comparison section, it is mentioned that  Klusowski & Barron (2016) analyze a ""similar"" problem and obtain worse generalization bounds. It would be important to know the exact setting in which they obtained their bounds and how do their assumptions compare with the ones made in this paper. The comparison seems incomplete without this.

4. The experiments showcase that the regularized estimator has a better path norm (and expectedly so) but almost similar (in case of MNIST actually better) test accuracy. This defeats the purpose of  showing that the regularized estimator has better generalization ability which is claimed in the introduction as well as the experiment section (calling it ""well-posed""). What this indeed shows is that even though the path norm might be big, the generalization of the estimator is till very good contradicting the statements made. 

5. The numbers shown in Figure 1 and the numbers reported in Table 2 do no match: while the plot shows that the scaled path norm is around 60 for both MNIST and CIFAR-10, the corresponding numbers in the table are 507 and 162. Can you please point out the reason for this discrepancy?

6. Theorem 5 seems to suggest that in the noiseless case, the estimation error would scale as the spectral norm of f^*. Rather, in the noiseless setting, it seems that the correct scaling of the generalization error should be with respect to properties of the regularized estimator and the function class. Even though the spectral norm can be arbitrarily high, the generalization bound should only be dependent on the complexity of functions which can fit the current data well. It would be good to have a comment in the draft on why the current dependence is a better thing and examples where such generalization bounds are indeed better. ","The review is quite critical of the paper, pointing out several areas where the claims are not well-supported or even contradicted by the results. The reviewer acknowledges the theoretical work but deems it ""incremental."" The tone is professional and polite, using constructive criticism and suggesting improvements rather than resorting to harsh language.",-30.0,70.0
"Paper summary: 

As is made clear in the title, this paper sets out to answer the following question: “Why do deep convolutional networks generalize so poorly to small image transformations?”. It focuses on natural image transformations on translation and scaling (rotation is missing though).

The paper proposes two main explanations: 
-	Strided convolution, called subsampling in the paper, ignores the classical sampling theorem,
-	CNNs will not learn invariance because of the (photographers') biases contained in the datasets.

On a general level, the paper is a good read, it is well written and the figures clearly convey the message they’re intended to. Adversarial attacks and robustness of CNNs in general is a very interesting and important topic in ML. The originality of this work is in the approach of the problem, the paper tries to explain the reasons why CNNs are vulnerable. Related works put more emphasis on coming up with novel attacks/defense strategies. Considering natural attacks as done in this submission is particularly interesting as it is probably a more surprising shortcoming of CNNs compared to optimally designed attacks or highly unnatural perturbations. The argument about subsampling (stride) being the reason of not having translational invariance is nice, especially the theoretical insight with the Shannon-Nyquist theorem and the more figurative example on part detectors. There are nevertheless a few major concerns about this work:

Major Concerns:

Theoretical arguments:
The theoretical argument made in this paper is interesting but to make the point stronger a more in-depth explanation would be needed.
-	The step from Eq (2) to Eq (3) is not entirely clear “K does not depend on x_i”, maybe one extra sentence to explain this step would be useful. 
-	Terms introduced such as the basis function B and the set of transformations T could be better defined.
-	For the extension to other types of transformations “While the claim focuses on global translation, it can also be extended to piecewise constant transformations.” it would be important to point out what type of natural transformations can be included in this set.

Empirical evidence:
Experiments are not fully convincing. Additional empirical evidence would be beneficial and necessary to support the claims of this:
-	“A natural criticism of these results is that they are somehow related to the image resizing and inpainting procedures that we used.” This is a very good point and the authors following arguments are not fully convincing. Results with different transformation procedures mentioned in the rest of the paragraph (and probably more) should be included to convince the reader.
-	The theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification. In fact, feature maps of the CNNs that the authors consider do indeed contain many high frequencies.
-	The argument made in part 4 about the photographer’s bias seems valid for general natural transformations, but it does not apply to small transformations such as 1-pixel translations presented in the paper. Also, evidence that datasets without (or less) photographers' bias are less susceptible to natural attacks would make the argument in the paper a lot stronger. 
-	When using 6x6 avg pooling for the VGG16 architecture ”recognition performance decreases somewhat” . Results are only preliminary in the paper, but this statement needs a more thorough experimental backing. It should come with convincing quantitative evidence.
-	Please include some results or citation on other work about test time augmentation to support the statement “still only provides partial invariance”.

References and phrasing:
Generally previous work is well referenced in this paper, although there are some formulations that can be slightly modified to make a clear distinction between what is novel and what is previous work:
-	As is very well shown in the introduction, there is a lot of work on generating adversarial examples that drastically change the output of a CNN. This should be made clear in the abstract, in fact the sentence “In this paper we show that modern CNNs [...] also happens with other realistic small image transformations”  seems to indicate that this is the novel work in the paper. This is also why I believe the first sentence “Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations.” is somewhat contestable. 
-	“We find that modern deep CNNs are not invariant to translations, scalings and other realistic image transformations” as the paper points out earlier this is not a novel finding, so I would use a formulation that makes that clear and gives more emphasis to your own arguments as of why this happens.

Further Comments :
-	Part 5 ""Implications for Practical Systems"" could be moved to discussion as there is no new point and it seems more a reflection on what was already stated.
-	The final sentence of the abstract “Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.” is not necessary, this is clearly true but it isn’t really contested in the ML community.
-	“despite the architecture being explicitly designed to provide such invariances” I agree that this has motivated the use and design of CNNs in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.
-	”jaggedness is greater for the modern, deeper, networks compared to the less modern VGG16 network” might be worth interesting to consider if the residuals have anything to do with it.
","The review is generally positive, acknowledging the paper's readability, interesting approach, and the validity of some arguments. However, it also raises major concerns, particularly regarding the theoretical depth, experimental validation, and clarity in distinguishing novel contributions. The reviewer uses phrases like ""good read,"" ""interesting,"" and ""nice,"" indicating a positive attitude. While the review points out weaknesses, it does so constructively, suggesting improvements and further investigations rather than dismissing the work. The language is formal, objective, and avoids harsh criticism, maintaining a polite and professional tone.",60.0,80.0
"The paper talks about a method to combine preconditioning at the per feature level and Nesterov-like acceleration for SGD optimization.

The explanation of the method in Section 3 should be self-contained.  The main result, computational context, etc., are poorly described, so that it would not be easily understandable to a non-expert.

What was the reason for the choice of the mini batch size of 128.  I would guess that you would actually see interesting differences for the method by varying this parameter.

How does this compare with the FLAG method of Chen et al from AISTATS, which is motivated by similar issues and addresses similar concerns, obtaining stronger results as far as I can tell?

The figures and captions and inserts are extremely hard to read, so much so that I have to trust it when the authors tell me that their results are better.

The empirical evaluation for ""convex problems"" is for LS regression.  Hmm.  Is there not a better convex problem that can be used to illustrate the strength and weaknesses of the method.  If not, why don't you compare to a state-of-the-art least squares solver.

For the empirical results, what looks particularly interesting is some tradeoffs, e.g, a slower initial convergence, that are shown.  Given the limited scope of the empirical evaluations, it's difficult to tell whether there is much to argue for the method.  But those tradeoffs are seen in other contexts, e.g., with subsampled second order methods, and it would be good to understand those tradeoffs, since that might point to where and if a methods such as this is useful.

The conclusions in the conclusion are overly broad.","The review is critical of the paper, pointing out several weaknesses in clarity, methodology, and results. While it acknowledges some interesting aspects, the overall tone suggests significant revisions are needed. The language, however, remains professional and avoids personal attacks, focusing on the work itself.",-30.0,60.0
"This paper studied the Donsker-Varadhan lower bound of KL-divergence. The authors show that with high probability, the DV lower bound is upper bounded by log of the sample size, so if the true KL-divergence is very large, then exponential sample size is needed to make the DV lower bound tight. The same argument holds true for any distribution-free high-confidence lower bound (such as DV lower bound) for KL divergence. Then the authors proposed to use an upper bound for entropy instead of lower bound for mutual information. 

The idea of the paper is interesting and the proof of Theorems 1 and 2 are valid. Especially I like the idea of Theorem 1, which proves that any distribution-free high-confidence lower bound for KL divergence is upper bounded by log of sample size. This idea is similar to the paper in (Gao et al 15') which shows that mutual information estimator is upper bounded by log(N).

However, this paper contains many fatal flaws, which significantly weaken the quality of this paper. Precisely,

1. The DV lower bound is just an alternative of mutual information, helping MMI predictive coding algorithm to find good coding functions C_x and C_y. The goal of MMI predictive coding is not to estimate the mutual information I(C_x(x), C_y(y)) precisely, instead, the goal is to find good coding functions. The fact that DV lower bound is small means that we can not estimate mutual information through DV lower bound, but it does not directly imply that we can not find the coding functions. I expect some experiments to show that when mutual information is large, MMI predictive coding using DV lower bound can not find good coding functions.

2. In Section 3, the formula after the proof of Outlier Risk Lemma and before Theorem 1 (btw, it is better to have numbers for these formula) seems to be problematic. The first formula shows that E_{z~q} e^{F(z)} >= (1/N)e^{F_max}, then we plug it in (4). But in (4) there is negative ln of E_{z~q} e^{F(z)}, so we should have KL(P,Q) <= something, correct? This may be a typo but this typo is so important such that it affect the readability a lot. Theorem 1 is correct but the paragraphs before Theorem 1 confuse the reader a lot.

3. In Section 4, are you considering classical entropy for discrete random variables, or differential entropy for continuous random variables? I assume you are considering the latter, since most people of machine learning community are interested in continuous random variables. Then your statement of I(X;Y) <= H(X) is incorrect, since for continuous random variables, H(X|Y) can be negative. See (Thomas & Cover, Chapter 8) for a reference.

4. Related to problem 3, if you are considering continuous random variables, then the statement I(X;Y)=H(X)+H(Y)-H(X,Y) is not always correct. There are cases that H(X) is infinite, H(Y) is infinite, H(X,Y) is infinite but I(X;Y) is finite. These cases does not only exist in mathematical books, but also exists in practice, especially when the data is located on a low-dimensional manifold embedded in a high-dimensional space. Therefore, your approach of decompose the mutual information is not always possible.

5. Regarding to your proposed optimization problem in Section 6 (also it is better to have a number), I have some concerns. Since it involves max over \Psi outside, and inf over \Theta and inf over \Phi inside, so I wonder how do you solve this problem? Can you guarantee that the solution can provide good coding functions C_x and C_y? Also, it seems that this optimization problem is proposed as an improvement over the DV lower bound method, so I wish to see some experiment showing that this method is better than the DV lower bound method, at least for some synthetic datasets.

Because of the above mentioned flaws (especially 3 and 4, and lack of experiments), I think the paper is below the standard of ICLR conference.

References:
[1] Efficient Estimation of Mutual Information for Strongly Dependent Variables, by Gao, Ver Steeg and Galstyan, AISTATS15'
[2] Elements of Information Theory, 2nd edition, by Thomas and Cover.



","The reviewer acknowledges the interesting idea and valid proofs but lists several major flaws, including a lack of experimental validation and potential theoretical inconsistencies. The tone is critical, pointing out ""fatal flaws"" and deeming the paper ""below the standard."" However, the language remains professional and avoids personal attacks.",-40.0,60.0
"In the paper, the authors combine the federated method, sparse compression, quantization and propose Sparse Binary Compression method for deep learning optimization.  Beyond previous methods, the method in this paper achieves excellent results in the experiments. The paper is written very clearly and easy to follow. 

The following are my concerns,
1. In the introduction, the authors emphasize that there is a huge compression in the upstream communication. How about the downstream communication, I think the server should also send gradients to clients. The averaged gradient is not compressed anymore. 

2. I think the method used in the paper is not federated learning. Federated learning averages the models from multiple clients. however, in the paper, the proposed methods are averaging gradients instead. It is called local updates, and is a well-known tradeoff between communication and computation in the convex optimization.

3. I want to point out that the similar local update (federated learning) technique has already explored, and proved not work well. In [1] the authors showed that deploying the local update simply may lead to divergence. Therefore, the iterations of the local update are constrained to be very small. e.g. less than 64.  Otherwise, it leads to divergence. I also got similar results in my experience.  The temporal sparsity in the paper looks very small. I am curious about why it works in this paper.

4. Another issue is the results in the experiments. It is easy to find out that resnet50 can get 76.2% on Imagenet according to [2]. However, the baseline is 73.7% in the paper.  I didn't check the result for resnet18 on cifar10 or resnet34 on cifar 100, because people usually don't use bottleneck block for cifars.

5. In Table 2, Federated average always has worse results than other compared methods. Could you explain the reason?  If using federated average is harmful to the accuracy, it should also affect the result of the proposed method. 


[1] Zhang, Sixin, Anna E. Choromanska, and Yann LeCun. ""Deep learning with elastic averaging SGD."" Advances in Neural Information Processing Systems. 2015
[2]https://github.com/D-X-Y/ResNeXt-DenseNet","The review starts with positive remarks, highlighting the clarity and novelty of the paper's proposed method. However, it quickly delves into serious concerns regarding the technical validity and experimental setup. The reviewer questions the authors' understanding of federated learning and points out potential flaws in their approach, even suggesting that some results might be inaccurate. The tone, while direct and critical, maintains a professional and academic decorum. There's no personal attack, and the reviewer provides specific examples and references to support their critique.",-10.0,60.0
"This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. 

I find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. 

Here I detail my main concerns:


1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, $p(y|x) = \int p(y|x,w)p(w|\Theta)dw$. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). 

In the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound
$\ln p(y*|x*) >= E_q[\ln p(y*|x*,w)] - KL(q|p)$
In this case, q(w) = p(w|\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the $\Theta$ are learnt by maximum log-likelihood and do not have a $q$ associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time.   

My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. 

3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. 

So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. 

Minor comments:

1. The generative model for Variational dropout is the same than the generative model for the ""conditional model"", eq. (3). 

2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.

3. Section 3.3. I find some parts are difficult to understand. ""suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation."" Later, I can understand authors are referring to the possibility of reducing the dropout rate. ","The reviewer acknowledges the relevance of the paper's topic and the novelty of the approach but expresses concerns about the clarity and persuasiveness of the arguments. They find the paper difficult to follow and disagree with some of the authors' reasoning, particularly regarding the use of different dropout rates during training and testing. The reviewer also points out specific areas where the presentation could be improved. Overall, the feedback indicates that the reviewer finds the paper's direction promising but believes it requires significant revisions to improve clarity and address the raised concerns. The language used is critical but professional and remains within the bounds of academic discourse.",-20.0,50.0
"The paper at hand describes an approach to enable neural networks to take arbitrary structured data (basically any data that is not easily represented as a fixed-dimensional vector) as input. The paper describes how such data can be represented as a set (e.g. a sequence is represented as a set of index + data) and then an auxiliary network called the set aggregating network (SAN)  is used to represent the data in a high dimensional latent space. In addition to the idea, the paper provides a theoretical analysis of the approach which shows that with a sufficiently high dimensional representation the network is able to learn a unique representation for each input example. 

Good in this paper: 
 - nicely on topic - this is truly about learning representations
 - interesting idea with some (albeit not-surprising) theoretical analysis

Not yet great in this paper: 
 - the paper feels a bit premature in multiple ways, to me most importantly the experiments appear to be really rushed. 
  Looking at table 1 - it is really unclear how to read this. The table is hardly explained and it would be good to actually compare the method to the state of the art. I understand that the authors care more about generality here - but it's much easier to build something generic that is very far from the state of the art than to build something that is closer to the state of the art. Also - I feel it would have been interesting to allow for a more direct comparison of the SAN results with the other methods. Similarly, in Table 2 - how far away is this from the state of the art.
- the variable size image-recognition task seems a bit artificial - I believe that scaling images to a fixed size is a reasonable idea and is well understood and also something that humans can work with. Dealing with variable size images for the purpose of not having a fixed size vector seems artificial and unnecessary - in this case here specifically the images are same size to begin with. By using SAN you loose a lot of the understanding of computer vision research of the last decade (e.g. it's clear that CNNs are a good idea - with SAN - you cannot really do that anymore) - so, I feel this experiment here doesn't add anything. 

I feel these comments can probably be addressed by rethinking the experimental evaluation. At this point, I think the paper provides a nice idea with a theoretical analysis - but it doesn't provide enough experimental evidence that this works. 

Minor comments: 
 - p1: Analogically -> just drop the word
- p1: citation of Brin & Page -> this seems a bit out of place - yes, it is a method working on graphs, but it is not relevant in the context of the paper - to the best of my knowledge there are no neural networks in this. 
- p2: where Radon - >where the Radon
- p2: ""One can easily see "" -> I cannot. Please ellaborate","The reviewer provides a mixed opinion, acknowledging the interesting idea and theoretical analysis but expressing concerns about the experimental validation and suggesting improvements. They find the experiments rushed and lacking comparison to the state-of-the-art. The language used is generally polite and constructive, offering specific suggestions for improvement.",40.0,60.0
"# Summary

This submission proposes a multi-task convolutional neural network architecture for end-to-end driving (going from an RGB image to controls) evaluated using the CARLA open source simulator. The architecture consists of an encoder and three decoders on top: two for perception (depth prediction and semantic segmentation), and one for driving controls prediction. The network is trained in a two-step supervised fashion: first training the encoder and perception decoders (using depth and semantic segmentation ground truth), second freezing the encoder and training the driving module (imitation learning on demonstrations). The network is evaluated on the standard CARLA benchmark showing better generalization performance in new driving conditions (town and weather) compared to the CARLA baselines (modular pipeline, imitation learning, RL). Qualitative results also show that failure modes are easier to interpret by looking at predicted depth maps and semantic segmentation results.


# Strengths

Simplicity of the approach: the overall architecture described above is simple (cf. Figure 1), combining the benefits of the modular and end-to-end approaches into a feed-forward CNN. The aforementioned two-stage learning algorithm is also explained clearly. Predicted depth maps and semantic segmentation results are indeed more interpretable than attention maps (as traditionally used in end-to-end driving).

Evaluation of the driving policy: the evaluation is done with actual navigation tasks using the CARLA (CoRL'18) benchmark, instead of just off-line behavior cloning accuracy (often used in end-to-end driving papers, easier to overfit to, not guaranteed to transfer to actual driving).

Simple ablative analysis: Table 2 quantifies the generalization performance benefits of pretraining and freezing the encoder on perception tasks (esp. going from 16% to 62% of completed episodes in the new town and weather dynamic navigation scenario).


# Weaknesses

## Writing

I have to start with the most obvious one. The paper is littered with typos and grammatical errors (way too many to list). For instance, the usage of ""the"" and ""a"" is almost non-existent. Overall, the paper is really hard to read and needs a thorough pass of proof-reading and editing. Also, please remove the acknowledgments section: I think it is borderline breaking the double-blind submission policy (I don't know these persons, but if I did that would be a breach of ICLR submission policy). Furthermore, I think its contents are not very professional for a submission at a top international academic venue, but that is just my opinion. 


## Novelty

This is the main weakness for me. The architecture is very close to at least the following works:
- Xu, H., Gao, Y., Yu, F. and Darrell, T., End-to-end learning of driving models from large-scale video datasets (CVPR'17): this reference is missing from the paper, whereas it is very closely related, as it also shows the benefit of a segmentation decoder on top of a shared encoder for end-to-end driving (calling it privileged training);
- Codevilla et al's Conditional Imitation Learning (ICRA'18): the only novelty in the current submission w.r.t. CIL is the addition of the depth and segmentation decoders;
- Müller, M., Dosovitskiy, A., Ghanem, B., & Koltun, V., Driving Policy Transfer via Modularity and Abstraction (CoRL'18): the architecture also uses a shared perception module and segmentation (although in a mediated way instead of auxiliary task) to show better generalization performance (including from sim to real).

Additional missing related works include:
- Kim, J. and Canny, J.F., Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention (ICCV'17): uses post-hoc attention interpretation of ""black box"" end-to-end networks;
- Sauer, A., Savinov, N. and Geiger, A., Conditional Affordance Learning for Driving in Urban Environments (CoRL'18): also uses a perception module in the middle of the CIL network showing better generalization performance in CARLA (although a bit lower than the results in the current submission).
- Pomerleau, D.A., Alvinn: An autonomous land vehicle in a neural network (NIPS'89): the landmark paper for end-to-end driving with neural networks!


## Insights / significance

In light of the aforementioned prior art, I believe the claims are correct but already reported in other publications in the community (cf. references above). In particular, the proposed approach uses a lot more strongly labeled data (depth and semantic segmentation supervision in a dataset of 40,000 images) than the competing approaches mentioned above. For instance, the modular pipeline in the original CARLA paper uses only 2,500 labeled images, and I am sure its performance would be vastly improved with 40,000 images, but this is not evaluated, hence the comparison in Table 1 being unfair in my opinion. This matters because the encoder in the proposed method is frozen after training on the perception tasks, and the main point of the experiments is to convince that it results in a great (fixed) intermediate representation, which is in line with the aforementioned works doing mediated perception for driving.

The fine-tuning experiments are also confirming what is know in the litterature, namely that simple fine-tuning can lead to catastrophic forgetting (Table 3).

Finally, the qualitative evaluation of failure cases (5.3) leads to a trivial conclusion: a modular approach is indeed more interpretable than an end-to-end one. This is actually by design and the main advocated benefit of modular approaches: failure in the downstream perception module yields failure in the upstream driving module that builds on top of it. As the perception module is, by design, outputting a human interpretable representation (e.g., a semantic segmentation map), then this leads to better interpretation overall.


## Reproducibility

There are not enough details in section 3.1 about the deep net architecture to enable re-implementation (""structure similar to SegNet"", no detailed description of the number of layers, non-linearities, number of channels, etc).

Will the authors release the perception training dataset collected in CARLA described in Section 4.2?



# Recommendation

Although the results of the proposed multi-task network on the CARLA driving benchmark are good, it is probably due to using almost two orders of magnitude more labeled data for semantic segmentation and depth prediction than prior works (which is only practical because the experiments are done in simulation). Prior work has confirmed that combining perception tasks like semantic segmentation with end-to-end driving networks yield better performance, including using a strongly related approach (Xu et al). In addition to the lack of novelty or new insights, the writing needs serious attention.

For these reasons, I believe this paper is not suitable for publication at ICLR.","The reviewer acknowledges the good results of the paper but is not impressed by the novelty of the approach. They find the paper to be similar to previous work and criticizes the writing style. The reviewer is also critical of the significance of the findings, suggesting they are trivial or already known. Overall, the tone is rather negative, bordering on dismissive at times, particularly in the ""Novelty"" and ""Insights / significance"" sections.",-50.0,20.0
"The paper presents a VAE that uses labels to separate the learned representation into an invariant and a covariant part. The method is validated using experiments on the MNIST dataset.

The writing in this paper is somewhat problematic. Although it is hard to put the finger on a particularly severe instance, the paper is filled with vague and hyperbolic statements. Words like ""efficiently"", ""meaningful"", ""natural"", etc. are sprinkled throughout to confer a positive connotation, often without having a specific meaning in their context or adding any information. Where the meaning is somewhat clear, the claims are often not supported by evidence. Sometimes the claims are so broad that it is not clear what kind of evidence could support such a claim.

A relatively large amount of space is used to explain the general concept of invariant/covariant learning, which, as a general concept, is widely understood and not novel. There are other instances of overclaiming, such as ""The goal of CoVAE is to provide an approach to probabilistic modelling that enables meaningful representations [...]"". In fact, CoVAE is a rather specific model(class), rather than an approach to probabilistic modelling.

The paper is at times meandering. For instance, the benefits of and motivation for the proposed approach are not simply stated in the introduction and then demonstrated in the rest of the paper, but instead the paper states some benefits and motivations, explains some technical content, mentions some more benefits, repeats some motivations stated before, etc.

Many researchers working on representation learning hope to discover the underlying learning principles that lead to representations that seem natural to a human being. In this paper, labels are used to guide the representation into the ""right"" representation. It is in my opinion not very surprising that one can use labels to induce certain qualities deemed desirable in the representation.

To conclude, because of the writing, limited novelty, and limited experiments, I think this paper currently does not pass the bar for ICLR.","The review is highly critical of the paper, pointing out issues with writing style, overclaiming, limited novelty, and insufficient experiments. The reviewer explicitly states that the paper 'does not pass the bar for ICLR.' However, the criticism is presented in a professional and analytical manner, focusing on the paper's shortcomings rather than resorting to personal attacks.",-60.0,60.0
"This is nicely written paper analyzing the effect of various pre-training methods and shows that language models are very effective on sequence tagging tasks (POS, CCG). The experiments are well motivated and well described.

Regarding Table 1: which one of the ""LM forward"" models was used in the subsequent experiments? 

Are the input embeddings for the random init LSTM pre-trained or are they also randomly initialized?","The review starts with positive statements about the paper, such as ""nicely written"" and ""well motivated and well described."" This indicates a positive sentiment. The questions asked are neutral and framed as requests for clarification rather than criticisms. Therefore, the politeness is also high.",75.0,90.0
"The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below. 
The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. This also means, that training will be able to done faster. Besides this the training parameters can also be learnt. These are the main contributions. The denoising task experiment shows positive results. The idea could be used in the future by others looking for faster model inference and training.

If a Manhattan distance d is used i.e. dx,dy<k in equation (6), why is this a FullCRF? It seems like the new CRF is no longer a fully connected one. 

Page 5, first paragraph describing how the reorganization in the GPU is avoided is not very clear. It would be helpful to a reader to have more details and explanations about this.

It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get. Comparing to the Deeplab results etc for the non-trained case, the non-trained model still seems to be performing competitively. Table 2 of Table 3 does not really bring out the advantage of training. The +C, +T, +CT don't seem to be hugely different in terms of validation metrics. Note that Table 3 does not mention other models that might not be trained (assuming that those results are in Table 2) but the text also mentions that the training is not completely fair.

In section 5, Unary, it is mentioned that the network is not trained on larger datasets like other work, why?
And under CRF, what does iterations are unrolled mean?

In section 5.1, why does the random flipping help in simulating inaccuracies?


Minor points:
Abstract: Add space after ""GPUs."".
Would be good to define what Q, *, ' indicate in paragraph 4, page 2.
""hight"" -> ""height"" in section 4.1

","The review starts with positive remarks, highlighting the paper's clarity, relevance, and contribution (speed and trainability). However, it raises several questions and requests for clarification, indicating that the reviewer doesn't fully grasp all aspects of the work. While the tone is mostly neutral, asking ""why"" certain choices were made could be interpreted as slightly challenging the authors' decisions. Overall, the sentiment leans towards positive due to the initial praise and the constructive nature of the feedback.",60.0,70.0
"The main idea is to incorporate linguistic-based constrains in the form of dependency trees into different variations of relation networks.
In general, the paper is well written and organized, the presented problem is well motivated, and the approach is very strait forward. The experimental setting is comprehensive, and the results are indeed competitive in a wide range of tasks.
I think that using linguistic knowledge to improve Neural networks performance is very promising field, I think that you could get a much more substantial gains when applying your method in less resource-rich setups (maybe using some small subset of training for the SNLI and question duplication datasets).
It seems that your method relies heavily on previous works (RN, RNN-RN, latent dependency trees ,intra-sentence attention), can you please state clearly what your contribution is? does your model has any advantages over current state-of-the-art methods?   

edit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. ","The reviewer starts with positive remarks, praising the paper's clarity, organization, problem motivation, approach, experimental setting, and results. They also find the research direction promising. However, they express concerns about the novelty and significance of the contribution, suggesting it might be more suitable for a short paper. The reviewer acknowledges the good aspects but leans towards suggesting a minor revision due to concerns about novelty.",50.0,70.0
"Summary: 

This paper proposes a method to get feedback from humans in an autonomous vehicle (AV). Labels are collected such that the human actually moves a steering wheel and depending on the steering wheel angle disagreement with the direction the vehicle is actually moving a feedback value is collected which is used to weight the scalar loss function used to learn from these demonstrations. 

Experiments on a simple driving simulator is presented. 

Comments: 

I think this paper is attempting to address an important problem in imitation learning that is encountered quite often in DAgger, AggreVate and variants where the expert feedback is provided on the state distribution induced by the learnt policy via a mixture policy. In DAgger (where the corrections are one-step as opposed to AggreVate where the expert takes over and shows the full demonstration to get Q(s,a)) it is difficult to actually provide good feedback especially when the expert demonstrations are not getting executed on the vehicle and hence hard for humans to ascertain what would be the actual effect of the actions they are recommending. In fact there is always a tendency to overcorrect which leads to instability in DAgger iterations. 

The paper proposes using a modified feedback algorithm on page 6 whose magnitude and sign is based on how much the correction signal is in agreement or disagreement with the current policy being executed on the vehicle. 

Unfortunately this paper is very confusingly written at the moment. I had to take multiple passes and still can't figure out many claims and discussions: 

- ""To the best of our knowledge, no research so far has focused on using any kind of human feedback in the context of AV control with learning from demonstration"" - This is not true. See: 

""Learning Monocular Reactive UAV Control in Cluttered Natural Environments, Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert"" who used DAgger for autonomous driving of a drone with human pilot feedback. 

- Lots of terms are introduced without definition or forward references. Example: \theta and \hat{\theta} are provided early-on are refered to on page 3 in the middle of the page but only defined at the end of the page in 3.1. 

- Lots of confusing statements have been made without clear discussion like ""...we could also view our problem as a contextual bandit, since the feedback for every action falls in the same range..."" This was a baffling statement since contextual bandit is a one-step RL problem where there is no credit assignment problem unlike sequential decision-making settings as being dealt with in this paper. Perhaps something deeper was meant but it was not clear at all from text. 

- The paper is strewn with typos, is really verbose and seems to be written in a rush. For example, ""Since we are off-policy the neural network cannot not influence the probability of seeing an example again, and this leads can lead to problems.""

- The experiments are very simple and it is not clear whether the images in figure 2 are the actual camera images used (which would be weird since they are from an overhead view which is not what human safety drivers would actually see) or hand-drawn illustrations.

","The reviewer raises serious concerns about the paper, including factual errors, lack of clarity, confusing statements, typos, and simplistic experiments. While they acknowledge the importance of the problem being addressed, the numerous negative points suggest a negative sentiment. However, the language used, while direct, maintains a professional and critical tone without resorting to personal attacks or disrespectful language.",-50.0,50.0
"This is an empirical study on the ability for DQNs trained with/without regularization to perform well on variants of the same environment (e.g. increasing difficulty of a game). The paper is well written, the experimental methodology is clear & sound, and the significance is around improved sample efficiency via warm starting from a regularized DQN to fine tune. The error bounds for the regularized models results seem uncomfortably large in some cases. Overall it looks like a good methodological paper that can inform others on taking regularization more seriously when training DQNs. Evaluating on a modified ALE environment is great, but it would have been better to see this having similar impact in real life applications.","The review is largely positive. The reviewer finds the paper well-written with a clear methodology and interesting findings ('good methodological paper', 'significance', 'great'). While they point out a potential issue with error bounds and suggest a further area of investigation, these are framed as constructive suggestions rather than strong criticisms. ",75.0,100.0
"This work considers a version of importance sampling of states from the replay buffer.  Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM. The trajectories with lower rank are preferred at sampling.

Main issues:

1. Estimating rank from a density estimator

- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.

- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)

2. Generalization issues

- the method is not applicable to episodes of different length
- the approach assumes existence of a state to goal function f(s)->g
- although the paper does not expose this point (it is discussed the HER paper)

3. Scaling issues

- length of the vector grows linearly with the episode length
- length of the vector grows linearly with the size of the goal vector

For long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior.

4. Minor issues

- 3.3 ""It is known that PER can become very expensive in computational time"" - please supply a reference
 

- 3.3 ""After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors"" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?). Here is from the PER paper ""Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage""
","The review is critical of the paper's methodology, pointing out several issues and requesting clarification. While the reviewer does not explicitly reject the paper, the tone and the number of concerns suggest a negative sentiment. However, the language used is professional and not disrespectful, indicating neutrality in terms of politeness.",-50.0,0.0
"This paper propose a new approach to dialogue modeling by introducing two
innovations over an established dialogue model: the HRED (Hierarchical
Recurrent Encoder-Decoder) network. The innovations are: (1) adding a GAN
objective to the standard MLE objective of the HRED model; and (2)
modifying the HRED model to include an attention mechanism over the local
conditioning information (i.e. the ""call"" before the present ""response"").  

Writing: The writing was mostly ok, though there were some issues early in
Section 2. The authors rather awkwardly transition from a mathematical
formalism that included the two halves of the dialogue as X (call) and Y
(response), to a formalism that only considers a single sequence X. 

Novelty and Impact:  The proposed approach explicitly combines an established
model with two components that are themselves well-established.
It's fair to say that the novelty is relatively weak. The model development
is sensible, but reasonably straightforward. It isn't clear to me that a
careful reader of the literature in this area (particularly the GAN for
text literature) will learn that much from this paper. 

Experiments: Overall the empirical evaluation shows fairly convincingly
that the proposed model is effective. I do wonder why would the hredGAN
model outperform the hred model on perplexity. The hred model is
directly optimizing MLE which is directly related to the perplexity
measure, while the hredGAN include an additional objective that should
(perhaps) sacrifice likelihood. This puzzling result was not discussed and
really should be.

The generated responses, given in table 3 -- while showing some improvement
over hred and Vhred (esp. in terms of response length and specificity) --
do not fit the context particularly well. This really just shows we still
have some way to go before this challenging task is solved. 

It would be useful if the authors could run an ablation study to help
resolve the relative contributions of the two innovations (GAN and
attention) to the improvements in results. Perhaps the improvement in
perplexity (discussed above) is do to the use of attention. 

Detailed comments / questions

- In the paragraph between Eqns 2 and 3, the authors seem to suggest that
  teacher forcing is an added heuristic -- however this is just the
  correct evaluation of the MLE objective. 

- In discussing the combined MLE-GAN objective in Eqn. 8 Does the MLE
  objective use teacher forcing? Some earlier text (discussed above) leads
  me to suspect that it does not. 
","The review is mostly critical of the paper's novelty and has some concerns about the experimental results. While the reviewer acknowledges the effectiveness of the proposed model, they point out the lack of novelty and raise questions about the perplexity results. The language used is formal and professional, but the criticism, while constructive, lowers the politeness score.",-20.0,60.0
"This paper proposes a deep probabilistic model for temporal data that leverages latent variables to switch between different learned linear dynamics. The probability distributions are parameterized by deep neural networks and learning is performed end-to-end with amortized variational inference using inference networks.

There has been a lot of recent research trying to combine probabilistic models and deep learning to define powerful transition models that can be learned in an unsupervised way, to be used for model-based RL. This paper fits in this research area, and presents a nice combination of several interesting ideas presented in related works (switching variables, structured inference networks, merging updates as in the Kalman filter). The novelty of this paper in terms of original ideas is limited, the novel part lies in the clever combination of known approaches.

The paper reads well, but I found the explanation and notation in section 4 quite confusing (although easy to improve). The authors propose a structured variational approximation, but the factorization assumptions are not clear from the notation (I had to rely on Figure 2a to fully understand them). 
- In the first line of equation 7 it seems that the variational approximation q_phi for z_t only depends on x_t, but it is actually dependent also on the future x through s_t and q_meas
- The first line of section 4.1.1 shows that q_phi depends on x_{1:T}. However from figure 2a it seems that it only directly depends on x_{t:T}, and that the dependence on x_{1:t-1} is modelled through the dependence on z_{t-1}. 
- Is there a missing s_t in q_trans in the first line of (7)?
- why do you keep the dependence on future outputs in q_meas if it is not used in the experiments and not shown in figure 2a? It only makes the notation more confusing.
- You use f_phi to denote all the function in 4.1.1 (with different inputs). It would be clearer to use a different letter or for example add numbers (e.g. f^1_\phi) 
- Despite being often done in VAE papers, it feels strange to me to introduce the inference model (4.1) before the generative model (4.2), as the inference model defines an approximation to the true posterior which is derived from the generative model. One could in principle use other type of approximate inference techniques while keeping the generative model unchanged. 

It is difficult for me to understand how useful are in practice the switching variables. Reading the first part of the paper it seems that the authors will use discrete random variables, but they actually use for s_t continuous relaxiations of discrete variables (concrete distribution), or gaussian variables. As described in appendix B2 by the authors, training models with such continuous relaxations is often challenging in terms of hyper-parameter tuning. One may even wonder if it is worth the effort: could you have used instead a deterministic s_t parameterized for example as a bidirectional LSTM with softmax output? This may give equivalent results and remove a lot of complexity. Also, the fact that the gaussian switching variables perform better in the experiments is an indication that this may be the case.

To be able to detect walls the z variables basically need to learn to represent the position of the agent and encoding the information on the position of the walls in the connection to s_t.  Would you then need to train the model from scratch for any new environment?

Minor comment:
- in the softmax equation (6) there are missing brackets: lambda is at the denominator both for g and the log
","The reviewer provides a generally positive assessment, acknowledging the paper's contribution as a clever combination of existing approaches. They find the work interesting and well-written, particularly praising the combination of probabilistic models and deep learning. However, they express some reservations about the clarity and necessity of certain aspects, such as the notation in section 4 and the use of switching variables. The reviewer's detailed critique, including specific questions and suggestions for improvement, indicates a constructive and engaged attitude. While the reviewer doesn't explicitly state excitement, their detailed engagement with the work implies a positive sentiment. The language used is objective and professional, lacking any harsh or disrespectful language.",60.0,80.0
"+ Theoretic explanation for the scoring function.
+ (Promise for) Online provided source code.
+ The paper is well-written.

- The authors missed [1] which also introduces a generative model for knowledge graph embeddings. 
- The use of the datasets FB15k-237 and WN18RR instead of FB15k and WN18 (without inverse relations) would enable a better empirical evaluation. By using the flawed FB15k and WN18 datasets, the evaluation is biased towards the usage of inverse relations which should not exist in a link prediction evaluation dataset.
- The authors are not mentioning and comparing to walk based approaches like node2vec [2], Deepwalk [3], and rdf2vec [4]. 


Due to the missing comparisons to the mentioned references above and the possible bias in the evaluation, I am leaning towards rejecting the paper.


Minor comments:

. Abbreviations like h for head are used before they are introduced.
. ""The the"" -> ""The""
. ""triples are likely too be obvious examples"" -> ""triples are likely to be obvious examples""


[1] Xiao, Han, Minlie Huang, and Xiaoyan Zhu. ""TransG: A generative model for knowledge graph embedding."" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016.

[2] Grover, Aditya, and Jure Leskovec. ""node2vec: Scalable feature learning for networks."" Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2016.

[3]  Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. ""Deepwalk: Online learning of social representations."" Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.

[4] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko Paulheim. ""RDF2Vec: RDF Graph Embeddings and Their Applications."" SWJ http://www.semantic-web-journal.net/content/rdf2vec-rdf-graph-embeddings-and-their-applications-1","The reviewer highlights both positive and negative aspects of the paper. While praising the theoretical explanation, code availability, and writing quality, they express concerns about missing references, potential bias in dataset selection, and the lack of comparison with relevant walk-based approaches. These concerns lead them to lean towards rejection, indicating a negative sentiment. However, the language remains professional and polite throughout, suggesting constructive criticism rather than harsh negativity.",-25.0,75.0
"Good results; providing some insights on the selection of activation function.  

This paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.)
The two principal results of this paper are 
1. Initializing the network critically on the edge of chaos.  
2. Identifying some conditions on the activation functions which allow good ""information flow"" through the network.   

The first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. 

The second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.  



cons:
1. I don't think the experimental results are convincing enough for the reasons below:
    1.1. All experiments are conducted over MNIST with testing accuracy around 96%.  The authors should consider using large datasets (at least Cifar10).
    1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine.   
    1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: ""Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"" 


2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. 

3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. 

4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically.  


In sum, the paper has some interesting theoretical results but the empirical results are not convincing.  


Other comments:
1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. 
2. Consider replacing ""Proposition 4"" by  ""Theorem"", since it is the main result of the paper.  

","The review acknowledges the paper's good theoretical contributions, particularly highlighting the novelty of Proposition 5. However, it expresses concerns about the empirical validation, finding it unconvincing due to limitations in dataset choice, network size, and training methodology. The reviewer also questions some claims and suggests improvements in result presentation. While the review is critical, it maintains a professional and constructive tone, focusing on specific areas for improvement rather than resorting to personal attacks or dismissive language.",40.0,70.0
"This very interesting paper is based on the sensorimotor contingency theory, which grounds the perception of the agent (motor perception and sensory perception) in the capacity to learn to predict future sensory experience and to build a compact internal representation of proprioception and motor states. They show that through active experience (exploration) of an environment, an agent can encode its motor state in a way that captures both the topology and the relative distances. The authors show that in the case of an environment consistent with sensorimotor transitions and with some changes in the environment that are not inconsistent with the sensorimotor transitions (resets?), the network can learn the representation h of motor that project to the actual position of the agent in the environment (albeit in very contrived 2D toy tasks).

The model does not rely on RL at all; rather, it uses the representation of motor states (proprioception) to make predictions about the sensory observations of the environment. If such an agent were to act, I presume that there would be a search procedure across motor states to make the agent reach a desired state. The paper merits publication at ICLR provided very extensive revisions are made, and I am listing here the improvements to be made.

It is cool to cite Kant, Poincaré and Nicod, whose philosophical work underlies subsequent work on representing space and sensory experience. When citing Kant, please cite the primary source, not the 1998 re-edition.

There are missing references to Wayne et al (2018) ""Unsupervised Predictive Memory in a Goal-Directed Agent"" arXiv:1803.10760 and Ha & Schmidhuber ""World Models"" arXiv:1803.10122, where an agent is shown to build a representation of the world that can be decoded into spatial position and even a map of obstacles, on previously unseen environments, using only prediction of images, rewards and actions. Please include them in your work as they considerably change the narrative of section 2 (related work). Essentially, while the claims of the paper are interesting and relevant for the representation learning community, similar work has already been done, at much larger scale, from visual observations, using RL and self-supervised learning.

Section 2 is also somewhat overly critical of previous work: in (Banino et al, 2018; Cueva & Wei, 2018) ""rely[ing] on extraneous spatial supervision signals [do not] counter any claim of autonomy"", first because these signals can come from sensory perception (e.g., smell) and also because the agent is still autonomous at test time. Similarly, the depth prediction task in (Mirowski et al, 2017) is rather intuitive (stereo-vision).

Part 3 is difficult to parse: it would help to use the word proprioception (or explain why it is inappropriate) when talking about motor states, and exteroception (sensing the environment). I understood the first assumption, which is local continuity in sensory and motor space as well as the ambiguity of redundant motor systems that can generate the same sensory states, but not the second assumption. From what I understand, there are two invariants in building the model of proprioception: invariance to the topology of the environment and to the distances between objects, but then this is hard to reconcile with the setup of (in)consistent transitions in a moving environment and consistent transitions in a static environment. Please rewrite this section in a way that is easier to parse for people who know state-space models and RL for navigation and grasping (who may be your audience). Moreover, all the references point to a single work, which suggests that it is a very peculiar way of approaching a much more general problem of sensorimotor prediction, and therefore begs for a clear and simple explanation.

The architecture of the model is interesting: typically deep RL papers encode the sensory observations s into a hidden representation h, to take actions and produce a motor state m. Here, the current and future motor states m_t and m_{t+1} are embedded into h_t and h_{t+1} using a siamese MLP and used, in combination with the current sensory observation s_t, to make a prediction of s_{t+1}. This is somewhat related to learning the dynamics in model-based RL; please look into and cite Pathak et al (2017) ""Curiosity-driven exploration by self-supervised prediction"", ICML and other work on intrinsic curiosity.

The experiments are in very simplistic 2D grid world environments, but it makes the analysis and understanding of the 3D representations h much more simpler to follow. On the other hand, the discrete world task is very contrived (especially the weird mapping from m to h and from p to s) and hard to relate to existing work. One difficult problem that is solved by (Banino et al, 2018) is that of path integration in 2D from egocentric velocity. Could the authors present results on such a nonlinear case?

Revision: Score updated from 6 to 7.","The reviewer finds the paper interesting and relevant, suggesting it merits publication with extensive revisions. This points to a positive sentiment, around 70-80. However, the review also lists several criticisms and calls for significant improvements, pulling the sentiment down. The language is largely professional and polite, but phrases like ""overly critical"" and ""very contrived""  detract from the overall politeness. Therefore, the politeness score would be around 60-70.",75.0,65.0
"This paper proposes to introduce a new domain, the uncertain domain, to better handle the division between seen/unseen domains in open-set and generalized zero-shot learning. The approach handles test samples estimated to be from the seen class in one way, and ones that belong to either the unseen or uncertain domain in another way. This idea handles the problem that test instances may incorrectly be attributed to one of the seen classes. The authors evaluate their approach on several relevant datasets against a wide variety of methods for OSL and ZSL, and show convincing results. 

I have three concerns. One is that the method sections of the paper are fairly lengthy, including an extensive explanation of prior work, e.g. EVT, so time is spent reading before the reader gets to the interesting part of the proposed method, and this time could be better focused around the contributions of *this* work. 

For the G-ZSL experiments, most of the methods seem to be older methods tackling ZSL not G-ZSL so perhaps more relevant baselines could be found.

On a related note, it would be good to include some qualitative examples that might reveal some intuitive reasons for the large margin between the performance of the proposed work, and other approaches; in some cases this margin seems rather large, and while the authors attempt to explain it, something beyond a textual explanation might be useful. ","The review is overall positive. The reviewer acknowledges the paper's contribution with phrases like ""convincing results"" and finds the idea interesting. However, it also provides constructive criticism and suggestions for improvement, which indicates a neutral-to-positive stance rather than an overwhelmingly positive one.",60.0,80.0
"Summary: This work provides an analysis of the directional distribution of of stochastic gradients in SGD. The basic claim is that the distribution, when modeled as a von Mises-Fisher distribution, becomes more uniform as training progresses. There is experimental verification of this claim, and some results suggesting that the SNR is more correlated with their measure of uniformity than with the norm of the gradients.

Quality: The proofs appear correct to me. 

Clarity: The paper is generally easy to read.

Originality & Significance: I don't know of this specific analysis existing in the literature, so in that sense it may be original. Nonetheless, I think there are serious issues with the significance. The idea that there are two phases of optimization is not particularly new (see for example Bertsekas 2015) and the paper's claim that uniformity of direction increases as SGD convergence is easy to see in a simple example. Consider f_i(x) = |x-b_i|^2  quadratics with different centers. Clearly the minimum will be the centroid. Outside of a ball of certain radius from the centroid all of the gradients grad f_i point in the same direction, closer to the minimum they will point towards their respective centers. It is pretty clear, then that uniformity goes up as convergence proceeds, depending on the arrangement of the centers.

The analysis in the paper is clearly more general and meaningful than the toy example, but I am not seeing what the take-home is other than the insight generated by the toy example. The paper would be improved by clarifying how this analysis provides additional insight, providing more analysis on the norm SNR vs uniformity experiment at the end. 

Pros:
- SGD is a central algorithm and further analysis laying out its properties is important
- Thorough experiments.

Cons:
- It is not entirely clear what the contribution is.

Specific comments:
- The comment at the top of page 4 about the convergence of the minibatch gradients is a bit strange. This could also be seen as the reason that analysis of the convergence of SGD rely on annealed step sizes. Without annealing step-sizes, it's fairly clear that SGD will converge to a kind of stochastic process.

- The paper would be stronger if the authors try to turn this insight into something actionable, either by providing a theoretical result that gives guidance or some practical algorithmic suggestions that exploit it.

Dimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey. ArXiv 2015.","The reviewer acknowledges the correctness of the proofs and the clarity of the paper. However, they express concerns about the significance of the work, questioning the novelty of the findings and their practical implications. While acknowledging the thorough experiments and the importance of analyzing SGD, the reviewer suggests the paper would be stronger if it provided more actionable insights or algorithmic suggestions. The language used is critical but professional and respectful.",20.0,60.0
"Volumetric Convolution, Automatic Representation Learning in Unit Ball

This work proposes to tackle the challenging problem of learning on unit balls. The method uses volumetric convolutions based on the Zernike polynomial trick, which makes it convenient to use on convolution networks. Invariance to 3D rotation enables a transformation to a volumetric space, where convolutions could be used in a conventional process. Clarity of the methodology may benefit from a motivation discussed from a global perspective. The reader is currently facing heavy mathematical concepts fairly quickly with global rationale on the proposed choices, in particular, in the explanation of symmetry analysis. Clarity on the use of 2D and 3D features could also benefit from more details on what is exactly proposed. Results are shown on an object recognition task achieving performance comparable with the state-of-the-art. 

Positive
+ Tackles the difficult problem of extending graph learning to arbitrary topologies, particularly on unit balls
+ The contributions are multifold -therotical framework for modeling volumetric convolutions over functions defined on unit-balls, -derivation of the formulation, to make it usable by neural nets, -measures of axial symmetry on unit-balls

Specific comments
- How to handle mixed topologies, for instance, with random presence of holes in the meshes
- Extension beyond unit balls?
- Fundamentaly, arbitrary genus-0 meshes are topologically equivalent to a sphere, however, there can be severe metric distorsion when transforming shapes to a sphere (e.g, transforming a banana to a sphere, the ends gets severely atrophied) - Does this pose a problem - how to handle these metric distorsion?
- Zernike polynomials are based on the spherical harmonics - could this be generalized to arbitrary graph harmonics? Beyond spherical shapes?



","The review acknowledges the significance of the work and its contributions using positive language (""Clarity of the methodology may benefit...""). It provides constructive feedback and suggestions for improvement rather than harsh criticism (""Clarity on the use of 2D and 3D features could also benefit from more details...""). The questions posed suggest interest in the work and its potential extensions. Overall, the tone is encouraging and helpful.",60.0,80.0
"This paper first proposed a variant of experience replay to achieve better data efficiency in off-policy RL. The RACER algorithm was then developed, by modifying the approximated advantage function in the NAF algorithm. The proposed methods were finally tested on the MuJoCo environment to show the competitive performance.

This paper is in general well written. The ideas look interesting, even though they are mostly small modification of the previous works. The experiments also show the promise of the proposed methods. One of my concerns is regarding the generality of ReF-ER. I am wondering if it can be also applied to the Atari domain to boost the performance there, similar to the prioritized experience replay paper. I understand that the requirement of GPUs is beyond the hardware configuration in this work, but that would be an important contribution to the community. My other questions and comments are as follows.
- Regarding the parametric form of f^w in Eq. (7), what are the definitions for L_+ and L_-? What are the benefits of introducing min and max there, compared with the form in Eq. (11), as used in NAF? Does it cause any problems during optimization?
- The y axis in Figure 3 is for KL (\pi || \mu), while the text below used KL(\mu || \pi) and the description regarding the change of C also seems to be inaccurate. 
- In Figure 4, do you have any explanation why using PER leads to worse performance for NAF?
- For the implementation, did you use any parallelization to speed up the algorithm?","The reviewer provides a generally positive overview of the paper, stating that it is ""well-written"" and that the ideas are ""interesting."" They do offer constructive criticism and suggestions for improvement, but the overall tone is encouraging and helpful rather than overly critical. ",60.0,80.0
"The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment. The method builds on imitation learning (behavioural cloning) to model the agent’s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours. Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent’s environment. While learning to model the target agent’s inner state, the RL reward is generated based on the difference of the target agent’s inner state between consecutive time steps.

The approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines. Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours. It would be highly beneficial to evaluate these aspects. Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness. 

Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4). 

Minor issues:
- Reward formulations for the baselines as part of the appendix.
- Same scale for the y-axes across figures

","The review is mostly positive. The reviewer acknowledges the novelty of the proposed method and its effectiveness in the evaluated toy scenarios. They also highlight the paper's clarity and the interesting direction it presents. However, they express concerns about the method's scalability and the need for further evaluation in more complex environments. The language used is constructive and professional throughout the review.",60.0,80.0
"===============================
I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.)

However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited.

As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation. 

===============================

Contributions:

The main contribution of this paper is the proposed phredGAN, which is a persona-based GAN framework for multi-turn dialogue modeling. Specifically, a persona-based HRED generator is developed, with two different kinds of discriminator design. Experiments are conducted on both the UDC and the TV series transcript datasets.  

Weaknesses:

(1) Novelty: I would say the novelty of this paper is rather limited. This paper heavily rely on the previous hredGAN work (Olabiyi et al., 2018), and extends it by injecting attributes into the system, borrowing ideas from the persona-based Seq2seq model (Li et al, 2016b). 

phredGAN_a is a straightforward extension of hredGAN, while phredGAN_d further introduces a collaborative discriminator that tries to predict the attribute that generated the input utterance. However, in summary, I think this paper is not novel enough. 

(2) Presentation: The paper is generally easy to follow and understand. However, I would say the paper is poorly written, and needs further polishing. For example, Table 1 & 2 are pretty ugly. 

(3) Evaluation: Generally, I think the experiments are not convincing and also not well-executed, with detailed comments listed below. 

Questions:

(1) In phredGAN_a, as shown in Eqn. (4), the attribute is used as input of the discriminator, while in phredGAN_d, as shown in Eqn. (5) & (6), the attribute is used as the target of the discriminator. My question is: why not use the attribute as both input & output? That is, why not combine (4) & (6), instead of using (5) & (6)? Please clarify this. 

(2) In experiments, Section 3.1, the authors mention that the generator and the discriminator use a shared encoder. However, the generator and discriminator has a different role. Since the encoder is shared, then: in one step, we update the encoder to minimize the GAN objective, in the alternative step, we update the encoder again to maximize the GAN objective. So, how to deal with this conflicting role of encoder during the training? Please clarify this. 

(3) From Table 2, it seems that it is difficult to see that phredGAN is better than hredGAN. Can you provide some explanations here?

(4) In Table 4, if the responses generated by hredGAN can be provided, that would be better to demonstrate the advantage of phredGAN. How does phredGAN compare with hredGAN qualitatively?

(5) From Table 1 & 2, it seems to me there is no metric that is specifically designed to evaluate whether the model captures the attribute information. Is there a way to quantitatively evaluate this? For example, pretrain an attribute classifier, or use the collaborative discriminator in the phredGAN_d model to measure how the generated response reflect the attribute. If we can observe the performance of phredGAN is better than that of hredGAN, that would be helpful for the paper.  

(6) Since the task is challenging, and the automatic metrics designed for this task is not perfect, like other papers, I think human evaluation is essential and desired for this task. However, such human evaluation is lacked in this paper.
  ","The reviewer acknowledges the authors' efforts in addressing previous feedback and improving the paper, leading to a score increase. However, they maintain their stance on the paper's limited novelty and incremental contribution, ultimately recommending rejection. The language used is professional and provides constructive criticism without resorting to personal attacks or disrespectful remarks.",-30.0,80.0
"This paper proposes an augmentation of traditional neural network learning to allow for the inference of causal effects. Specifically, they modify the data sampling procedure of SGD during training to use matched samples that are paired via propensity score matching. Experimental results on a number of dataset show that the proposed methodology is comparable to alternative machine learning based causal inference methods. 

Overall, I think this is a nice idea. I have two main concerns: 
(1) The use of small batches for matching. Figure 2 does alleviate this concern to an extent, but there is a large literature in statistics and the social sciences on the effect that the quality of matches have on the final causal estimand. It is quite possible that this particular dataset is more amenable to PSM. It is also worth noting that while there is bias reduction shown in figure 2, it is not overwhelming. 

(2) The use of propensity scores for matching. One of the insights from the heterogeneous treatment effect literature is that it is not difficult to find cases where the propensity of treatment is identical for two sets of covariates that otherwise do not obey any real balance. This can lead to large biases in the final estimate. Given that PSM is still a relatively widely used practice, I don’t think that its use is a ground for rejection in itself, but given that neural networks are often used to estimate complex causal relations when they are used and this paper is interested in individual treatment effects it is worth noting. 

I found the experimental setup to do a very good job in covering large portions of the behavior of the algorithm. The final results are a little underwhelming–the proposed method does not appear to clearly define a new state of the art for the tasks it is applied to–but it is often competitive and the paper presents an interesting idea.
","The reviewer states ""Overall, I think this is a nice idea"", which clearly indicates a positive sentiment. However, the reviewer also points out limitations and suggests areas of improvement, indicating that the sentiment is not overly positive. Therefore, a sentiment score of 50 is appropriate. The language used is professional, respectful, and constructive, suggesting a high level of politeness (80).",50.0,80.0
"Summary. The paper considers the robustness of neural nets against adversarial attacks. More precisely, the authors experimentally investigate the robustness of ensembles of neural nets. They empirically show that adversarially trained ensembles of 2 neural nets are more robust than ensembles of 2 adversarially trained neural nets.

Pros.
* Robustness of neural nets is a challenging problem of interest for ICLR
* The paper is easy to read
* Experimental results compare different algorithms for 2 neural nets

Cons.
* The study is experimental
* It is limited to gradient-based attacks
* It is limited to ensembles of size 2
* The Ensemble2Adv is a single NN model and not an ensemble model. 

Evaluation.
The problem is significant and the use of ensemble methods for robustness against adversarial attacks is a promising line of research. The experimental study in this paper opens new lines of research in this direction. But, in my opinion, the paper is not ready for publication at ICLR. Detailed comments follow but the study is limited to k=2; the main finding is limited to the comparison between bagging two adversarially trained neural nets (SeparateEnsemble2Adv) and learning adversarially the average of two neural nets (Ensemble2adv). In my opinion, Ensemble2adv is a single model of double size and not an ensemble model thus somehow contradicting the main claim of the paper.

Detailed comments.
* Introduction, end of §2, it is said that non-gradient based attacks are still effective. But in the sequel you only consider gradient-based attacks and never discussed this question.
* Introduction, contributions, it should be made clear at the beginning of the paper that you will consider ensembles of size 2 and only gradient-based attacks.
* Section 2. The momentum-based attack should be cited and could be considered. ""Boosting adversarial attacks with momentum, Dong et al, CVPR18""
* Section 3, §2, the discussion on ensemble methods is unprecise. Ensemble methods have different objectives. For instance, Bagging-like methods  aim at reducing the generalization error while others as Boosting aim at augmenting the capacity of individual models.
* Section 3. Here is my main concern on this paper. The classical method would be bagging of neural nets with different initializations. The neural nets could be adversarially trained. This would lead to the so-called SeparateEnsemble2Adv. Here, the authors consider another method. Their method can be viewed as k(=2) copies of the same neural network with different initializations and an additional layer computing the average of the k(=2) outputs. Then adversarially learn the obtained model which leads to the so-called Ensemble2Adv algorithm. This algorithm is not an ensemble method as such. In my opinion for k=2, it is equivalent to doubling the size of a neural net, adding averaging of the outputs, and adversarially training the obtained neural net.
* Note recent advances in ensemble NNs with papers such as Averaging weights leads to wider optima ..., Izmailov et al, UAI18; Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Garipov et al, arXiv:1802.10026
* Section 4.1. Here comes the limitation k=2. The case k=4 is considered in table 1 but is not discussed elsewhere in the paper.
* Section 4.1. I am not convinced by DoubleAdv. It is one way of doubling the size of a neural net but I am not convinced that this is the more efficient. As said before, in my opinion, Ensemble2Adv is another way for doubling the size. And many more should exist.
* Section 5. In my opinion, the main comparisons should concern SeparateEnsemble2Adv and Ensemble2Adv. Also other methods doubling the size should be considered. 
* Section 5. For k greater than 2, SeparateEnsemblekAdv should be the better method because the adversarial learning phase could be easily parallelized.
* I am not convinced by the discussion in Section 6.
* Typos. and -> an l-13, p5; IFGSM5, l-19 p6; then l-6 p7; to due l-6 p9
* Biblio. Please give complete references
","The review acknowledges the significance of the problem and the potential of the research direction. However, it expresses several concerns, particularly regarding the limited scope of the study (focusing on ensembles of size 2 and gradient-based attacks) and the interpretation of Ensemble2Adv as a single model rather than an ensemble. The reviewer suggests the paper is not ready for publication, indicating a negative sentiment. While the reviewer provides detailed and constructive criticism, the language remains polite and professional throughout.",-40.0,60.0
"This paper has two main contributions: 
 1) it extends normalizing flows to discrete settings (exploiting relaxation ideas from Jang et al and Maddison et al).
 2) it presents an approximate fixed-point update rule for autoregressive time-series that can exploit GPU parallelism.

Overall, I think the work is solid. Contribution 1 isn't very novel, but is useful and the authors did a good job there.

Contribution 2 seems more interesting, but is not as well studied. When is the fixed point update expected to work? 
What assumptions does it imply? How does performance improve with the number of steps K? Does simulating for a finite steps emphasize the effect of early z's?
I'm a bit surprised that the authors did not attempt to study this part of their algorithm in isolation. They make a claims 
but never look at this in detail.

That said, the authors do a good job showing the method ""works"", and figures 3F and 3G are particularly nice. 
In 3G, is ""autoregressive"" supposed to converge to flow eventually?
Why don't the authors also use time as the x-axis in figure 2F (like 3F)?

My biggest complaint about the paper is the writing, which does not introduce and present ideas in a clear sequential 
manner, making the paper hard to read. I realize ELBO is standard, but at least some description of the setup in equation 1 
is warranted. What is x,z,\theta etc? Any paper should aim to be minimally self-contained. This continues throughout the paper, which does not really attempt to place the contribution in the larger literature, but rather just reports what the authors did and observed.

Some more examples:

Page 3: ""so we need to evaluate \hat{Q}"". This isn't defined. The authors should mention what \hat{Q} and \bar{Q} are.
Similarly for P. After a couple of passes through the paragraph, I could figure what the authors meant, but they 
should introduce the notation they use.

In section 2, while defining their model, they do not mention the dimension of z_t until after equation 8
(and even here, it has to be inferred).

What is x in 6b? What is the generative model they are doing inference on?

Section 2.2: it's not clear to me how convergence is defined even in the discrete case. I feel this discussion 
also really belongs to section 2.1

While I can understand what section 2.3 is trying to say, I could not really follow the notation.

I could not understand figure 1E and the associated sentence in section 2.4

What is the take-away of section 2.3 and 2.4? The authors seem to imply working with the discrete model is 
better in their experiments. Maybe forewarn the reader here?

The experiments are a bit hard to follow. It is inspired by a neuroscience application, but uses only simulated data. This is fine, but rather than describe the setup in mathematical/time-series language, it is complicated the with neuroscience jargon. As such, it feels disjointed and disconnected from the rest of the paper. I already complained that earlier sections do not describe the modeling setup, this is one way the paper could be improved.

In figure 2A and 3A, are the s's actually z's?","The reviewer finds the work solid and interesting, highlighting contributions like extending normalizing flows and the approximate fixed-point update rule. However, they express concerns about clarity, suggesting the paper lacks sequential introduction of ideas and sufficient explanation of notation and model setup. While acknowledging the strength of the results (figures 3F and 3G), the reviewer criticizes the writing style and organization, finding them hard to follow and disconnected, especially in the experiment description. The language, while direct and critical, maintains a professional and constructive tone.",40.0,60.0
"The paper presents a new Generative Adversarial Network (GAN) for learning a  
target distribution that is defined as the difference between two other 
distributions. Applications in semi-supervised learning and adversarial training 
are considered in the experimental evaluation and results are presented in 
computer vision tasks. 

The paper is not very well written and can be hard to follow. One very important 
issue for me was motivation for defining the target distribution as a difference 
between two other distributions. I am not familiar with this area, but reading 
through the introduction it was never clear to me why this is a useful scenario, 
in practice. Furthermore, some statements in the introduction felt quite 
arbitrary. For example, the authors state that PixelCNN ""does not have a latent 
representation"" in a manner that makes it sound as if that is a bad thing. If 
indeed it is, then why so? It would be very helpful to motivate the setting more 
and to provide a couple of examples of where this method would be useful, in the 
introduction. Also, regarding the MNIST example in the end of page 1, what is 
the ""universal set""? This paragraph also felt a bit arbitrary and unclear.

Some comments about the rest of the paper:
  - The theoretical results of section 3 are just stated/listed, but are not 
    connected to algorithm 1. Please connect them to the different parts of the 
    algorithm and state in a couple sentences what they imply for the algorithm.
  - Right after theorem 1, which assumption are you referring to when you say 
    ""the assumption in Theorem 1""?
  - The reformulation of section 3.1 is never justified. What led you to use 
    this reformulation and why do you think it is more stable in practice?
  - You should mention in the caption of table 4, what quantity you are 
    computing.

Note that my evaluation for this paper is based mainly on the way it is written 
as, in its current state, it is hard for me to judge what is novel and what is 
useful, and what readers are supposed to take in by reading this paper. The main 
question that the paper definitely needs to answer, but does not do so currently 
(in my opinion) is:

  When is this method useful to readers? For solving which problems and under 
  what conditions? And also, when is this method bad and should not be used?

== Experiments ==

Section 5.1 is hard to follow and I don't quite get how it connects to the rest.

Also, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your 
method does not need to rely on an additional density estimation network. Even 
if that is true, I cannot see how it is a useful remark given that the method of 
Dai et al. seems to always beat your method.

== Style ==

In figure 1, no labels or legends are provided making it hard to figure out 
what's going on at a glance. It would be very helpful to include labels and a 
legend.

Equation 2 is not written correctly. The equals sign only refers to ""V(G, D)"" 
and not the min-max of that, right? Please make that explicit by first defining 
""V(G, D)"" alone.","The reviewer raises several concerns about the paper's clarity, motivation, and connection between theoretical results and the algorithm. They find it difficult to assess the paper's novelty and usefulness due to these issues. While the reviewer provides constructive criticism and detailed feedback, their difficulty understanding the core concepts suggests a negative sentiment. However, the language remains polite and professional throughout, focusing on areas for improvement rather than outright criticism.",-40.0,60.0
"The main contribution of this paper is a new proposed score to evaluate models that yield uncertainty values for regression.

As constituted, the paper can not be published into one of the better ML conferences. The novelty here is very limited. Furthermore there are other weaknesses to the study.

First, the stated goal of the ""metric"" is that ""reflects the correlation between the true error and the estimated uncertainty ... (and is) scale independent and robust to outliers."" Given this goal (and the name of the paper) it is perplexing why the correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of true error and predicted error (\sigma_i) was not tried as baseline score. The correlation would have some ""scale independence"" and I'm sure that there are robust estimates (a simple thing would be to consider the median instead of mean, but there are probably more sophisticated approaches). This just feels like an obvious omission. If one wants to mix both predictive quality and correctness of uncertainty assessments then one could just scale the mean absolute error by the correlation: MAE/Corr, which would lead to a direct comparison to the  proposed MeRCI.

Second, the paper does a poor job of justifying MeRCI. On toy data MeRCI is justified by a confluence with existing scores. Then on the depth prediction task, where there are discrepancies among the scores, MeRCI is largely justified qualitatively  on a single image (Figure 4). A qualitative argument on a single instance in a single task can not cut it. The paper must put forth some systematic and more comprehensive comparison of scores.

Even with the above issues resolved, the paper would have to do more for publication. I would want to see either some proof of a property of the proposed score(s), or to use the proposed score to inform training, etc.","The review starts with a negative statement about the paper's publishability and points out a lack of novelty and weaknesses. The reviewer then lists several major issues, questioning the methodology and justification of the proposed approach. The language, while direct, avoids harsh or disrespectful vocabulary. The final paragraph suggests significant revisions and additional contributions are needed, indicating a negative sentiment overall.",-60.0,40.0
"Summary
The manuscript proposes to use power iterations in an approximate ""whitening layer"" to optimize the slowness objective of SFA in a very general setting. A set of experiments illustrates that this way of doing nonlinear SFA is meaningful.

Quality
Although the idea is pretty straight forward and the paper shows qualitative results on a number of datasets, the relative merit of the approach is empirically not well characterized.

Clarity
The manuscript is in general well written and the technical content is well accessible. However the description of the whitening layer implementation needs some more details.

Originality
The idea of using a whitening layer together with the slowness objective has not been explored before. There is a second ICLR 2019 submission (Pfau et al.) with a very similar idea, though.

Empirical Evaluation
The approximate whitening should lead to a trade-off between whitening and slowness optimization. I miss an experiment illustrating that trade-off. Also the comparison to nonlinear SFA using expansion or kernelization of hierarchical SFA is empirically not properly characterized. In the end, if one takes the slowness objective seriously, one would use the method yielding slower results.

Significance
The manuscript introduces a way of running nonlinear SFA with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.

Reproducibility
The data is either synthetic or publicly available. The Keras implementation of the PowerWhitening layer as well as the entire neural network along with its optimization schedule is not shared. Hence, there should be some effort involved to reproduce the experiments.

Pros and Cons
1+) The idea of an approximate whitening layer is conceptually simple and clear.
2-) The description of the practical implementation of the power iteration is slightly imprecise.
3-) The algorithm scales badly in the number of output dimensions. This scaling is bad in a computational sense and also in a statistical sense.

Details
a) Section 6.1: Why do you need to add the noise term? What is the statistical meaning of this added noise?
b) Section 6.1: the solutions if comparable -> the solutions is comparable
c) References: Shaham -> ICLR 2018 paper
d) References: nyström -> Nyström
e) The name for the algorithm ""Power SFA"" is a little bit bold.","The review acknowledges the manuscript's merits, such as a clear idea and well-written aspects. However, it raises significant concerns about the empirical evaluation, particularly the lack of clarity on the trade-off between whitening and slowness optimization. The reviewer also points out missing comparisons and scalability issues. While the language is direct and critical in places, it maintains a professional and constructive tone overall.",20.0,60.0
"This paper proposes a new metric called the image score that compares the similarity of activation between a given image with a pool of groundtruth images. The paper finds it useful for semi-supervised learning with self-teaching, where the network picks the most confident sample and use the network prediction as the label. It finds that the proposed method is better than 1) not using the unlabeled data and 2) using softmax as an indicator for model prediction certainty.

Motivation: The introduction begins by motivating the interpretability story of deep learning, but I don’t see gaining any more interpretability by reading the rest of the paper. The paper proposes to improve interpretability by assigning a score to each individual example, but then the obtained scores are not properly analyzed in the paper, and only final classification accuracy is evaluated. What are the training samples that makes the model make certain decision at test time? How to measure the correlation between the usefulness of training samples and the proposed image score? These questions left unanswered in the paper. Figure 1 helps a little bit, but then the top row is not necessarily the bad images, but maybe hard examples that needs extra attention to learn. Therefore, I think the end results presented in the experiments do not align with the motivation. Rather than shooting for interpretability, this is just another semi-supervised learning paper.

Models: The major issue of this paper is the model formulation that is not well motivated. The intuition of how the authors come up with the equation for computing the image score is not well explained. Hence the formulation seems very ad-hoc, and it is unclear why this is the selected method.

Experiments: As a semi-supervised learning paper, a common setting for CIFAR-10 is to use 4k labeled images. Here, the method uses 30k, which is 7.5x the size of the usual setting. It also does not compare to prior semi-supervised learning work (e.g. one of the recent one is: https://arxiv.org/abs/1711.00258). The only two baselines discussed here are weak. Also the improvement from the baselines by using the proposed method is not very significant.

Comparison: Figure 2-4 shows some positive correlation between the accuracy and score, which is fair, but it doesn’t compare to any baselines--the only one we have is softmax baseline and it is not shown in the figure.

In conclusion, I couldn’t see how the paper improves interpretability as claimed in the introduction. The proposed method seems ad-hoc, without any justification. Being considered as a semi-supervised learning paper, it lack significant amount of comparison to prior work and adopting a common semi-supervised benchmark. Due to the above reasons, I recommend reject.

---
Minor points:
“...almost all of the existed works investigate only the models and ignore the relationship between models and samples”. This is over-exaggerated. I believe most of the visualization techniques are dependent on the actual input samples. It is true to say about “training samples” not “samples” in general.

“all correctly classified images should have similar chain of activation, while incorrectly classified images should have very different activations both within themselves and with correctly classified images”. This claim seems not backed up. How do you know it is the case for “all” correctly classified images? What defines similar/different?","The review is highly critical of the paper, pointing out major flaws in the motivation, model formulation, experiments, and comparison with prior work. The reviewer recommends rejection, indicating a strongly negative sentiment. While the language is direct and critical, it maintains a professional and academic tone without resorting to personal attacks or overly harsh language.",-80.0,60.0
"The paper provides a new system that combines a number of neural networks to predict chemical reactions. The paper brings together a number of interesting methods to create a system that outperforms the state of the art.

Good about this paper: 
 - reported performance: the authors report a small but very consistent performance improvement.
 - the authors propose an approach that puts together many pieces to become an effective approach to chemical reaction prediction. 
 -

Problematic with this paper
 - this paper is impossible to understand if you only refer to the ten pages of content. There are at least 5 pointers in the paper where the authors refer to the appendix for details. Details in many of these cases are necessary to even understand what is really being done:  p3: rewards p4: message passing functions, p5: updating states, p9: training details. Further, The paper has some details that are unnecessary - e.g. the discussion of global vs. local network on p4 - this could go into the appendix (or be dropped entirely)

 - the model uses a step-wise reward in the training procedure (p3) -> positive reward for each correct subaction. It is not clear from the paper whether the model requires this at test time too (which should not be available). It's not clear what the authors do in testing. I feel that a clean RL protocol would only use rewards during training that are also available in testing (and a final reward)

 - eq 7: given there is an expontential in the probability - how often will the sampling not pick the top candidate? feels like it will mostly pick the top candidate. 

 - eq 9: it's unclear what this would do if the same pair of atoms is chosen twice (or more often)

 - the results presented in table 3: it appears that GTPN alone (and with beam search) is worse than the previous state of the art. only the various post processing steps make it better than the previous methods. It's not clear whether the state of the art methods in the table use similar postprocessing steps or whether they would also improve their results if the same postprocessing steps were applied. 
 

minor stuff: 
p2: Therefore, one can view GTPN as RL -> I don't think there is a causality. Just drop ""Therefore""
p2: standard RL loss -> what is that? 
eq. 2: interestin gchoice to add the vectors - wouldn't it be easier to just concatenate?
p4: what does ""NULL"" mean? how is this encoded?
p4 bottom: this is quite uncommon notation for me. Not a blocker but took me a while to parse and decrypt.
p5: how are the coefficients tuned?","The review acknowledges the paper's strengths, such as its novel approach and performance improvement. However, it also raises several significant concerns regarding clarity, missing details, and potential flaws in the methodology. The reviewer suggests improvements and requests clarifications, indicating a desire to see the paper strengthened. Overall, the tone is critical but constructive, aiming to guide the authors towards a more robust publication. ",40.0,70.0
"The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.

1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.

2) Properties of the minimizer
The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.

3) Scaling property
I find this section confusing. Specifically,
a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?
b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?
c) The idea of “restarting” is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don’t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this “property” seem to be used to simply rescale the a and w parameters.
d) The authors claim that “the scaling law (Proposition 3.2) should play a significant role” to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?

4) Convergence rate
It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,
a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn’t a constant step size also yield convergence in that case?
b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.

5) Saddles for neural nets
The authors claim they “have not encountered convergence to saddles” for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?

6) Extension of the analysis to deep neural networks
The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?

7) Experiments
How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?
","The review starts by acknowledging the relevance of the paper's topic but then proceeds to list a large number of concerns, particularly highlighting a significant overlap with prior work (Kolher et al. 2018). The reviewer questions several aspects of the paper's methodology and findings, suggesting the results might be weaker than previous work. The language used, while direct and critical, maintains a professional and academic tone. There's no personal attack or disrespectful wording.",-50.0,60.0
"The scope of the paper is interesting: to additionally learn the nonlinear activation function of the neuron.

The insights provided in section 2 with eqs (2)-(5) are interesting and naturally build on the previous work of Poggio & Girosi (1990) and Smola (1998). I found this a nice new insight and the strongest part of the paper. It is e.g. revealing to see to which P and L the rectifier nonlinearity is corresponding.

On the other hand I also have a number of suggestions for further improvement:

- Section 1: related to the overall function to be learned, the authors state ""this general problem has been already solved"". I think this statement is not completely correct, because depending on the choice of the stabilizer one obtains different optimal representations (e.g. Gaussian RBF or thin plate splines) as explained in Poggio & Girosi (1990). The theory does not tell what the best stabilizer is.

Additional relevant work that would be good to mention at this point, in the area of kernel methods, is e.g. learning the kernel.

- It seems that no other existing work on deep kernel machines has been mentioned in the paper, while in the conclusions the authors state ""In this paper we have introduced Kernel-based deep neural networks"". 

- Related to the training set T_N the notation e^kappa is not explained. It is not clear how this is related to eq (1).

- It would be good to comment on the difference between (3)(4) and Poggio & Girosi (1990).

- unnumbered eq after (5): are there multiple solutions to the problem (non-convex)? 

- The explanation of the recurrent network at the end of section 2 is too limited. Moreover, LSTM is not just a neuron nonlinearity, but a recurrent network with a particular structure. To which P and L would LSTM correspond?

- Fig.2: some of the nonlinearities look quite complicated and some of them are oscillatory (is this desirable? it reminds us of overfitting). Often one is interested in activation functions with a ""simple shape"" like sigmoid, tanh, relu. A more complicated nonlinearity may reduce the interpretability of the model.

- The examples given are rather conceptual (though nice) examples of the proposed method. However, no comparisons with other methods have been made yet in terms of generalization performance, e.g. on a few standard classification benchmark data sets, in comparison with other deep or shallow models.

A possible drawback of the proposed method might be (or maybe not) that additional unknown parameters need to be learned, which could possibly lead to worse generalization. It might be good to further investigate this.
","The review starts with positive remarks, highlighting interesting insights and building upon previous work. However, it quickly transitions into a series of constructive criticisms and suggestions for improvement. The reviewer points out potential flaws and areas where the paper could be strengthened. While the feedback is direct and detailed, it maintains a professional and respectful tone, suggesting ways to enhance the paper's quality and impact. Overall, the sentiment leans slightly towards the critical side, aiming to guide the authors towards a more robust and comprehensive study.",20.0,70.0
"The key idea of this paper is to expand the network for training on new tasks which is termed as C-Net, and train an additional generative model which is used for predicting task id (which is called H-Net), and use the task id for selecting weights from the C-Net.

Pros:
1. It is relatively easy to understand the paper. 
2. The originality of this paper lies in the usage of generative model to predict task id (H-Net). To my knowledge has not been proposed before.
3. In contrast to previous works in multi-task learning, which assumes task id is available both during training and inference, this work tries to remove the need of task id during inference, which makes it closer to the general definition of continual learning.

Cons:
1. Expanding the network for new tasks is not a novel contribution of this paper, it has already been proposed in previous works on multi-task learning. Doing expansion on all of the layers does not qualify for a major contribution in my opinion.
2. The experimental comparison is not very fair in my opinion, 
     a. Comparing accuracy of C-Net to other methods is not very useful. Because this methods expands the network for every new task, while other methods (EWC, LwF) has limited to no expansion in the network. Given that the single network result is far from state of the art (table 3), I suppose model size could contribute to the accuracy boost.
     b. It is not explicitly stated in the paper whether the output neurons are shared between tasks or an individual set of output neurons are used for different tasks, but from the rest of the paper I suppose disjoint neurons are used. Then the comparison between EWC and this work is not fair because EWC shares the output neurons among tasks.
This is not to blame this paper for not making fair comparison, since given different assumptions between methods (availability of task id, shared output neurons etc.), it is usually difficult to fairly compare between continual learning methods.  This problem is raised in another submission https://openreview.net/forum?id=ByGVui0ctm. The point here is that the accuracy of C-Net is not a good measure of how good this method is.
3. I disagree with the point that MNIST and SVHN are similar, they have very different distributions and are very easy to tell apart with a model. One concern is that the generative H-Net may fail to work once the distributions of the tasks overlap to some extent. e.g. cifar10 vs cifar100.

As a conclusion, the key contribution of this work is using generative model to determine task id which removes the need for task id during inference. It is relatively insufficient for publication on ICLR.","The reviewer acknowledges the originality of the paper's idea and praises its clarity. However, they find the experimental comparison unfair and the key contribution somewhat insufficient for publication. The tone is critical but professional and provides constructive feedback.",20.0,60.0
"In this paper, the authors propose a SOSELETO (source selection for target optimization) framework to transfer learning and training with noisy labels. The intuition is some source instances are more informative than the others. Specifically, source instances are weighted and the weights are learned in a bilevel optimization scheme. Experimental studies on both training with noisy label problems and transfer learning problems demonstrate the effectiveness of the proposed SOSELETO.

Overall, this paper is well-written, and easy to follow. The intuition is clear and reasonable, although it is not new. Regarding the technical section, I have the following comments:
(1)	The paper assumes that the source and target domains share the same feature representation parameters \theta. This is a widely used assumption in the existing works. However, these works usually have a specific part to align two domains to support the assumption, e.g. adversarial loss or MMD. In objective of SOSELETO, I do not see such a domain alignment part. I am wondering whether the assumption is still valid in this case. From the experimental study, I find SOSELETO achieves very good results in transfer learning problems. I am wondering whether the performance would be further improved if a domain alignment objective is added in the weighted source loss.
(2)	Each source has a weight, and thus there are n^s \alpha. As mini-batch is used in the training, I am wondering whether batches are overlapping or not. If overlapping, how to decide the final \alpha_i for x^s_i as you may obtain several \alpha_i in batches. 
(3)	Another point is abouth \lambda_p. In the contents, you omit the last term Q \alpha_m \lambda_p in eq.(4) as you use the fact that it is very small. I am not convincing on this omission as \lambda_p is also a weight for the entire derivative. Moreover, if \lambda_p is very small, the convergence would be very slow. In the experimental studies, you use different \lambda_p for different problems. Then, what’s the rule of setting \lambda_p given a new problem?

Regarding the experimental results, the experimental settings for the section 4.2 are not very clear to me. You may need to clearly state the train and test set (e.g. data size) for each method.  
","The reviewer provides constructive criticism and insightful questions, indicating a positive stance towards the paper's core ideas. While they point out the intuition isn't entirely novel and have specific technical questions, their overall tone is encouraging and geared towards improving the paper. The reviewer acknowledges the paper's clarity and finds the experimental results promising. The use of phrases like ""well-written,"" ""easy to follow,"" and ""interesting"" further suggests a positive disposition. However, the presence of areas for improvement prevents it from being a wholeheartedly positive review.",60.0,80.0
"This paper describes a method of training neural networks without update locking. The idea is a small modification on top of Czarnecki et al. Critic training, where instead of using final loss as a critic target, one bootstraps from critics on other layers. In particular, if only one module is present, these two approaches are actually identical. To be more precise, the only difference between these two methods is that (7) in Critic training would change to l(L_i, L_N). As a consequence, method becomes forward unlocked too. It is worth noting, that in the appendix of Czarnecki et al. it is shown that this particular method (critic training) under simple conditions actually ""degenerates"" to deep supervision (which is forward unlocked too). Consequently unlocking property as such is not a big contribution of the proposed method. Rest of the paper includes following elements:
- empirical evaluation showing improvement over critic training by 0.4% in CIFAR10 and 0.9% in CIFAR100 when using 3 splits.
- expansion on using the model for progressive inference.

Given standard deviation of errors in Table 1. it is not clear how significant these improvements are. How many samples were used to estimate these quantities? It is worth noting, that Critic training was showed to be outperformed by Sobolev Training in the same paper authors cite, but its performance is not reported despite looking like a well defined baseline. In particular, can these two methods be combined? 

I believe that this is an interesting research direction, however paper in its current form seems as a small incremental improvement over sota, and could be significantly improved by for example:
- providing more comprehensive evaluation (including estimating accuracy to lower std errors)
- adding other baseline solutions (such as Sobolev training, cDNI, or deep supervision)
- considering any form of convergence/dynamics analysis of the proposed approach



","The review acknowledges the paper presents an interesting research direction and the proposed method is a small improvement over existing methods. However, the reviewer also points out several limitations and suggests significant improvements. Overall, the sentiment is mixed, leaning slightly towards the positive side due to the interest in the research direction. The language used is polite and professional, providing constructive criticism and suggestions for improvement.",20.0,80.0
"This paper deals with the open set classification problem, where in addition to the known classes, the method should also be able to recognize the unknown class. The main idea is based on two parts: learning a discriminative representation, and a threshold based detection rule. To learn the embedding, the authors propose to minimize the inner class distance (between each instance to its center) and enlarge the distance between centers. The outlier score of an instance is computed as the minimum distance between known class prototypes. Experiments on various datasets show the ability of the learned method.

I'm not completely sure whether the whole approach is novel or not in the open set recognition domain, but both parts are not novel enough. Pulling similar instances together and pushing dissimilar ones away is the main idea in embedding learning. The ii-loss is similar to the triplet-center loss in the paper ""He et al. Triplet-Center Loss for Multi-View 3D Object Retrieval. CVPR18"". 

Although in the experiments the proposed method achieves good results in most cases, the reviewer suggests the authors comparing with more baselines to make the work solid.
1. Comparing with other embedding learning methods with the same outlier detection score. 
The authors should prove that the proposed embedding is important enough in the open set case. For example, using the center loss (Wen et al. A discriminative feature learning approach for deep face recognition. ECCV16), triplet-center loss, triplet loss (computing class centers after embedding).

2. Discuss more on the outlier score part. 
How to differentiate the known class outlier and new class? Will the problem be more difficult when the unknown class contains more heterogeneous classes? The authors can also apply existing open set recognition rule on the learned embedding.

Some detailed questions:
1. What's the difference between ""the network weights are first updated to minimize on ii-loss and then in a separate step updated to minimize cross entropy loss"" and optimize both loss terms simultaneously?
2. ""We assume that a certain percent of the training set to be noise/outliers"", how to determine the concrete value? Is 1% the helpful one for all cases?
3. Since there is not optimize over the unknown classes in training, could the reason for ""the unknown class instances fully occupy the open space between the known classes"" is the unknown classes are randomly sampled from the whole class set? For example, if classes about animals are known classes and classes about scene compose the unknown class, will the unknown class also occupy the whole space in this case?
4. What is the motivation of making ""the unknown class instances fully occupy the open space between the known classes""?","The reviewer acknowledges the paper's contributions (""The main idea is based on two parts..."", ""Experiments on various datasets show the ability..."") but suggests significant improvements. They find the novelty somewhat lacking and point out areas for improvement and further experiments. The tone is critical but professional and suggests concrete steps for improvement, indicating a willingness to see the paper published after revisions. Therefore, the sentiment is slightly positive, leaning towards neutral.",20.0,60.0
"The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks.  It seems like the main argument is to use them because you get a performance improvement with them. 

While the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence.  Additional justifications for why (or when) to use this should be described.

Why does the method outperform particularly when dropout is included?

I also expect the lack of monotonicity in the q-activation functions to lead to the creation of (exponentially) more local minima.  Any comments?

Quality: the experiments need some further work.
Clarity: aside from a few points, the paper is written clearly.
Originality: the work appears original to me
Significance: TBD, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be SOTA).

Small points:
""By prop 2, g_q(x) agrees with with original activation function"".  What does ""agrees with"" mean?
""Fig 2. Darker color --> lighter color?""
""(Conclusion) ... can goes[sic] deeper on the error surface."" To me, the experiments only show marginally better performance","The review is lukewarm in its overall assessment. While it acknowledges the originality of the work and potential benefits (improved early training performance), it also raises several concerns. The reviewer questions the justification for using q-activation functions, especially given the lack of convergence in the experiments and potential issues with local minima. The tone is direct and critical, pointing out specific flaws and demanding further investigation, but it maintains a professional and constructive approach. There are no personal attacks or disrespectful language used.",20.0,60.0
"This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. 

The proposed method is closely related to 1) “Learning Montezuma’s Revenge from a Single Demonstration” a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma’s revenge game 2) “Reverse Curriculum Generation for Reinforcement Learning” by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. 

The approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. 

The Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. 

Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). 

I commend the authors for honestly reporting their method’s shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form.
","The reviewer highlights several shortcomings in the paper, including lack of novelty, unclear comparisons, and questionable choices in parameters and experimental setup. They find the results not fully convincing and suggest the work lacks significance and quality for ICLR standards. While the reviewer acknowledges the authors' honesty in reporting shortcomings, the overall tone suggests substantial revisions are needed. Therefore, the sentiment leans negative.",-60.0,50.0
"This is a clear and well written paper that attempts to improve our ability to predict in the setting of massive multi-label data which, as the authors highlight, is an increasingly import problem in biology and healthcare. 

Strengths:
The idea of using the hierarchical structure of the labels is innovative and well-motivated. The experimental design and description of the methods is excellent. 

Weaknesses:
Overall the results are not consistently strong and there is a key baseline missing. The approach only seems help in the ""rare label, small data"" regime, which limits the applicability of the method but is still worthy of consideration. 

My biggest reservation is that the authors did not include a baseline where the classes are reweighted according to their frequency. Multilabel binary cross-entropy is very easy to modify to incorporate class weights (e.g. upweight the minority class for each label) and without this baseline I am unable to discern how well the method works relative to this simple baseline.

One more dataset would also strengthen the results, and since I am suggesting more work I will also try to be helpful and be specific. Predicting mesh terms from abstracts would qualify as a massive multilabel task and there is plenty of public data available here: https://www.nlm.nih.gov/databases/download/pubmed_medline.html 

Finally, there is one relevant paper that the authors may wish to consider in their review section: https://www.biorxiv.org/content/early/2018/07/10/365965","The reviewer provides both positive and negative feedback, ultimately suggesting major revisions. While they praise the paper's clarity, innovation, and experimental design, they also point out limitations in the results and a significant missing baseline. The recommendation for additional experiments and data suggests a sentiment leaning towards potentially positive if the authors address the concerns. Therefore, the sentiment is slightly above neutral.",20.0,70.0
"The papers proposed to use normalizing flow policies instead of Gaussian policies to improve exploration and achieve better sample complexity in practice. While I believe this idea has not specifically been tried in previous literature and the vague intuition that NF leads to more exploration that helps learning a better policy, the novelty of combining these two seems limited, and the paper does not seem to provide enough justification to using NF policies instead of alternative policy distributions both in theory and in the experiments.

1. About Section 4.2. I believe that the normalizing flow in question would transform the volume of a Gaussian? So there would exist some parameter setting for a flow model that also shrinks volume, thereby resulting in lower variance policies? The arguments would thereby depend heavily on the specific architecture and initialization of the flow model, which is not discussed in detail. 

Also, why is finding a high variance policy better in terms of the trust region argument? Isn't the whole point of using trust region that the new policy should be closer to old policy to prevent performance degradation? I also think that a fair comparison would be compare KL between normalizing flow policies, instead of KL between NF and Gaussian.

2. The TRPO experiments seem wrong -- at least the results don't match what is reported in the ACKTR paper for Reacher and InverseDoublePendulum envs -- there the TRPO policy at least learns something. Also TRPO in general does not perform as bad as it may seem, see ""Deep RL that matters"" paper by Henderson et al. Maybe this is because of using OpenAI baselines code which seems to have worse TRPO performance.

There is also no experiments on ACKTR on the small Mujoco tasks (even in the Appendix), which seems to be a rather big oversight given the authors have already done even harder tasks for ACKTR + NF.

Moreover I think a fair comparison is to use almost the same architecture for implicit and gaussian, where the only difference is where you sample the noise. For Gaussian with flows, you can first use an MLP to produce deterministic outputs and then use flow to generate the mean actions. Otherwise it is impossible to say whether the architecture or the implicit distribution contributes more to the success.

One could also use truncated Gaussian distributions / Beta distributions / Gaussian + tanh, since Mujoco actions beyond (-1, 1) is treated as -1 or 1, so Gaussian should already be bad. It is unclear whether NF is able to outperform these settings. 

Minor points:

- Fix citations. Please use \citep throughout.
- Is Equation (6) correct? Seems like \Sigma_i should be the inverse of g_i(\epsilon)? Also this is the ""change of variables formula"" not ""chain rule"".
- Why is normalizing flow not part of the background?
- Add legends in Figure (1)
- Figure 2(c), I believe with max entropy you could already obtain diverse ant trajectories?
- I believe in the context of generative models, ""implicit"" typically means the case where likelihood is not tractable? Here the likelihood is perfectly tractable.","The reviewer expresses several concerns about the paper, questioning the novelty and justification of the approach, finding issues with the experiments and comparisons, and pointing out technical details that need clarification. They also list several minor points that need addressing. The tone is critical and points out several flaws in the work, suggesting significant revisions are needed. However, the language is framed constructively, suggesting specific improvements and further investigations rather than outright dismissal. Therefore, the sentiment is rather negative but still within the range of professional academic feedback.",-50.0,50.0
"This paper proposes a simple method for knowledge distillation. The teacher and student models are matched using MMD objectives, the author demonstrates different variants of matching kernels specializes to previously proposed variants of knowledge distillation.

- The extensive evaluation suggests that the MMD with polynomial kernel provides better results than the previously proposed method.
-  It is interesting to see that MMD based transfer has more advantage on the object detection tasks.
- Can the author provides more insights into the behavior of different kernels, for example visualizing, the gradient map might help us to understand why certain kernel works better than another one?
- Did you consider translation invariance or other spatial properties when designing your kernels?

In summary, this is an interesting paper with good empirical results. The technique being used generalization is quite straightforward, but the paper also includes a good amount of discussion on why the proposed approach could be better and I think that really helps the reader.
","The reviewer finds the paper interesting, with good empirical results and a good amount of discussion. They highlight the straightforwardness of the technique but appreciate the discussion on its potential advantages. The questions posed are constructive and aim to further the understanding of the proposed method. All of this points towards a positive sentiment with polite and constructive language.",75.0,90.0
"[Relevance] Is this paper relevant to the ICLR audience? yes

[Significance] Are the results significant? somewhat

[Novelty] Are the problems or approaches novel? reasonably

[Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations.

[Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat

[Clarity] Is the paper well-organized and clearly written? yes, except the experiments

Confidence: 2/5

Seen submission posted elsewhere: No (but I did find it on arXiv after writing the review)

Detailed comments:

In this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (“normal” RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases.

=== Comments

I found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the “non-decaying time series” observations.

 However, I had difficult coming up with an example of a “static decay feature”. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a “static decay” feature rather than just a “static” feature.)

My main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While “meaningfulness” may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.

=== Typos, etc.

The plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used.
","The reviewer finds the approach reasonable and novel, leading to a somewhat positive sentiment. However, they express concerns about clarity in the experiments and provide constructive criticism. The language is polite and professional throughout, suggesting areas for improvement without resorting to negativity.",40.0,80.0
"This paper shows that gradient descent mostly happens in a tiny subspace which is spanned by the top eigenvectors of the Hessian. Empirical results are shown to support the claim. This finding is interesting and provides us some insights to design more efficient optimization algorithms. Overall, this paper is interesting and easy to follow. 

The experiments in section 2 do a decent job supporting the claim that gradient descent happens in a tiny subspace and the subspace is mostly preserved over long periods of training. However, I would like to add a couple more points to the discussion: 

- It's not surprising that the magnitude of gradient is larger in the high curvature directions, which means that the learning would always first happen in top subspace if the learning rate is small enough. It would be interesting to tune the parameter of learning rate to see if the phenomena would occur across different learning rate (especially large learning rate).
- The argument of gradient descent happening in a tiny space is quite obvious if the Hessian has only a few large eigenvalues. Therefore, it would be interesting to discuss the spectrum of the Hessian a little bit.
- Contrary to plain gradient descent, natural gradient is able to learn low curvature directions (small eigenvalues). It would be interesting to show some experiments with natural gradient methods.

Following section 2, the authors give a toy model to further backup their claims. However, I find the example is too restrictive and may not explain why the subspace would be preserved over the training. If I understand right, the loss function of the toy model is convex and Hessian is a constant over time. For this kind of toy model, the Hessian (or equivalently the Fisher matrix) only depends on the input distribution, so it's easy to see that the Hessian would be low-rank and preserved throughout the training. However, neural networks are highly non-convex, so it's unclear to me whether the implications of the toy model would generalize. I encourage the authors to analyze more complicated models. 

To summarize, I think this paper is interesting and well-written. However, it lacks convincing explanation why the subspace would preserve over the training (to me, it's more interesting than the point that gradient descent happens in tiny subspace). Anyway, it is not completely reasonable to expect all such possible discussions to take place at once. ","The reviewer starts with positive remarks, highlighting the paper's interesting findings and clarity. While they raise valid concerns and suggest improvements, they maintain a constructive and encouraging tone throughout. The suggestions focus on expanding the analysis and addressing potential limitations, rather than dismissing the work. The reviewer acknowledges the value of the research and encourages further exploration, indicating a positive overall sentiment.",60.0,80.0
"This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations. The algorithm works on any symmetric input data. The techniques in this paper combines two previous approaches: 1. the algorithm Convotron for learning a single convolutional filter (while the second layer has fixed weight) on any symmetric input distributions; 2. non-convex optimization for low rank matrix factorization.

The main observation in the paper is that if the overlap in the convolutions is not large (in the sense that each location of the convolution has at least one input coordinate that is not used in any other locations), then the weight that corresponds to the non-overlapping part and the weights in the second layer can be computed by a matrix factorization step (the paper gives a way to estimate a gradient that is similar to the gradient for a linear neural network, and then the problem is very similar to a rank-1 matrix factorization). After this step, we know the second layer and the algorithm can generalize the previous Convotron algorithm to learn the full convolutional filter.

This is an interesting observation that allows the algorithm to learn a two-layer neural network. On the other hand this two layer neural network is still a bit limited as there is still only one convolutional filter, and in particular there is only one local and global optimum (up to scaling the two layers). The observation also limited how much the patches can overlap which was not a problem in the original convotron algorithm. 

Overall I feel the paper is interesting but a bit incremental.","The reviewer acknowledges the paper's interesting observation and contribution but points out limitations. They call the work ""incremental"" which is a neutral to mildly positive term in peer review. The language is analytical and lacks strong positive or negative markers.",20.0,50.0
"Summary
=======
This paper introduces spread divergences. Spread divergences are obtained by taking the divergence between smoothed/noisy versions of two distributions. A spread divergence between two distributions of non-overlapping support can be defined even when the corresponding divergence is not. The authors discuss conditions under which the data generating process can be identified by minimizing spread divergences and apply spread divergences to the examples of PCA, ICA, and noiseless VAE.

Review
======
With a lot of papers focusing on generative modeling, divergence minimization is of great relevance to the ICLR community. Adding noise to distributions to ensure overlapping support is intuitive and has been used to stabilize training of GANs, but I am unaware of any work focusing on questions of identifiability and efficiency. I especially like the example of slowing EM in ICA with small noise. Here, some empirical results are lacking which analyze the speed/correctness of the identification of parameters for various choices of divergence/model noise. These would have greatly enhanced the paper. Instead, the available space was used to show model samples, which I find less helpful.

In Section 3.2 and Section 6 the authors argue that choosing noise which maximizes the spread divergence is optimal or at least preferable. This seems counterintuitive, given that the point of the noise was to make the distributions more similar. Please elaborate on why maximizing the divergence is a good strategy.

Minor
=====
The paper seems hastily written, with some grammar issues, typos, and sloppy use of LaTeX, e.g.:

– ""-\log"" instead of ""\log"" in definition of KL divergence in the introduction
– ""Section 2.2"" not ""section(2.2)"", ""Equation 24"" not ""equation(24)""
– ""model (Tipping & Bishop, 1999)"" instead of ""model Tipping & Bishop (1999)""
– ""\mid"" instead of ""|""
– ""x"" instead of ""y"" in Equation 28

Please provide a reference for the EM algorithm of ICA.","The reviewer finds the paper's topic relevant and interesting, highlighting the novelty of spread divergences. They praise the example of slowing EM in ICA. However, they also express concerns about missing empirical analysis and counterintuitive arguments regarding noise maximization. While the reviewer provides constructive criticism and suggestions for improvement, the overall tone remains positive and encouraging. The language used is polite and professional, focusing on specific aspects of the paper without resorting to personal attacks or harsh language. The minor comments further emphasize the constructive approach by pointing out concrete areas for improvement.",60.0,80.0
"This paper proposes an effective attack technique for the widely used optical flow based classification models in white-box and black-box settings. The most interesting result is on the sparsity and frame salience, which could have a lot of applications. But the main idea is to transfer the standard attack techniques from image to video domain. I have some concerns as below. 

1. Page 3, ""...while the temporal stream alone achieves 83.7%. Thus, if the motion stream can be fooled, the entire model is compromised.""

This statement is not exactly correct. Motion stream is better on UCF101 and HMDB51 dataset, which are two medium scale action recognition dataset. On other large-scale datasets like Sports1M, Kinetics, ActivityNet, etc., motion stream performs much worse than spatial stream. Hence, the motivation of the paper is unclear. Especially for real-world applications, due to real-time requirement, people usually just use the spatial stream. Hence, the current flow attack setting has limited usage. It is very important to show attack results on spatial stream as well. 

2. For FlowNet2, authors use gradient of the loss wrt the input images. However, FlowNet2 is a very large network consisting of 5 FlowNets. I am curious what the gradients will look like after the long back-propagation. Can authors comment on this by drawing a figure of magnitude distribution of gradients in the very early layers? 


Due to limited novelty, I recommend an initial rating of 5. 



","The reviewer acknowledges the potential of the paper's findings in sparsity and frame salience, but expresses concerns about the limited novelty and questions the motivation behind the chosen approach. They point out a factual error and suggest additional experiments. The language used is critical but professional and provides concrete suggestions for improvement. Overall, the tone leans towards the negative side due to the raised concerns.",-20.0,60.0
"The primary purpose of this paper, from what I understand, is to show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. This can potentially be useful in helping to identify artificial samples in the real world.

I think that the reviewers did an excellent job of probing into different statistical perspectives, such as looking at the continuity of the distribution of pixel intensities and the various higher moments of spectral data. I also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.

One of the first findings is that, with MNIST and CIFAR, the pixel intensities of fake samples are noticeably different when viewed from the perspective of a Kolgomorov-Smirnov Test or Jensen-Shannon Divergence comparison. This is an interesting observation, but less so than it would be if compared to something such as a variational autoencoder (VAE), which fits a KL distribution explicitly. IWGAN and LSGAN are using different metrics in their loss functions (such as Wassertein and least squares), and thus the result is not surprising or novel. I think if the authors had somehow shown how they worked their metrics into IWGAN or LSGAN to achieve better results, this could have been interesting.

Another observation the authors make is about the smoothness of the GAN distributions. This may not be so easily wrapped into the loss function, but it seems easily remedied as a post-processing step, or perhaps even a smoothing layer in the network itself. Nevertheless, this is an observation that I have not seen discussed in the literature so there is merit to at least noting the difference. It is confusing that on page 4, the authors state that they hypothesized that the smoothness was due to the pixel values themselves, and chose to alter the distribution of the original pixels in [0,1]. However, they state that in Figure 5, the smoothness remained ""as expected."" Did the authors misspeak here?

I found the music and speech experiments very interesting. The authors note that the synthetic Bach chorales, for instance, introduce many transitions that are not seen in the training and testing set of real Bach chorales. This, again, is interesting to note, but not surprising as the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized. I do not believe these observations to be paper-worthy by themselves. However, the authors I believe have a good start on creating papers in which they specifically address these issues, showing how they can create better synthetic samples by incorporating their observations.

As to the writing style, there are many places where the writing is not quite clear. I would suggest getting an additional party to help proofread to avoid grammatical mistakes. I do not believe that the mistakes are so egregious as to impede understanding. However, it could distract from the importance on the authors future innovations if not corrected.

One last note. The title of the paper is ""TequilaGAN: How to Easily Identify GAN Samples."" This makes it seem as if the authors were introducing another type of GAN, like LSGAN or DCGAN. However, they are not. As a matter of fact, nowhere else in the paper is the word ""TequilaGAN"" mentioned. This title seems a bit sensational and misleading.

In the end, although I did find this paper to be an interesting read, I cannot recommend it for publication in ICLR.

----

Edit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication.","The reviewer finds the paper interesting and acknowledges the novelty of the observations. However, they don't believe the findings are groundbreaking enough for an ICLR publication. The reviewer also points out some writing issues. Overall, the review leans towards the positive side due to the acknowledgment of the paper's merit, but the recommendation to reject it brings the sentiment down.",30.0,70.0
"In this paper, the authors investigate the accuracy-efficiency tradeoff for neural language models. In particular, they explore how different compression strategies impact the accuracy (and flops), and more interestingly, also how it impacts the power use for a RaspberryPi. The authors consider the QRNNs and SRUs for this purpose and use standard datasets for their analysis. I am torn about this paper. On one hand, I feel that the analysis is interesting, thoughtful and detailed; the power usage statistics bring a different perspective to the compression community. The section on inference time pruning was especially interesting to read. On the other hand however, there is limited novelty in the setup. The authors use standard, well known, compression algorithms on common neural language modeling architectures and datasets and use out-of-the-box tools for their ultimate analysis. Further, the paper needs additional work before it can be accepted in my opinion. I detail my arguments below:

- The authors begin by discussing SwiftKey and similar apps but I'm not sure if its clear that they use neural language modeling as the backend. Do the authors have a source to validate this claim?
- Knowledge distillation is another algorithm that has been found to be quite competitive in compressing models into smaller versions of themselves. Have the authors experimented with that? 
- The R@3 is an good metric but I suggest that the authors look at mean reciprocal rank (MRR) instead. This removes the arbitrary-ness of ""3"" while ensuring that the metric of interest is the accuracy and not probability of being correct (perplexity). 
- Can you comment on the sensitivity of the results to the RPi frameworks? For instance, the RPi deployment tools, architecture, and variance in the predictions? 
- Along the same line, I'm curious how generalizable the RPi results are for other computing architectures. For those of us who are not experts on hardware, it would be nice to read about whether similar tradeoffs will exist in other architectures such as mobile phones, GPUs or CPUs. 
- Could the authors add some meta-analysis about the results? If the perplexity goes up as a consequence of compression, what kinds of tokens it that localized to? Is it primarily rare words that the model is less confident about, or are the probabilities for most words getting skewed?
- Finally, I feel that such an exploration will catch on only if the tools are open-sourced and made easy to replicate/use. If there were a blog or article summarizing the steps needed to replicate the power measurement (including sources from where to buy the necessary hardware), more people would be inclined on adding such an analysis to future neural language modeling work. 

I am willing to revisit my rating, as necessary, once I read through the rebuttal. 


UPDATE:

After reading the rebuttal, I am increasing my score to 6. The authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains. ","The reviewer provides a mixed review, acknowledging the interesting aspects of the paper while also pointing out limitations. They find the analysis interesting and detailed, particularly the power usage statistics. However, they also note a lack of novelty and suggest several improvements, indicating a need for further work. The reviewer's willingness to revisit their rating after the rebuttal suggests they are open to changing their opinion, leaning towards a more positive sentiment overall.",40.0,70.0
"The authors argue that catastrophic forgetting may cause mode collapse and oscillation, and propose a novel plug-and-play  regularizer that can be applied to a variety of GANs' training process to counter catastrophic forgetting of the discriminator. The regularizer is a clever adaption of EWC and IS into the context of GAN training. With the authors' formulation, this regularizer will account for the discriminator's parameter from all previous ""tasks"" (snapshots taken at certain iterations) with extra memory budget of only one set of parameters, while assigning higher regularization strengths to parameters learned from recent tasks. Experiments demonstrate such regularizer improves GAN models including DCGAN, SN-DCGAN, WGAN-GP on image generation tasks and textGAN on text generation tasks.

Pros:
The paper is well-written. The formulation of online memory and controlled forgetting are clever, giving rise to the adaption of EWC and IS as a practical regularizer to overcome the problem of catastrophic forgetting in GANs. The experiments also demonstrate the regularizer is superior than historical averaging and SN on the synthetic dataset, and it is able to improve multiple GAN models in both image and text generation tasks.

Cons/Suggestions:
1. Although I can see the method is working, the empirical evidence to support ""mode oscillation"" is not strong enough for me. I think in order for continual learning to make perfect sense, mode oscillation should be an obvious issue for GANs; otherwise, we probably don't need remembering the history, as the generator is probably evolving towards the right direction even in the vanilla approach. Still, since there have been several papers showing history is important, it should be helpful in some sense. In Figure 1, I cannot tell whether in (d), the generator returned to the previous space (probably refers to (a)). Even the centers of mass of (a) and (d) look different for me. Figure 2 (left) only shows the distribution of generated data is changing as the training proceeds in vanilla GANs, since few of them (some shallow blue lines) have low peaks in previous datasets. If the mode oscillates and the generator returns to previous state, there should at least be another peak along the line, which is missing in curves on later datasets (darker blue ones). (I guess I have understood this figure correctly, but Figure 2 seems horizontally flipped to me. Since you are testing on previous fake datasets, and the accuracy should drop on previous datasets; however, the accuracy drops on later datasets in the figure.)

2. I doubt the authors may not have tried enough sets of hyper parameters for baseline models. In table 1, the variance of GAN, GAN + l2 weight and GAN + SN are significantly higher than the others. I don't think with l2 weight regularizer, the model will be much more unstable than the authors' approach.

3. The authors didn't give the results of their regularizer with LeakGAN on text generation. Currently their model has lower test BLEU than LeakGAN, which indicates lower fluency, but its self BLEU is lower than LeakGAN, which indicates higher diversity. It would be much better if the proposed method can surpass LeakGAN on both metrics.

4. Using inception score on mixture of eight Gaussians may not make much sense, if they are using the ImageNet pre-trained model, since such a model is not trained to fit this distribution. Still, the author has reported symmetric KL. 

5. The authors did not specify their inception score on real Celeb-A and CIFAR10 images. 


Overall, I tend to accept this paper for its contribution on methods. It would be even better if my concerns could be addressed.

Edit: after seeing the review of Reviewer 3, I find the proposed method seems to be the same as Online EWC and I have downgraded the rating. The authors should address these concerns.","The reviewer acknowledges the paper's merits, praising its clever approach and experimental results. This points towards a positive sentiment. However, they also raise several concerns and suggest improvements, indicating it's not perfect. The tone is professional and suggestions are constructive, suggesting politeness. Overall, it leans positive but with room for improvement.",60.0,80.0
"This paper gives various PAC-Bayesian generalization guarantees and some
empirical results on parameter perturbation in training using an algorithm
motivated by the theory.

The fundamental issue addressed in this paper is whether parameter
perturbation during training improves generalization and, if so, what
theoretical basis exists for this phenomenon.  For continuously
parameterized models, PAC-Bayesian bounds are fundamentally based on
parameter perturbation (non-singular posteriors).  So PAC-Bayesian
theory is naturally tied to parameter perturbation issues.  A more
refined question is whether the size of the perturbation should be
done on a per-parameter bases and whether per-parameter noise levels
should be adaptive --- should the appropriate noise level for each
parameter be adjusted on the basis of statistics in the training data.
Adam and RMS-prop both adapt per-parameter learning rate eta_i to be
proportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running
estimate of the expectation over a draw of a training point of the
square of the gradient of the loss with respect to parameter i.  At
the end of the day, this paper, based on PAC-Bayesian analysis,
proposes that a very similar adaptation be made to per-parameter noise
during training but where E g_i^2 is replaced by the RMS value \sqrt{E
g_i^2}.  It seems that all theoretical analyses require the square
root --- the units need to work.  A fundamental theoretical question,
perhaps unrelated to this paper, is why in learning rate adaptation the
square root hurts the performance.

This paper can be evaluated on both theoretical and empirical grounds.
At a theoretical level I have several complaints.  First, the
theoretical analysis seem fairly mechanical and without theoretical
innovation. Second, the analysis obscures the prior being used (the
learning bias). The paper first states an assumption that each
parameter is a-priori taken to be uniform over |w_i| <= \tau_i and the
KL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where
sigma_i is the width of a uniform posterior over a smaller interval.
But later they say that they approximate tau_i by |w_i| + kappa_i with
kappa_i = \gamma |w_i| + epsilon.  I believe this works out to be
essentially a log-uniform prior on |w_i| (over some finite range of
log |w_i|).  This seems quite reasonable but should be made explicit.

The paper ignores the possibility that the prior should be centered at
the random initialization of the parameters.  This was found to be
essential in Dziugaite and Roy and completely changes the dependence
of k_i on w_i.

Another complaint is that the Hoefding bound is very loose in cases
where the emperical loss is small compared to its upper bound.  The
analysis can be more intuitively related to practice by avoiding the
rescaling of the loss into the interval [0,1] and writing expressions
in terms of a maximum bound on the loss L_max.  When hat{L} << L_max
(almost always the case in practice) the relative Chernoff bound is
much tighter and significantly alters the analysis.  See McAllester's
PAC-Bayesian tutorial.

The theoretical discussion on re-parameterization misses an important
point, in my opinoin, relative to the need to impose a learning bias
(the no-free-lunch theorem).  All L_2 generalization bounds can be
interpreted in terms of a Gaussian prior on the parameters.  In all
such cases the prior (the learning bias) is not invariant to
re-parameterization.  All L_2 generalization bounds are subject to the
same re-parameterization criticism.  A prior tied to a particular
parameterization is standard practice in machine learning for in all
L_2 generalization bounds, including SVMs.  I do think that a
log-uniform prior (rather than a Gaussian prior) is superior and
greatly reduces sensitivity to re-parameterization as noted by the
authors (extremely indirectly).

I did not find the empirical results to very useful.  The value of
parameter perturbation in training remains an open question. Although
it is rarely done in practice today, it is an important fundamental
question. A much more thorough investigation is needed before any
conclusions can be drawn with confidence. Experimentation with
perturbation methods would seem more informative than theory given the
current state of the art in relating theory to practice.
","The review is critical of the paper's theoretical approach, finding it to be somewhat incremental and lacking in innovation. The reviewer also points out several technical concerns and suggests areas for improvement. While the reviewer acknowledges the importance of the research question, they find the empirical results to be inconclusive and call for more thorough investigation. Overall, the tone is professional and critical but not disrespectful.",-20.0,60.0
"This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. 

On the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. 

I think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed.

The validation and test dataset have been modified to remove ""samples"" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets.

The use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment ""doyou"" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison.

Some minor comments:
- A typo in the introduction ""... semi-Markov model. The the characters inside ..."".
- Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented.
- Is it possible to efficiently generate a sequence?

[Updated after reconsidering other reviews]
Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.","The reviewer finds the paper interesting and well-motivated, indicating a positive sentiment. They highlight the novelty and significance of the approach. While they point out some limitations and suggest improvements, their tone remains constructive and polite throughout. The reviewer acknowledges the paper's contribution to language modeling, further reinforcing the positive sentiment. ",60.0,70.0
"This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem 
using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. 

In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on 
a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. 

The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as 

Brandon Amos, Lei Xu, J. Zico Kolter:
Input Convex Neural Networks. 
ICML 2017: 146-155

Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. ","The reviewer finds the work solid and important, highlighting its contribution to understanding deep learning complexity. While they suggest practical insights and further discussion on the implications of exponential time, their tone remains positive and encouraging. They see the work as a bridge for future research. The reviewer suggests additional references and explanations for broader appeal, indicating a desire to improve the paper rather than criticizing it.",75.0,75.0
"Summary: The paper suggests a method to reducing the space consumption of training neural nets, in exchange for additional training time. The method stores in memory only a subset of the intermediate tensors computed in the forward step, and then in the backward step it re-computes the missing tensors as they are needed by interpolating forward (again) between the stored tensors. The paper also gives a combinatorial algorithm for choosing which tensors to store on a given computation DAG annotated with vertex costs.

Evaluation: I generally like the paper. The proposed method is simple and straightforward, and seems to lead to a noticeable improvement in space usage during training.
The part related to decomposing a DAG into ""close sets"" looks like it might overlap with existing literature in graph theory; I don't have concrete references but the authors may want to check this. The ACG solver algorithm looks somewhat wasteful in terms of the degree of the polynomial running time, but since actual computation graphs are tiny in computational terms, I guess this is not really an issue.
One point not addressed in the experiments is what is the overhead incurred in training time by the two space-efficient methods over usual training. I suppose one expects the training time be less than twice longer, since the re-forwards amount to one additional complete forward step per forward-backward pair.
Another thing that would be interesting to see is the actual stored vertices (V_R) that were chosen empirically for the rows in table 1 (or at least some rows). Since the computational graphs of the tested networks are small, and some are well-known, I imagine it should doable (though this is merely a suggestion).

Conclusion: The paper is simple and looks reasonable, with no major issues that I could detect.","The reviewer states ""I generally like the paper"" and finds the method ""simple and straightforward"", which points to a positive sentiment. While the reviewer offers constructive criticism and suggestions for improvement, they are not framed negatively and do not detract from the overall positive impression. The language used is neutral and professional throughout the review.",65.0,80.0
"I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. 

PS: I am downgrading my confidence in my evaluation.

---

Paper 93 proposes an empirical evaluation of the memorization properties of convnets. More specifically, it evaluates three aspects:
-	First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier. The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.
-	Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model. For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions. It is shown experimentally that one can detect (even partial) leakage with such a technique.
-	Third, it evaluates whether we can detect that a single images was used to train a convnet. Two simple techniques are proposed. The first one considers that a sample is part of the training set if it correctly classified. The second one considers that a sample is part of the training set if its loss is below a threshold. It is shown experimentally that one can make such a decision with moderate accuracy.

On the positive side:
-	This is a topic that should be of broad interest to the ICLR community.
-	The paper is generally well-written.
-	The experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.

On the negative side:
-	It is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?
-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?
-	Section 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.
-	The experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy. Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3). This seems to be too low to be of practical use. This might be because the Bayes and MAT attacks are too simplistic. Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?
","The reviewer starts by stating they are not confident in championing the paper, which is a negative sentiment. They list more negative than positive points, and the language used to describe the paper's shortcomings is quite direct (e.g., ""somewhat disappointing"", ""too simplistic""). However, they do acknowledge the importance of the topic and the generally good writing, indicating some positivity. Overall, the tone leans negative but maintains a professional and not overly harsh demeanor.",-30.0,40.0
"To generate a sequence of high-level visual elements for recreation or translation of images, the authors propose differentiable ""canvas"" networks and ""drawer"" networks based on convolutional neural networks. One of the main ideas is the replacement of the ""canvas"" networks instead of non-differentiable ""renderer"" to end-to-end train the whole model with mean-squared error loss. It seems to be a novel approach to optimize drawing actions. It is reasonable to use separate networks to approximate the behavior of renderer and to fix the parameters of the ""canvas"" networks to maintain the pretrained rendering capability.

Integrating the high-level visual constructs for recreation or translation of images is to eliminate or attenuate visual artifacts and blurriness, as mentioned in the introduction of the paper. Qualitative comparison with the other state-of-the-art methods is shown in Figure 6f; however, it fails to show significant improvement over them. Quantitative results do not include in the comparison, but only for the ablation study to determine the proposing method. Although the paper proposes an interesting approach to enhance an image generation task, the provided evidence is weak to support the argument, which should be useful for their criteria.

Moreover, experimental details fall short to ensure the validity of experiments. How do you split the dataset as train/val/test? Are the reporting figures (L2 loss) from test results? How are the statistics of the datasets you used?

In Related Work, the authors describe ""reinforcement learning methods can be unstable and often depend on large amounts of training samples."" Many RL methods use various techniques to stabilize the learning, and this argument alone cannot be the grounding that the supervised approach is better than RL. Unsupervised learning also needs a large amount of data. What is the point of this paragraph (the second paragraph in Related Work)?


Quality: 
  Figure 1-3 are taking too much space, which might lead to exceeding 8 pages. 

Clarity:
  The experimental procedure is not clear. Please clarify the issues mentioned above. It is not hinder to understand the content; however, the writing can be improved by proof-reading and correcting a few grammatical errors.

Originality and significance:
  Using the differentiable ""canvas"" networks to avoid non-differentiable ""renderer"" is a novel approach as far as I know. 

Pros:
  Differentiable drawing networks are underexplored in our community.

Cons:
  It failed to show the excellency over pixel-wise generation methods and limited to simple visual elements, line drawings or box generations. This work does not explore ""brush strokes"" in paintings.


Minor comments:

- In Related Work, the inline citation should be ""Simhon & Dudek (2004)"" instead of ""(Simhon & Dudek, 2004)"", and this may apply to the others.

- In Figure 2, the Hint should be x_n, the current state, or target image X for regeneration (X' for translation)?

- In 4.1, a typo, ""Out state consists of"" to ""Our state consists of"".",The review acknowledges the novelty of the approach but expresses concerns about the lack of strong evidence to support its effectiveness. The reviewer points out weaknesses in the experimental validation and results. The tone is critical but professional and provides constructive feedback for improvement. The language is polite and professional throughout.,20.0,60.0
"The paper studies failure modes of deep and narrow networks. I find this research extremely valuable and interesting. In addition to that, the paper focuses on as small as possible models, for which the undesired behavior occurs. That is another great positive, too much of a research in DL focuses on the most complex and general cases in my opinion. I would be more than happy to give this paper a very strong recommendation, if not for numerous flaws in presentation. If those get improved, I am very eager to increase my rating. Here are the things that I think need an improvement:
1. The formulation of theorems.
The paper strives for mathematical style. Yet the formulations of the theorems are very colloquial. Expression ""by assuming random weights"" is not what one wants to see in a rigorous math paper. The formulations of the theorems need to be made rigorous and easy to understand, the assumptions need to be clearly stated and all concepts used strictly defined.
2. Too many theorems
9 (!) theorems is way too much. Theorem is a significant contribution. I strongly suggest having 1-2 strong theorems, and downgrading more technical lemmas to a lemma and proposition status.
In addition - the problem studied is really a study of bad local minimas for neural networks. More mentions of the previous work related to the topic would improve the scientific quality additionally, in my opinion.
","The reviewer clearly states that they find the research ""extremely valuable and interesting"" and would give a ""very strong recommendation"" if the presentation was improved. This points to a positive sentiment. The reviewer also provides constructive criticism and specific suggestions for improvement in a professional and respectful manner, indicating politeness. However, the numerous flaws mentioned slightly lower the overall positivity. Therefore, the sentiment score is high but not perfect, and the politeness score is very high.",85.0,90.0
"The paper proposes a data-dependent regularization method which is coupled with softmax loss to train deep neural networks for classification. The paper turns to Orthogonal Low-rank Embedding (OLE) loss for the geometric constraint that one class of data/feature are assumed to reside on a low-rank subspace that subspaces of different classes are orthogonal ideally. The probability in the softmax is then modeled as cosine similarity between data feature and the class-specific subspaces. In this way, geometric loss and softmax loss have the common goal for optimization. Moreover, during training, the geometry enforced on one batch of features is simultaneously validated on a separate batch using a validation loss. The experiments seem to suggest such a model helps avoid overfitting/memorizing noisy training data. The paper reads well and is easy to follow.

However, the paper is limited in technical novelty and practical significance. Here are some concerns -- 

1) The paper only studies one method based on OLE, though it cites the center loss [19]. How does the center loss behave in face of noisy training label? Would it also be able to refuse to fit the noisy training data?

2) As each class has its own (low-rank) subspace, and the rank is reduced by imposing the nuclear norm. It seems that the proposed method is hard to extend to many classes (class number is larger than the dimension)?

3) The datasets in the experiments are quite small in scale and class number. It is not persuasive unless tested on larger scale data or with large class number.

4) The proposed method seems to be limited in deal with discrete labels (e.g., classification), is it easy to extend to continuous target, say regression problems like depth estimation and surface normal estimation?

5) While the authors claim as a main contribution that the proposed GRSVNet is a general framework, it is hard to see how this framework can be used in other tasks other than classification.

6) The experiments are less persuasive. It's better to add the error bar to see the improvement by the proposed method is not due to random initialization. Running time should also be compared, as nuclear norm seems to be time consuming.","The review starts with a positive note, acknowledging the clarity and readability of the paper. However, it quickly transitions into outlining limitations, questioning novelty and significance. The bulk of the review focuses on concerns and suggestions for improvement, indicating a lukewarm reception overall. While the language is direct and critical, it maintains a professional and constructive tone, avoiding personal attacks or disrespectful language.",20.0,70.0
"To exploit the near neighbor/manifold features, this paper proposes to combine k-nearest neighbors of each training data point into the neural network models.  Specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning. Besides, the final label of the classification task will be learned, a sequence of nearest neighbor labels and a sequence of out-of-sample feature vectors (for oversampling) will be also learned in the same time, similar with the multi-task approaches. Since the proposed models are based k-nearest neighbor calculations, which is time-consuming, they also design an algorithm for the ‘out-of-core’ situation, say load a small portion of data each time to approximately calculate the neighbors. Experiments show that some proposed models work better than baselines in classification and oversampling.
Strong points:
(1) As similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.
(2) The proposed models work well on the ‘out-of-core’ situation, which shows that the models are robust.
Concerns or suggestions:
(1) The training data $x$ is just one data point, it is not a sequence of data. So the idea to model it in a sequence to sequence setting does not make sense.
(2) K-nearest neighbors are a set but not a sequence. To model them as a sequence is also strange. The i-th nearest neighbor does not necessarily dependent on the i-1-th nearest neighbor. For example, we consider the one-dimensional case, the focus data may lie between its first and second nearest neighbors. In this case, there is no clear sequence dependence from the second neighbor to the first neighbor. 
(3) The experiments are not sufficient. They only compare with some weak baselines, such as KNN. As the classification task, there are many state-of-the-art models. Besides of these standard classification models, we strongly suggest comparing with the previous method, Wang et al. (2017), which also proposes to combine the k-nearest neighbors into memory network models. I am surprised that the authors did not compare with this very related work. In my opinion, the idea of utilizing nearest neighbors as external memory in Wang et al. (2017) makes more senses.
(4) The experimental results of some proposed sub-models (key parts of final models) are even worse than the basic kNN model. I should say that the results are not good enough to support the proposed methods. ","The reviewer raises several serious concerns about the paper's core ideas and methodology. They find flaws in the application of sequence models to non-sequential data and point out a lack of comparison with stronger baselines and a particularly relevant previous work.  The criticism, while direct, maintains a professional and analytical tone without resorting to personal attacks or overly negative language. ",-50.0,50.0
"The authors propose a data-dependent dropout variant that produces dropout candidates based on their predictive saliency / relevance. Results are reported for 4 datasets (Cifar10, Cifar100, Caltech256 and UCF101) and 4 different models (CNN-2, AlexNet, VGG16 and VGG19), and suggest an increase in generalization performance over other dropout approaches (curriculum dropout, standard dropout, no dropout), as well as increase in the network's plasticity as measured by some existing metrics from the literature. The authors conclude that Excitation Dropout results in better network utilization and offers advantages for network compression (in the sense of neuron pruning).

Overall I find the idea to be interesting and fairly novel, and commend the authors for the fluid writing style. However, I find key issues with the testing and experiments. Specifically, the lack of confidence bounds for individual results makes it impossible to determine whether the reported incremental improvements are actually significant over those of existing approaches. Likewise, I criticize the choice of methods the authors have chosen to compare against, as several other data-dependent dropout approaches (e.g. Information Dropout) exist that may be conceptually closer (and therefore more comparable) to the proposed approach. I also question the choice of tested network architectures and the placement of the dropout layer.

The paper could be of high significance if all claims in the paper could be backed up by experiments that show the advantage of Excitation Dropout to be not a random effect. I will therefore give a lower score for the paper in its current form, but am willing to revise my rating the major points below are properly addressed.

Pros:
+ novel mechanism to improve dropout, results seemingly superior over other methods
+ achieves better utilization of network resources and achieves robustness to dropping out neurons at test time

Cons:
- results without error bars, unclear if advantage is significant
- did not compare against most relevant competing methods


MAJOR POINTS
Section 2 - The comparison to Moreiro et al. is not entirely clear. A fairer comparison would be with some of the other methods listed which also focus on answering the question of which neurons to dropout, or approaches which determine the dropout policy based on information gained from the data, such as Information Dropout (Achille & Soatto). The authors state that Morerio et al are the state-of-the-art in dropout techniques, however based on the results presented here (Figure 3) it seems to perform just as well as standard dropout. Perhaps there are architecture-specific or data-specific issues? In any case this example undermines the confidence of the claims.

Section 3.2, equation 3 - is there some theoretical underpinning as to how this equation was modelled, or was it chosen simply because it covers the expected corner cases described in paragraph 4 of this section? Also, given the intuition in this paragraph (e.g. p_EB = 1 / N), it is correct to assume this equation models the dropout probability but only for fully connected layers? What about dropout in convolutional layers? Though some previous statements do point to the usage of dropout predominantly for fully connected layers, I feel that this context is missing here and should be explicitly addressed. The caption to e.g. Table 1 seems to imply the authors add a single dropout layer in one of the fully connected layers, however this begs the question as to why this positioning was chosen - why only one dropout layer, and why precisely at that location? The scope of the claims should be adapted accordingly.

Section 4.2 - ""After convergence, ED demonstrates a significant improvement in performance compared to other methods"". If five trained models were used, then some sense of measure of uncertainty should be given throughout. For example, in the Cifar10 results for Figure 3, it is difficult to say whether the marginal improvement from about 80% (standard dropout and curriculum dropout) to about 82% (excitation dropout) is significant or not. Perhaps this would be less of an issue if the authors had worked with e.g. ImageNet, but for these smaller datasets it would definitely be worth to be on the safe side. I highly suspect that statistically speaking (perhaps with the exception of the results on Caltech256), the effects of all of these dropout variants are indistinguishable from each other. I urge the authors to include a measure of the standard deviation / 95% confidence interval across the models that were tested.

The results presented sub-section 4.3 do not justify the claim that the models trained with Excitation Dropout tend to be more informative. Perhaps the definition of ""informative"" should be expanded upon in length. Can the authors show that the alternative paths learned by the models augmented with Excitation Dropout indeed carry complimentary information and not just redundant information?

Figure 5 shows interesting results, but once again begs the question of whether there is any significant difference between standard dropout and curriculum dropout. I encourage the authors to include confidence bounds for each trace. Likewise, there is an inherent bias in the results, in that the leftmost figure compares EB and CD in the context in which EB was trained, i.e. dropping of ""most salient"" neurons. The comparison is one-sided, however, as no results are reported from the context in which CD was trained, i.e. dropping neurons more frequently as training progresses. Comparing these results would bring to light whether the performance boost see in Figure 5 is a function of ""overfitting"" to the training manner or not.
Also, I believe the results for the second column (dropping out least relevant neurons) are misleading. To the best of my understanding, as p_c increases, at some point neurons start to be dropped that actually have high relevance. This could explain why all curves start out similarly and EB slowly begins to stick out - at this point the EB models once again start to be used in the context within which they were trained, in contrast to the other approaches. The authors should perhaps also explicity clarify why this second column gives any more information than the first.



MINOR POINTS

The authors propose Excitation Dropout as a guided regularization technique. Batch normalization is another standard regularization technique which is often compared with dropout. In particular, for deep CNNs, batch normalization is known to work very well, often better than the standard dropout. In the experiments here, to what extent was batch normalization and / or any other widely utilized network regularizers used? Is it possible that the regularizing effect found here actually comes from one of these? I.e. were the models that were not trained from scratch trained with batch normalization? It would be good if more data could be provided for EB vs. other regularizing techniques, if the claim is that EB is a novel regularizer.

Section 3.1 - ""We choose to use EB since it produces a valid probability distribution for each network layer"". Though this is a nice property, were there any other considerations for choosing the saliency method? Recent work (Adebayo et al, ""Sanity Checks for Saliency Maps"") has shown that even some well established saliency techniques are actually independent from both the model and data. As this approach relies heavily on the correctness of EB, I feel that a further justification should be given to validate its use for this scenario other than just based on the type of output it produces.

Section 3.1, equation 2 - more detail and reasoning should be given as to why connections with negative weights are excluded from the computation of the conditional probability, if possible without referring the reader to the EB paper. Why is this justified? Is this probability modelled for a specific activation function? 

The authors do not provide the details of the CNN-2 architecture (even in the appendix) and simply refer to another article. If the majority of the results presented in the paper are based on this network (including a reference made to a specific layer of the network in subsection 4.2) – which is not commonly known – why not to detail the network architecture and save additional effort for the reader?

How are the class-wise training and test images chosen for Caltech256 dataset?

The authors test the CNN-2 architecture on Cifar10 and Cifar100, and AlexNet, VGG16, and VGG19 on UCF101. I feel that at least a couple architectures should be validated with more than a single dataset, or the authors should justify the current matching between architectures and datasets. Table 2 is unclear regarding what models were used for what datasets (caption could be interpreted to mean that VGG16 was also used for Cifar and Caltech, however other statements seem to say otherwise).

""To prove that the actual boost in accuracy with ED is not provided by the choice of specific masks,..."" I suggest that the authors rephrase or explain this sentence in more detail. To the best of my understanding, it is precisely the fact that different masks are used, each reflective of the particular input used to generate the forward activations, that gives boost in performance over ""standard"" dropout methods by identifying salient paths in the network.

Although it is a very important experimental detail, only in the end of sub-section 4.2, it becomes clear in which layers Excitation Dropout was applied. 

Y-axis labels are missing for the left panels in Figure 3.

The authors randomly choose to abbreviate Excitation Dropout as ED in some paragraphs, while write the full form in others.  

Table 2 - It is not clear that the ""Neurons ON"" metric refers to the ""average percentage of zero activations"" explained below. 

Table 2 - How is peak p_EB measured? Is this an average over a set of test images after convergence? If so, I similarly suggest for confidence bounds to be introduced. It would be interesting to compare this to intermediate values (e.g. after every epoch) during training. Same question for entropy of activations and entropy of pEB. This information would be useful for reproducibility.

Table 2 - Where do the delta values in Table 2 come from? If empirically determined, it should be stated explicitly.

Table 2 - In general, because the metrics provided in Table 2 are averages (second paragraph of this Section 4.3), both (to the best of my understanding) across input subsets (e.g. averging results over many test inputs) and models (caption to Table 1), I feel Table 2 in its current form raises confusion given the lack of confidence bounds. I recommend the authors to clarify what type of averaging was done and to introduce e.g. standard deviations across all reported scores. The authors should refrain from using the term ""significantly"" while describing results if no statistical testing was done, or explicitly clarify their usage of this term.

Table 2 - In general, Table 2 reports results on selected metrics which, if the authors' hypothesis is correct, should have a clear trend as training progresses. An interesting idea to explore would be to include an analysis (in the appendix) of how these factors change over the course of the training procedure. Intuitively, it seems plasticity is something that should be learned slowly over time, so if these plots were to reveal something different, it would be indicative that something else is going on.

Figure 4 - Judging heatmaps is difficult as it depends on the visual perception of the reader. Thus, it is difficult to judge whether, as the authors claim, ED is indeed less ""peaky"" than the other alternatives. I suggest that the authors use a perceptually uniform heatmap, and to acompany these figures with e.g. the histogram of the heatmap values. Likewise, it is unclear how the multi-model aspect of the testing plays a role in generating these results. From the 5 originally trained models, how was the model selected that generated these results? Was there averaging of any kind?

Figure 5: the text is too small to be readble

Is ""re-wiring"" the most appropriate term to use to describe what is happening at inference time? Although different paths may be used, the network connections themselves are fixed and thus this is a potential source for confusion.

What do numbers in Table-4 in the appendix represent? Test accuracy? ","The review is mostly positive in its description of the paper's novelty and potential impact. However, the reviewer raises several major concerns regarding the experimental setup and statistical analysis, which significantly impacts their confidence in the results. The language used is formal, professional, and critical but not disrespectful, suggesting a high degree of politeness.",20.0,80.0
"The paper studies discrete time dynamical systems with a non-linear state equation.  They assume the non-linear function is assumed to be \beta-increasing like leaky ReLU. Under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running SGD on a fixed length trajectory gives logarithmic convergence.

The paper is well-written and proves strong convergence properties. The deterministic result does not seem very novel and uses the idea of one-point strong convexity which has been studied in various prior works. However the bounding of the condition number of the data matrix is interesting and guarantees are near-optimal. The faster convergence for odd activations is a good observation. Overall, I think the paper is good. I do list some concerns:
Questions/concerns:
- The deterministic theorem (Theorem 4.1) seems similar to Theorem 3 in [1] with SGD instead of GD. Also under the distribution being symmetric, it can be derived from [2] with $k=1$. 
- Can the ideas be extended to other commonly used activations such as ReLUs/Sigmoids? Sigmoids have exponentially small slope near origin.
- The proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated. So the data is sufficiently uncorrelated taking enough data. What happens if this data at each step is not gaussian?
- In the unstable setting, the solution proposed just samples from different trajectories which by default are independent hence correlation is not an issue, this seems a bit like cheating. 
- In RNNs, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.
- If SGD was performed on only one truncated series, do the results still hold?

Other comments:
- There has been previous work on generalized linear models which work in more general settings like GLMtron [3]. The authors should update prior work on generalized linear models as well as neural networks.
- Typo on Page 2 y_t = h_{t+1} not y_t = h_t.

[1] Dylan J. Foster, Ayush Sekhari, and Karthik Sridharan. Uniform Convergence of Gradients for Non-Convex Learning and Optimization. NIPS 2018.
[2] Surbhi Goel, Adam Klivans, and Raghu Meka. Learning One Convolutional Layer with Overlapping Patches. ICML 2018.
[3] Sham M. Kakade et al. Efficient learning of generalized linear and single index models with isotonic regression. NIPS 2011.


--------------
I would be maintaining the same score. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. ","The reviewer states ""Overall, I think the paper is good."" which indicates positive sentiment. They also mention the paper is well-written and proves strong convergence properties. While they have some concerns and suggestions for improvement, their overall tone is constructive and polite.",60.0,80.0
"This paper presents a new approach to an active learning problem where the idea is to train a classifier to distinguish labeled and unlabeled datapoints and select those that look the most like unlabeled.

The paper is clearly written and easy to follow. The idea is quite novel and evokes interesting thoughts. I appreciated that the authors provide links and connections to other problems. Another positive aspect is that evaluation methodology is quite sound and includes comparison to many recent algorithms for AL with neural networks. The analysis of Section 5.5 is quite interesting.
However, I have a few concerns regarding the methodology. First of all, I am not completely convinced by the fact that selecting the samples that resemble the most unlabeled data is beneficial for the classifier. It seem that in this case just the data from under-explored regions will be selected at every new iteration. If this is the purpose, some simpler methods, for example, relying on density sampling, can be used. Could you elaborate how you method would compare to them? I can see this method as a way to measure the representativeness of datapoints, but I would see it as a component of AL, not an AL alone. What would happen it is combined with Uncertainty and you use it to labeled the points that are both uncertain and resemble unlabeled data? 
Besides, the proposed approach does not take the advantage of all the information that is available to AL, in particular, it does not use at the information about labels. I believe that labels contain a lot of useful information for making an informed selection decision and ignoring it when it is available is not rational.  
Next, I have conceptual difficulties understanding what would happen to a classifier at next iteration when it is trained on the data that was determined by the previous classifier. Seems that the training data is non-iid and might cause some strange bias. In addition to this, it sounds a bit strange to use classification where overfitting is acceptable.
Finally, the results of the experimental evaluation do not demonstrate a significant advantage of the proposed method and thus it is unclear is there is a benefit of using this method in practice. 

Questions:
- Could you elaborate why DAL strategy does not end up doing just random sampling?
- Nothing restrict DAL from being applied with classifiers other than neural networks and smaller problems. How do you think DAL would work on simpler datasets and classifiers?
- How does the classifier (that distinguished between labeled and unlabeled data) deal with very unbalanced classes? I suppose that normally unlabeled set is much bigger than labeled. What does 98% accuracy mean in this case?
- How many experiments were run to produce each figure? Are error bars of most experiments so small that are almost invisible?

Small comments:
- I think in many cases citep command should be used instead of cite. 
- Can you explain more about the paragraph 3 of related work where you say that uncertainty-based approach would be different from margin-based approach if the classifier is neural network?
- Last sentence before 3.1: how do you guarantee in this case that the selected examples are not similar to each other (that was mentioned as a limitation for batch uncertainty selection, last paragraph on page 1)?
- It was hard to understand the beginning of 5.5, at first it sounds like the ranking of methods is going to be analysed.
- I am not sure ""discriminative"" is a good name for this algorithm. It suggested that is it opposite to ""generative"" (query synthesis?), but then all AL that rank datapoints with some scoring function are ""discriminative"".","The reviewer provides both positive and negative feedback, but the overall tone suggests that they are not fully convinced by the paper's approach. While they acknowledge the novelty and clarity of the paper, they raise several methodological concerns and are not impressed by the experimental results. The numerous questions and requests for clarification further indicate a lack of complete understanding and potential skepticism. Therefore, the sentiment leans slightly towards the negative side. The language used is polite and professional, employing constructive criticism and avoiding harsh or disrespectful language.",-20.0,80.0
"This paper centers around adding a reward term that, as I understand it, rewards the agent for having seen sequences of rewards that have low variability. This is an interesting idea, however I find the clarity of the main part of the paper (section 4.1, where this new term is defined) quite poor. That section makes several seemingly arbitrary choices that are not properly explained, which makes one wonder if those choices were made mostly to make the empirical results look good or if there are some more fundamental and general concepts being captured there. In particular, I have to wonder where the 100 in the definition of R_H comes from, and also how sigma_max would be determined (it is very hard to get a good intuition on such quantities as an RL practitioner).  

The paper also introduces “hot-wire exploration”, basically trying the same action for a while during the initial stage, which is a nice exploration heuristic for Atari, but I am not sure how generally applicable the idea is beyond the Atari testbed.

In general, I am always a bit wary of experimental results that were obtained as a result of introducing additional hyper-parameters or functional forms. However, the results look pretty good, and the authors do manage to show some amount of hyperparameter robustness, which makes me wish the design choices had been more clearly explained..
","The reviewer expresses a mixed sentiment. While intrigued by the core idea, they find the clarity lacking and question the rationale behind certain choices, hinting at potential cherry-picking. The reviewer acknowledges the positive results and some robustness analysis but remains cautious due to the introduction of additional parameters. The language used is professional and avoids direct negativity, focusing on constructive feedback.",20.0,60.0
"This paper proposes a technique for channel-wise quantization of CNNs
to 8-bit, fixed point precision. The authors propose several
techniques for analyzing the statistical properties of output channel
activations in order to select the best fractional bit length for each
channel. Experimental results on eleven different CNN architectures
demonstrate that the approaches proposed result in significantly less
accuracy loss when compared to a layer-wise baseline.

The paper has the following strengths:

 1. The experimental results on eleven different architectures (of
    varying depth and breadth) are convincing, and are consistently
    better than layer-wise MAX for choosing fractional bit length.

The paper has the following weak points:

 1. There is not much coherence between the description of the
    approach in section 2.1, Figure 1, Algorithm 1, and
    Figure 2. Notation is used in Algorithm 1 which is never defined.
 2. Related to the previous point, the proposed technique has a lot of
    moving parts and I don't feel that it would be easy to reproduce
    the results of the paper. There are some vague statements, like
    ""We resolve this complication by pre-coordinating the fractional
    lengths of the weights"", which require significantly more
    precision. This issue -- one of the main issues with channel-wise
    versus layer-wise quantization -- is never returned to in the
    definition of the method.
 3. The experimental comparison with layer-wise quantization is
    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN
    quantization? The results comparing channel-wise and layer-wise
    MAX are already convincing, but are the moment-analysis approaches
    not equally applicable to layer-wise quantization?
    State-of-the-art results that are less sensitive to outliers
    should be included in Table 1. A comparison with layer-wise
    approaches would be nice to have also in Figure 4 to show
    sensitivity to profiling set size.

The experimental results in the paper are impressive, and the analysis
motivating the approach is convincing. However, there are presentation
and clarity issues in the technical development, and the comparative
analysis is lacking broader comparisons with the state-of-the-art (to
be fair, the authors recognize that layer-wise MAX as a baseline is
particularly susceptible to outliers). These two aspects combined,
however, lead me to the opinion that this work is just not quite ready
for publication at ICLR.
","The review acknowledges the strengths of the paper, particularly the impressive experimental results and convincing analysis. However, it also points out significant weaknesses in clarity, reproducibility, and the lack of comparison with state-of-the-art methods. The reviewer ultimately finds the work promising but not yet ready for publication due to these weaknesses. The tone is critical but professional and constructive.",50.0,70.0
"This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The analysis relies on the negative curvature in the directions normal to the stable manifolds of all critical points that are not global minimizer.

Exploring the surface of a function and analyzing the structure of the negative curvature normal to the stable manifolds is an interesting idea. However, I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. See details below. Therefore, I have reservations about the paper.

*) The terminology ""stable manifolds"" is used from the first page, while its formal definition is given on page 4.
*) P3, the dictionary learning problem is not formally given. It is stated in the paper that the task is to find A and X, given Y. However, what optimization problem does the author consider? Is it \min_{A, X} \|Y - A X\|_F^2? assuming both dictionary A and sparse code X are unknown or \min_{A} \|Y - A X\|_F^2 assuming only dictionary is unknown?
*) P3, second paragraph in Section 3: what is the variable q? It is not defined before.
*) P3, third paragraph in Section 3: What is the function row()? Why does row(Y) equal row(X)?
*) P3: How does the dictionary learning problem reformulate into the problem in the third paragraph of Section 3? If I understand correctly, the task is to find A, X such that A^* Y = X since A is orthogonal. Consider the first column in A and denote it by q. Then the first column of X is approximated by q^* Y. Since X is sparse, the task is to find q so that q^* Y as sparse as possible. But how about the other columns in matrix $A$? 
*) The Riemannian gradient algorithm is not stated in this paper.


",The reviewer finds the core idea interesting but has significant reservations due to lack of clarity and completeness in the paper. They find the paper difficult to understand and point out missing definitions and unexplained transitions. The language used doesn't express strong negativity but highlights the issues as points for improvement. ,-30.0,60.0
"PAPER SUMMARY:

This paper introduces a biologically motivated black-box attack algorithm. 
The target model in this case is DNN applied to the ASR context (automatic speech recognition system). 

NOVELTY & SIGNIFICANCE:

The proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.

This however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.

It is also unclear how this mutation component improves over the existing work (more on this in the sections below).

Another issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:

Chen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.
ZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. 
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM

Cheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.
Query-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457

While these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.

TECHNICAL SOUNDNESS:

I find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.

CLARITY:

The paper is clearly written.

EMPIRICAL RESULTS:

I do not understand this statement:

""That 35% of random attacks were successful in this respect highlights the fact that black box
adversarial attacks are definitely possible and highly effective at the same time""

Why is 35% successful attack rate a positive result? The result tends to suggest that this is an attack with low success rate. 

The 2nd paragraph in 3.2 seems to give a vague explanation: ""the vast majority of failure cases are only a few edit distances away from the target. 

This suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity"".

Given the above statement, I do not see why the authors didn't actually ""run the algorithm for a few more iterations"" to verify it ...

I am also curious why is the success rate of the proposed method is significantly lower than that of the existing system -- I assume ""single word black box"" is the work of (Alzantot et al., 2018).

I find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?

REVIEW SUMMARY:

The paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). ","The review is highly critical of the paper. The reviewer finds the novelty and significance to be incremental, the technical soundness questionable, and the empirical evaluation sloppy. They point out several flaws and missing comparisons with relevant literature. The language used, while direct, maintains a professional and critical tone without resorting to personal attacks or overly negative phrasing.",-50.0,50.0
